
class Model(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.w = torch.tensor([[1.0, 0.0], [0.5, 4.0]]) # The weight matrix for computing the normalized dot
        self.b = torch.tensor([[1.0], [0.1]]) # The bias matrix for computing the softmax
        self.key = torch.tensor([[-0.3073, 0.2261, -0.1469, 0.0600, -0.1718, 0.1826, -0.1676, -0.4189],
                            [-0.2473, 0.0856, -0.1045, 0.4278, -0.1690, 0.8706, 0.1572, -0.6718],
                            [-0.7533, 0.0437, -0.0448, 0.1866, -0.8607, 0.0397, 0.5535, 0.3993],
                            [-0.3455, 0.6086, 0.0174, 1.1969, 0.6267, -0.0806, 0.8720, 0.1198],
                            [-0.3281, 0.0461, 0.0992, 0.7770, -0.5023, 0.5227, 0.0764, 0.6155],
                            [-0.1405, -0.4982, 0.3425, -0.1186, -0.9292, -0.6057, -0.2639, -0.3138],
                            [-0.5871, -0.4767, -0.4198, -0.6022, -0.6159, -0.1603, 0.3352, 0.0089],
                            [-0.3490, -0.2934, -0.0613, 0.2419, 0.4606, -0.3861, 0.1215, 0.1089]]) # Pre-computed key tensor
        self.value = torch.tensor([[0.2807, 0.0834],
                            [-0.8931, -0.5047],
                            [0.1609, -0.1620],
                            [-1.5982, -0.9024],
                            [-0.2110, -0.5462],
                            [-0.9919, -0.8625],
                            [-0.0876, 0.1611],
                            [-0.2481, 0.1376],
                            [-0.8218, -0.4155],
                            [-0.1206, -1.5935],
                            [-1.0358, 1.1839],
                            [-0.8898, 0.0717],
                            [-0.6896, -0.1762],
                            [-0.3200, -0.6392],
                            [-0.9167, 0.3089],
                            [0.4587, 1.2132],
                            [-1.0953, -0.1311],
                            [-1.6978, 0.5265],
                            [-0.8640, 1.7382],
                            [0.5554, -1.8606],
                            [-0.3293, -0.0756],
                            [1.0338, 1.1269],
                            [0.0105, 0.3123],
                            [-0.4241, 1.3648],
                            [-0.9185, 0.7237],
                            [0.1229, 1.2844],
                            [1.5419, -0.2420],
                            [-2.1251, 0.0748]]) # Pre-computed value tensor
        self.query = torch.tensor([[-0.4497, -0.4810, 0.3259, -0.9274, -0.9296, 0.7059, -0.6732, -0.4189],
                                [-0.2535, -0.7743, 1.6732, 0.0856, 0.4278, -0.3111, -0.6430, -0.6718],
                                [0.4064, 0.5856, -0.7086, 0.0437, 0.5452, 0.7731, 0.3892, 0.3993],
                                [-0.0240, -0.1890, -0.0856, 0.4691, 0.9784, 1.1112, -0.0887, 0.1198],
                                [-0.4577, -0.6185, 0.0992, 1.1969, 0.6267, 1.0489, -0.1465, 0.6155],
                                [0.4156, -0.5391, -0.1940, -0.4982, -0.9292, -0.8915, 0.4740, -0.3138],
                                [0.9968, -0.6227, -0.1445, -0.4767, -0.6022, -0.8528, 0.1237, 0.0089],
                                [0.0245, -0.4847, 0.0224, -0.2934, 0.2419, 0.5214, 0.0294, 0.1089]]) # Pre-computed query tensor
        self.mask = torch.tensor([[[10000, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]],
                                [[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]],
                                [[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]],
                                [[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]],
                                [[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]],
                                [[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]],
                                [[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]],
                                [[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]],
                                [[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]],
                                [[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]],
                                [[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]],
                                [[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]],
                                [[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]],
                                [[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]],
                                [[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]],
                                [[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]],
                                [[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]]], dtype=torch.float) # Attention mask
        self.v1 = torch.tensor([[0, 0], [-0.0078, -0.0221], [0.0044, -0.0192], [0.0149, -0.0029], [0.0159, -0.0003], [0.0081, -0.0164], [-0.0151, -0.0169], [-0.0081, -0.0212], [-0.0052, -0.0316], [-0.0169, 0.0024], [-0.0159, -0.0333], [0.0210, -0.0323], [-0.0109, -0.0273], [0.0070, -0.0077], [0.0076, -0.0053], [-0.0090, 0.0005], [0.0170, -0.0310], [0.0080, -0.0091], [-0.0033, -0.0268], [-0.0030, -0.0296], [-0.0211, -0.0123]]) # The output from computing the attention weights over query, key, value

    def forward(self, x1):
        o1 = x1 @ self.key.T * (1 / math.sqrt(self.query.size(-1))) # Compute the scaled dot product over key
        o2 = o1 + self.mask # Add the attention mask
        self.o3 = torch.nn.functional.softmax(o2, dim=-1) # Apply softmax
        o4 = self.o3 @ self.value # Compute the dot product with value
        return o4

# Initializing the model
m = Model()

# Inputs to the model
x1 = torch.tensor([[[[-0.3073, 0.2261, -0.1469, 0.0600, -0.1718, 0.1826, -0.1676, -0.4189],
                    [-0.2473, 0.0856, -0.1045, 0.4278, -0.1690, 0.8706, 0.1572, -0.6718],
                    [-0.7533, 0.0437, -0.0448, 0.1866, -0.8607, 0.0397, 0.5535, 0.3993],
                    [-0.3455, 0.6086, 0.0174, 1.1969, 0.6267, -0.0806, 0.8720, 0.1198],
                    [-0.3281, 0.0461, 0.0992, 0.7770, -0.5023, 0.5227, 0.0764, 0.6155],
                    [-0.1405, -0.4982, 0.3425, -0.1186, -0.9292, -0.6057, -0.2639, -0.3138],
                    [-0.5871, -0.4767, -0.4198, -0.6022, -0.6159, -0.1603, 0.3352, 0.0089],
                    [-0.3490, -0.2934, -0.0613, 0.2419, 0.4606, -0.3861, 0.1215, 0.1089]]]])

{
    "opt": "binary_unary=0",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 32, 5, stride=2, padding=2)\n        self.conv2 = torch.nn.Conv2d(32, 32, 3, stride=1, padding=1)\n    def forward(self, x):\n        self.conv1.groups = 32\n        v1 = self.conv1(x)\n        self.conv2.groups = 32\n        v2 = self.conv2(v1)\n        return v2\n# Inputs to the model\nx = torch.randn(1, 3, 256, 256)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(64, 32, 3, stride=1, padding=1)\n        self.pool1 = torch.nn.AvgPool2d(kernel_size=13, stride=1)\n        self.conv2 = torch.nn.Conv2d(32, 8, 3, stride=1, padding=1)\n        self.pool2 = torch.nn.AvgPool2d(kernel_size=9, stride=1)\n        self.conv3 = torch.nn.Conv2d(8, 2, 3, stride=1, padding=1)\n    def forward(self, x):\n        v1 = self.conv1(x)\n        v2 = self.pool1(v1) # Pooling kernel size: 3 x 3\n        v3 = torch.relu(v2)\n        v4 = self.conv2(v3)\n        v5 = self.pool2(v4) # Pooling kernel size 2 x 2\n        v6 = torch.relu(v5)\n        v7 = self.conv3(v6)\n        v8 = v7.view(1, -1)\n        return v8\n# Inputs to the model\nx = torch.randn(1, 3, 224, 224)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(16, 16, 7, stride=1, padding=3)\n        self.conv2 = torch.nn.Conv2d(16, 16, 7, stride=1, padding=3)\n        self.conv3 = torch.nn.Conv2d(16, 16, 7, stride=1, padding=3)\n        self.conv4 = torch.nn.Conv2d(16, 16, 7, stride=1, padding=3)\n        self.conv5 = torch.nn.Conv2d(16, 16, 7, stride=1, padding=3)\n    def forward(self, x1, x2, x3, x4, x5, x6, x7, x8):\n        v1 = self.conv1(x1)\n        v2 = v1 + x2\n        v3 = torch.relu(v2)\n        v4 = v3 + x2\n        v5 = torch.relu(v4)\n        v6 = self.conv2(v5)\n        v7 = v6 + x3\n        v8 = torch.relu(v7)\n        v9 = v8 + x3\n        v10 = torch.relu(v9)\n        v11 = self.conv3(v10)\n        v12 = v11 + x4\n        v13 = torch.relu(v12)\n        v14 = self.conv4(v4)\n        v15 = 1 + v14\n        v16 = v15 + x5\n        v17 = torch.relu(v16)\n        v18 = self.conv5(v17)\n        v19 = v18 + x6\n        v20 = torch.relu(v19)\n        v21 = torch.relu(v20)\n        v22 = v21 + x7\n        v23 = torch.relu(v22)\n        v24 = self.conv6(v23)\n        v25 = v24 + x8\n        return v25\n# Inputs to the model\nx1 = torch.randn(1, 16, 64, 64)\nx2 = torch.randn(1, 16, 64, 64)\nx3 = torch.randn(1, 16, 64, 64)\nx4 = torch.randn(1, 16, 64, 64)\nx5 = torch.randn(1, 16, 64, 64)\nx6 = torch.randn(1, 16, 64, 64)\nx7 = torch.randn(1, 16, 64, 64)\nx8 = torch.randn(1, 16, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(16, 24, 5, stride=2, padding=2)\n        self.conv2 = torch.nn.Conv2d(16, 16, 5, stride=2, padding=2)\n        self.conv3 = torch.nn.Conv2d(16, 72, 5, stride=2, padding=2)\n    def forward(self, x1, x2):\n        v1 = x1\n        v2 = self.conv1(v1)\n        v3 = v2 + 1\n        v4 = torch.relu(v3)\n        v5 = v4\n        v6 = self.conv2(v5)\n        v7 = v6\n        v8 = torch.relu(v7)\n        v9 = v8\n        v10 = self.conv3(v9)\n        v11 = x2\n        v12 = v11\n        v13 = self.conv1(v12)\n        v14 = v10 + v13\n        v15 = torch.relu(v14)\n        v16 = v15\n        v17 = self.conv2(v16)\n        v18 = v17\n        v19 = torch.relu(v18)\n        v20 = v19\n        v21 = self.conv3(v20)\n        v22 = v21\n        v23 = v22 + 1\n        v24 = torch.relu(v23)\n        v25 = v24\n        v26 = v25 + 1\n        v27 = torch.relu(v26)\n        return v27\n# Inputs to the model\nx1 = torch.randn(1, 16, 64, 64)\nx2 = torch.randn(1, 16, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(16, 16, 7, stride=3, padding=3, dilation=2)\n        self.conv2 = torch.nn.Conv2d(16, 16, 7, stride=3, padding=3, dilation=2)\n        self.conv3 = torch.nn.Conv2d(16, 16, 7, stride=3, padding=3, dilation=2)\n    def forward(self, x1, x2, x3, x4, x5):\n        v1 = self.conv1(x1)\n        v2 = self.conv2(x2)\n        v3 = v1 + v2\n        v4 = torch.relu(v3)\n        v5 = self.conv3(v4)\n        v6 = v5 + x3\n        v7 = torch.relu(v6)\n        v8 = v7 + x4\n        v9 = torch.relu(v8)\n        return v9\n# Inputs to the model\nx1 = torch.randn(1, 16, 64, 64)\nx2 = torch.randn(1, 16, 64, 64)\nx3 = torch.randn(1, 16, 64, 64)\nx4 = torch.randn(1, 16, 64, 64)\nx5 = torch.randn(1, 16, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(16, 16, 7, stride=1, padding=3)\n        self.conv2 = torch.nn.Conv2d(16, 16, 7, bias=True, stride=1, padding=3)\n        self.conv3 = torch.nn.Conv2d(16, 16, 7, bias=True, stride=1, padding=3)\n    def forward(self, x1, x2, x3):\n        v1 = x1 + self.conv1(x2)\n        v2 = torch.relu(v1)\n        v3 = 1 + v2\n        v4 = self.conv2(v3)\n        v5 = 2 + v4\n        v6 = 3 + x3\n        v7 = self.conv3(v6)\n        v8 = 4 + v7\n        v9 = torch.relu(v8)\n        return v9\n# Inputs to the model\nx1 = torch.randn(1, 16, 64, 64)\nx2 = torch.randn(1, 16, 64, 64)\nx3 = torch.randn(1, 16, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(16, 16, 7, stride=1, padding=3)\n        self.conv2 = torch.nn.Conv2d(16, 16, 7, stride=1, padding=3)\n        self.conv3 = torch.nn.Conv2d(16, 16, 7, stride=1, padding=3)\n        self.conv4 = torch.nn.Conv2d(16, 16, 7, stride=1, padding=3)\n    def forward(self, x1, x2, x3, x4, x5, x6, x7):\n        v1 = self.conv1(x1)\n        v2 = x2 + v1\n        v3 = torch.relu(v2)\n        v4 = self.conv2(v3)\n        v5 = self.conv3(v4)\n        v6 = v5 + x3\n        v7 = torch.relu(v6)\n        v8 = x4 + v7\n        v9 = torch.relu(v8)\n        v10 = self.conv4(v9)\n        v11 = v10 + x5\n        v12 = torch.relu(v11)\n        v13 = x6 + v12\n        v14 = torch.relu(v13)\n        v15 = x7 + v14\n        v16 = torch.relu(v15)\n        return v16\n# Inputs to the model\nx1 = torch.randn(1, 16, 64, 64)\nx2 = torch.randn(1, 16, 64, 64)\nx3 = torch.randn(1, 16, 64, 64)\nx4 = torch.randn(1, 16, 64, 64)\nx5 = torch.randn(1, 16, 64, 64)\nx6 = torch.randn(1, 16, 64, 64)\nx7 = torch.randn(1, 16, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(16, 16, 7, stride=3, padding=3)\n        self.conv2 = torch.nn.Conv2d(16, 16, 7, stride=3, padding=3)\n        self.conv3 = torch.nn.Conv2d(16, 16, 7, stride=3, padding=3)\n    def forward(self, x1, x2, x3, x4, x5):\n        v1 = self.conv1(x1)\n        v2 = self.conv2(x2)\n        v3 = v1 + 1\n        v4 = torch.relu(v3)\n        v5 = self.conv3(v4)\n        v6 = v1 + v5\n        v7 = torch.relu(v6)\n        return v7\n# Inputs to the model\nx1 = torch.randn(1, 16, 64, 64)\nx2 = torch.randn(1, 16, 64, 64)\nx3 = torch.randn(1, 16, 64, 64)\nx4 = torch.randn(1, 16, 64, 64)\nx5 = torch.randn(1, 16, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(16, 16, 7, stride=1, padding=3)\n        self.conv2 = torch.nn.Conv2d(16, 16, 7, stride=1, padding=3)\n        self.conv3 = torch.nn.Conv2d(16, 16, 7, stride=1, padding=3)\n        self.conv4 = torch.nn.Conv2d(16, 16, 7, stride=1, padding=3)\n        self.conv5 = torch.nn.Conv2d(16, 16, 7, stride=1, padding=3)\n        self.conv6 = torch.nn.Conv2d(16, 16, 7, stride=1, padding=3)\n    def forward(self, x1, x2, x3, x4, x5, x6):\n        v1 = self.conv1(x1)\n        v2 = x2 + v1\n        v3 = torch.relu(v2)\n        v4 = self.conv2(v3)\n        v5 = x3 + v4\n        v6 = torch.relu(v5)\n        v7 = x4 + v6\n        v8 = torch.relu(v7)\n        v9 = self.conv3(v8)\n        v10 = x5 + v9\n        v11 = torch.relu(v10)\n        v12 = x6 + v11\n        v13 = torch.relu(v12)\n        v14 = self.conv4(v13)\n        v15 = x1 + v14\n        v16 = torch.relu(v15)\n        v17 = x2 + v16\n        v18 = torch.relu(v17)\n        v19 = self.conv5(v18)\n        v20 = x3 + v19\n        v21 = torch.relu(v20)\n        v22 = x4 + v21\n        v23 = torch.relu(v22)\n        v24 = self.conv6(v23)\n        v25 = x5 + v24\n        v26 = torch.relu(v25)\n        return v26\n# Inputs to the model\nx1 = torch.randn(1, 16, 64, 64)\nx2 = torch.randn(1, 16, 64, 64)\nx3 = torch.randn(1, 16, 64, 64)\nx4 = torch.randn(1, 16, 64, 64)\nx5 = torch.randn(1, 16, 64, 64)\nx6 = torch.randn(1, 16, 64, 64)",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(16, 16, 1, stride=1, padding=0)\n        self.conv2 = torch.nn.Conv2d(16, 16, 1, stride=1, padding=0)\n        self.conv3 = torch.nn.Conv2d(16, 16, 1, stride=1, padding=0)\n    def forward(self, x1, x2, x3, x4, x5, x6):\n        v1 = self.conv1(x1)\n        v2 = v1 + x1\n        v3 = torch.relu(v2)\n        v4 = self.conv2(x2)\n        v5 = 1 + v4\n        v6 = torch.relu(v5)\n        v7 = 2 + self.conv3(x3)\n        v8 = torch.relu(v7)\n        v9 = v8 + self.conv1(x4)\n        v10 = torch.relu(v9)\n        v11 = v10 + self.conv2(x5)\n        v12 = torch.relu(v11)\n        v13 = 3 + self.conv3(x6)\n        v14 = torch.relu(v13)\n        v15 = self.conv1(v14)\n        v16 = 2 * v15\n        v17 = torch.relu(v16)\n        v18 = self.conv2(v17)\n        v19 = 1 * v18\n        v20 = torch.relu(v19)\n        v21 = self.conv3(v20)\n        v22 = 1 * v21\n        v23 = torch.relu(v22)\n        return v23\n# Inputs to the model\nx1 = torch.randn(1, 16, 64, 64)\nx2 = torch.randn(1, 16, 64, 64)\nx3 = torch.randn(1, 16, 64, 64)\nx4 = torch.randn(1, 16, 64, 64)\nx5 = torch.randn(1, 16, 64, 64)\nx6 = torch.randn(1, 16, 64, 64)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 32, 5, stride=2, padding=2)\n        self.conv2 = torch.nn.Conv2d(32, 32, 3, stride=1, padding=1)\n    def forward(self, x):\n        self.conv1.groups = 32\n        v1 = self.conv1(x)\n        self.conv2.groups = 32\n        v2 = self.conv2(v1)\n        return v2\n# Inputs to the model\nx = torch.randn(1, 3, 256, 256)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(64, 32, 3, stride=1, padding=1)\n        self.pool1 = torch.nn.AvgPool2d(kernel_size=13, stride=1)\n        self.conv2 = torch.nn.Conv2d(32, 8, 3, stride=1, padding=1)\n        self.pool2 = torch.nn.AvgPool2d(kernel_size=9, stride=1)\n        self.conv3 = torch.nn.Conv2d(8, 2, 3, stride=1, padding=1)\n    def forward(self, x):\n        v1 = self.conv1(x)\n        v2 = self.pool1(v1) # Pooling kernel size: 3 x 3\n        v3 = torch.relu(v2)\n        v4 = self.conv2(v3)\n        v5 = self.pool2(v4) # Pooling kernel size 2 x 2\n        v6 = torch.relu(v5)\n        v7 = self.conv3(v6)\n        v8 = v7.view(1, -1)\n        return v8\n# Inputs to the model\nx = torch.randn(1, 3, 224, 224)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(16, 16, 7, stride=1, padding=3)\n        self.conv2 = torch.nn.Conv2d(16, 16, 7, stride=1, padding=3)\n        self.conv3 = torch.nn.Conv2d(16, 16, 7, stride=1, padding=3)\n        self.conv4 = torch.nn.Conv2d(16, 16, 7, stride=1, padding=3)\n        self.conv5 = torch.nn.Conv2d(16, 16, 7, stride=1, padding=3)\n    def forward(self, x1, x2, x3, x4, x5, x6, x7, x8):\n        v1 = self.conv1(x1)\n        v2 = v1 + x2\n        v3 = torch.relu(v2)\n        v4 = v3 + x2\n        v5 = torch.relu(v4)\n        v6 = self.conv2(v5)\n        v7 = v6 + x3\n        v8 = torch.relu(v7)\n        v9 = v8 + x3\n        v10 = torch.relu(v9)\n        v11 = self.conv3(v10)\n        v12 = v11 + x4\n        v13 = torch.relu(v12)\n        v14 = self.conv4(v4)\n        v15 = 1 + v14\n        v16 = v15 + x5\n        v17 = torch.relu(v16)\n        v18 = self.conv5(v17)\n        v19 = v18 + x6\n        v20 = torch.relu(v19)\n        v21 = torch.relu(v20)\n        v22 = v21 + x7\n        v23 = torch.relu(v22)\n        v24 = self.conv6(v23)\n        v25 = v24 + x8\n        return v25\n# Inputs to the model\nx1 = torch.randn(1, 16, 64, 64)\nx2 = torch.randn(1, 16, 64, 64)\nx3 = torch.randn(1, 16, 64, 64)\nx4 = torch.randn(1, 16, 64, 64)\nx5 = torch.randn(1, 16, 64, 64)\nx6 = torch.randn(1, 16, 64, 64)\nx7 = torch.randn(1, 16, 64, 64)\nx8 = torch.randn(1, 16, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(16, 24, 5, stride=2, padding=2)\n        self.conv2 = torch.nn.Conv2d(16, 16, 5, stride=2, padding=2)\n        self.conv3 = torch.nn.Conv2d(16, 72, 5, stride=2, padding=2)\n    def forward(self, x1, x2):\n        v1 = x1\n        v2 = self.conv1(v1)\n        v3 = v2 + 1\n        v4 = torch.relu(v3)\n        v5 = v4\n        v6 = self.conv2(v5)\n        v7 = v6\n        v8 = torch.relu(v7)\n        v9 = v8\n        v10 = self.conv3(v9)\n        v11 = x2\n        v12 = v11\n        v13 = self.conv1(v12)\n        v14 = v10 + v13\n        v15 = torch.relu(v14)\n        v16 = v15\n        v17 = self.conv2(v16)\n        v18 = v17\n        v19 = torch.relu(v18)\n        v20 = v19\n        v21 = self.conv3(v20)\n        v22 = v21\n        v23 = v22 + 1\n        v24 = torch.relu(v23)\n        v25 = v24\n        v26 = v25 + 1\n        v27 = torch.relu(v26)\n        return v27\n# Inputs to the model\nx1 = torch.randn(1, 16, 64, 64)\nx2 = torch.randn(1, 16, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(16, 16, 7, stride=3, padding=3, dilation=2)\n        self.conv2 = torch.nn.Conv2d(16, 16, 7, stride=3, padding=3, dilation=2)\n        self.conv3 = torch.nn.Conv2d(16, 16, 7, stride=3, padding=3, dilation=2)\n    def forward(self, x1, x2, x3, x4, x5):\n        v1 = self.conv1(x1)\n        v2 = self.conv2(x2)\n        v3 = v1 + v2\n        v4 = torch.relu(v3)\n        v5 = self.conv3(v4)\n        v6 = v5 + x3\n        v7 = torch.relu(v6)\n        v8 = v7 + x4\n        v9 = torch.relu(v8)\n        return v9\n# Inputs to the model\nx1 = torch.randn(1, 16, 64, 64)\nx2 = torch.randn(1, 16, 64, 64)\nx3 = torch.randn(1, 16, 64, 64)\nx4 = torch.randn(1, 16, 64, 64)\nx5 = torch.randn(1, 16, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(16, 16, 7, stride=1, padding=3)\n        self.conv2 = torch.nn.Conv2d(16, 16, 7, bias=True, stride=1, padding=3)\n        self.conv3 = torch.nn.Conv2d(16, 16, 7, bias=True, stride=1, padding=3)\n    def forward(self, x1, x2, x3):\n        v1 = x1 + self.conv1(x2)\n        v2 = torch.relu(v1)\n        v3 = 1 + v2\n        v4 = self.conv2(v3)\n        v5 = 2 + v4\n        v6 = 3 + x3\n        v7 = self.conv3(v6)\n        v8 = 4 + v7\n        v9 = torch.relu(v8)\n        return v9\n# Inputs to the model\nx1 = torch.randn(1, 16, 64, 64)\nx2 = torch.randn(1, 16, 64, 64)\nx3 = torch.randn(1, 16, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(16, 16, 7, stride=1, padding=3)\n        self.conv2 = torch.nn.Conv2d(16, 16, 7, stride=1, padding=3)\n        self.conv3 = torch.nn.Conv2d(16, 16, 7, stride=1, padding=3)\n        self.conv4 = torch.nn.Conv2d(16, 16, 7, stride=1, padding=3)\n    def forward(self, x1, x2, x3, x4, x5, x6, x7):\n        v1 = self.conv1(x1)\n        v2 = x2 + v1\n        v3 = torch.relu(v2)\n        v4 = self.conv2(v3)\n        v5 = self.conv3(v4)\n        v6 = v5 + x3\n        v7 = torch.relu(v6)\n        v8 = x4 + v7\n        v9 = torch.relu(v8)\n        v10 = self.conv4(v9)\n        v11 = v10 + x5\n        v12 = torch.relu(v11)\n        v13 = x6 + v12\n        v14 = torch.relu(v13)\n        v15 = x7 + v14\n        v16 = torch.relu(v15)\n        return v16\n# Inputs to the model\nx1 = torch.randn(1, 16, 64, 64)\nx2 = torch.randn(1, 16, 64, 64)\nx3 = torch.randn(1, 16, 64, 64)\nx4 = torch.randn(1, 16, 64, 64)\nx5 = torch.randn(1, 16, 64, 64)\nx6 = torch.randn(1, 16, 64, 64)\nx7 = torch.randn(1, 16, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(16, 16, 7, stride=3, padding=3)\n        self.conv2 = torch.nn.Conv2d(16, 16, 7, stride=3, padding=3)\n        self.conv3 = torch.nn.Conv2d(16, 16, 7, stride=3, padding=3)\n    def forward(self, x1, x2, x3, x4, x5):\n        v1 = self.conv1(x1)\n        v2 = self.conv2(x2)\n        v3 = v1 + 1\n        v4 = torch.relu(v3)\n        v5 = self.conv3(v4)\n        v6 = v1 + v5\n        v7 = torch.relu(v6)\n        return v7\n# Inputs to the model\nx1 = torch.randn(1, 16, 64, 64)\nx2 = torch.randn(1, 16, 64, 64)\nx3 = torch.randn(1, 16, 64, 64)\nx4 = torch.randn(1, 16, 64, 64)\nx5 = torch.randn(1, 16, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(16, 16, 7, stride=1, padding=3)\n        self.conv2 = torch.nn.Conv2d(16, 16, 7, stride=1, padding=3)\n        self.conv3 = torch.nn.Conv2d(16, 16, 7, stride=1, padding=3)\n        self.conv4 = torch.nn.Conv2d(16, 16, 7, stride=1, padding=3)\n        self.conv5 = torch.nn.Conv2d(16, 16, 7, stride=1, padding=3)\n        self.conv6 = torch.nn.Conv2d(16, 16, 7, stride=1, padding=3)\n    def forward(self, x1, x2, x3, x4, x5, x6):\n        v1 = self.conv1(x1)\n        v2 = x2 + v1\n        v3 = torch.relu(v2)\n        v4 = self.conv2(v3)\n        v5 = x3 + v4\n        v6 = torch.relu(v5)\n        v7 = x4 + v6\n        v8 = torch.relu(v7)\n        v9 = self.conv3(v8)\n        v10 = x5 + v9\n        v11 = torch.relu(v10)\n        v12 = x6 + v11\n        v13 = torch.relu(v12)\n        v14 = self.conv4(v13)\n        v15 = x1 + v14\n        v16 = torch.relu(v15)\n        v17 = x2 + v16\n        v18 = torch.relu(v17)\n        v19 = self.conv5(v18)\n        v20 = x3 + v19\n        v21 = torch.relu(v20)\n        v22 = x4 + v21\n        v23 = torch.relu(v22)\n        v24 = self.conv6(v23)\n        v25 = x5 + v24\n        v26 = torch.relu(v25)\n        return v26\n# Inputs to the model\nx1 = torch.randn(1, 16, 64, 64)\nx2 = torch.randn(1, 16, 64, 64)\nx3 = torch.randn(1, 16, 64, 64)\nx4 = torch.randn(1, 16, 64, 64)\nx5 = torch.randn(1, 16, 64, 64)\nx6 = torch.randn(1, 16, 64, 64)",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(16, 16, 1, stride=1, padding=0)\n        self.conv2 = torch.nn.Conv2d(16, 16, 1, stride=1, padding=0)\n        self.conv3 = torch.nn.Conv2d(16, 16, 1, stride=1, padding=0)\n    def forward(self, x1, x2, x3, x4, x5, x6):\n        v1 = self.conv1(x1)\n        v2 = v1 + x1\n        v3 = torch.relu(v2)\n        v4 = self.conv2(x2)\n        v5 = 1 + v4\n        v6 = torch.relu(v5)\n        v7 = 2 + self.conv3(x3)\n        v8 = torch.relu(v7)\n        v9 = v8 + self.conv1(x4)\n        v10 = torch.relu(v9)\n        v11 = v10 + self.conv2(x5)\n        v12 = torch.relu(v11)\n        v13 = 3 + self.conv3(x6)\n        v14 = torch.relu(v13)\n        v15 = self.conv1(v14)\n        v16 = 2 * v15\n        v17 = torch.relu(v16)\n        v18 = self.conv2(v17)\n        v19 = 1 * v18\n        v20 = torch.relu(v19)\n        v21 = self.conv3(v20)\n        v22 = 1 * v21\n        v23 = torch.relu(v22)\n        return v23\n# Inputs to the model\nx1 = torch.randn(1, 16, 64, 64)\nx2 = torch.randn(1, 16, 64, 64)\nx3 = torch.randn(1, 16, 64, 64)\nx4 = torch.randn(1, 16, 64, 64)\nx5 = torch.randn(1, 16, 64, 64)\nx6 = torch.randn(1, 16, 64, 64)\n"
            ],
            "g_time": 24.782506227493286
        }
    }
}
{
    "opt": "sfdp=4",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.q1 = torch.nn.Parameter(torch.randn(3, 2))\n        self.k5 = torch.nn.Parameter(torch.randn(3, 2))\n        self.v2 = torch.nn.Parameter(torch.randn(3, 2))\n        self.mask = torch.nn.Parameter(mask.squeeze(0))\n    def forward(self):\n        qk = self.q1 @ self.k5.transpose(-2, -1) / math.sqrt(self.q1.size(-1))\n        qk = qk + self.mask\n        attn_weight = torch.softmax(qk, dim=-1)\n        output = attn_weight @ self.v2\n        return output\n# Inputs to the model\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, q1, k55, v22, msk):\n        qk = q1 @ k55.transpose(-2, -1) / math.sqrt(q1.size(-1))\n        qk = qk + msk\n        attn_weight = torch.softmax(qk, dim=-1)\n        output = attn_weight @ v22\n        return output\n# Inputs to the model\nquery = torch.randn(1, 64, 56, 56)\nkey = torch.randn(1, 64, 56, 56)\nvalue = torch.randn(1, 64, 56, 56)\nattn_mask = (torch.rand(1, 56, 56) > 0.7).fill_(-1000000000.0)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, query, key, value, mask):\n        qk = query @ key.transpose(-2, -1) / math.sqrt(query.size(-1))\n        output = torch.softmax(qk, dim=-1) @ value\n        return output\n# Inputs to the model\nq = torch.randn(1, 64, 56, 56)\nk = torch.randn(1, 64, 56, 56)\nv4 = torch.randn(1, 64, 56, 56)\nmask = (torch.rand(1, 56, 56) > 0.7).fill_(-1000000000.0)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, q,  K, v,  mask):\n        QK = q @ K.transpose(-2, -1)\n        QK = QK + mask\n        attn_weight = torch.softmax(QK, dim=-1)\n        output = attn_weight @ v\n        return output\n# Inputs to the model\nq = torch.randn(1, 64, 56, 56)\nK = torch.randn(1, 64, 56, 56)\nV = torch.randn(1, 64, 56, 56)\nmask = (torch.rand(1, 56, 56) > 0.7).fill_(-1000000000.0)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, q, ke_w, val, mask):\n        qk = query @ key.transpose(-2, -1) / math.sqrt(query.size(-1))\n        qk = qk + mask\n        attn_weight = torch.softmax(qk, dim=-1)\n        output = attn_weight @ value\n        return output\n# Inputs to the model\nq = torch.randn(1, 64, 56, 56)\nk = torch.randn(1, 64, 56, 56)\nv3 = torch.randn(1, 64, 56, 56)\nmask = (torch.rand(1, 56, 56) > 0.7).fill_(-1000000000.0)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, Q7, k, v3, mask):\n        qk = Q7 @ k.transpose(-2, -1) / math.sqrt(Q7.size(-1))\n        qk = qk + mask\n        attn_weight = torch.softmax(qk, dim=-1)\n        output = attn_weight @ v3\n        return output\n# Inputs to the model\nQ1 = torch.randn(1, 64, 56, 56)\nK5 = torch.randn(1, 64, 56, 56)\nv2 = torch.randn(1, 64, 56, 56)\nmask = (torch.rand(1, 56, 56) > 0.7).fill_(-1000000000.0)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, query, k, v, mask):\n        qk = query @ k.transpose(-2, -1) / math.sqrt(query.size(-1))\n        qk = qk + mask\n        attn_weight = torch.softmax(qk, dim=-1)\n        output = attn_weight @ v\n        return output\n# Inputs to the model\nq = torch.randn(1, 64, 56, 56)\nkey = torch.randn(128, 64, 56, 56)\nvalue = torch.randn(128, 64, 56, 56)\nmask = (torch.rand(1, 56, 56) > 0.7).fill_(-1000000000.0)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, q3, k5, v2, mask2):\n        qk = q3 @ k5.transpose(-2, -1) / math.sqrt(q3.size(-1))\n        qk = qk + mask2\n        attn_weight = torch.softmax(qk, dim=-1)\n        output = attn_weight @ v2\n        return output\n# Inputs to the model\nQ = torch.randn(1, 64, 56, 56)\nK = torch.randn(1, 64, 56, 56)\nV = torch.randn(1, 64, 56, 56)\nmask = (torch.rand(1, 56, 56) > 0.7).fill_(-1000000000.0)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, q1, k1, v3, mask):\n        qk = q1 @ k1.transpose(-2, -1) / math.sqrt(q1.size(-1))\n        qk = qk + mask\n        attn_weight = torch.softmax(qk, dim=-1)\n        output = attn_weight @ v3\n        return output\n# Inputs to the model\nQ = torch.randn(1, 64, 56, 56)\nK = torch.randn(1, 64, 56, 56)\nV = torch.randn(1, 64, 56, 56)\nmask = (torch.rand(1, 56, 56) > 0.7).fill_(-1000000000.0)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, Q, k, v, mask):\n        qk = Q @ k.transpose(-2, -1) / math.sqrt(Q.size(-1))\n        qk = qk + mask\n        attn_weight = torch.softmax(qk, dim=-1)\n        output = attn_weight @ v\n        return output\n# Inputs to the model\nq1 = torch.randn(1, 64, 56, 56)\nK = torch.randn(1, 64, 56, 56)\nV = torch.randn(1, 64, 56, 56)\nmask = (torch.rand(1, 56, 56) > 0.7).fill_(-1000000000.0)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.q1 = torch.nn.Parameter(torch.randn(3, 2))\n        self.k5 = torch.nn.Parameter(torch.randn(3, 2))\n        self.v2 = torch.nn.Parameter(torch.randn(3, 2))\n        self.mask = torch.nn.Parameter(mask.squeeze(0))\n    def forward(self):\n        qk = self.q1 @ self.k5.transpose(-2, -1) / math.sqrt(self.q1.size(-1))\n        qk = qk + self.mask\n        attn_weight = torch.softmax(qk, dim=-1)\n        output = attn_weight @ self.v2\n        return output\n# Inputs to the model\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, q1, k55, v22, msk):\n        qk = q1 @ k55.transpose(-2, -1) / math.sqrt(q1.size(-1))\n        qk = qk + msk\n        attn_weight = torch.softmax(qk, dim=-1)\n        output = attn_weight @ v22\n        return output\n# Inputs to the model\nquery = torch.randn(1, 64, 56, 56)\nkey = torch.randn(1, 64, 56, 56)\nvalue = torch.randn(1, 64, 56, 56)\nattn_mask = (torch.rand(1, 56, 56) > 0.7).fill_(-1000000000.0)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, query, key, value, mask):\n        qk = query @ key.transpose(-2, -1) / math.sqrt(query.size(-1))\n        output = torch.softmax(qk, dim=-1) @ value\n        return output\n# Inputs to the model\nq = torch.randn(1, 64, 56, 56)\nk = torch.randn(1, 64, 56, 56)\nv4 = torch.randn(1, 64, 56, 56)\nmask = (torch.rand(1, 56, 56) > 0.7).fill_(-1000000000.0)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, q,  K, v,  mask):\n        QK = q @ K.transpose(-2, -1)\n        QK = QK + mask\n        attn_weight = torch.softmax(QK, dim=-1)\n        output = attn_weight @ v\n        return output\n# Inputs to the model\nq = torch.randn(1, 64, 56, 56)\nK = torch.randn(1, 64, 56, 56)\nV = torch.randn(1, 64, 56, 56)\nmask = (torch.rand(1, 56, 56) > 0.7).fill_(-1000000000.0)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, q, ke_w, val, mask):\n        qk = query @ key.transpose(-2, -1) / math.sqrt(query.size(-1))\n        qk = qk + mask\n        attn_weight = torch.softmax(qk, dim=-1)\n        output = attn_weight @ value\n        return output\n# Inputs to the model\nq = torch.randn(1, 64, 56, 56)\nk = torch.randn(1, 64, 56, 56)\nv3 = torch.randn(1, 64, 56, 56)\nmask = (torch.rand(1, 56, 56) > 0.7).fill_(-1000000000.0)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, Q7, k, v3, mask):\n        qk = Q7 @ k.transpose(-2, -1) / math.sqrt(Q7.size(-1))\n        qk = qk + mask\n        attn_weight = torch.softmax(qk, dim=-1)\n        output = attn_weight @ v3\n        return output\n# Inputs to the model\nQ1 = torch.randn(1, 64, 56, 56)\nK5 = torch.randn(1, 64, 56, 56)\nv2 = torch.randn(1, 64, 56, 56)\nmask = (torch.rand(1, 56, 56) > 0.7).fill_(-1000000000.0)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, query, k, v, mask):\n        qk = query @ k.transpose(-2, -1) / math.sqrt(query.size(-1))\n        qk = qk + mask\n        attn_weight = torch.softmax(qk, dim=-1)\n        output = attn_weight @ v\n        return output\n# Inputs to the model\nq = torch.randn(1, 64, 56, 56)\nkey = torch.randn(128, 64, 56, 56)\nvalue = torch.randn(128, 64, 56, 56)\nmask = (torch.rand(1, 56, 56) > 0.7).fill_(-1000000000.0)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, q3, k5, v2, mask2):\n        qk = q3 @ k5.transpose(-2, -1) / math.sqrt(q3.size(-1))\n        qk = qk + mask2\n        attn_weight = torch.softmax(qk, dim=-1)\n        output = attn_weight @ v2\n        return output\n# Inputs to the model\nQ = torch.randn(1, 64, 56, 56)\nK = torch.randn(1, 64, 56, 56)\nV = torch.randn(1, 64, 56, 56)\nmask = (torch.rand(1, 56, 56) > 0.7).fill_(-1000000000.0)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, q1, k1, v3, mask):\n        qk = q1 @ k1.transpose(-2, -1) / math.sqrt(q1.size(-1))\n        qk = qk + mask\n        attn_weight = torch.softmax(qk, dim=-1)\n        output = attn_weight @ v3\n        return output\n# Inputs to the model\nQ = torch.randn(1, 64, 56, 56)\nK = torch.randn(1, 64, 56, 56)\nV = torch.randn(1, 64, 56, 56)\nmask = (torch.rand(1, 56, 56) > 0.7).fill_(-1000000000.0)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, Q, k, v, mask):\n        qk = Q @ k.transpose(-2, -1) / math.sqrt(Q.size(-1))\n        qk = qk + mask\n        attn_weight = torch.softmax(qk, dim=-1)\n        output = attn_weight @ v\n        return output\n# Inputs to the model\nq1 = torch.randn(1, 64, 56, 56)\nK = torch.randn(1, 64, 56, 56)\nV = torch.randn(1, 64, 56, 56)\nmask = (torch.rand(1, 56, 56) > 0.7).fill_(-1000000000.0)\n"
            ],
            "g_time": 8.573529958724976
        }
    }
}
{
    "opt": "binary=8",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.seq1 = torch.nn.Sequential(torch.nn.Conv1d(2, 2, 2, stride=2,\n                                                         padding=1), torch.nn.Conv1d(2, 3, 3, stride=3, padding=1))\n        self.seq2 = torch.nn.Sequential(torch.nn.Conv1d(2, 3, 2, stride=2,\n                                                         padding=1), torch.nn.Conv1d(3, 3, 3, stride=3, padding=1))\n    def forward(self, x1, x2):\n        v1 = self.seq1(x1)\n        v2 = self.seq2(x2)\n        v3 = v1 + v2\n        return v3\n# Inputs to the model\nx1 = torch.randn(2, 2, 12)\nx2 = torch.randn(2, 2, 12)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 4, 3, padding=1, stride=1)\n        self.conv2 = torch.nn.Conv2d(3, 4, 3, padding=1, stride=1)\n        self.conv3 = torch.nn.Conv2d(3, 4, 3, padding=1, stride=1)\n        self.conv4 = torch.nn.Conv2d(3, 4, 3, padding=1, stride=1)\n    def forward(self, x1, x2, x3, x4):\n        v1 = self.conv1(x1)\n        v2 = self.conv2(x2)\n        v_cat = torch.cat([v1, v2], dim=1)\n        v3 = self.conv3(x3)\n        v4 = self.conv4(x4)\n        v_cat = torch.cat([v_cat, v3, v4], dim=1)\n        v_out = torch.cat([v_cat, v_cat], dim=1)\n        return v_out\n# Inputs to the model\nx1 = torch.randn(1, 3, 30, 34)\nx2 = torch.randn(1, 3, 30, 34)\nx3 = torch.randn(1, 3, 30, 34)\nx4 = torch.randn(1, 3, 30, 34)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 2, 1, padding=1, stride=1)\n        self.conv2 = torch.nn.Conv2d(3, 2, 1, padding=1, stride=1)\n    def forward(self, x1, x2):\n        v1 = self.conv1(x1)\n        v2 = self.conv2(torch.nn.functional.relu(x2))\n        v3 = v1 + v2\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\nx2 = torch.zeros(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 8, 1, padding=0, stride=1)\n    def forward(self, x1, x2):\n        v1 = self.conv1(x1)\n        v2 = x2 - v1\n        v3 = torch.nn.functional.relu(v2)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 3, 16, 16)\nx2 = torch.randn(1, 3, 16, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 64, 3, padding=1, stride=1)\n        self.conv2 = torch.nn.Conv2d(3, 64, 3, padding=1, stride=2)\n        self.conv3 = torch.nn.Conv2d(3, 64, 3, padding=1, stride=4)\n        self.conv4 = torch.nn.Conv2d(3, 64, 3, padding=1, stride=8)\n    def forward(self, x):\n        v1 = self.conv1(x)\n        v2 = self.conv2(x)\n        v3 = self.conv3(x)\n        v4 = self.conv4(x)\n        v5 = v1 + v2 + v3 + v4\n        v6 = torch.sigmoid(v5)\n        return v6\n# Inputs to the model\nx = torch.randn(1, 3, 32, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 64, 5, padding=2, stride=1)\n        self.conv2 = torch.nn.Conv2d(3, 64, 5, padding=2, stride=1)\n    def forward(self, x1, x2):\n        v1 = self.conv1(x1)\n        v2 = self.conv2(x2)\n        v3 = v1 + v2\n        v4 = torch.relu(v3)\n        v5 = torch.tanh(v4)\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\nx2 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 32, 3, stride=1, padding=1, bias=False)\n        self.bn1 = torch.nn.BatchNorm2d(32, 1e-05, 0.1, True)\n        self.conv2 = torch.nn.Conv2d(3, 32, 3, stride=1, padding=1, bias=False)\n        self.bn2 = torch.nn.BatchNorm2d(32, 1e-05, 0.1, True)\n        self.conv3 = torch.nn.Conv2d(3, 3, 1, stride=1, padding=0, bias=False)\n    def forward(self, x):\n        v1 = self.bn1(self.conv1(x))\n        v2 = self.bn2(self.conv2(x))\n        v3 = v1 + v2\n        v4 = self.conv3(v3)\n        return v4\n# Inputs to the model\nx = torch.randn(5, 3, 128, 128)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 64, 3, padding=1, stride=1)\n        self.conv2 = torch.nn.Conv2d(3, 64, 3, padding=1, stride=1)\n        self.fc = torch.nn.Linear(64, 64)\n    def forward(self, x1, x2):\n        v1 = self.conv1(x1)\n        v2 = self.conv2(x2)\n        v3 = torch.nn.functional.relu(v1 + v2)\n        v4 = torch.nn.functional.tanh(v3)\n        v5 = v4.view(-1, 64)\n\n        v6 = self.fc(v5)\n        return v6\n# Inputs to the model\nx1 = torch.randn(2, 3, 64, 64)\nx2 = torch.randn(2, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 32, 3, padding=1, stride=1)\n        self.relu = torch.nn.ReLU()\n        self.bn1 = torch.nn.BatchNorm2d(32)\n        self.conv2 = torch.nn.Conv2d(32, 32, 3, padding=1, stride=1)\n        self.conv3 = torch.nn.Conv2d(32, 32, 3, padding=1, stride=1)\n        self.conv4 = torch.nn.Conv2d(32, 32, 3, padding=1, stride=1)\n        self.conv5 = torch.nn.Conv2d(32, 64, 3, padding=1, stride=1)\n        self.bn2 = torch.nn.BatchNorm2d(64)\n        self.conv6 = torch.nn.Conv2d(64, 64, 3, padding=1, stride=1)\n        self.conv7 = torch.nn.Conv2d(64, 64, 1, padding=1, stride=1)\n        self.bn3 = torch.nn.BatchNorm2d(64)\n    def forward(self, x1, x2, x31, x32, x41, x42):\n        y1 = self.conv1(x1)\n        y2 = self.conv1(x2)\n        w0 = self.relu(y1 + y2)\n        w1 = self.bn1(w0)\n        w2 = self.conv2(w1)\n        w3 = self.conv3(w1) + self.conv4(w2)\n        w4 = self.conv5(w3)\n        w5 = self.relu(w4)\n        y3 = self.bn2(w4)\n        w6 = self.conv6(y3)\n        w7 = self.conv7(w5)\n        w8 = self.bn3(w6 + w7)\n        return w8\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\nx2 = torch.randn(1, 3, 64, 64)\nx3 = torch.randn(1, 3, 96, 96)\nx4 = torch.randn(1, 3, 96, 96)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 64, 3, padding=1, stride=2)\n        self.conv2 = torch.nn.Conv2d(3, 64, 3, padding=1, stride=2)\n    def forward(self, x1, x2):\n        v1 = self.conv1(x1)\n        v2 = self.conv2(x2)\n        v3 = torch.nn.functional.relu(v1 + v2)\n        v4 = torch.nn.functional.tanh(v3)\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 3, 128, 128)\nx2 = torch.randn(1, 3, 128, 128)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.seq1 = torch.nn.Sequential(torch.nn.Conv1d(2, 2, 2, stride=2,\n                                                         padding=1), torch.nn.Conv1d(2, 3, 3, stride=3, padding=1))\n        self.seq2 = torch.nn.Sequential(torch.nn.Conv1d(2, 3, 2, stride=2,\n                                                         padding=1), torch.nn.Conv1d(3, 3, 3, stride=3, padding=1))\n    def forward(self, x1, x2):\n        v1 = self.seq1(x1)\n        v2 = self.seq2(x2)\n        v3 = v1 + v2\n        return v3\n# Inputs to the model\nx1 = torch.randn(2, 2, 12)\nx2 = torch.randn(2, 2, 12)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 4, 3, padding=1, stride=1)\n        self.conv2 = torch.nn.Conv2d(3, 4, 3, padding=1, stride=1)\n        self.conv3 = torch.nn.Conv2d(3, 4, 3, padding=1, stride=1)\n        self.conv4 = torch.nn.Conv2d(3, 4, 3, padding=1, stride=1)\n    def forward(self, x1, x2, x3, x4):\n        v1 = self.conv1(x1)\n        v2 = self.conv2(x2)\n        v_cat = torch.cat([v1, v2], dim=1)\n        v3 = self.conv3(x3)\n        v4 = self.conv4(x4)\n        v_cat = torch.cat([v_cat, v3, v4], dim=1)\n        v_out = torch.cat([v_cat, v_cat], dim=1)\n        return v_out\n# Inputs to the model\nx1 = torch.randn(1, 3, 30, 34)\nx2 = torch.randn(1, 3, 30, 34)\nx3 = torch.randn(1, 3, 30, 34)\nx4 = torch.randn(1, 3, 30, 34)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 2, 1, padding=1, stride=1)\n        self.conv2 = torch.nn.Conv2d(3, 2, 1, padding=1, stride=1)\n    def forward(self, x1, x2):\n        v1 = self.conv1(x1)\n        v2 = self.conv2(torch.nn.functional.relu(x2))\n        v3 = v1 + v2\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\nx2 = torch.zeros(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 8, 1, padding=0, stride=1)\n    def forward(self, x1, x2):\n        v1 = self.conv1(x1)\n        v2 = x2 - v1\n        v3 = torch.nn.functional.relu(v2)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 3, 16, 16)\nx2 = torch.randn(1, 3, 16, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 64, 3, padding=1, stride=1)\n        self.conv2 = torch.nn.Conv2d(3, 64, 3, padding=1, stride=2)\n        self.conv3 = torch.nn.Conv2d(3, 64, 3, padding=1, stride=4)\n        self.conv4 = torch.nn.Conv2d(3, 64, 3, padding=1, stride=8)\n    def forward(self, x):\n        v1 = self.conv1(x)\n        v2 = self.conv2(x)\n        v3 = self.conv3(x)\n        v4 = self.conv4(x)\n        v5 = v1 + v2 + v3 + v4\n        v6 = torch.sigmoid(v5)\n        return v6\n# Inputs to the model\nx = torch.randn(1, 3, 32, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 64, 5, padding=2, stride=1)\n        self.conv2 = torch.nn.Conv2d(3, 64, 5, padding=2, stride=1)\n    def forward(self, x1, x2):\n        v1 = self.conv1(x1)\n        v2 = self.conv2(x2)\n        v3 = v1 + v2\n        v4 = torch.relu(v3)\n        v5 = torch.tanh(v4)\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\nx2 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 32, 3, stride=1, padding=1, bias=False)\n        self.bn1 = torch.nn.BatchNorm2d(32, 1e-05, 0.1, True)\n        self.conv2 = torch.nn.Conv2d(3, 32, 3, stride=1, padding=1, bias=False)\n        self.bn2 = torch.nn.BatchNorm2d(32, 1e-05, 0.1, True)\n        self.conv3 = torch.nn.Conv2d(3, 3, 1, stride=1, padding=0, bias=False)\n    def forward(self, x):\n        v1 = self.bn1(self.conv1(x))\n        v2 = self.bn2(self.conv2(x))\n        v3 = v1 + v2\n        v4 = self.conv3(v3)\n        return v4\n# Inputs to the model\nx = torch.randn(5, 3, 128, 128)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 64, 3, padding=1, stride=1)\n        self.conv2 = torch.nn.Conv2d(3, 64, 3, padding=1, stride=1)\n        self.fc = torch.nn.Linear(64, 64)\n    def forward(self, x1, x2):\n        v1 = self.conv1(x1)\n        v2 = self.conv2(x2)\n        v3 = torch.nn.functional.relu(v1 + v2)\n        v4 = torch.nn.functional.tanh(v3)\n        v5 = v4.view(-1, 64)\n\n        v6 = self.fc(v5)\n        return v6\n# Inputs to the model\nx1 = torch.randn(2, 3, 64, 64)\nx2 = torch.randn(2, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 32, 3, padding=1, stride=1)\n        self.relu = torch.nn.ReLU()\n        self.bn1 = torch.nn.BatchNorm2d(32)\n        self.conv2 = torch.nn.Conv2d(32, 32, 3, padding=1, stride=1)\n        self.conv3 = torch.nn.Conv2d(32, 32, 3, padding=1, stride=1)\n        self.conv4 = torch.nn.Conv2d(32, 32, 3, padding=1, stride=1)\n        self.conv5 = torch.nn.Conv2d(32, 64, 3, padding=1, stride=1)\n        self.bn2 = torch.nn.BatchNorm2d(64)\n        self.conv6 = torch.nn.Conv2d(64, 64, 3, padding=1, stride=1)\n        self.conv7 = torch.nn.Conv2d(64, 64, 1, padding=1, stride=1)\n        self.bn3 = torch.nn.BatchNorm2d(64)\n    def forward(self, x1, x2, x31, x32, x41, x42):\n        y1 = self.conv1(x1)\n        y2 = self.conv1(x2)\n        w0 = self.relu(y1 + y2)\n        w1 = self.bn1(w0)\n        w2 = self.conv2(w1)\n        w3 = self.conv3(w1) + self.conv4(w2)\n        w4 = self.conv5(w3)\n        w5 = self.relu(w4)\n        y3 = self.bn2(w4)\n        w6 = self.conv6(y3)\n        w7 = self.conv7(w5)\n        w8 = self.bn3(w6 + w7)\n        return w8\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\nx2 = torch.randn(1, 3, 64, 64)\nx3 = torch.randn(1, 3, 96, 96)\nx4 = torch.randn(1, 3, 96, 96)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 64, 3, padding=1, stride=2)\n        self.conv2 = torch.nn.Conv2d(3, 64, 3, padding=1, stride=2)\n    def forward(self, x1, x2):\n        v1 = self.conv1(x1)\n        v2 = self.conv2(x2)\n        v3 = torch.nn.functional.relu(v1 + v2)\n        v4 = torch.nn.functional.tanh(v3)\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 3, 128, 128)\nx2 = torch.randn(1, 3, 128, 128)\n"
            ],
            "g_time": 19.033406019210815
        }
    }
}
{
    "opt": "binary_unary=8",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.c1 = torch.nn.Conv2d(32, 64, (6, 3), stride=(1, 1), padding=(1, 0), dilation=(3, 4), groups=32, bias=True)\n    def forward(self, x):\n        return torch.add(self.c1(x), x)\n# Inputs to the model\nx1 = torch.randn(1, 32, 113, 73)\nx2 = torch.randn(1, 32, 113, 73)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(1, 8, 3, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = self.conv1(x1)\n        v3 = self.conv1(x1)\n        v4 = v1.repeat(3, 1, 1, 1) + v2.repeat(3, 1, 1, 1) + v3.repeat(3, 1, 1, 1)\n        v5 = torch.relu(v4)\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 1, 32, 96)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(16, 192, 1, padding=0, stride=1, dilation=1, groups=1, bias=True)\n    def forward(self, x1, x2, x3, x4, x5, x6):\n        v1 = self.conv(x1)\n        v2 = self.conv(x2)\n        v3 = self.conv(x3)\n        v4 = self.conv(x4)\n        v5 = self.conv(x5)\n        v6 = self.conv(x6)\n        v7 = v1 + v2 + v3 + v4 + v5 + v6\n        v8 = torch.relu(v7)\n        return v8\n# Inputs to the model\nx1 = torch.randn(1, 16, 16, 16)\nx2 = torch.randn(1, 16, 16, 16)\nx3 = torch.randn(1, 16, 16, 16)\nx4 = torch.randn(1, 16, 16, 16)\nx5 = torch.randn(1, 16, 16, 16)\nx6 = torch.randn(1, 16, 16, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(1, 1, 1, bias=True, groups=1, dilation=1, padding=0, stride=1)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 + v2\n        v4 = torch.relu(v3)\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 1, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 16, 3, stride=1, padding=1)\n        self.conv2 = torch.nn.Conv2d(16, 32, 3, stride=1, padding=1)\n        self.conv3 = torch.nn.Conv2d(32, 64, 3, stride=1, padding=1)\n        self.conv4 = torch.nn.Conv2d(64, 128, 3, stride=1, padding=1)\n    def forward(self, x):\n        v1 = self.conv1(x)\n        v2 = self.conv2(v1)\n        v3 = self.conv3(v2+v1)\n        v4 = self.conv4(v1+v3+v2+v1)\n        v5 = self.conv1(v1+v3+v2+v1+v4)\n        v6 = torch.relu(v5)\n        return v6\n# Inputs to the model\nx = torch.randn(1, 3, 32, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.model1 = torch.nn.Conv2d(4, 32, (5, 33), stride=(2, 15), padding=(1, 7), dilation=(2, 8), groups=16, bias=True)\n        self.model2 = torch.nn.Conv2d(4, 32, (5, 33), stride=(2, 15), padding=(1, 7), dilation=(2, 8), groups=16, bias=True)\n        self.model3 = torch.nn.Conv2d(4, 32, (5, 33), stride=(2, 15), padding=(1, 7), dilation=(2, 8), groups=16, bias=True)\n    def forward(self, x1, x2, x3):\n        v1 = self.model1(x1)\n        v2 = self.model2(x2)\n        v3 = self.model3(x3)\n        v4 = v1 + v2 + v3\n        v5 = torch.relu(v4)\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 4, 1024, 416)\nx2 = torch.randn(1, 4, 1024, 416)\nx3 = torch.randn(1, 4, 1024, 416)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(1, 8, 3, stride=1, padding=2)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = v1 + self.conv1(x1)\n        v3 = torch.relu(v2)\n        return torch.relu(v3)\n# Inputs to the model\nx1 = torch.randn(1, 1, 32, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(1, 8, 3, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = self.conv1(x1)\n        v3 = self.conv1(x1)\n        v4 = v1 + v2 + v3\n        v5 = torch.relu(v4)\n        v6 = torch.relu(v5)\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 1, 32, 96)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, x2, x3, x4, x5):\n        v1 = torch.nn.functional.interpolate(x1, scale_factor=1.0, size=None, mode='bicubic', align_corners=None)\n        v2 = torch.nn.functional.interpolate(x2, scale_factor=2.0, size=None, mode='bicubic', align_corners=None)\n        v3 = torch.nn.functional.interpolate(x3, scale_factor=3.0, size=None, mode='bicubic', align_corners=None)\n        v4 = torch.nn.functional.interpolate(x4, scale_factor=1.0, size=None, mode='bicubic', align_corners=None)\n        v5 = torch.nn.functional.interpolate(x5, scale_factor=1.0, size=None, mode='bicubic', align_corners=None)\n        v11 = v1 + v2 + v3\n        v12 = v11 + v4 + v5\n        v13 = torch.relu(v12)\n        return v13\n# Inputs to the model\nx1 = torch.randn(1, 16, 32, 32, requires_grad = True)\nx2 = torch.randn(1, 16, 32, 32, requires_grad = True)\nx3 = torch.randn(1, 16, 32, 32, requires_grad = True)\nx4 = torch.randn(1, 16, 32, 32, requires_grad = True)\nx5 = torch.randn(1, 16, 32, 32, requires_grad = True)\n# model ends\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(24, 192, 1, padding=0, stride=1, dilation=1, groups=3, bias=True)\n        self.conv2 = torch.nn.Conv2d(24, 192, 1, padding=0, stride=1, dilation=1, groups=3, bias=True)\n        self.conv3 = torch.nn.Conv2d(24, 192, 1, padding=0, stride=1, dilation=1, groups=3, bias=True)\n    def forward(self, x1, x2, x3):\n        v1 = self.conv1(x1)\n        v2 = self.conv2(x2)\n        v3 = self.conv3(x3)\n        v4 = v1 + v2 + v3\n        v5 = torch.relu(v4)\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 24, 64, 64)\nx2 = torch.randn(1, 24, 64, 64)\nx3 = torch.randn(1, 24, 64, 64)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.c1 = torch.nn.Conv2d(32, 64, (6, 3), stride=(1, 1), padding=(1, 0), dilation=(3, 4), groups=32, bias=True)\n    def forward(self, x):\n        return torch.add(self.c1(x), x)\n# Inputs to the model\nx1 = torch.randn(1, 32, 113, 73)\nx2 = torch.randn(1, 32, 113, 73)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(1, 8, 3, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = self.conv1(x1)\n        v3 = self.conv1(x1)\n        v4 = v1.repeat(3, 1, 1, 1) + v2.repeat(3, 1, 1, 1) + v3.repeat(3, 1, 1, 1)\n        v5 = torch.relu(v4)\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 1, 32, 96)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(16, 192, 1, padding=0, stride=1, dilation=1, groups=1, bias=True)\n    def forward(self, x1, x2, x3, x4, x5, x6):\n        v1 = self.conv(x1)\n        v2 = self.conv(x2)\n        v3 = self.conv(x3)\n        v4 = self.conv(x4)\n        v5 = self.conv(x5)\n        v6 = self.conv(x6)\n        v7 = v1 + v2 + v3 + v4 + v5 + v6\n        v8 = torch.relu(v7)\n        return v8\n# Inputs to the model\nx1 = torch.randn(1, 16, 16, 16)\nx2 = torch.randn(1, 16, 16, 16)\nx3 = torch.randn(1, 16, 16, 16)\nx4 = torch.randn(1, 16, 16, 16)\nx5 = torch.randn(1, 16, 16, 16)\nx6 = torch.randn(1, 16, 16, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(1, 1, 1, bias=True, groups=1, dilation=1, padding=0, stride=1)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 + v2\n        v4 = torch.relu(v3)\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 1, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 16, 3, stride=1, padding=1)\n        self.conv2 = torch.nn.Conv2d(16, 32, 3, stride=1, padding=1)\n        self.conv3 = torch.nn.Conv2d(32, 64, 3, stride=1, padding=1)\n        self.conv4 = torch.nn.Conv2d(64, 128, 3, stride=1, padding=1)\n    def forward(self, x):\n        v1 = self.conv1(x)\n        v2 = self.conv2(v1)\n        v3 = self.conv3(v2+v1)\n        v4 = self.conv4(v1+v3+v2+v1)\n        v5 = self.conv1(v1+v3+v2+v1+v4)\n        v6 = torch.relu(v5)\n        return v6\n# Inputs to the model\nx = torch.randn(1, 3, 32, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.model1 = torch.nn.Conv2d(4, 32, (5, 33), stride=(2, 15), padding=(1, 7), dilation=(2, 8), groups=16, bias=True)\n        self.model2 = torch.nn.Conv2d(4, 32, (5, 33), stride=(2, 15), padding=(1, 7), dilation=(2, 8), groups=16, bias=True)\n        self.model3 = torch.nn.Conv2d(4, 32, (5, 33), stride=(2, 15), padding=(1, 7), dilation=(2, 8), groups=16, bias=True)\n    def forward(self, x1, x2, x3):\n        v1 = self.model1(x1)\n        v2 = self.model2(x2)\n        v3 = self.model3(x3)\n        v4 = v1 + v2 + v3\n        v5 = torch.relu(v4)\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 4, 1024, 416)\nx2 = torch.randn(1, 4, 1024, 416)\nx3 = torch.randn(1, 4, 1024, 416)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(1, 8, 3, stride=1, padding=2)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = v1 + self.conv1(x1)\n        v3 = torch.relu(v2)\n        return torch.relu(v3)\n# Inputs to the model\nx1 = torch.randn(1, 1, 32, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(1, 8, 3, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = self.conv1(x1)\n        v3 = self.conv1(x1)\n        v4 = v1 + v2 + v3\n        v5 = torch.relu(v4)\n        v6 = torch.relu(v5)\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 1, 32, 96)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, x2, x3, x4, x5):\n        v1 = torch.nn.functional.interpolate(x1, scale_factor=1.0, size=None, mode='bicubic', align_corners=None)\n        v2 = torch.nn.functional.interpolate(x2, scale_factor=2.0, size=None, mode='bicubic', align_corners=None)\n        v3 = torch.nn.functional.interpolate(x3, scale_factor=3.0, size=None, mode='bicubic', align_corners=None)\n        v4 = torch.nn.functional.interpolate(x4, scale_factor=1.0, size=None, mode='bicubic', align_corners=None)\n        v5 = torch.nn.functional.interpolate(x5, scale_factor=1.0, size=None, mode='bicubic', align_corners=None)\n        v11 = v1 + v2 + v3\n        v12 = v11 + v4 + v5\n        v13 = torch.relu(v12)\n        return v13\n# Inputs to the model\nx1 = torch.randn(1, 16, 32, 32, requires_grad = True)\nx2 = torch.randn(1, 16, 32, 32, requires_grad = True)\nx3 = torch.randn(1, 16, 32, 32, requires_grad = True)\nx4 = torch.randn(1, 16, 32, 32, requires_grad = True)\nx5 = torch.randn(1, 16, 32, 32, requires_grad = True)\n# model ends\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(24, 192, 1, padding=0, stride=1, dilation=1, groups=3, bias=True)\n        self.conv2 = torch.nn.Conv2d(24, 192, 1, padding=0, stride=1, dilation=1, groups=3, bias=True)\n        self.conv3 = torch.nn.Conv2d(24, 192, 1, padding=0, stride=1, dilation=1, groups=3, bias=True)\n    def forward(self, x1, x2, x3):\n        v1 = self.conv1(x1)\n        v2 = self.conv2(x2)\n        v3 = self.conv3(x3)\n        v4 = v1 + v2 + v3\n        v5 = torch.relu(v4)\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 24, 64, 64)\nx2 = torch.randn(1, 24, 64, 64)\nx3 = torch.randn(1, 24, 64, 64)\n"
            ],
            "g_time": 13.576044797897339
        }
    }
}
{
    "opt": "splitwithsizes_cat_replace",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.features1 = torch.nn.Sequential(*(torch.nn.Conv2d(32, 1, 3, 1, 1) for _ in range(8)))\n        self.features2 = torch.nn.Sequential(*(torch.nn.Conv2d(1, 1, 3, 1, 1) for _ in range(6)))\n    def forward(self, v1):\n        split_tensors = torch.split(v1, [1, 1, 1], dim=1)\n        concatenated_tensor = torch.cat(split_tensors, dim=1)\n        return (concatenated_tensor, torch.split(v1, [1, 1, 1], dim=1))\n# Inputs to the model\nx1 = torch.randn(1, 3, 64,64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.features = torch.nn.ModuleList([torch.nn.Conv2d(3, 32, 3, 1, 1, bias=False),torch.nn.ReLU(),torch.nn.Conv2d(32, 32, 1, 1, 1, bias=False),torch.nn.Conv2d(32, 32, 3, 1, 1, bias=False),torch.nn.ReLU()])\n    def forward(self, x):\n        split_tensors = torch.split(x, [1, 1], dim=1)\n        concatenated_tensor = torch.cat(split_tensors, dim=1)\n        return (concatenated_tensor, torch.split(x, [1, 1], dim=1))\n# Inputs to the model\nx1 = torch.randn(1, 3, 32, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.a = torch.nn.Parameter(torch.randn((9, 9, 3, 3)), requires_grad=True)\n    def forward(self, v1):\n        split_tensors = torch.split(v1, [3, 3, 3], dim=2)\n        concatenated_tensor = torch.cat(split_tensors, dim=2)\n        return (concatenated_tensor, torch.split(v1, [3, 3, 3], dim=2))\n# Inputs to the model\nx1 = torch.randn(1, 1, 27, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.features = torch.eye(256)\n        self.features_1 = torch.nn.Linear(256, 128, bias=False)\n    def forward(self, v1):\n        split_tensors = torch.split(v1, [1, 1, 1], dim=1)\n        concatenated_tensor = torch.cat(split_tensors, dim=1)\n        v0 = self.features_1(self.features.mm(torch.squeeze(split_tensors[1], dim=1).t()))\n        v2 = torch.cat([self.features_1(self.features_1(self.features.mm(\n            torch.squeeze(split_tensors[2], dim=1).t())))], dim=1)\n        return (concatenated_tensor, v0, v2, torch.split(v1, [1, 1, 1], dim=1))\n# Inputs to the model\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.features = torch.nn.Sequential(*(torch.nn.Conv2d(32, 32, 3, 1, 1) for _ in range(3)))\n    def forward(self, v1):\n        split_tensors = torch.split(v1, [1, 1, 1], dim=1)\n        concatenated_tensor = torch.cat(split_tensors, dim=1)\n        return (concatenated_tensor, torch.split(v1, [1, 1, 1], dim=1))\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.features1 = torch.nn.Sequential(*(torch.nn.Conv2d(32, 1, 3, 1, 1) for _ in range(5)))\n    def forward(self, v1):\n        split_tensors = v1.split(1, dim=1)\n        concatenated_tensor = torch.cat(split_tensors, dim=1)\n        return (concatenated_tensor, v1.split(1, dim=1))\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.features1 = torch.nn.Sequential(*(torch.nn.Conv2d(32, 1, 3, 1, 1) for _ in range(3)))\n    def forward(self, v2, v3):\n        split_tensors = torch.split(v2, [4, 3, 5], dim=2)\n        concatenated_tensor = torch.cat(split_tensors, dim=2)\n        return (concatenated_tensor, torch.split(v3, [1, 1, 1], dim=1))\n# Inputs to the model\nx1 = torch.randn(1, 32, 64, 32)\nx2 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.features1 = torch.nn.Sequential(*(torch.nn.Conv2d(3, 4, 3, 1, 1) for _ in range(3)))\n    def forward(self, v1):\n        split_tensors = torch.split(v1, [1, 2], dim=2)\n        concatenated_tensor = torch.cat(split_tensors, dim=2)\n        return (concatenated_tensor, torch.split(v1, [1, 2], dim=2))\n# Inputs to the model\nx1 = torch.randn(1, 3, 16, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.features1 = torch.rand(2, 3, 3, 3)\n        self.features2 = torch.nn.Conv2d(3, 64, 3, 1, 1)\n        self.features3 = torch.nn.Conv2d(64, 3, 3, 1, 1, bias=False)\n    def forward(self, x, y):\n        x1 = self.features1(x)\n        x2 = self.features2(x1)\n        x3 = self.features3(x2)\n        split_tensors1 = torch.split(x1, 2, dim=1)\n        concatenated_tensor = torch.cat(split_tensors1, dim=1)\n        return (concatenated_tensor, x3)\n# Inputs to the model\ntorch.manual_seed(0)\nx1 = torch.randn(1, 64, 1, 1)\ny1 = torch.randn(1, 3, 1, 1)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.features = torch.nn.Sequential(torch.nn.Conv2d(3, 32, 7, 1, 3), torch.nn.ReLU(), torch.nn.Conv2d(32, 3, 7, 1, 3), torch.nn.ReLU())\n    def forward(self, v1):\n        split_tensors = torch.split(v1, [1, 2], dim=1)\n        concatenated_tensor = torch.cat(split_tensors, dim=1)\n        return (concatenated_tensor, torch.split(v1, [1, 2], dim=1))\n# Inputs to the model\nx1 = torch.randn(1, 3, 32, 32)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.features1 = torch.nn.Sequential(*(torch.nn.Conv2d(32, 1, 3, 1, 1) for _ in range(8)))\n        self.features2 = torch.nn.Sequential(*(torch.nn.Conv2d(1, 1, 3, 1, 1) for _ in range(6)))\n    def forward(self, v1):\n        split_tensors = torch.split(v1, [1, 1, 1], dim=1)\n        concatenated_tensor = torch.cat(split_tensors, dim=1)\n        return (concatenated_tensor, torch.split(v1, [1, 1, 1], dim=1))\n# Inputs to the model\nx1 = torch.randn(1, 3, 64,64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.features = torch.nn.ModuleList([torch.nn.Conv2d(3, 32, 3, 1, 1, bias=False),torch.nn.ReLU(),torch.nn.Conv2d(32, 32, 1, 1, 1, bias=False),torch.nn.Conv2d(32, 32, 3, 1, 1, bias=False),torch.nn.ReLU()])\n    def forward(self, x):\n        split_tensors = torch.split(x, [1, 1], dim=1)\n        concatenated_tensor = torch.cat(split_tensors, dim=1)\n        return (concatenated_tensor, torch.split(x, [1, 1], dim=1))\n# Inputs to the model\nx1 = torch.randn(1, 3, 32, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.a = torch.nn.Parameter(torch.randn((9, 9, 3, 3)), requires_grad=True)\n    def forward(self, v1):\n        split_tensors = torch.split(v1, [3, 3, 3], dim=2)\n        concatenated_tensor = torch.cat(split_tensors, dim=2)\n        return (concatenated_tensor, torch.split(v1, [3, 3, 3], dim=2))\n# Inputs to the model\nx1 = torch.randn(1, 1, 27, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.features = torch.eye(256)\n        self.features_1 = torch.nn.Linear(256, 128, bias=False)\n    def forward(self, v1):\n        split_tensors = torch.split(v1, [1, 1, 1], dim=1)\n        concatenated_tensor = torch.cat(split_tensors, dim=1)\n        v0 = self.features_1(self.features.mm(torch.squeeze(split_tensors[1], dim=1).t()))\n        v2 = torch.cat([self.features_1(self.features_1(self.features.mm(\n            torch.squeeze(split_tensors[2], dim=1).t())))], dim=1)\n        return (concatenated_tensor, v0, v2, torch.split(v1, [1, 1, 1], dim=1))\n# Inputs to the model\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.features = torch.nn.Sequential(*(torch.nn.Conv2d(32, 32, 3, 1, 1) for _ in range(3)))\n    def forward(self, v1):\n        split_tensors = torch.split(v1, [1, 1, 1], dim=1)\n        concatenated_tensor = torch.cat(split_tensors, dim=1)\n        return (concatenated_tensor, torch.split(v1, [1, 1, 1], dim=1))\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.features1 = torch.nn.Sequential(*(torch.nn.Conv2d(32, 1, 3, 1, 1) for _ in range(5)))\n    def forward(self, v1):\n        split_tensors = v1.split(1, dim=1)\n        concatenated_tensor = torch.cat(split_tensors, dim=1)\n        return (concatenated_tensor, v1.split(1, dim=1))\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.features1 = torch.nn.Sequential(*(torch.nn.Conv2d(32, 1, 3, 1, 1) for _ in range(3)))\n    def forward(self, v2, v3):\n        split_tensors = torch.split(v2, [4, 3, 5], dim=2)\n        concatenated_tensor = torch.cat(split_tensors, dim=2)\n        return (concatenated_tensor, torch.split(v3, [1, 1, 1], dim=1))\n# Inputs to the model\nx1 = torch.randn(1, 32, 64, 32)\nx2 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.features1 = torch.nn.Sequential(*(torch.nn.Conv2d(3, 4, 3, 1, 1) for _ in range(3)))\n    def forward(self, v1):\n        split_tensors = torch.split(v1, [1, 2], dim=2)\n        concatenated_tensor = torch.cat(split_tensors, dim=2)\n        return (concatenated_tensor, torch.split(v1, [1, 2], dim=2))\n# Inputs to the model\nx1 = torch.randn(1, 3, 16, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.features1 = torch.rand(2, 3, 3, 3)\n        self.features2 = torch.nn.Conv2d(3, 64, 3, 1, 1)\n        self.features3 = torch.nn.Conv2d(64, 3, 3, 1, 1, bias=False)\n    def forward(self, x, y):\n        x1 = self.features1(x)\n        x2 = self.features2(x1)\n        x3 = self.features3(x2)\n        split_tensors1 = torch.split(x1, 2, dim=1)\n        concatenated_tensor = torch.cat(split_tensors1, dim=1)\n        return (concatenated_tensor, x3)\n# Inputs to the model\ntorch.manual_seed(0)\nx1 = torch.randn(1, 64, 1, 1)\ny1 = torch.randn(1, 3, 1, 1)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.features = torch.nn.Sequential(torch.nn.Conv2d(3, 32, 7, 1, 3), torch.nn.ReLU(), torch.nn.Conv2d(32, 3, 7, 1, 3), torch.nn.ReLU())\n    def forward(self, v1):\n        split_tensors = torch.split(v1, [1, 2], dim=1)\n        concatenated_tensor = torch.cat(split_tensors, dim=1)\n        return (concatenated_tensor, torch.split(v1, [1, 2], dim=1))\n# Inputs to the model\nx1 = torch.randn(1, 3, 32, 32)\n"
            ],
            "g_time": 9.417703866958618
        }
    }
}
{
    "opt": "binary_unary=6",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(16, 32)\n        self.other = torch.tensor([2.0, 4.0, 6.0, 8.0, 10.0, 12.0, 14.0, 16.0, 18.0, 20.0, 2.0, 4.0, 6.0, 8.0, 10.0, 12.0, 14.0, 16.0, 18.0, 20.0]).reshape([2, 10])\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 - self.other\n        v3 = torch.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(5, 10)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 - (-10)\n        v3 = torch.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 5)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, other):\n        v1 = torch.nn.functional.linear(x1, torch.randn(x1.shape[1], 1))\n        v2 = v1 - other\n        v3 = F.relu(v2)\n        return v3\n\n# Initializing the model\na1 = torch.randn(5, 3)\nm = Model()\n\n# Inputs to the model\nother = torch.randn(5, 1)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(4, 8)\n\n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 - 1.5\n        v3 = torch.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 4)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(10, 2)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 - other\n        v3 = relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nX1 = torch.randn(4, 2, requires_grad = True)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(256, 256)\n \n    def forward(self, x1):\n        x1 = F.adaptive_avg_pool2d(x1, output_size=(1, 1))\n        x1 = torch.flatten(x1, 1)\n        v1 = self.linear(x1)\n        v2 = v1 - 0.6\n        v3 = F.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 256, 256, 256)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(8, 4)\n        self.other = torch.tensor([1, -2, 3, -4])\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 - self.other\n        v3 = torch.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n \n    def forward(self, x1):\n        t1 = self.linear(x1)\n        t2 = t1 - 0.2\n        t3 = torch.relu(t2)\n        v1 = t3 + 1 # add one\n        return v1\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(10, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n \n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 - 0.5\n        v3 = torch.nn.functional.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 - 1.0\n        v3 = torch.nn.functional.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(16, 32)\n        self.other = torch.tensor([2.0, 4.0, 6.0, 8.0, 10.0, 12.0, 14.0, 16.0, 18.0, 20.0, 2.0, 4.0, 6.0, 8.0, 10.0, 12.0, 14.0, 16.0, 18.0, 20.0]).reshape([2, 10])\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 - self.other\n        v3 = torch.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(5, 10)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 - (-10)\n        v3 = torch.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 5)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, other):\n        v1 = torch.nn.functional.linear(x1, torch.randn(x1.shape[1], 1))\n        v2 = v1 - other\n        v3 = F.relu(v2)\n        return v3\n\n# Initializing the model\na1 = torch.randn(5, 3)\nm = Model()\n\n# Inputs to the model\nother = torch.randn(5, 1)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(4, 8)\n\n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 - 1.5\n        v3 = torch.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 4)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(10, 2)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 - other\n        v3 = relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nX1 = torch.randn(4, 2, requires_grad = True)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(256, 256)\n \n    def forward(self, x1):\n        x1 = F.adaptive_avg_pool2d(x1, output_size=(1, 1))\n        x1 = torch.flatten(x1, 1)\n        v1 = self.linear(x1)\n        v2 = v1 - 0.6\n        v3 = F.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 256, 256, 256)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(8, 4)\n        self.other = torch.tensor([1, -2, 3, -4])\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 - self.other\n        v3 = torch.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n \n    def forward(self, x1):\n        t1 = self.linear(x1)\n        t2 = t1 - 0.2\n        t3 = torch.relu(t2)\n        v1 = t3 + 1 # add one\n        return v1\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(10, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n \n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 - 0.5\n        v3 = torch.nn.functional.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 - 1.0\n        v3 = torch.nn.functional.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n"
            ],
            "g_time": 8.37969422340393
        }
    }
}
{
    "opt": "sfdp=0",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.key = torch.nn.Parameter(torch.randn(50, 11, 47, 20))\n    def forward(self, x1):\n        q = x1\n        k = x1\n        v = x1\n        inv_scale = math.sqrt(k.size(1))\n        scaled_dot_product = torch.matmul(q, k.transpose(-2, -1)) / inv_scale\n        attention_weights = scaled_dot_product.softmax(dim=-1)\n        output = attention_weights.matmul(v)\n        return output\n# Inputs to the model\nx1 = torch.randn(64, 90, 3, 29)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.key = torch.nn.Parameter(torch.randn(7, 70, 10, 10))\n    def forward(self, x1):\n        q = x1\n        k = x1\n        v = x1\n        inv_scale = math.sqrt(k.size(1))\n        scaled_dot_product = torch.matmul(q, k.transpose(-2, -1)) / inv_scale\n        attention_weights = scaled_dot_product.softmax(dim=-1)\n        output = attention_weights.matmul(v)\n        return output\n# Inputs to the model\nx1 = torch.randn(63, 17, 26, 52)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.key = torch.nn.Parameter(torch.randn(8, 70, 68, 96))\n    def forward(self, x1):\n        q = x1\n        k = x1\n        v = x1\n        inv_scale = math.sqrt(k.size(1))\n        scaled_dot_product = torch.matmul(q, k.transpose(-2, -1)) / inv_scale\n        attention_weights = scaled_dot_product.softmax(dim=-1)\n        output = attention_weights.matmul(v)\n        return output\n# Inputs to the model\nx1 = torch.randn(2, 13, 96, 29)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.key = torch.nn.Parameter(torch.randn(73, 76, 61, 69))\n    def forward(self, x1):\n        q = x1\n        k = x1\n        v = x1\n        inv_scale = math.sqrt(k.size(1))\n        scaled_dot_product = torch.matmul(q, k.transpose(-2, -1)) / inv_scale\n        attention_weights = scaled_dot_product.softmax(dim=-1)\n        output = attention_weights.matmul(v)\n        return output\n# Inputs to the model\nx1 = torch.randn(73, 92, 71, 11)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.key = torch.nn.Parameter(torch.randn(41, 43, 42, 77))\n    def forward(self, x1):\n        q = x1\n        k = x1\n        v = x1\n        inv_scale = math.sqrt(k.size(1))\n        scaled_dot_product = torch.matmul(q, k.transpose(-2, -1)) / inv_scale\n        attention_weights = scaled_dot_product.softmax(dim=-1)\n        output = attention_weights.matmul(v)\n        return output\n# Inputs to the model\nx1 = torch.randn(5, 15, 8, 15)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.key = torch.nn.Parameter(torch.randn(51, 4, 99, 31))\n    def forward(self, x1):\n        q = x1\n        k = x1\n        v = x1\n        inv_scale = math.sqrt(k.size(1))\n        scaled_dot_product = torch.matmul(q, k.transpose(-2, -1)) / inv_scale\n        attention_weights = scaled_dot_product.softmax(dim=-1)\n        output = attention_weights.matmul(v)\n        return output\n# Inputs to the model\nx1 = torch.randn(13, 59, 99, 66)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.key = torch.nn.Parameter(torch.randn(50, 12, 69, 90))\n    def forward(self, x1):\n        q = x1\n        k = x1\n        v = x1\n        inv_scale = math.sqrt(k.size(1))\n        scaled_dot_product = torch.matmul(q, k.transpose(-2, -1)) / inv_scale\n        attention_weights = scaled_dot_product.softmax(dim=-1)\n        output = attention_weights.matmul(v)\n        return output\n# Inputs to the model\nx1 = torch.randn(19, 49, 98, 67)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.key = torch.nn.Parameter(torch.randn(26, 37, 97, 70))\n    def forward(self, x1):\n        q = x1\n        k = x1\n        v = x1\n        inv_scale = math.sqrt(k.size(1))\n        scaled_dot_product = torch.matmul(q, k.transpose(-2, -1)) / inv_scale\n        attention_weights = scaled_dot_product.softmax(dim=-1)\n        output = attention_weights.matmul(v)\n        return output\n# Inputs to the model\nx1 = torch.randn(28, 64, 82, 63)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.key = torch.nn.Parameter(torch.randn(20, 76, 58, 64))\n    def forward(self, x1):\n        q = x1\n        k = x1\n        v = x1\n        inv_scale = math.sqrt(k.size(1))\n        scaled_dot_product = torch.matmul(q, k.transpose(-2, -1)) / inv_scale\n        attention_weights = scaled_dot_product.softmax(dim=-1)\n        output = attention_weights.matmul(v)\n        return output\n# Inputs to the model\nx1 = torch.randn(27, 1, 50, 80)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.key = torch.nn.Parameter(torch.randn(18, 86, 84, 23))\n    def forward(self, x1):\n        q = x1\n        k = self.key\n        v = x1\n        inv_scale = math.sqrt(k.size(1))\n        scaled_dot_product = torch.matmul(q, k.transpose(-2, -1)) / inv_scale\n        attention_weights = scaled_dot_product.softmax(dim=-1)\n        output = attention_weights.matmul(v)\n        return output\n# Inputs to the model\nx1 = torch.randn(8, 66, 19, 47)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.key = torch.nn.Parameter(torch.randn(50, 11, 47, 20))\n    def forward(self, x1):\n        q = x1\n        k = x1\n        v = x1\n        inv_scale = math.sqrt(k.size(1))\n        scaled_dot_product = torch.matmul(q, k.transpose(-2, -1)) / inv_scale\n        attention_weights = scaled_dot_product.softmax(dim=-1)\n        output = attention_weights.matmul(v)\n        return output\n# Inputs to the model\nx1 = torch.randn(64, 90, 3, 29)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.key = torch.nn.Parameter(torch.randn(7, 70, 10, 10))\n    def forward(self, x1):\n        q = x1\n        k = x1\n        v = x1\n        inv_scale = math.sqrt(k.size(1))\n        scaled_dot_product = torch.matmul(q, k.transpose(-2, -1)) / inv_scale\n        attention_weights = scaled_dot_product.softmax(dim=-1)\n        output = attention_weights.matmul(v)\n        return output\n# Inputs to the model\nx1 = torch.randn(63, 17, 26, 52)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.key = torch.nn.Parameter(torch.randn(8, 70, 68, 96))\n    def forward(self, x1):\n        q = x1\n        k = x1\n        v = x1\n        inv_scale = math.sqrt(k.size(1))\n        scaled_dot_product = torch.matmul(q, k.transpose(-2, -1)) / inv_scale\n        attention_weights = scaled_dot_product.softmax(dim=-1)\n        output = attention_weights.matmul(v)\n        return output\n# Inputs to the model\nx1 = torch.randn(2, 13, 96, 29)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.key = torch.nn.Parameter(torch.randn(73, 76, 61, 69))\n    def forward(self, x1):\n        q = x1\n        k = x1\n        v = x1\n        inv_scale = math.sqrt(k.size(1))\n        scaled_dot_product = torch.matmul(q, k.transpose(-2, -1)) / inv_scale\n        attention_weights = scaled_dot_product.softmax(dim=-1)\n        output = attention_weights.matmul(v)\n        return output\n# Inputs to the model\nx1 = torch.randn(73, 92, 71, 11)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.key = torch.nn.Parameter(torch.randn(41, 43, 42, 77))\n    def forward(self, x1):\n        q = x1\n        k = x1\n        v = x1\n        inv_scale = math.sqrt(k.size(1))\n        scaled_dot_product = torch.matmul(q, k.transpose(-2, -1)) / inv_scale\n        attention_weights = scaled_dot_product.softmax(dim=-1)\n        output = attention_weights.matmul(v)\n        return output\n# Inputs to the model\nx1 = torch.randn(5, 15, 8, 15)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.key = torch.nn.Parameter(torch.randn(51, 4, 99, 31))\n    def forward(self, x1):\n        q = x1\n        k = x1\n        v = x1\n        inv_scale = math.sqrt(k.size(1))\n        scaled_dot_product = torch.matmul(q, k.transpose(-2, -1)) / inv_scale\n        attention_weights = scaled_dot_product.softmax(dim=-1)\n        output = attention_weights.matmul(v)\n        return output\n# Inputs to the model\nx1 = torch.randn(13, 59, 99, 66)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.key = torch.nn.Parameter(torch.randn(50, 12, 69, 90))\n    def forward(self, x1):\n        q = x1\n        k = x1\n        v = x1\n        inv_scale = math.sqrt(k.size(1))\n        scaled_dot_product = torch.matmul(q, k.transpose(-2, -1)) / inv_scale\n        attention_weights = scaled_dot_product.softmax(dim=-1)\n        output = attention_weights.matmul(v)\n        return output\n# Inputs to the model\nx1 = torch.randn(19, 49, 98, 67)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.key = torch.nn.Parameter(torch.randn(26, 37, 97, 70))\n    def forward(self, x1):\n        q = x1\n        k = x1\n        v = x1\n        inv_scale = math.sqrt(k.size(1))\n        scaled_dot_product = torch.matmul(q, k.transpose(-2, -1)) / inv_scale\n        attention_weights = scaled_dot_product.softmax(dim=-1)\n        output = attention_weights.matmul(v)\n        return output\n# Inputs to the model\nx1 = torch.randn(28, 64, 82, 63)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.key = torch.nn.Parameter(torch.randn(20, 76, 58, 64))\n    def forward(self, x1):\n        q = x1\n        k = x1\n        v = x1\n        inv_scale = math.sqrt(k.size(1))\n        scaled_dot_product = torch.matmul(q, k.transpose(-2, -1)) / inv_scale\n        attention_weights = scaled_dot_product.softmax(dim=-1)\n        output = attention_weights.matmul(v)\n        return output\n# Inputs to the model\nx1 = torch.randn(27, 1, 50, 80)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.key = torch.nn.Parameter(torch.randn(18, 86, 84, 23))\n    def forward(self, x1):\n        q = x1\n        k = self.key\n        v = x1\n        inv_scale = math.sqrt(k.size(1))\n        scaled_dot_product = torch.matmul(q, k.transpose(-2, -1)) / inv_scale\n        attention_weights = scaled_dot_product.softmax(dim=-1)\n        output = attention_weights.matmul(v)\n        return output\n# Inputs to the model\nx1 = torch.randn(8, 66, 19, 47)\n"
            ],
            "g_time": 6.69045090675354
        }
    }
}
{
    "opt": "pointless_cumsum_replacement",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1):\n        b = {}\n        a = {}\n        b['dtype'] = torch.int16\n        b['layout'] = torch.strided\n        b['device'] = torch.device('cuda:0')\n        a['dtype'] = torch.int32\n        a['layout'] = torch.strided\n        a['device'] = torch.device('cuda:0')\n        a['dtype_to'] = torch.int64\n        a['dtype_from'] = torch.int32\n        b['dtype_to'] = torch.int64\n        b['dtype_from'] = torch.int32\n        t1 = torch.full([512, 32], 1, dtype=b['dtype'], layout=b['layout'], device=b['device'], pin_memory=False)\n        t2 = t1.to(dtype=a['dtype'])\n        t3 = torch.cumsum(t2, 1)\n        return t3\n# Inputs to the model\nx1 = torch.randn(512, 32, dtype=torch.int16, device='cuda:0')\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1):\n        b = {}\n        a = {}\n        b['dtype'] = torch.float32\n        b['layout'] = torch.strided\n        b['device'] = torch.device('cuda:0')\n        a['dtype'] = torch.bool\n        a['layout'] = torch.strided\n        a['device'] = torch.device('cuda:0')\n        a['dtype_to'] = torch.float64\n        a['dtype_from'] = torch.bool\n        b['dtype_to'] = torch.float64\n        b['dtype_from'] = torch.bool\n        t1 = torch.full([127, 256], 1, dtype=b['dtype'], layout=b['layout'], device=b['device'], pin_memory=False)\n        t2 = t1.to(dtype=a['dtype'])\n        t3 = torch.cumsum(t2, 1)\n        return t3\n# Inputs to the model\nx1 = torch.randn(127, 256, dtype=torch.float32, device='cuda:0')\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1):\n        b = {}\n        a = {}\n        b['dtype'] = torch.float32\n        b['layout'] = torch.strided\n        b['device'] = torch.device('cuda:0')\n        a['dtype'] = torch.int32\n        a['layout'] = torch.strided\n        a['device'] = torch.device('cuda:0')\n        a['dtype_to'] = torch.float64\n        a['dtype_from'] = torch.int32\n        b['dtype_to'] = torch.int32\n        b['dtype_from'] = torch.float32\n        t1 = torch.full([32, 64], 1, dtype=b['dtype'], layout=b['layout'], device=b['device'], pin_memory=False)\n        t2 = t1.to(dtype=a['dtype'])\n        t3 = torch.cumsum(t2, 1)\n        return t3\n# Inputs to the model\nx1 = torch.randn(32, 64, dtype=torch.float32, device='cuda:0')\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1):\n        b = {}\n        a = {}\n        b['dtype'] = torch.int8\n        b['layout'] = torch.sparse\n        b['device'] = torch.device('cpu')\n        a['dtype'] = torch.float16\n        a['layout'] = torch.strided\n        a['device'] = torch.device('cpu')\n        a['dtype_to'] = torch.float32\n        a['dtype_from'] = torch.float16\n        b['dtype_to'] = torch.int8\n        b['dtype_from'] = torch.float16\n        t1 = torch.full([256, 2, 512, 512], 1, dtype=b['dtype'], layout=b['layout'], device=b['device'], pin_memory=False)\n        t2 = t1.to(dtype=a['dtype'])\n        t3 = torch.cumsum(t2, 1)\n        return t3\n# Inputs to the model\nx1 = torch.randn(256, 2, 512, 512, device='cpu')\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1):\n        b = {}\n        a = {}\n        b['dtype'] = torch.uint8\n        b['layout'] = torch.strided\n        b['device'] = torch.device('cpu')\n        a['dtype'] = torch.int32\n        a['layout'] = torch.strided\n        a['device'] = torch.device('cpu')\n        a['dtype_to'] = torch.float32\n        a['dtype_from'] = torch.uint8\n        b['dtype_to'] = torch.bool\n        b['dtype_from'] = torch.float32\n        t1 = torch.full([128, 40], 1, dtype=b['dtype'], layout=b['layout'], device=b['device'], pin_memory=False)\n        t2 = t1.to(dtype=a['dtype'])\n        t3 = torch.cumsum(t2, 1)\n        return t3\n# Inputs to the model\nx1 = torch.randint(0, 10, (128, 40), dtype=torch.uint8, device='cuda:0')\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1):\n        b = {}\n        a = {}\n        b['dtype'] = torch.bool\n        b['layout'] = torch.strided\n        a['dtype'] = torch.int16\n        a['layout'] = torch.strided\n        a['device'] = torch.device('cpu')\n        a['dtype_to'] = torch.float16\n        a['dtype_from'] = torch.int64\n        b['dtype_to'] = torch.bool\n        b['dtype_from'] = torch.float16\n        b['device'] = torch.device('cpu')\n        t1 = torch.full([256, 64], 1, dtype=b['dtype'], layout=b['layout'], device=b['device'], pin_memory=False)\n        t2 = t1.to(dtype=a['dtype'])\n        t3 = torch.cumsum(t2, 1)\n        return t3\n# Inputs to the model\nx1 = torch.randn(256, 64, dtype=torch.int16, device='cpu')\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1):\n        b = {}\n        a = {}\n        b['dtype'] = torch.float64\n        b['layout'] = torch.strided\n        b['device'] = torch.device('cuda:1')\n        a['dtype'] = torch.int32\n        a['layout'] = torch.strided\n        a['device'] = torch.device('cuda:0')\n        a['dtype_to'] = torch.float64\n        a['dtype_from'] = torch.int32\n        b['dtype_to'] = torch.float64\n        b['dtype_from'] = torch.float32\n        t1 = torch.full([64, 128], 1, dtype=b['dtype'], layout=b['layout'], device=b['device'], pin_memory=False)\n        t2 = t1.to(dtype=a['dtype'])\n        t3 = torch.cumsum(t2, 1)\n        return t3\n# Inputs to the model\nx1 = torch.randn(64, 128, dtype=torch.float64, device='cuda:1')\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1):\n        b = {}\n        a = {}\n        b['dtype'] = torch.float32\n        b['layout'] = torch.strided\n        b['device'] = torch.device('cuda:0')\n        a['dtype'] = torch.float16\n        a['layout'] = torch.strided\n        a['device'] = torch.device('cuda:0')\n        a['dtype_to'] = torch.float32\n        a['dtype_from'] = torch.float16\n        b['dtype_to'] = torch.float32\n        b['dtype_from'] = torch.float16\n        t1 = torch.full([8, 1024], 1, dtype=b['dtype'], layout=b['layout'], device=b['device'], pin_memory=False)\n        t2 = t1.to(dtype=a['dtype'])\n        t3 = torch.cumsum(t2, 1)\n        return t3\n# Inputs to the model\nx1 = torch.randn(8, 1024, dtype=torch.float32, device='cuda:0')\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1):\n        b = {}\n        a = {}\n        b['dtype'] = torch.float32\n        b['layout'] = torch.strided\n        b['device'] = torch.device('cuda:0')\n        a['dtype'] = torch.float16\n        a['layout'] = torch.strided\n        a['device'] = torch.device('cuda:0')\n        a['dtype_to'] = torch.float32\n        a['dtype_from'] = torch.float16\n        b['dtype_to'] = torch.float32\n        b['dtype_from'] = torch.float16\n        t1 = torch.full([64, 128], 1, dtype=b['dtype'], layout=b['layout'], device=b['device'], pin_memory=False)\n        t2 = t1.to(dtype=a['dtype'])\n        t3 = torch.cumsum(t2, 1)\n        return t3\n# Inputs to the model\nx1 = torch.randn(64, 128, dtype=torch.float32, device='cuda:0')\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1):\n        b = {}\n        a = {}\n        b['dtype'] = torch.float16\n        b['layout'] = torch.strided\n        b['device'] = torch.device('cuda:0')\n        a['dtype'] = torch.float32\n        a['layout'] = torch.strided\n        a['device'] = torch.device('cuda:0')\n        a['dtype_to'] = torch.float64\n        a['dtype_from'] = torch.float32\n        b['dtype_to'] = torch.float16\n        b['dtype_from'] = torch.float64\n        t1 = torch.full([16, 32], 1, dtype=b['dtype'], layout=b['layout'], device=b['device'], pin_memory=False)\n        t2 = t1.to(dtype=a['dtype'])\n        t3 = torch.cumsum(t2, 1)\n        return t3\n# Inputs to the model\nx1 = torch.randn(16, 32, dtype=torch.float16, device='cuda:0')\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1):\n        b = {}\n        a = {}\n        b['dtype'] = torch.int16\n        b['layout'] = torch.strided\n        b['device'] = torch.device('cuda:0')\n        a['dtype'] = torch.int32\n        a['layout'] = torch.strided\n        a['device'] = torch.device('cuda:0')\n        a['dtype_to'] = torch.int64\n        a['dtype_from'] = torch.int32\n        b['dtype_to'] = torch.int64\n        b['dtype_from'] = torch.int32\n        t1 = torch.full([512, 32], 1, dtype=b['dtype'], layout=b['layout'], device=b['device'], pin_memory=False)\n        t2 = t1.to(dtype=a['dtype'])\n        t3 = torch.cumsum(t2, 1)\n        return t3\n# Inputs to the model\nx1 = torch.randn(512, 32, dtype=torch.int16, device='cuda:0')\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1):\n        b = {}\n        a = {}\n        b['dtype'] = torch.float32\n        b['layout'] = torch.strided\n        b['device'] = torch.device('cuda:0')\n        a['dtype'] = torch.bool\n        a['layout'] = torch.strided\n        a['device'] = torch.device('cuda:0')\n        a['dtype_to'] = torch.float64\n        a['dtype_from'] = torch.bool\n        b['dtype_to'] = torch.float64\n        b['dtype_from'] = torch.bool\n        t1 = torch.full([127, 256], 1, dtype=b['dtype'], layout=b['layout'], device=b['device'], pin_memory=False)\n        t2 = t1.to(dtype=a['dtype'])\n        t3 = torch.cumsum(t2, 1)\n        return t3\n# Inputs to the model\nx1 = torch.randn(127, 256, dtype=torch.float32, device='cuda:0')\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1):\n        b = {}\n        a = {}\n        b['dtype'] = torch.float32\n        b['layout'] = torch.strided\n        b['device'] = torch.device('cuda:0')\n        a['dtype'] = torch.int32\n        a['layout'] = torch.strided\n        a['device'] = torch.device('cuda:0')\n        a['dtype_to'] = torch.float64\n        a['dtype_from'] = torch.int32\n        b['dtype_to'] = torch.int32\n        b['dtype_from'] = torch.float32\n        t1 = torch.full([32, 64], 1, dtype=b['dtype'], layout=b['layout'], device=b['device'], pin_memory=False)\n        t2 = t1.to(dtype=a['dtype'])\n        t3 = torch.cumsum(t2, 1)\n        return t3\n# Inputs to the model\nx1 = torch.randn(32, 64, dtype=torch.float32, device='cuda:0')\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1):\n        b = {}\n        a = {}\n        b['dtype'] = torch.int8\n        b['layout'] = torch.sparse\n        b['device'] = torch.device('cpu')\n        a['dtype'] = torch.float16\n        a['layout'] = torch.strided\n        a['device'] = torch.device('cpu')\n        a['dtype_to'] = torch.float32\n        a['dtype_from'] = torch.float16\n        b['dtype_to'] = torch.int8\n        b['dtype_from'] = torch.float16\n        t1 = torch.full([256, 2, 512, 512], 1, dtype=b['dtype'], layout=b['layout'], device=b['device'], pin_memory=False)\n        t2 = t1.to(dtype=a['dtype'])\n        t3 = torch.cumsum(t2, 1)\n        return t3\n# Inputs to the model\nx1 = torch.randn(256, 2, 512, 512, device='cpu')\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1):\n        b = {}\n        a = {}\n        b['dtype'] = torch.uint8\n        b['layout'] = torch.strided\n        b['device'] = torch.device('cpu')\n        a['dtype'] = torch.int32\n        a['layout'] = torch.strided\n        a['device'] = torch.device('cpu')\n        a['dtype_to'] = torch.float32\n        a['dtype_from'] = torch.uint8\n        b['dtype_to'] = torch.bool\n        b['dtype_from'] = torch.float32\n        t1 = torch.full([128, 40], 1, dtype=b['dtype'], layout=b['layout'], device=b['device'], pin_memory=False)\n        t2 = t1.to(dtype=a['dtype'])\n        t3 = torch.cumsum(t2, 1)\n        return t3\n# Inputs to the model\nx1 = torch.randint(0, 10, (128, 40), dtype=torch.uint8, device='cuda:0')\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1):\n        b = {}\n        a = {}\n        b['dtype'] = torch.bool\n        b['layout'] = torch.strided\n        a['dtype'] = torch.int16\n        a['layout'] = torch.strided\n        a['device'] = torch.device('cpu')\n        a['dtype_to'] = torch.float16\n        a['dtype_from'] = torch.int64\n        b['dtype_to'] = torch.bool\n        b['dtype_from'] = torch.float16\n        b['device'] = torch.device('cpu')\n        t1 = torch.full([256, 64], 1, dtype=b['dtype'], layout=b['layout'], device=b['device'], pin_memory=False)\n        t2 = t1.to(dtype=a['dtype'])\n        t3 = torch.cumsum(t2, 1)\n        return t3\n# Inputs to the model\nx1 = torch.randn(256, 64, dtype=torch.int16, device='cpu')\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1):\n        b = {}\n        a = {}\n        b['dtype'] = torch.float64\n        b['layout'] = torch.strided\n        b['device'] = torch.device('cuda:1')\n        a['dtype'] = torch.int32\n        a['layout'] = torch.strided\n        a['device'] = torch.device('cuda:0')\n        a['dtype_to'] = torch.float64\n        a['dtype_from'] = torch.int32\n        b['dtype_to'] = torch.float64\n        b['dtype_from'] = torch.float32\n        t1 = torch.full([64, 128], 1, dtype=b['dtype'], layout=b['layout'], device=b['device'], pin_memory=False)\n        t2 = t1.to(dtype=a['dtype'])\n        t3 = torch.cumsum(t2, 1)\n        return t3\n# Inputs to the model\nx1 = torch.randn(64, 128, dtype=torch.float64, device='cuda:1')\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1):\n        b = {}\n        a = {}\n        b['dtype'] = torch.float32\n        b['layout'] = torch.strided\n        b['device'] = torch.device('cuda:0')\n        a['dtype'] = torch.float16\n        a['layout'] = torch.strided\n        a['device'] = torch.device('cuda:0')\n        a['dtype_to'] = torch.float32\n        a['dtype_from'] = torch.float16\n        b['dtype_to'] = torch.float32\n        b['dtype_from'] = torch.float16\n        t1 = torch.full([8, 1024], 1, dtype=b['dtype'], layout=b['layout'], device=b['device'], pin_memory=False)\n        t2 = t1.to(dtype=a['dtype'])\n        t3 = torch.cumsum(t2, 1)\n        return t3\n# Inputs to the model\nx1 = torch.randn(8, 1024, dtype=torch.float32, device='cuda:0')\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1):\n        b = {}\n        a = {}\n        b['dtype'] = torch.float32\n        b['layout'] = torch.strided\n        b['device'] = torch.device('cuda:0')\n        a['dtype'] = torch.float16\n        a['layout'] = torch.strided\n        a['device'] = torch.device('cuda:0')\n        a['dtype_to'] = torch.float32\n        a['dtype_from'] = torch.float16\n        b['dtype_to'] = torch.float32\n        b['dtype_from'] = torch.float16\n        t1 = torch.full([64, 128], 1, dtype=b['dtype'], layout=b['layout'], device=b['device'], pin_memory=False)\n        t2 = t1.to(dtype=a['dtype'])\n        t3 = torch.cumsum(t2, 1)\n        return t3\n# Inputs to the model\nx1 = torch.randn(64, 128, dtype=torch.float32, device='cuda:0')\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1):\n        b = {}\n        a = {}\n        b['dtype'] = torch.float16\n        b['layout'] = torch.strided\n        b['device'] = torch.device('cuda:0')\n        a['dtype'] = torch.float32\n        a['layout'] = torch.strided\n        a['device'] = torch.device('cuda:0')\n        a['dtype_to'] = torch.float64\n        a['dtype_from'] = torch.float32\n        b['dtype_to'] = torch.float16\n        b['dtype_from'] = torch.float64\n        t1 = torch.full([16, 32], 1, dtype=b['dtype'], layout=b['layout'], device=b['device'], pin_memory=False)\n        t2 = t1.to(dtype=a['dtype'])\n        t3 = torch.cumsum(t2, 1)\n        return t3\n# Inputs to the model\nx1 = torch.randn(16, 32, dtype=torch.float16, device='cuda:0')\n"
            ],
            "g_time": 10.720001220703125
        }
    }
}
{
    "opt": "unary=22",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(32, 8, bias=False)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.tanh(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.tanh(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(5, 10)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.tanh(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 5)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 128)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.tanh(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 1)\n \n    def forward(self, x2):\n        v7 = self.linear(x2)\n        v8 = torch.tanh(v7)\n        return v8\nm = Model()\n\n# Inputs to the model\nx2 = torch.randn(2, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(5, 7)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.tanh(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(3, 5)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(8, 16, bias=False)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.tanh(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(64, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc1 = torch.nn.Linear(3072, 4096)\n \n    def forward(self, x1):\n        x2 = self.fc1(x1)\n        x3 = torch.tanh(x2)\n        return x3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3072)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(20, 16)\n\n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.tanh(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 20)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.tanh(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(32, 8, bias=False)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.tanh(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.tanh(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(5, 10)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.tanh(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 5)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 128)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.tanh(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 1)\n \n    def forward(self, x2):\n        v7 = self.linear(x2)\n        v8 = torch.tanh(v7)\n        return v8\nm = Model()\n\n# Inputs to the model\nx2 = torch.randn(2, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(5, 7)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.tanh(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(3, 5)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(8, 16, bias=False)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.tanh(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(64, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc1 = torch.nn.Linear(3072, 4096)\n \n    def forward(self, x1):\n        x2 = self.fc1(x1)\n        x3 = torch.tanh(x2)\n        return x3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3072)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(20, 16)\n\n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.tanh(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 20)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.tanh(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n"
            ],
            "g_time": 4.292922019958496
        }
    }
}
{
    "opt": "binary=0",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(16, 64, 7, stride=1, padding=3, dilation=1)\n    def forward(self, x1, other=1):\n        v1 = self.conv(x1)\n        v2 = v1 + other\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 16, 80, 80)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(62, 16, 1, stride=1, padding=1, dilation=1)\n    def forward(self, x1, other=1, padding0=1, padding1=1, padding2=1, padding3=None):\n        v1 = self.conv(x1)\n        v2 = v1 + other\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 62, 144, 57)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(1, 1, 1, stride=1, dilation=1, groups=16, padding=1)\n    def forward(self, x1, other=1):\n        v1 = self.conv(x1)\n        v2 = v1 + other\n        return v2\n# Inputs to the model\nx1 = torch.randn(16, 1, 107, 107)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(2, 32, 1, stride=1, padding=1, dilation=1)\n    def forward(self, x1, padding0=None, padding1=None):\n        v1 = self.conv(x1)\n        if padding0 == None:\n            padding0 = torch.randn(v1.shape)\n        return v1\n# Inputs to the model\nx1 = torch.randn(1, 2, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(32, 32, 7, stride=15, dilation=1)\n    def forward(self, x, padding0=None, padding1=None, padding2=1, padding3=None, t2_shape=1):\n        v1 = self.conv(x)\n        if padding0 == None:\n            padding0 = torch.randn(v1.shape)\n        v2 = v1 + padding0\n        return v2\n# Inputs to the model\nx = torch.randn(1, 32, 304, 304)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(10, 6, 3, stride=1, dilation=1)\n    def forward(self, features, padding0=1, padding1=0):\n        convolution = torch.nn.functional.conv2d(features, self.conv.weight, self.conv.bias,  self.conv.stride, self.conv.padding, self.conv.dilation, self.conv.groups)\n        return convolution\n# Inputs to the model\nfeatures = torch.randn(1, 10, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(6, 8, 1, stride=2, padding=1)\n    def forward(self, x1, other=1):\n        v1 = self.conv(x1)\n        v2 = v1 + other\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 6, 10, 10)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(64, 1, 2, stride=2, dilation=1)\n    def forward(self, x1, other=1, padding1=None, padding2=None):\n        v1 = self.conv(x1)\n        if padding2 == None:\n            padding2 = torch.randn(v1.shape)\n        v2 = v1 + other\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 64, 32, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(20, 20, 2, stride=2, dilation=2)\n    def forward(self, x1, other, padding0, padding1=None):\n        if padding1 == None:\n            padding1 = torch.randn(x1.shape)\n        v1 = self.conv(x1)\n        v2 = v1 + other\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 20, 80, 80)\nother = 1\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(8, 15, 3, stride=1, dilation=2)\n    def forward(self, x1, other=1, padding0=None, padding1=None, padding2=1, padding3=None, padding4=None):\n        v1 = self.conv(x1)\n        if padding3 == None:\n            padding3 = torch.randn(v1.shape)\n        if padding2 == None:\n            padding2 = torch.randn(v1.shape)\n        v2 = v1 + other\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 8, 80, 80)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(16, 64, 7, stride=1, padding=3, dilation=1)\n    def forward(self, x1, other=1):\n        v1 = self.conv(x1)\n        v2 = v1 + other\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 16, 80, 80)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(62, 16, 1, stride=1, padding=1, dilation=1)\n    def forward(self, x1, other=1, padding0=1, padding1=1, padding2=1, padding3=None):\n        v1 = self.conv(x1)\n        v2 = v1 + other\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 62, 144, 57)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(1, 1, 1, stride=1, dilation=1, groups=16, padding=1)\n    def forward(self, x1, other=1):\n        v1 = self.conv(x1)\n        v2 = v1 + other\n        return v2\n# Inputs to the model\nx1 = torch.randn(16, 1, 107, 107)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(2, 32, 1, stride=1, padding=1, dilation=1)\n    def forward(self, x1, padding0=None, padding1=None):\n        v1 = self.conv(x1)\n        if padding0 == None:\n            padding0 = torch.randn(v1.shape)\n        return v1\n# Inputs to the model\nx1 = torch.randn(1, 2, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(32, 32, 7, stride=15, dilation=1)\n    def forward(self, x, padding0=None, padding1=None, padding2=1, padding3=None, t2_shape=1):\n        v1 = self.conv(x)\n        if padding0 == None:\n            padding0 = torch.randn(v1.shape)\n        v2 = v1 + padding0\n        return v2\n# Inputs to the model\nx = torch.randn(1, 32, 304, 304)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(10, 6, 3, stride=1, dilation=1)\n    def forward(self, features, padding0=1, padding1=0):\n        convolution = torch.nn.functional.conv2d(features, self.conv.weight, self.conv.bias,  self.conv.stride, self.conv.padding, self.conv.dilation, self.conv.groups)\n        return convolution\n# Inputs to the model\nfeatures = torch.randn(1, 10, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(6, 8, 1, stride=2, padding=1)\n    def forward(self, x1, other=1):\n        v1 = self.conv(x1)\n        v2 = v1 + other\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 6, 10, 10)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(64, 1, 2, stride=2, dilation=1)\n    def forward(self, x1, other=1, padding1=None, padding2=None):\n        v1 = self.conv(x1)\n        if padding2 == None:\n            padding2 = torch.randn(v1.shape)\n        v2 = v1 + other\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 64, 32, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(20, 20, 2, stride=2, dilation=2)\n    def forward(self, x1, other, padding0, padding1=None):\n        if padding1 == None:\n            padding1 = torch.randn(x1.shape)\n        v1 = self.conv(x1)\n        v2 = v1 + other\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 20, 80, 80)\nother = 1\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(8, 15, 3, stride=1, dilation=2)\n    def forward(self, x1, other=1, padding0=None, padding1=None, padding2=1, padding3=None, padding4=None):\n        v1 = self.conv(x1)\n        if padding3 == None:\n            padding3 = torch.randn(v1.shape)\n        if padding2 == None:\n            padding2 = torch.randn(v1.shape)\n        v2 = v1 + other\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 8, 80, 80)\n"
            ],
            "g_time": 6.381519079208374
        }
    }
}
{
    "opt": "unary=4",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 64)\n \n    def forward(self, x2):\n        t1 = self.linear(x2)\n        t2 = t1 * 0.5\n        t3 = t1 * 0.7071067811865476\n        t4 = torch.erf(t3)\n        t5 = t4 + 1\n        t6 = t2 * t5\n        return t6\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx2 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(i, o)\n    \n    def forward(self, x):\n        v1 = self.linear(x)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx = torch.randn(B, i)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(32, 64, bias=True)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(8, 16, bias=False)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(8, 2)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(16, 32)\n \n    def forward(self, x2):\n        v1 = self.linear(x2)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx2 = torch.randn(1, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(128, 64)\n \n    def forward(self, x2):\n        v1 = self.linear(x2)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx2 = torch.randn(1, 128)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(2, 3)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 2)\n",
                "\nclass Model(torch.nn.Module):\n  def __init__(self):\n    super().__init__()\n    self.linear = torch.nn.Linear(20, 20)\n\n  def forward(self, x1):\n    v1 = self.linear(x1)\n    v2 = v1 * 0.5\n    v3 = v1 * 0.7071067811865476\n    v4 = torch.erf(v3)\n    v5 = v4 + 1\n    v6 = v2 * v5\n    return v6\n\n# Initialize the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(20)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 64)\n \n    def forward(self, x2):\n        t1 = self.linear(x2)\n        t2 = t1 * 0.5\n        t3 = t1 * 0.7071067811865476\n        t4 = torch.erf(t3)\n        t5 = t4 + 1\n        t6 = t2 * t5\n        return t6\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx2 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(i, o)\n    \n    def forward(self, x):\n        v1 = self.linear(x)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx = torch.randn(B, i)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(32, 64, bias=True)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(8, 16, bias=False)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(8, 2)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(16, 32)\n \n    def forward(self, x2):\n        v1 = self.linear(x2)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx2 = torch.randn(1, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(128, 64)\n \n    def forward(self, x2):\n        v1 = self.linear(x2)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx2 = torch.randn(1, 128)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(2, 3)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 2)\n",
                "\nclass Model(torch.nn.Module):\n  def __init__(self):\n    super().__init__()\n    self.linear = torch.nn.Linear(20, 20)\n\n  def forward(self, x1):\n    v1 = self.linear(x1)\n    v2 = v1 * 0.5\n    v3 = v1 * 0.7071067811865476\n    v4 = torch.erf(v3)\n    v5 = v4 + 1\n    v6 = v2 * v5\n    return v6\n\n# Initialize the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(20)\n"
            ],
            "g_time": 6.615143060684204
        }
    }
}
{
    "opt": "unary=2",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(5, 53, (25,), (2,), (0,), 1)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * v1 * v1\n        v4 = v3 * 0.044715\n        v5 = v1 + v4\n        v6 = v5 * 0.7978845608028654\n        v7 = torch.tanh(v6)\n        v8 = v7 + 1\n        v9 = v2 * v8\n        return v9\n# Inputs to the model\nx1 = torch.randn(5, 5, 5, 5)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(34, 67, (121, 93), (10, 27), 6)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * v1 * v1\n        v4 = v3 * 0.044715\n        v5 = v1 + v4\n        v6 = v5 * 0.7978845608028654\n        v7 = torch.tanh(v6)\n        v8 = v7 + 1\n        v9 = v2 * v8\n        return v9\n# Inputs to the model\nx1 = torch.randn(1, 34, 14, 22)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(5, 5, (20, 5), stride=1, group=1, dilation=1, padding=(19, 4))\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * v1 * v1\n        v4 = v3 * 0.044715\n        v5 = v1 + v4\n        v6 = v5 * 0.7978845608028654\n        v7 = torch.tanh(v6)\n        v8 = v7 + 1\n        v9 = v2 * v8\n        return v9\n# Inputs to the model\nx1 = torch.randn(1, 5, 13, 23)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(4, 17, 7, 1, 0)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * v1 * v1\n        v4 = v3 * 0.044715\n        v5 = v1 + v4\n        v6 = v5 * 0.7978845608028654\n        v7 = torch.tanh(v6)\n        v8 = v7 + 1\n        v9 = v2 * v8\n        return v9\n# Inputs to the model\nx1 = torch.randn(1, 4, 44, 29)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose3d(17, 18, (4, -1, 5), (3, 17, 6), (1, 4, 2))\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * v1 * v1\n        v4 = v3 * 0.044715\n        v5 = v1 + v4\n        v6 = v5 * 0.7978845608028654\n        v7 = torch.tanh(v6)\n        v8 = v7 + 1\n        v9 = v2 * v8\n        return v9\n# Inputs to the model\nx1 = torch.randn(1, 17, 76, 343)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(7, 54, 9, stride=7, padding=1, output_padding=9)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * v1 * v1\n        v4 = v3 * 0.044715\n        v5 = v1 + v4\n        v6 = v5 * 0.7978845608028654\n        v7 = torch.tanh(v6)\n        v8 = v7 + 1\n        v9 = v2 * v8\n        return v9\n# Inputs to the model\nx1 = torch.randn(1, 7, 5, 20)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(1, 1, (7, 6), stride=(9, 4), padding=(6, 5))\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * v1 * v1\n        v4 = v3 * 0.044715\n        v5 = v1 + v4\n        v6 = v5 * 0.7978845608028654\n        v7 = torch.tanh(v6)\n        v8 = v7 + 1\n        v9 = v2 * v8\n        return v9\n# Inputs to the model\nx1 = torch.randn(1, 1, 14, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(8, 82, (62, 98), 18, 6, 2)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * v1 * v1\n        v4 = v3 * 0.044715\n        v5 = v1 + v4\n        v6 = v5 * 0.7978845608028654\n        v7 = torch.tanh(v6)\n        v8 = v7 + 1\n        v9 = v2 * v8\n        return v9\n# Inputs to the model\nx1 = torch.randn(17, 8, 89, 86)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(2, 1, 25, 1, 21)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * v1 * v1\n        v4 = v3 * 0.044715\n        v5 = v1 + v4\n        v6 = v5 * 0.7978845608028654\n        v7 = torch.tanh(v6)\n        v8 = v7 + 1\n        v9 = v2 * v8\n        return v9\n# Inputs to the model\nx1 = torch.randn(1, 2, 12, 42)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(5, 1, (17, 19), 1, 1)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * v1 * v1\n        v4 = v3 * 0.044715\n        v5 = v1 + v4\n        v6 = v5 * 0.7978845608028654\n        v7 = torch.tanh(v6)\n        v8 = v7 + 1\n        v9 = v2 * v8\n        return v9\n# Inputs to the model\nx1 = torch.randn(1, 5, 63, 99)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(5, 53, (25,), (2,), (0,), 1)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * v1 * v1\n        v4 = v3 * 0.044715\n        v5 = v1 + v4\n        v6 = v5 * 0.7978845608028654\n        v7 = torch.tanh(v6)\n        v8 = v7 + 1\n        v9 = v2 * v8\n        return v9\n# Inputs to the model\nx1 = torch.randn(5, 5, 5, 5)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(34, 67, (121, 93), (10, 27), 6)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * v1 * v1\n        v4 = v3 * 0.044715\n        v5 = v1 + v4\n        v6 = v5 * 0.7978845608028654\n        v7 = torch.tanh(v6)\n        v8 = v7 + 1\n        v9 = v2 * v8\n        return v9\n# Inputs to the model\nx1 = torch.randn(1, 34, 14, 22)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(5, 5, (20, 5), stride=1, group=1, dilation=1, padding=(19, 4))\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * v1 * v1\n        v4 = v3 * 0.044715\n        v5 = v1 + v4\n        v6 = v5 * 0.7978845608028654\n        v7 = torch.tanh(v6)\n        v8 = v7 + 1\n        v9 = v2 * v8\n        return v9\n# Inputs to the model\nx1 = torch.randn(1, 5, 13, 23)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(4, 17, 7, 1, 0)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * v1 * v1\n        v4 = v3 * 0.044715\n        v5 = v1 + v4\n        v6 = v5 * 0.7978845608028654\n        v7 = torch.tanh(v6)\n        v8 = v7 + 1\n        v9 = v2 * v8\n        return v9\n# Inputs to the model\nx1 = torch.randn(1, 4, 44, 29)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose3d(17, 18, (4, -1, 5), (3, 17, 6), (1, 4, 2))\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * v1 * v1\n        v4 = v3 * 0.044715\n        v5 = v1 + v4\n        v6 = v5 * 0.7978845608028654\n        v7 = torch.tanh(v6)\n        v8 = v7 + 1\n        v9 = v2 * v8\n        return v9\n# Inputs to the model\nx1 = torch.randn(1, 17, 76, 343)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(7, 54, 9, stride=7, padding=1, output_padding=9)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * v1 * v1\n        v4 = v3 * 0.044715\n        v5 = v1 + v4\n        v6 = v5 * 0.7978845608028654\n        v7 = torch.tanh(v6)\n        v8 = v7 + 1\n        v9 = v2 * v8\n        return v9\n# Inputs to the model\nx1 = torch.randn(1, 7, 5, 20)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(1, 1, (7, 6), stride=(9, 4), padding=(6, 5))\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * v1 * v1\n        v4 = v3 * 0.044715\n        v5 = v1 + v4\n        v6 = v5 * 0.7978845608028654\n        v7 = torch.tanh(v6)\n        v8 = v7 + 1\n        v9 = v2 * v8\n        return v9\n# Inputs to the model\nx1 = torch.randn(1, 1, 14, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(8, 82, (62, 98), 18, 6, 2)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * v1 * v1\n        v4 = v3 * 0.044715\n        v5 = v1 + v4\n        v6 = v5 * 0.7978845608028654\n        v7 = torch.tanh(v6)\n        v8 = v7 + 1\n        v9 = v2 * v8\n        return v9\n# Inputs to the model\nx1 = torch.randn(17, 8, 89, 86)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(2, 1, 25, 1, 21)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * v1 * v1\n        v4 = v3 * 0.044715\n        v5 = v1 + v4\n        v6 = v5 * 0.7978845608028654\n        v7 = torch.tanh(v6)\n        v8 = v7 + 1\n        v9 = v2 * v8\n        return v9\n# Inputs to the model\nx1 = torch.randn(1, 2, 12, 42)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(5, 1, (17, 19), 1, 1)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * v1 * v1\n        v4 = v3 * 0.044715\n        v5 = v1 + v4\n        v6 = v5 * 0.7978845608028654\n        v7 = torch.tanh(v6)\n        v8 = v7 + 1\n        v9 = v2 * v8\n        return v9\n# Inputs to the model\nx1 = torch.randn(1, 5, 63, 99)\n"
            ],
            "g_time": 9.35193657875061
        }
    }
}
{
    "opt": "sfdp=2",
    "response": {
        "0": {
            "raw": [
                "\nclass MultiHeadAttention(torch.nn.Module):\n    def __init__(self, emb, heads=8, dropout=0):\n        super().__init__()\n\n        self.emb = emb\n        self.heads = heads\n        self.dropout = dropout\n\n        self.head_dim = emb\n        if emb % heads!= 0:\n          raise ValueError(f\"Embedding dimension {emb} should be divisible by number of heads {heads}\")\n        self.depth = self.head_dim // self.heads\n\n        self.query_w = nn.Linear(emb, emb, bias=False)\n        self.key_w = nn.Linear(emb, emb, bias=False)\n        self.value_w = nn.Linear(emb, emb, bias=False)\n        self.layer_norm = nn.LayerNorm(emb)\n    \n    def forward(self, query, key, value, attn_mask=None):\n        q = self.query_w(query)\n        k = self.key_w(key)\n        v = self.value_w(value)\n\n        bs = q.size(0)\n\n        qk = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.depth)\n        qk.masked_fill_(attn_mask.unsqueeze(1) == 1, -1e4)\n\n        attn = torch.softmax(qk, dim=-1)\n        attn = torch.nn.functional.dropout(attn, p=self.dropout, training=self.training)\n\n        output = torch.matmul(attn, v)\n        output = output.permute(0, 2, 1, 3).contiguous()\n        output = output.view(bs, -1, self.emb)\n\n        result = self.layer_norm(output + query)\n\n        return result\n\n# Initializing the model\nm = MultiHeadAttention(128)\n\n# Input to the model\nkey = torch.randn(16, 128, 56, 56)\nvalue = torch.randn(16, 128, 56, 56)\nattn_mask = torch.ones(16, 128, 1, 56).cumsum(-1)!= 0\nquery = torch.randn(16, 128, 56, 56)\nresult = m(query, key, value, attn_mask)\n\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n \n    def forward(self, query, key, value):\n        v1 = torch.matmul(query, key.transpose(-2, -1))\n        v2 = v1.div(1.0)\n        v3 = v2.softmax(dim=-1)\n        v4 = torch.nn.functional.dropout(v3, p=0.)\n        v5 = v4.matmul(value)\n        return v5\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 8, 7, 7)\nx2 = torch.randn(1, 8, 7, 7)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.dropout_qk = torch.nn.Dropout(0.3)\n \n    def forward(self, x1, x2, x3, x4):\n        v1 = torch.matmul(x1, x2)\n        v2 = v1.div(0.001)\n        v3 = v2.softmax(dim=-1)\n        v4 = self.dropout_qk(v3)\n        output = torch.mul(v4, x3)\n        return v5, v6, v7\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 2, 3)\nx2 = torch.randn(1, 2, 3)\nx3 = torch.randn(1, 2, 4)\nx4 = torch.randn(1, 2, 4)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, num_heads: int = 2):\n        super().__init__()\n        self.multihead_attn = nn.MultiheadAttention(embed_dim, num_heads)\n \n    def forward(self, x1):\n        # x1 is the query\n        # x2 should be padding\n        x2 = torch.zeros_like(x1)\n        x3 = torch.zeros_like(x1)\n        output, attn = self.multihead_attn(query=x1, key=x1, value=x1, key_padding_mask=x2, need_weights=x3)\n        return output\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model and their shape (please change the shape according to your model's inputs)\nx1 = torch.randn(1, 20, 512)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear1 = torch.nn.Linear(3, 4, bias=False)\n        self.linear2 = torch.nn.Linear(4, 5, bias=False)\n\n    def forward(self, q1_p, k1_p):\n        q1 = self.linear1(q1_p)\n        k1 = self.linear2(k1_p)\n        qk = torch.matmul(q1, k1.transpose(-2, -1))\n        inv_scale_factor = float(2 / 3) * torch.rsqrt(toq(torch.einsum('b h n, b h n -> b h', q1, k1).sum(-1)))\n        scaled_qk = qk.div(inv_scale_factor)\n        softmax_qk = scaled_qk.softmax(dim=-1)\n        return softmax_qk\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nq1_p = torch.randn(3, 4)\nk1_p = torch.randn(5, 4)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n \n    def forward(self, q1, k2, v3):\n        qk = torch.matmul(q1, k2)\n        inv_scale_factor = (self.head_dim ** -0.5)\n        scaled_qk = qk * inv_scale_factor\n        softmax_qk = scaled_qk.softmax(dim=-1)\n        dropdout_qk = torch.nn.functional.dropout(softmax_qk, p=self.dropout_p)\n        output = dropdout_qk * v3\n        return output\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nq1 = torch.randn(1, 4, 256)\nk2 = torch.randn(1, 4, 256)\nv3 = torch.randn(1, 4, 64, 256)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, x1, x2, x3, x4):\n        qk = torch.matmul(x1, x2.transpose(-2, -1))\n        scaled_qk = qk.div(1)\n        softmax_qk = scaled_qk.softmax(dim=-1)\n        dropout_qk = torch.nn.functional.dropout(softmax_qk, p=0.0)\n        output = dropout_qk.matmul(x3)\n        return output\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(20, 30, 10)\nx2 = torch.randn(20, 30, 10)\nx3 = torch.randn(20, 10, 40)\nx4 = torch.randn(30, 40, 50)\n",
                "\nnum_heads = 10\nattention_dropout_probability = 0.1\n\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.qkv_project = torch.nn.Conv2d(32, num_heads, 2, stride=2)\n        self.out_project = torch.nn.Conv2d(32, 32, 2, stride=2)\n \n    def forward(self, x1):\n        qkv = self.qkv_project(x1)\n        q, k, v = torch.chunk(qkv, 3, dim=1) # Split the output of the convolution into 3 parts of a fixed size\n        scaled_qk = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(q.shape[-1]) # Compute the dot product between the query and the key and scale it\n        softmax_qk = scaled_qk.softmax(dim=-1) # Apply softmax to the scaled dot product\n        dropout_qk = torch.nn.functional.dropout(softmax_qk, p=self.dropout_p) # Apply dropout to the softmax output\n        output = dropout_qk.matmul(v) # Compute the dot product of the dropout output and the value\n        return self.out_project(output)\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 32, 64, 64)\n",
                "\nclass Model(nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.key = nn.Linear(8, 8)\n        self.value = nn.Linear(8, 8)\n        self.softmax = nn.Softmax(dim=-1)\n\n    def forward(self, query):\n        key = self.key(query)\n        value = self.value(query)\n        qk = torch.matmul(query, key.transpose(-2, -1))\n        scale = torch.sqrt(torch.tensor(key.shape[-2]).float())\n        inv_scale_factor = 1.0 / scale\n        scaled_qk = qk.mul(inv_scale_factor)\n        softmax_qk = self.softmax(scaled_qk)\n        dropout_qk = nn.functional.dropout(softmax_qk, p=0.5)\n        output = torch.matmul(dropout_qk, value)\n        return output\n\n# Inputs to the model\nquery = torch.randn(1, 8, 64, 96)\n",
                "\nclass Model(torch.nn.Module):\n    def forward(self, query, key, value, inv_scale_factor, dropout_p):\n        qk = torch.matmul(query, key.transpose(-2, -1))\n        scaled_qk = qk.div(inv_scale_factor)\n        softmax_qk = scaled_qk.softmax(dim=-1)\n        dropout_qk = torch.nn.functional.dropout(softmax_qk, p=dropout_p)\n        output = dropout_qk.matmul(value)\n        return output\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nquery = torch.randn(1, 128, 10, 10)\nkey = torch.randn(1, 128, 20, 20)\nvalue = torch.randn(1, 128, 20, 20)\ninv_scale_factor = torch.randn(1)\ndropout_p = torch.randn(1)\n"
            ],
            "code": [
                "\nclass MultiHeadAttention(torch.nn.Module):\n    def __init__(self, emb, heads=8, dropout=0):\n        super().__init__()\n\n        self.emb = emb\n        self.heads = heads\n        self.dropout = dropout\n\n        self.head_dim = emb\n        if emb % heads!= 0:\n          raise ValueError(f\"Embedding dimension {emb} should be divisible by number of heads {heads}\")\n        self.depth = self.head_dim // self.heads\n\n        self.query_w = nn.Linear(emb, emb, bias=False)\n        self.key_w = nn.Linear(emb, emb, bias=False)\n        self.value_w = nn.Linear(emb, emb, bias=False)\n        self.layer_norm = nn.LayerNorm(emb)\n    \n    def forward(self, query, key, value, attn_mask=None):\n        q = self.query_w(query)\n        k = self.key_w(key)\n        v = self.value_w(value)\n\n        bs = q.size(0)\n\n        qk = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.depth)\n        qk.masked_fill_(attn_mask.unsqueeze(1) == 1, -1e4)\n\n        attn = torch.softmax(qk, dim=-1)\n        attn = torch.nn.functional.dropout(attn, p=self.dropout, training=self.training)\n\n        output = torch.matmul(attn, v)\n        output = output.permute(0, 2, 1, 3).contiguous()\n        output = output.view(bs, -1, self.emb)\n\n        result = self.layer_norm(output + query)\n\n        return result\n\n# Initializing the model\nm = MultiHeadAttention(128)\n\n# Input to the model\nkey = torch.randn(16, 128, 56, 56)\nvalue = torch.randn(16, 128, 56, 56)\nattn_mask = torch.ones(16, 128, 1, 56).cumsum(-1)!= 0\nquery = torch.randn(16, 128, 56, 56)\nresult = m(query, key, value, attn_mask)\n\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n \n    def forward(self, query, key, value):\n        v1 = torch.matmul(query, key.transpose(-2, -1))\n        v2 = v1.div(1.0)\n        v3 = v2.softmax(dim=-1)\n        v4 = torch.nn.functional.dropout(v3, p=0.)\n        v5 = v4.matmul(value)\n        return v5\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 8, 7, 7)\nx2 = torch.randn(1, 8, 7, 7)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.dropout_qk = torch.nn.Dropout(0.3)\n \n    def forward(self, x1, x2, x3, x4):\n        v1 = torch.matmul(x1, x2)\n        v2 = v1.div(0.001)\n        v3 = v2.softmax(dim=-1)\n        v4 = self.dropout_qk(v3)\n        output = torch.mul(v4, x3)\n        return v5, v6, v7\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 2, 3)\nx2 = torch.randn(1, 2, 3)\nx3 = torch.randn(1, 2, 4)\nx4 = torch.randn(1, 2, 4)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, num_heads: int = 2):\n        super().__init__()\n        self.multihead_attn = nn.MultiheadAttention(embed_dim, num_heads)\n \n    def forward(self, x1):\n        # x1 is the query\n        # x2 should be padding\n        x2 = torch.zeros_like(x1)\n        x3 = torch.zeros_like(x1)\n        output, attn = self.multihead_attn(query=x1, key=x1, value=x1, key_padding_mask=x2, need_weights=x3)\n        return output\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model and their shape (please change the shape according to your model's inputs)\nx1 = torch.randn(1, 20, 512)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear1 = torch.nn.Linear(3, 4, bias=False)\n        self.linear2 = torch.nn.Linear(4, 5, bias=False)\n\n    def forward(self, q1_p, k1_p):\n        q1 = self.linear1(q1_p)\n        k1 = self.linear2(k1_p)\n        qk = torch.matmul(q1, k1.transpose(-2, -1))\n        inv_scale_factor = float(2 / 3) * torch.rsqrt(toq(torch.einsum('b h n, b h n -> b h', q1, k1).sum(-1)))\n        scaled_qk = qk.div(inv_scale_factor)\n        softmax_qk = scaled_qk.softmax(dim=-1)\n        return softmax_qk\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nq1_p = torch.randn(3, 4)\nk1_p = torch.randn(5, 4)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n \n    def forward(self, q1, k2, v3):\n        qk = torch.matmul(q1, k2)\n        inv_scale_factor = (self.head_dim ** -0.5)\n        scaled_qk = qk * inv_scale_factor\n        softmax_qk = scaled_qk.softmax(dim=-1)\n        dropdout_qk = torch.nn.functional.dropout(softmax_qk, p=self.dropout_p)\n        output = dropdout_qk * v3\n        return output\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nq1 = torch.randn(1, 4, 256)\nk2 = torch.randn(1, 4, 256)\nv3 = torch.randn(1, 4, 64, 256)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, x1, x2, x3, x4):\n        qk = torch.matmul(x1, x2.transpose(-2, -1))\n        scaled_qk = qk.div(1)\n        softmax_qk = scaled_qk.softmax(dim=-1)\n        dropout_qk = torch.nn.functional.dropout(softmax_qk, p=0.0)\n        output = dropout_qk.matmul(x3)\n        return output\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(20, 30, 10)\nx2 = torch.randn(20, 30, 10)\nx3 = torch.randn(20, 10, 40)\nx4 = torch.randn(30, 40, 50)\n",
                "\nnum_heads = 10\nattention_dropout_probability = 0.1\n\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.qkv_project = torch.nn.Conv2d(32, num_heads, 2, stride=2)\n        self.out_project = torch.nn.Conv2d(32, 32, 2, stride=2)\n \n    def forward(self, x1):\n        qkv = self.qkv_project(x1)\n        q, k, v = torch.chunk(qkv, 3, dim=1) # Split the output of the convolution into 3 parts of a fixed size\n        scaled_qk = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(q.shape[-1]) # Compute the dot product between the query and the key and scale it\n        softmax_qk = scaled_qk.softmax(dim=-1) # Apply softmax to the scaled dot product\n        dropout_qk = torch.nn.functional.dropout(softmax_qk, p=self.dropout_p) # Apply dropout to the softmax output\n        output = dropout_qk.matmul(v) # Compute the dot product of the dropout output and the value\n        return self.out_project(output)\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 32, 64, 64)\n",
                "\nclass Model(nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.key = nn.Linear(8, 8)\n        self.value = nn.Linear(8, 8)\n        self.softmax = nn.Softmax(dim=-1)\n\n    def forward(self, query):\n        key = self.key(query)\n        value = self.value(query)\n        qk = torch.matmul(query, key.transpose(-2, -1))\n        scale = torch.sqrt(torch.tensor(key.shape[-2]).float())\n        inv_scale_factor = 1.0 / scale\n        scaled_qk = qk.mul(inv_scale_factor)\n        softmax_qk = self.softmax(scaled_qk)\n        dropout_qk = nn.functional.dropout(softmax_qk, p=0.5)\n        output = torch.matmul(dropout_qk, value)\n        return output\n\n# Inputs to the model\nquery = torch.randn(1, 8, 64, 96)\n",
                "\nclass Model(torch.nn.Module):\n    def forward(self, query, key, value, inv_scale_factor, dropout_p):\n        qk = torch.matmul(query, key.transpose(-2, -1))\n        scaled_qk = qk.div(inv_scale_factor)\n        softmax_qk = scaled_qk.softmax(dim=-1)\n        dropout_qk = torch.nn.functional.dropout(softmax_qk, p=dropout_p)\n        output = dropout_qk.matmul(value)\n        return output\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nquery = torch.randn(1, 128, 10, 10)\nkey = torch.randn(1, 128, 20, 20)\nvalue = torch.randn(1, 128, 20, 20)\ninv_scale_factor = torch.randn(1)\ndropout_p = torch.randn(1)\n"
            ],
            "g_time": 16.929981470108032
        }
    }
}
{
    "opt": "binary_unary=2",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 6, 5, stride=2, padding=2)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 - 0.5\n        v3 = F.relu(v2)\n        v4 = F.max_pool2d(v3, 2)\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 8, 5, stride=1, padding=2)\n        self.conv2 = torch.nn.Conv2d(8, 16, 3, stride=2, padding=1)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = v1 - 0.4\n        v3 = F.relu(v2)\n        v4 = self.conv2(v3)\n        v5 = v4 - 0.8\n        v6 = F.relu(v5)\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 3, 32, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 56, 3, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 - 0.5\n        v3 = F.relu(v2)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 3, 224, 224)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 12, 7, stride=2, padding=3)\n        self.conv2 = torch.nn.Conv2d(12, 4, 3, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 - 5\n        v3 = F.relu(v2)\n        v4 = self.conv2(v3)\n        v5 = v4 - 5\n        v6 = F.relu(v5)\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 96, 3, stride=2, padding=1)\n        self.conv2 = torch.nn.Conv2d(96, 96, 1, stride=1, padding=0)\n        self.conv3 = torch.nn.Conv2d(96, 32, 1, stride=1, padding=0)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = self.conv2(v1)\n        v3 = self.conv3(v2)\n        v4 = v3 - 128\n        v5 = F.relu(v1 - v4)\n        return v1\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv1d(2, 3, 2, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 + 0.5\n        v3 = F.relu(v2)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 10, 5, stride=1, padding=2)\n        self.conv2 = torch.nn.Conv2d(10, 4, 3, stride=2, padding=1)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = v1 - 0.5\n        v3 = F.relu(v2)\n        v4 = self.conv2(v3)\n        v5 = v4 - 20\n        v6 = F.relu(v5)\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 64, 33, stride=2, padding=17)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = v1 - 0.71\n        v3 = F.relu(v2)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 3, 32, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 8, 5, stride=1, padding=2)\n        self.bn1 = torch.nn.BatchNorm2d(8)\n        self.bn2 = torch.nn.BatchNorm2d(8)\n        self.conv2 = torch.nn.Conv2d(8, 8, 5, stride=2, padding=2)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = self.bn1(v1)\n        v3 = v2 - 0.1\n        v4 = F.relu(v3)\n        v5 = self.bn2(v4)\n        v6 = v5 - 0.1\n        v7 = F.relu(v6)\n        v8 = self.conv2(v7)\n        v9 = self.bn(v8)\n        v10 = v9 - 0.1\n        v11 = F.relu(v10)\n        return v11\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 6, 3, stride=1, padding=1)\n        self.conv2 = torch.nn.Conv2d(6, 6, 3, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = v1 - 5\n        v3 = F.relu(v2)\n        v4 = self.conv2(v3)\n        v5 = v4 + 5\n        v6 = F.relu(v5)\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 3, 224, 224)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 6, 5, stride=2, padding=2)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 - 0.5\n        v3 = F.relu(v2)\n        v4 = F.max_pool2d(v3, 2)\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 8, 5, stride=1, padding=2)\n        self.conv2 = torch.nn.Conv2d(8, 16, 3, stride=2, padding=1)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = v1 - 0.4\n        v3 = F.relu(v2)\n        v4 = self.conv2(v3)\n        v5 = v4 - 0.8\n        v6 = F.relu(v5)\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 3, 32, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 56, 3, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 - 0.5\n        v3 = F.relu(v2)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 3, 224, 224)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 12, 7, stride=2, padding=3)\n        self.conv2 = torch.nn.Conv2d(12, 4, 3, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 - 5\n        v3 = F.relu(v2)\n        v4 = self.conv2(v3)\n        v5 = v4 - 5\n        v6 = F.relu(v5)\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 96, 3, stride=2, padding=1)\n        self.conv2 = torch.nn.Conv2d(96, 96, 1, stride=1, padding=0)\n        self.conv3 = torch.nn.Conv2d(96, 32, 1, stride=1, padding=0)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = self.conv2(v1)\n        v3 = self.conv3(v2)\n        v4 = v3 - 128\n        v5 = F.relu(v1 - v4)\n        return v1\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv1d(2, 3, 2, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 + 0.5\n        v3 = F.relu(v2)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 10, 5, stride=1, padding=2)\n        self.conv2 = torch.nn.Conv2d(10, 4, 3, stride=2, padding=1)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = v1 - 0.5\n        v3 = F.relu(v2)\n        v4 = self.conv2(v3)\n        v5 = v4 - 20\n        v6 = F.relu(v5)\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 64, 33, stride=2, padding=17)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = v1 - 0.71\n        v3 = F.relu(v2)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 3, 32, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 8, 5, stride=1, padding=2)\n        self.bn1 = torch.nn.BatchNorm2d(8)\n        self.bn2 = torch.nn.BatchNorm2d(8)\n        self.conv2 = torch.nn.Conv2d(8, 8, 5, stride=2, padding=2)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = self.bn1(v1)\n        v3 = v2 - 0.1\n        v4 = F.relu(v3)\n        v5 = self.bn2(v4)\n        v6 = v5 - 0.1\n        v7 = F.relu(v6)\n        v8 = self.conv2(v7)\n        v9 = self.bn(v8)\n        v10 = v9 - 0.1\n        v11 = F.relu(v10)\n        return v11\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 6, 3, stride=1, padding=1)\n        self.conv2 = torch.nn.Conv2d(6, 6, 3, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = v1 - 5\n        v3 = F.relu(v2)\n        v4 = self.conv2(v3)\n        v5 = v4 + 5\n        v6 = F.relu(v5)\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 3, 224, 224)\n"
            ],
            "g_time": 9.540024280548096
        }
    }
}
{
    "opt": "unary=17",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv0 = torch.nn.ConvTranspose2d(in_channels=3, out_channels=32, kernel_size=(3, 32), stride=(2, 1), padding=(1, 0))\n        self.conv1 = torch.nn.ConvTranspose2d(in_channels=32, out_channels=32, kernel_size=(1, 64), stride=(1, 1), padding=(1, 0))\n    def forward(self, x1):\n        v1 = self.conv0(x1)\n        v2 = torch.relu(v1)\n        v3 = self.conv1(v2)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.block0 = torch.nn.Sequential(torch.nn.ConvTranspose2d(3, 3, 3, padding=0, stride=1, output_padding=1), torch.nn.ReLU(inplace=False))\n    def forward(self, x1):\n        y = self.block0(x1)\n        return y\n# Inputs to the model\nx1 = torch.randn(1, 3, 96, 96)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.ConvTranspose2d(3, 32, 1, stride=3, padding=1)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        return v1\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.block0 = torch.nn.ConvTranspose2d(3, 12, 3, bias=False, padding=1, stride=2)\n        self.block1 = torch.nn.ReLU(inplace=False)\n        self.block2 = torch.nn.Sigmoid()\n    def forward(self, x1):\n        v1 = self.block0(x1)\n        v2 = self.block1(v1)\n        v3 = self.block2(v2)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 3, 128, 128)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.block0 = torch.nn.Sequential(torch.nn.ConvTranspose2d(3, 1, 1, bias=True, padding=0, stride=1), torch.nn.ReLU(inplace=False), torch.nn.Sigmoid())\n    def forward(self, x1):\n        y = self.block0(x1)\n        return y\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.ConvTranspose2d(3, 1, 1, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v3 = torch.cat([v1, x1], dim=1)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.block0 = torch.nn.Sequential(torch.nn.ConvTranspose2d(3, 1, 3, bias=True, padding=0, stride=2), torch.nn.ReLU(inplace=False), torch.nn.Sigmoid())\n    def forward(self, x1):\n        y = self.block0(x1)\n        return y\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.block0 = torch.nn.Sequential(torch.nn.ReflectionPad2d(1), torch.nn.Conv2d(1, 1, (6, 9), stride=(2, 2), bias=False))\n    def forward(self, x1):\n        y = self.block0(x1)\n        return y\n# Inputs to the model\nx1 = torch.randn(1, 1, 19, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.block0 = torch.nn.Sequential(torch.nn.ConvTranspose2d(3, 12, 1, bias=True, padding=0, stride=2), torch.nn.ReLU(inplace=False), torch.nn.Sigmoid())\n    def forward(self, x1):\n        y = self.block0(x1)\n        return y\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.block0 = torch.nn.Sequential(torch.nn.ConvTranspose2d(256, 128, 1, bias=False, padding=0), torch.nn.ReLU(inplace=False), torch.nn.Sigmoid())\n        self.block3 = torch.nn.Sequential(torch.nn.ConvTranspose2d(128, 32, 1, bias=False, padding=0), torch.nn.Tanh())\n        self.block1 = torch.nn.Sequential(torch.nn.ConvTranspose2d(32, 1, 3, padding=1, stride=2), torch.nn.ReLU(inplace=False))\n    def forward(self, x1):\n        y = self.block0(x1)\n        y0 = self.block3(y)\n        y1 = self.block1(y0)\n        return y1\n# Inputs to the model\nx1 = torch.randn(1, 256, 8, 8)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv0 = torch.nn.ConvTranspose2d(in_channels=3, out_channels=32, kernel_size=(3, 32), stride=(2, 1), padding=(1, 0))\n        self.conv1 = torch.nn.ConvTranspose2d(in_channels=32, out_channels=32, kernel_size=(1, 64), stride=(1, 1), padding=(1, 0))\n    def forward(self, x1):\n        v1 = self.conv0(x1)\n        v2 = torch.relu(v1)\n        v3 = self.conv1(v2)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.block0 = torch.nn.Sequential(torch.nn.ConvTranspose2d(3, 3, 3, padding=0, stride=1, output_padding=1), torch.nn.ReLU(inplace=False))\n    def forward(self, x1):\n        y = self.block0(x1)\n        return y\n# Inputs to the model\nx1 = torch.randn(1, 3, 96, 96)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.ConvTranspose2d(3, 32, 1, stride=3, padding=1)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        return v1\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.block0 = torch.nn.ConvTranspose2d(3, 12, 3, bias=False, padding=1, stride=2)\n        self.block1 = torch.nn.ReLU(inplace=False)\n        self.block2 = torch.nn.Sigmoid()\n    def forward(self, x1):\n        v1 = self.block0(x1)\n        v2 = self.block1(v1)\n        v3 = self.block2(v2)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 3, 128, 128)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.block0 = torch.nn.Sequential(torch.nn.ConvTranspose2d(3, 1, 1, bias=True, padding=0, stride=1), torch.nn.ReLU(inplace=False), torch.nn.Sigmoid())\n    def forward(self, x1):\n        y = self.block0(x1)\n        return y\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.ConvTranspose2d(3, 1, 1, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v3 = torch.cat([v1, x1], dim=1)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.block0 = torch.nn.Sequential(torch.nn.ConvTranspose2d(3, 1, 3, bias=True, padding=0, stride=2), torch.nn.ReLU(inplace=False), torch.nn.Sigmoid())\n    def forward(self, x1):\n        y = self.block0(x1)\n        return y\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.block0 = torch.nn.Sequential(torch.nn.ReflectionPad2d(1), torch.nn.Conv2d(1, 1, (6, 9), stride=(2, 2), bias=False))\n    def forward(self, x1):\n        y = self.block0(x1)\n        return y\n# Inputs to the model\nx1 = torch.randn(1, 1, 19, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.block0 = torch.nn.Sequential(torch.nn.ConvTranspose2d(3, 12, 1, bias=True, padding=0, stride=2), torch.nn.ReLU(inplace=False), torch.nn.Sigmoid())\n    def forward(self, x1):\n        y = self.block0(x1)\n        return y\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.block0 = torch.nn.Sequential(torch.nn.ConvTranspose2d(256, 128, 1, bias=False, padding=0), torch.nn.ReLU(inplace=False), torch.nn.Sigmoid())\n        self.block3 = torch.nn.Sequential(torch.nn.ConvTranspose2d(128, 32, 1, bias=False, padding=0), torch.nn.Tanh())\n        self.block1 = torch.nn.Sequential(torch.nn.ConvTranspose2d(32, 1, 3, padding=1, stride=2), torch.nn.ReLU(inplace=False))\n    def forward(self, x1):\n        y = self.block0(x1)\n        y0 = self.block3(y)\n        y1 = self.block1(y0)\n        return y1\n# Inputs to the model\nx1 = torch.randn(1, 256, 8, 8)\n"
            ],
            "g_time": 7.807458400726318
        }
    }
}
{
    "opt": "unary=15",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 96, 9, stride=2, padding=0)\n        self.conv2 = torch.nn.Conv2d(96, 64, 9, stride=2, padding=0)\n        self.conv3 = torch.nn.Conv2d(64, 64, 6, stride=2, padding=0)\n        self.conv4 = torch.nn.Conv2d(64, 64, 6, stride=2, padding=0)\n        self.conv5 = torch.nn.Conv2d(64, 64, 6, stride=2, padding=0)\n    def forward(self, x1):\n        v1 = torch.relu(self.conv1(x1))\n        v2 = torch.relu(self.conv2(v1))\n        v3 = torch.relu(self.conv3(v2))\n        v4 = torch.relu(self.conv4(v3))\n        v5 = torch.relu(self.conv5(v4))\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 3, 160, 160)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 128, 1, stride=1, padding=0, bias=False)\n        self.bn1 = torch.nn.BatchNorm2d(128)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = torch.relu(self.bn1(v1))\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 3, 224, 224)\n",
                "\nclass Residual(nn.Module):\n    def __init__(self, in_features):\n        super(Residual, self).__init__()\n        conv_block = [nn.Conv2d(in_features, in_features, kernel_size=3, padding=1), nn.BatchNorm2d(in_features), nn.ReLU(inplace=True), nn.Conv2d(in_features, in_features, kernel_size=3, padding=1), nn.BatchNorm2d(in_features)]\n        self.conv_block = nn.Sequential(*conv_block)\n    def forward(self, x):\n        return x + x + self.conv_block(x)    \nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 128, 3, padding=1)\n        self.res1 = Residual(128)\n        self.res2 = Residual(128)\n        self.res3 = Residual(128)\n        self.res4 = Residual(128)\n    def forward(self, x1):\n        v1 = F.relu(self.conv1(x1))\n        v2 = self.res1(v1) + v1\n        v3 = self.res2(v2) + v2\n        v4 = self.res3(v3) + v3\n        v5 = self.res4(v4) + v4\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 1, 512, 512)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(16, 16, 3, stride=3, padding=4)\n    def forward(self, x1):\n        v1 = torch.nn.functional.interpolate(x1, scale_factor=2)\n        v2 = torch.nn.functional.leaky_relu(self.conv1(v1))\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 16, 226, 226)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 128, 3, stride=2, padding=1)\n        self.conv2 = torch.nn.Conv2d(8, 256, 3, stride=2, padding=1)\n        self.conv3 = torch.nn.Conv2d(128, 256, 3, stride=2, padding=1)\n        self.conv4 = torch.nn.Conv2d(256, 256, 3, stride=2, padding=1)\n        self.conv5 = torch.nn.Conv2d(256, 512, 3, stride=2, padding=1)\n        self.conv6 = torch.nn.Conv2d(512, 2048, 3, stride=2, padding=1)\n        self.conv7 = torch.nn.Conv2d(2048, 4096, 3, stride=2, padding=1)\n        self.conv8 = torch.nn.Conv2d(4096, 4096, 3, stride=2, padding=1)\n    def forward(self, x1):\n        v1 = torch.relu(self.conv1(x1))\n        v2 = torch.relu(self.conv2(v1))\n        v3 = torch.relu(self.conv3(v2))\n        v4 = torch.relu(self.conv4(v3))\n        v5 = torch.relu(self.conv5(v4))\n        v6 = torch.relu(self.conv6(v5))\n        v7 = torch.relu(self.conv7(v6))\n        v8 = torch.relu(self.conv8(v7))\n        return v8\n# Inputs to the model\nx1 = torch.randn(1, 3, 60, 60)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 60, 14, stride=11, padding=4)\n        self.conv2 = torch.nn.Conv2d(60, 80, 4, stride=4, padding=1)\n        self.conv3 = torch.nn.Conv2d(80, 3, 2, stride=2, padding=1)\n    def forward(self, x1):\n        v1 = torch.relu(self.conv1(x1))\n        v2 = torch.relu(self.conv2(v1))\n        v3 = torch.relu(self.conv3(v2))\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 3, 192, 192)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(25, 25, kernel_size=(7, 1), stride=(7, 1), padding=(3, 0))\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        return v1\n# Inputs to the model\nx1 = torch.randn(1, 25, 320, 1)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(32, 64, 2, stride=1, padding=0)\n        self.conv2 = torch.nn.Conv2d(64, 48, 3, stride=1, padding=0)\n        self.conv3 = torch.nn.Conv2d(48, 32, 3, stride=1, padding=0)\n        self.conv4 = torch.nn.Conv2d(32, 48, 3, stride=1, padding=0)\n        self.conv5 = torch.nn.Conv2d(48, 64, 2, stride=1, padding=0)\n    def forward(self, x1):\n        v1 = torch.relu(self.conv1(x1))\n        v2 = torch.relu(self.conv2(v1))\n        v3 = torch.relu(self.conv3(v2))\n        v4 = torch.relu(self.conv4(v3))\n        v5 = torch.relu(self.conv5(v4))\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 32, 40, 40)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(1, 8, 3, stride=1, padding=1)\n        self.conv2 = torch.nn.Conv2d(8, 16, 3, stride=1, padding=1)\n        self.conv3 = torch.nn.Conv2d(16, 32, 3, stride=1, padding=1)\n        self.conv4 = torch.nn.Conv2d(32, 64, 3, stride=1, padding=1)\n        self.conv5 = torch.nn.Conv2d(64, 96, 3, stride=1, padding=1)\n        self.conv6 = torch.nn.Conv2d(96, 128, 3, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = torch.relu(self.conv1(x1))\n        v2 = torch.relu(self.conv2(v1))\n        v3 = torch.relu(self.conv3(v2))\n        v4 = torch.relu(self.conv4(v3))\n        v5 = torch.relu(self.conv5(v4))\n        v6 = torch.relu(self.conv6(v5))\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 1, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(56, 6, 7, stride=1, padding=0)\n        self.conv2 = torch.nn.Conv2d(6, 59, 1, stride=2, padding=0)\n        self.conv3 = torch.nn.Conv2d(59, 55, 1, stride=1, padding=0)\n        self.conv4 = torch.nn.Conv2d(55, 93, 1, stride=1, padding=0)\n        self.conv5 = torch.nn.Conv2d(93, 57, 1, stride=1, padding=0)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = torch.relu(v1)\n        v3 = self.conv2(v2)\n        v4 = torch.relu(v3)\n        v5 = self.conv3(v4)\n        v6 = torch.relu(v5)\n        v7 = self.conv4(v6)\n        v8 = torch.relu(v7)\n        v9 = self.conv5(v8)\n        v10 = torch.relu(v9)\n        return v10\n# Inputs to the model\nx1 = torch.randn(1, 56, 256, 256)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 96, 9, stride=2, padding=0)\n        self.conv2 = torch.nn.Conv2d(96, 64, 9, stride=2, padding=0)\n        self.conv3 = torch.nn.Conv2d(64, 64, 6, stride=2, padding=0)\n        self.conv4 = torch.nn.Conv2d(64, 64, 6, stride=2, padding=0)\n        self.conv5 = torch.nn.Conv2d(64, 64, 6, stride=2, padding=0)\n    def forward(self, x1):\n        v1 = torch.relu(self.conv1(x1))\n        v2 = torch.relu(self.conv2(v1))\n        v3 = torch.relu(self.conv3(v2))\n        v4 = torch.relu(self.conv4(v3))\n        v5 = torch.relu(self.conv5(v4))\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 3, 160, 160)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 128, 1, stride=1, padding=0, bias=False)\n        self.bn1 = torch.nn.BatchNorm2d(128)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = torch.relu(self.bn1(v1))\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 3, 224, 224)\n",
                "\nclass Residual(nn.Module):\n    def __init__(self, in_features):\n        super(Residual, self).__init__()\n        conv_block = [nn.Conv2d(in_features, in_features, kernel_size=3, padding=1), nn.BatchNorm2d(in_features), nn.ReLU(inplace=True), nn.Conv2d(in_features, in_features, kernel_size=3, padding=1), nn.BatchNorm2d(in_features)]\n        self.conv_block = nn.Sequential(*conv_block)\n    def forward(self, x):\n        return x + x + self.conv_block(x)    \nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 128, 3, padding=1)\n        self.res1 = Residual(128)\n        self.res2 = Residual(128)\n        self.res3 = Residual(128)\n        self.res4 = Residual(128)\n    def forward(self, x1):\n        v1 = F.relu(self.conv1(x1))\n        v2 = self.res1(v1) + v1\n        v3 = self.res2(v2) + v2\n        v4 = self.res3(v3) + v3\n        v5 = self.res4(v4) + v4\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 1, 512, 512)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(16, 16, 3, stride=3, padding=4)\n    def forward(self, x1):\n        v1 = torch.nn.functional.interpolate(x1, scale_factor=2)\n        v2 = torch.nn.functional.leaky_relu(self.conv1(v1))\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 16, 226, 226)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 128, 3, stride=2, padding=1)\n        self.conv2 = torch.nn.Conv2d(8, 256, 3, stride=2, padding=1)\n        self.conv3 = torch.nn.Conv2d(128, 256, 3, stride=2, padding=1)\n        self.conv4 = torch.nn.Conv2d(256, 256, 3, stride=2, padding=1)\n        self.conv5 = torch.nn.Conv2d(256, 512, 3, stride=2, padding=1)\n        self.conv6 = torch.nn.Conv2d(512, 2048, 3, stride=2, padding=1)\n        self.conv7 = torch.nn.Conv2d(2048, 4096, 3, stride=2, padding=1)\n        self.conv8 = torch.nn.Conv2d(4096, 4096, 3, stride=2, padding=1)\n    def forward(self, x1):\n        v1 = torch.relu(self.conv1(x1))\n        v2 = torch.relu(self.conv2(v1))\n        v3 = torch.relu(self.conv3(v2))\n        v4 = torch.relu(self.conv4(v3))\n        v5 = torch.relu(self.conv5(v4))\n        v6 = torch.relu(self.conv6(v5))\n        v7 = torch.relu(self.conv7(v6))\n        v8 = torch.relu(self.conv8(v7))\n        return v8\n# Inputs to the model\nx1 = torch.randn(1, 3, 60, 60)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 60, 14, stride=11, padding=4)\n        self.conv2 = torch.nn.Conv2d(60, 80, 4, stride=4, padding=1)\n        self.conv3 = torch.nn.Conv2d(80, 3, 2, stride=2, padding=1)\n    def forward(self, x1):\n        v1 = torch.relu(self.conv1(x1))\n        v2 = torch.relu(self.conv2(v1))\n        v3 = torch.relu(self.conv3(v2))\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 3, 192, 192)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(25, 25, kernel_size=(7, 1), stride=(7, 1), padding=(3, 0))\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        return v1\n# Inputs to the model\nx1 = torch.randn(1, 25, 320, 1)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(32, 64, 2, stride=1, padding=0)\n        self.conv2 = torch.nn.Conv2d(64, 48, 3, stride=1, padding=0)\n        self.conv3 = torch.nn.Conv2d(48, 32, 3, stride=1, padding=0)\n        self.conv4 = torch.nn.Conv2d(32, 48, 3, stride=1, padding=0)\n        self.conv5 = torch.nn.Conv2d(48, 64, 2, stride=1, padding=0)\n    def forward(self, x1):\n        v1 = torch.relu(self.conv1(x1))\n        v2 = torch.relu(self.conv2(v1))\n        v3 = torch.relu(self.conv3(v2))\n        v4 = torch.relu(self.conv4(v3))\n        v5 = torch.relu(self.conv5(v4))\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 32, 40, 40)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(1, 8, 3, stride=1, padding=1)\n        self.conv2 = torch.nn.Conv2d(8, 16, 3, stride=1, padding=1)\n        self.conv3 = torch.nn.Conv2d(16, 32, 3, stride=1, padding=1)\n        self.conv4 = torch.nn.Conv2d(32, 64, 3, stride=1, padding=1)\n        self.conv5 = torch.nn.Conv2d(64, 96, 3, stride=1, padding=1)\n        self.conv6 = torch.nn.Conv2d(96, 128, 3, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = torch.relu(self.conv1(x1))\n        v2 = torch.relu(self.conv2(v1))\n        v3 = torch.relu(self.conv3(v2))\n        v4 = torch.relu(self.conv4(v3))\n        v5 = torch.relu(self.conv5(v4))\n        v6 = torch.relu(self.conv6(v5))\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 1, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(56, 6, 7, stride=1, padding=0)\n        self.conv2 = torch.nn.Conv2d(6, 59, 1, stride=2, padding=0)\n        self.conv3 = torch.nn.Conv2d(59, 55, 1, stride=1, padding=0)\n        self.conv4 = torch.nn.Conv2d(55, 93, 1, stride=1, padding=0)\n        self.conv5 = torch.nn.Conv2d(93, 57, 1, stride=1, padding=0)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = torch.relu(v1)\n        v3 = self.conv2(v2)\n        v4 = torch.relu(v3)\n        v5 = self.conv3(v4)\n        v6 = torch.relu(v5)\n        v7 = self.conv4(v6)\n        v8 = torch.relu(v7)\n        v9 = self.conv5(v8)\n        v10 = torch.relu(v9)\n        return v10\n# Inputs to the model\nx1 = torch.randn(1, 56, 256, 256)\n"
            ],
            "g_time": 15.593674421310425
        }
    }
}
{
    "opt": "unary=21",
    "response": {
        "0": {
            "raw": [
                "\nclass ModelLinear(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(1, 1)\n    def forward(self, x):\n        return self.linear(x)\n# Inputs to the model\nx = torch.randn(1, 1, 1, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.con1 = torch.nn.Conv2d(1, 11, 11, stride=(2,2), padding=(2,2), dilation=(2,2))\n        self.relu1 = torch.nn.ReLU()\n        self.conv2 = torch.nn.Conv2d(11, 16, 5, stride=(1,1), padding=(1,1), dilation=(1,1))\n        self.relu2 = torch.nn.ReLU()\n    def forward(self, x):\n        x = self.con1(x)\n        x = self.relu1(x)\n        x = self.conv2(x)\n        x = self.relu2(x)\n        return x\n# Inputs to the model\ninput = torch.randn(1, 1, 28, 28)\n",
                "\nclass ModelTanh(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 64, 1)\n    def forward(self, x):\n        v1 = self.conv(x)\n        v2 = torch.tanh(v1)\n        return v2\n# Inputs to the model\nx = torch.randn(1, 3, 1231, 1231)\n",
                "\nclass ModelTanh(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.prelu1 = torch.nn.PReLU(2)\n    def forward(self, x):\n        r1 = self.prelu1(x)\n        r2 = torch.tanh(r1)\n        return r2\n# Inputs to the model\nx = torch.randn(1, 2, 3, 4)\n",
                "\nclass TestTanh(nn.Module):\n    def __init__(self):\n        super(TestTanh, self).__init__()\n        self.act= nn.Tanh()\n    def forward(self, x):\n        # v1 = self.bn(x)\n        x = torch.tanh(x)\n        return x\n# Inputs to the model\nx = torch.torch.rand(1, 10, 10, 10)\n",
                "\nclass ModelTanh(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.op = torch.tensor([198.0686])\n    def forward(self, x):\n        v1 = x + self.op\n        v2 = torch.tanh(v1)\n        return v2\n# Inputs to the model\nx = torch.randn(1, 4, 28, 28)\n",
                "\nclass ModelTanh(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 5, kernel_size=(1, 1))\n \n    def forward(self, x):\n        x = self.conv1(x)\n        x = torch.tanh(x)\n        return x\n# Inputs to the model\nx = torch.randn(1, 1, 224, 224)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 8, 5, stride=3, padding=3, dilation=2, groups=1)\n        self.conv2 = torch.nn.Conv2d(8, 64, 3, stride=3, padding=1, dilation=1, groups=1)\n        self.fc1 = torch.nn.Linear(64, 1000)\n        self.bn = torch.nn.BatchNorm2d(64)\n        self.tanh = torch.nn.Tanh()\n        self.dropout = torch.nn.Dropout(p=0.2)\n        self.fc2 = torch.nn.Linear(1000, 1000)\n        self.conv3 = torch.nn.Conv2d(64, 4, 3, stride=2, padding=1)\n        self.maxpool = torch.nn.MaxPool2d(kernel_size=4, stride=2, padding=1)\n        self.conv4 = torch.nn.Conv2d(4, 20, 3, stride=2, padding=3)\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.conv2(x)\n        x = x.flatten(start_dim=1)\n        x = self.fc1(x)\n        x = self.tanh(self.bn(x))\n        x = self.dropout(x)\n        x = self.fc2(x)\n        x = self.tanh(self.bn(x))\n        x = self.conv3(x)\n        x = self.conv4(x)\n        x = self.maxpool(x)\n        return x\n# Inputs to the model\nx = torch.randn(1, 3, 208, 304)\n",
                "\nclass ModelTanh(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(5, 43, 2)\n    def forward(self, x):\n        v1 = self.conv(x)\n        v2 = torch.tanh(v1)\n        return v2\n# Inputs to the model\nx = torch.randn(1, 5, 200, 200)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.conv = torch.nn.Conv2d(3, 24, 1)\n    def forward(self, x):\n        v1 = self.conv(x)\n        v2 = x+v1\n        return v2\n# Inputs to the model\nx = torch.randn(1, 3, 208, 304)\n"
            ],
            "code": [
                "\nclass ModelLinear(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(1, 1)\n    def forward(self, x):\n        return self.linear(x)\n# Inputs to the model\nx = torch.randn(1, 1, 1, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.con1 = torch.nn.Conv2d(1, 11, 11, stride=(2,2), padding=(2,2), dilation=(2,2))\n        self.relu1 = torch.nn.ReLU()\n        self.conv2 = torch.nn.Conv2d(11, 16, 5, stride=(1,1), padding=(1,1), dilation=(1,1))\n        self.relu2 = torch.nn.ReLU()\n    def forward(self, x):\n        x = self.con1(x)\n        x = self.relu1(x)\n        x = self.conv2(x)\n        x = self.relu2(x)\n        return x\n# Inputs to the model\ninput = torch.randn(1, 1, 28, 28)\n",
                "\nclass ModelTanh(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 64, 1)\n    def forward(self, x):\n        v1 = self.conv(x)\n        v2 = torch.tanh(v1)\n        return v2\n# Inputs to the model\nx = torch.randn(1, 3, 1231, 1231)\n",
                "\nclass ModelTanh(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.prelu1 = torch.nn.PReLU(2)\n    def forward(self, x):\n        r1 = self.prelu1(x)\n        r2 = torch.tanh(r1)\n        return r2\n# Inputs to the model\nx = torch.randn(1, 2, 3, 4)\n",
                "\nclass TestTanh(nn.Module):\n    def __init__(self):\n        super(TestTanh, self).__init__()\n        self.act= nn.Tanh()\n    def forward(self, x):\n        # v1 = self.bn(x)\n        x = torch.tanh(x)\n        return x\n# Inputs to the model\nx = torch.torch.rand(1, 10, 10, 10)\n",
                "\nclass ModelTanh(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.op = torch.tensor([198.0686])\n    def forward(self, x):\n        v1 = x + self.op\n        v2 = torch.tanh(v1)\n        return v2\n# Inputs to the model\nx = torch.randn(1, 4, 28, 28)\n",
                "\nclass ModelTanh(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 5, kernel_size=(1, 1))\n \n    def forward(self, x):\n        x = self.conv1(x)\n        x = torch.tanh(x)\n        return x\n# Inputs to the model\nx = torch.randn(1, 1, 224, 224)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 8, 5, stride=3, padding=3, dilation=2, groups=1)\n        self.conv2 = torch.nn.Conv2d(8, 64, 3, stride=3, padding=1, dilation=1, groups=1)\n        self.fc1 = torch.nn.Linear(64, 1000)\n        self.bn = torch.nn.BatchNorm2d(64)\n        self.tanh = torch.nn.Tanh()\n        self.dropout = torch.nn.Dropout(p=0.2)\n        self.fc2 = torch.nn.Linear(1000, 1000)\n        self.conv3 = torch.nn.Conv2d(64, 4, 3, stride=2, padding=1)\n        self.maxpool = torch.nn.MaxPool2d(kernel_size=4, stride=2, padding=1)\n        self.conv4 = torch.nn.Conv2d(4, 20, 3, stride=2, padding=3)\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.conv2(x)\n        x = x.flatten(start_dim=1)\n        x = self.fc1(x)\n        x = self.tanh(self.bn(x))\n        x = self.dropout(x)\n        x = self.fc2(x)\n        x = self.tanh(self.bn(x))\n        x = self.conv3(x)\n        x = self.conv4(x)\n        x = self.maxpool(x)\n        return x\n# Inputs to the model\nx = torch.randn(1, 3, 208, 304)\n",
                "\nclass ModelTanh(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(5, 43, 2)\n    def forward(self, x):\n        v1 = self.conv(x)\n        v2 = torch.tanh(v1)\n        return v2\n# Inputs to the model\nx = torch.randn(1, 5, 200, 200)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.conv = torch.nn.Conv2d(3, 24, 1)\n    def forward(self, x):\n        v1 = self.conv(x)\n        v2 = x+v1\n        return v2\n# Inputs to the model\nx = torch.randn(1, 3, 208, 304)\n"
            ],
            "g_time": 13.695480585098267
        }
    }
}
{
    "opt": "sfdp=5",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.heads = 27\n        self.seq_len = 512\n        self.dim = 512 // self.heads\n    def forward(self, query, key, value, attn_mask):\n        qk = query @ key.transpose(-2, -1) / math.sqrt(query.size(-1))\n        qk = qk + attn_mask\n        attn_weight = torch.softmax(qk, dim=-1)\n        attn_weight = torch.dropout(attn_weight, 0.92, True)\n        output = attn_weight @ value\n        return output\n# Inputs to the model\nquery = torch.randn(1, 27, 512, 512)\nkey = torch.randn(1, 27, 512, 512)\nvalue = torch.randn(1, 27, 512, 512)\nattn_mask = torch.randn(1, 1, 512, 512)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.heads = 8\n        self.seq_len = 224\n        self.dim = 2048 // self.heads\n    def forward(self, query, key, value, attn_mask):\n        qk = query @ key.transpose(-2, -1) / math.sqrt(query.size(-1))\n        qk = qk + attn_mask\n        attn_weight = torch.softmax(qk, dim=-1)\n        output = attn_weight @ value\n        return output\n# Inputs to the model\nquery = torch.randn(1, 8, 224, 2048)\nkey = torch.randn(1, 8, 224, 2048)\nvalue = torch.randn(1, 8, 224, 2048)\nattn_mask = torch.randn(1, 8, 224, 224)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.heads = 2\n        self.seq_len = 2\n        self.dim = 4\n    def forward(self, query, key, value, attn_mask):\n        qk = query @ key.transpose(-2, -1) / math.sqrt(query.size(-1))\n        qk = qk + attn_mask\n        attn_weight = torch.softmax(qk, dim=-1)\n        attn_weight = torch.dropout(attn_weight, 0, train=False)\n        output = attn_weight @ value\n        return output\n# Inputs to the model\nquery = torch.randn(1, 2, 3, 4)\nkey = torch.randn(1, 2, 3, 4)\nvalue = torch.randn(1, 2, 3, 4)\nattn_mask = torch.randn(1, 1, 3, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.heads = 1\n        self.seq_len = 512\n        self.dim = 8192 // self.heads\n    def forward(self, query, key, value, attn_mask):\n        qk = query @ key.transpose(-2, -1) / math.sqrt(query.size(-1))\n        qk = qk + attn_mask\n        attn_weight = torch.softmax(qk, dim=-1)\n        attn_weight = torch.dropout(attn_weight, 0.5, True)\n        output = attn_weight @ value\n        return output\n# Inputs to the model\nquery = torch.randn(1, 1, 321, 8192)\nkey = torch.randn(1, 1, 321, 8192)\nvalue = torch.randn(1, 1, 321, 8192)\nattn_mask = torch.randn(1, 1, 321, 321)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.heads = 64\n        self.seq_len = 2048\n        self.dim = 512 // self.heads\n    def forward(self, query, key, value, attn_mask):\n        qk = query @ key.transpose(-2, -1) / math.sqrt(query.size(-1))\n        qk = qk + attn_mask\n        attn_weight = torch.softmax(qk, dim=-1)\n        attn_weight = torch.dropout(attn_weight, 0.5, True)\n        output = attn_weight @ value\n        return output\n# Inputs to the model\nquery = torch.randn(1, 64, 2048, 512)\nkey = torch.randn(1, 64, 2048, 512)\nvalue = torch.randn(1, 64, 2048, 512)\nattn_mask = torch.randn(1, 1, 2048, 2048)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.heads = 32\n        self.seq_len = 7\n        self.dim = 16\n    def forward(self, query, key, value, attn_mask):\n        qk = query @ key.transpose(-2, -1) / math.sqrt(query.size(-1))\n        qk = qk + attn_mask\n        attn_weight = torch.softmax(qk, dim=-1)\n        attn_weight = torch.dropout(attn_weight, 0.9, True)\n        output = attn_weight @ value\n        return output\n# Inputs to the model\nquery = torch.randn(1, 32, 7, 16)\nkey = torch.randn(1, 32, 7, 16)\nvalue = torch.randn(1, 32, 7, 16)\nattn_mask = torch.randn(1, 1, 7, 7)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.heads = 1\n        self.seq_len = 96\n        self.dim = 16\n    def forward(self, query, key, value, attn_mask):\n        qk = query @ key.transpose(-2, -1) / math.sqrt(query.size(-1))\n        qk = qk + attn_mask\n        attn_weight = torch.softmax(qk, dim=-1)\n        attn_weight = torch.dropout(attn_weight, 0.5, True)\n        output = attn_weight @ value\n        return output\n# Inputs to the model\nquery = torch.randn(1, 1, 96, 16)\nkey = torch.randn(1, 1, 96, 16)\nvalue = torch.randn(1, 1, 96, 16)\nattn_mask = torch.randn(1, 1, 96, 96)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.heads = 1\n        self.seq_len = 448\n        self.dim = 8192 // self.heads\n    def forward(self, query, key, value, attn_mask):\n        qk = query @ key.transpose(-2, -1) / math.sqrt(query.size(-1))\n        qk = qk + attn_mask\n        attn_weight = torch.softmax(qk, dim=-1)\n        attn_weight = torch.dropout(attn_weight, 0.5, True)\n        output = attn_weight @ value\n        return output\n# Inputs to the model\nquery = torch.randn(1, 1, 449, 8192)\nkey = torch.randn(1, 1, 449, 8192)\nvalue = torch.randn(1, 1, 449, 8192)\nattn_mask = torch.randn(1, 1, 449, 449)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.heads = 128\n        self.seq_len = 73\n        self.dim = 512 // self.heads\n    def forward(self, query, key, value, attn_mask):\n        qk = query @ key.transpose(-2, -1) / math.sqrt(query.size(-1))\n        qk = qk + attn_mask\n        attn_weight = torch.softmax(qk, dim=-1)\n        attn_weight = torch.dropout(attn_weight, 0.45, True)\n        output = attn_weight @ value\n        return output\n# Inputs to the model\nquery = torch.randn(500, 128, 73, 512)\nkey = torch.randn(500, 128, 73, 512)\nvalue = torch.randn(500, 128, 73, 512)\nattn_mask = torch.randn(500, 1, 73, 73)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.heads = 1\n        self.seq_len = 128\n        self.dim = 64\n    def forward(self, query, key, value, attn_mask):\n        qk = query @ key.transpose(-2, -1) / math.sqrt(query.size(-1))\n        qk = qk + attn_mask\n        attn_weight = torch.softmax(qk, dim=-1)\n        attn_weight = torch.dropout(attn_weight, 0., True)\n        output = attn_weight @ value\n        return output\n# Inputs to the model\nquery = torch.randn(1, 1, 128, 64)\nkey = torch.randn(1, 1, 128, 64)\nvalue = torch.randn(1, 1, 128, 64)\nattn_mask = torch.randn(1, 1, 128, 128)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.heads = 27\n        self.seq_len = 512\n        self.dim = 512 // self.heads\n    def forward(self, query, key, value, attn_mask):\n        qk = query @ key.transpose(-2, -1) / math.sqrt(query.size(-1))\n        qk = qk + attn_mask\n        attn_weight = torch.softmax(qk, dim=-1)\n        attn_weight = torch.dropout(attn_weight, 0.92, True)\n        output = attn_weight @ value\n        return output\n# Inputs to the model\nquery = torch.randn(1, 27, 512, 512)\nkey = torch.randn(1, 27, 512, 512)\nvalue = torch.randn(1, 27, 512, 512)\nattn_mask = torch.randn(1, 1, 512, 512)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.heads = 8\n        self.seq_len = 224\n        self.dim = 2048 // self.heads\n    def forward(self, query, key, value, attn_mask):\n        qk = query @ key.transpose(-2, -1) / math.sqrt(query.size(-1))\n        qk = qk + attn_mask\n        attn_weight = torch.softmax(qk, dim=-1)\n        output = attn_weight @ value\n        return output\n# Inputs to the model\nquery = torch.randn(1, 8, 224, 2048)\nkey = torch.randn(1, 8, 224, 2048)\nvalue = torch.randn(1, 8, 224, 2048)\nattn_mask = torch.randn(1, 8, 224, 224)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.heads = 2\n        self.seq_len = 2\n        self.dim = 4\n    def forward(self, query, key, value, attn_mask):\n        qk = query @ key.transpose(-2, -1) / math.sqrt(query.size(-1))\n        qk = qk + attn_mask\n        attn_weight = torch.softmax(qk, dim=-1)\n        attn_weight = torch.dropout(attn_weight, 0, train=False)\n        output = attn_weight @ value\n        return output\n# Inputs to the model\nquery = torch.randn(1, 2, 3, 4)\nkey = torch.randn(1, 2, 3, 4)\nvalue = torch.randn(1, 2, 3, 4)\nattn_mask = torch.randn(1, 1, 3, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.heads = 1\n        self.seq_len = 512\n        self.dim = 8192 // self.heads\n    def forward(self, query, key, value, attn_mask):\n        qk = query @ key.transpose(-2, -1) / math.sqrt(query.size(-1))\n        qk = qk + attn_mask\n        attn_weight = torch.softmax(qk, dim=-1)\n        attn_weight = torch.dropout(attn_weight, 0.5, True)\n        output = attn_weight @ value\n        return output\n# Inputs to the model\nquery = torch.randn(1, 1, 321, 8192)\nkey = torch.randn(1, 1, 321, 8192)\nvalue = torch.randn(1, 1, 321, 8192)\nattn_mask = torch.randn(1, 1, 321, 321)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.heads = 64\n        self.seq_len = 2048\n        self.dim = 512 // self.heads\n    def forward(self, query, key, value, attn_mask):\n        qk = query @ key.transpose(-2, -1) / math.sqrt(query.size(-1))\n        qk = qk + attn_mask\n        attn_weight = torch.softmax(qk, dim=-1)\n        attn_weight = torch.dropout(attn_weight, 0.5, True)\n        output = attn_weight @ value\n        return output\n# Inputs to the model\nquery = torch.randn(1, 64, 2048, 512)\nkey = torch.randn(1, 64, 2048, 512)\nvalue = torch.randn(1, 64, 2048, 512)\nattn_mask = torch.randn(1, 1, 2048, 2048)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.heads = 32\n        self.seq_len = 7\n        self.dim = 16\n    def forward(self, query, key, value, attn_mask):\n        qk = query @ key.transpose(-2, -1) / math.sqrt(query.size(-1))\n        qk = qk + attn_mask\n        attn_weight = torch.softmax(qk, dim=-1)\n        attn_weight = torch.dropout(attn_weight, 0.9, True)\n        output = attn_weight @ value\n        return output\n# Inputs to the model\nquery = torch.randn(1, 32, 7, 16)\nkey = torch.randn(1, 32, 7, 16)\nvalue = torch.randn(1, 32, 7, 16)\nattn_mask = torch.randn(1, 1, 7, 7)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.heads = 1\n        self.seq_len = 96\n        self.dim = 16\n    def forward(self, query, key, value, attn_mask):\n        qk = query @ key.transpose(-2, -1) / math.sqrt(query.size(-1))\n        qk = qk + attn_mask\n        attn_weight = torch.softmax(qk, dim=-1)\n        attn_weight = torch.dropout(attn_weight, 0.5, True)\n        output = attn_weight @ value\n        return output\n# Inputs to the model\nquery = torch.randn(1, 1, 96, 16)\nkey = torch.randn(1, 1, 96, 16)\nvalue = torch.randn(1, 1, 96, 16)\nattn_mask = torch.randn(1, 1, 96, 96)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.heads = 1\n        self.seq_len = 448\n        self.dim = 8192 // self.heads\n    def forward(self, query, key, value, attn_mask):\n        qk = query @ key.transpose(-2, -1) / math.sqrt(query.size(-1))\n        qk = qk + attn_mask\n        attn_weight = torch.softmax(qk, dim=-1)\n        attn_weight = torch.dropout(attn_weight, 0.5, True)\n        output = attn_weight @ value\n        return output\n# Inputs to the model\nquery = torch.randn(1, 1, 449, 8192)\nkey = torch.randn(1, 1, 449, 8192)\nvalue = torch.randn(1, 1, 449, 8192)\nattn_mask = torch.randn(1, 1, 449, 449)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.heads = 128\n        self.seq_len = 73\n        self.dim = 512 // self.heads\n    def forward(self, query, key, value, attn_mask):\n        qk = query @ key.transpose(-2, -1) / math.sqrt(query.size(-1))\n        qk = qk + attn_mask\n        attn_weight = torch.softmax(qk, dim=-1)\n        attn_weight = torch.dropout(attn_weight, 0.45, True)\n        output = attn_weight @ value\n        return output\n# Inputs to the model\nquery = torch.randn(500, 128, 73, 512)\nkey = torch.randn(500, 128, 73, 512)\nvalue = torch.randn(500, 128, 73, 512)\nattn_mask = torch.randn(500, 1, 73, 73)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.heads = 1\n        self.seq_len = 128\n        self.dim = 64\n    def forward(self, query, key, value, attn_mask):\n        qk = query @ key.transpose(-2, -1) / math.sqrt(query.size(-1))\n        qk = qk + attn_mask\n        attn_weight = torch.softmax(qk, dim=-1)\n        attn_weight = torch.dropout(attn_weight, 0., True)\n        output = attn_weight @ value\n        return output\n# Inputs to the model\nquery = torch.randn(1, 1, 128, 64)\nkey = torch.randn(1, 1, 128, 64)\nvalue = torch.randn(1, 1, 128, 64)\nattn_mask = torch.randn(1, 1, 128, 128)\n"
            ],
            "g_time": 10.841962814331055
        }
    }
}
{
    "opt": "unary=16",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(256, 256)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.relu(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 256)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(32, 64)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = F.relu(v1)\n        return v2\n\n# Initializing the model\nm1 = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(128, 1000)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.relu(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 128)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(1, 2, bias=True)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.relu(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 1)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.nn.functional.relu(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n_output_ = m(x1)\n\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(16, 32)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = F.relu(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 6)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.relu(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(64, 32)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.nn.functional.relu(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(96, 10)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.relu(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(64, 96)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.dense = torch.nn.Linear(16, 32)\n \n    def forward(self, x1):\n        v1 = self.dense(x1)\n        v2 = torch.relu(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 16)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(256, 256)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.relu(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 256)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(32, 64)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = F.relu(v1)\n        return v2\n\n# Initializing the model\nm1 = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(128, 1000)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.relu(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 128)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(1, 2, bias=True)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.relu(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 1)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.nn.functional.relu(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n_output_ = m(x1)\n\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(16, 32)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = F.relu(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 6)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.relu(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(64, 32)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.nn.functional.relu(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(96, 10)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.relu(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(64, 96)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.dense = torch.nn.Linear(16, 32)\n \n    def forward(self, x1):\n        v1 = self.dense(x1)\n        v2 = torch.relu(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 16)\n"
            ],
            "g_time": 4.767528057098389
        }
    }
}
{
    "opt": "unary=24",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(1, 32, (3, 2), stride=1, padding=(1, 0))\n    def forward(self, x):\n        negative_slope = -2.9034677\n        v1 = self.conv(x)\n        v2 = v1 > 0\n        v3 = v1 * negative_slope\n        v4 = torch.where(v2, v1, v3)\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 1, 176, 240)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_1 = torch.nn.Conv2d(1, 1, (1, 1), stride=1, padding=(0, 0))\n        self.conv_2 = torch.nn.Conv2d(1, 1, (1, 1), stride=1, padding=(0, 0))\n    def forward(self, x):\n        negative_slope = 0.0\n        v1 = self.conv_1(x)\n        v2 = self.conv_2(x)\n        v3 = v1 > 0\n        v4 = v2 > 0\n        v5 = (v3, v1, v2)\n        v6 = torch.where(v3, v1, v2)\n        v7 = [i for i in v5]\n        v8 = [i for i in v7]\n        v9 = v4 == v8\n        v10 = v3 == v4\n        return v10\n# Inputs to the model\nx1 = torch.randn(1, 1, 1, 1)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(13, 6, 1, stride=1, padding=0)\n    def forward(self, x):\n        negative_slope = 0.27763452\n        v1 = self.conv(x)\n        v2 = v1 > 0\n        v3 = v1 * negative_slope\n        v4 = torch.where(v2, v1, v3)\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 13, 200, 200)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(16, 2, (1, 2), stride=1, padding=(0, 0))\n    def forward(self, x):\n        negative_slope = 0.5981151\n        v1 = self.conv(x)\n        v2 = v1 > 0\n        v3 = v1 * negative_slope\n        v4 = torch.where(v2, v1, v3)\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 16, 18, 12)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(1, 9, (3, 4), stride=1, padding=(0, 1))\n    def forward(self, x):\n        negative_slope = 380.96417\n        v1 = self.conv(x)\n        v2 = v1 > 0\n        v3 = v1 * negative_slope\n        v4 = torch.where(v2, v1, v3)\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 1, 8, 7)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(4, 6, (1, 3), stride=1, padding=(0, 4))\n    def forward(self, x):\n        negative_slope = 10.28929\n        v1 = self.conv(x)\n        v2 = v1 > 0\n        v3 = v1 * negative_slope\n        v4 = torch.where(v2, v1, v3)\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 4, 54, 26)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(2, 4, 1, stride=1, padding=0)\n    def forward(self, x):\n        negative_slope = 1.0\n        v1 = self.conv(x)\n        v2 = v1 > 0\n        v3 = v1 * negative_slope\n        v4 = torch.where(v2, v1, v3)\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 2, 13, 14)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(2, 12, (2, 1), stride=2, padding=(0, 0))\n    def forward(self, x):\n        negative_slope = -0.9695\n        v1 = self.conv(x)\n        v2 = v1 > 0\n        v3 = v1 * negative_slope\n        v4 = torch.where(v2, v1, v3)\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 2, 100, 15)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(2, 1, 1, stride=1, padding=0)\n    def forward(self, x):\n        negative_slope = 0.6992813\n        v1 = self.conv(x)\n        v2 = v1 > 0\n        v3 = v1 * negative_slope\n        v4 = torch.where(v2, v1, v3)\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 2, 600, 3201)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(10, 24, (3, 2), stride=1, padding=(1, 0))\n    def forward(self, x):\n        negative_slope = 8.822705\n        v1 = self.conv(x)\n        v2 = v1 > 0\n        v3 = v1 * negative_slope\n        v4 = torch.where(v2, v1, v3)\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 10, 41, 12)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(1, 32, (3, 2), stride=1, padding=(1, 0))\n    def forward(self, x):\n        negative_slope = -2.9034677\n        v1 = self.conv(x)\n        v2 = v1 > 0\n        v3 = v1 * negative_slope\n        v4 = torch.where(v2, v1, v3)\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 1, 176, 240)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_1 = torch.nn.Conv2d(1, 1, (1, 1), stride=1, padding=(0, 0))\n        self.conv_2 = torch.nn.Conv2d(1, 1, (1, 1), stride=1, padding=(0, 0))\n    def forward(self, x):\n        negative_slope = 0.0\n        v1 = self.conv_1(x)\n        v2 = self.conv_2(x)\n        v3 = v1 > 0\n        v4 = v2 > 0\n        v5 = (v3, v1, v2)\n        v6 = torch.where(v3, v1, v2)\n        v7 = [i for i in v5]\n        v8 = [i for i in v7]\n        v9 = v4 == v8\n        v10 = v3 == v4\n        return v10\n# Inputs to the model\nx1 = torch.randn(1, 1, 1, 1)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(13, 6, 1, stride=1, padding=0)\n    def forward(self, x):\n        negative_slope = 0.27763452\n        v1 = self.conv(x)\n        v2 = v1 > 0\n        v3 = v1 * negative_slope\n        v4 = torch.where(v2, v1, v3)\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 13, 200, 200)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(16, 2, (1, 2), stride=1, padding=(0, 0))\n    def forward(self, x):\n        negative_slope = 0.5981151\n        v1 = self.conv(x)\n        v2 = v1 > 0\n        v3 = v1 * negative_slope\n        v4 = torch.where(v2, v1, v3)\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 16, 18, 12)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(1, 9, (3, 4), stride=1, padding=(0, 1))\n    def forward(self, x):\n        negative_slope = 380.96417\n        v1 = self.conv(x)\n        v2 = v1 > 0\n        v3 = v1 * negative_slope\n        v4 = torch.where(v2, v1, v3)\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 1, 8, 7)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(4, 6, (1, 3), stride=1, padding=(0, 4))\n    def forward(self, x):\n        negative_slope = 10.28929\n        v1 = self.conv(x)\n        v2 = v1 > 0\n        v3 = v1 * negative_slope\n        v4 = torch.where(v2, v1, v3)\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 4, 54, 26)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(2, 4, 1, stride=1, padding=0)\n    def forward(self, x):\n        negative_slope = 1.0\n        v1 = self.conv(x)\n        v2 = v1 > 0\n        v3 = v1 * negative_slope\n        v4 = torch.where(v2, v1, v3)\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 2, 13, 14)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(2, 12, (2, 1), stride=2, padding=(0, 0))\n    def forward(self, x):\n        negative_slope = -0.9695\n        v1 = self.conv(x)\n        v2 = v1 > 0\n        v3 = v1 * negative_slope\n        v4 = torch.where(v2, v1, v3)\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 2, 100, 15)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(2, 1, 1, stride=1, padding=0)\n    def forward(self, x):\n        negative_slope = 0.6992813\n        v1 = self.conv(x)\n        v2 = v1 > 0\n        v3 = v1 * negative_slope\n        v4 = torch.where(v2, v1, v3)\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 2, 600, 3201)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(10, 24, (3, 2), stride=1, padding=(1, 0))\n    def forward(self, x):\n        negative_slope = 8.822705\n        v1 = self.conv(x)\n        v2 = v1 > 0\n        v3 = v1 * negative_slope\n        v4 = torch.where(v2, v1, v3)\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 10, 41, 12)\n"
            ],
            "g_time": 9.506876468658447
        }
    }
}
{
    "opt": "unary=14",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose_31 = torch.nn.ConvTranspose2d(1, 1, 3, stride=1, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv_transpose_31(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 1, 10, 10)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose13 = torch.nn.ConvTranspose2d(1024, 1024, 1, stride=1, padding=0)\n    def forward(self, x1):\n        v1 = self.conv_transpose13(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 1024, 28, 28)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose_2 = torch.nn.ConvTranspose2d(648, 19, 1, stride=1, padding=0)\n    def forward(self, x1):\n        v1 = self.conv_transpose_2(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 648, 56, 39)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose57 = torch.nn.ConvTranspose2d(65, 56, 3, stride=1, groups=4, padding=1)\n    def forward(self, x1):\n        v1 = self.conv_transpose57(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 65, 100, 25)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transposed = torch.nn.ConvTranspose2d(640, 640, 1, stride=1, padding=0)\n    def forward(self, x1):\n        v1 = self.conv_transposed(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 640, 48, 48)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose_64 = torch.nn.ConvTranspose2d(8, 3, 5, stride=2, groups=1, padding=2)\n    def forward(self, x1):\n        v1 = self.conv_transpose_64(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 8, 110, 90)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(512, 1, 3, stride=1, padding=1, groups=2)\n        self.conv_transpose16 = torch.nn.ConvTranspose2d(32, 128, 3, stride=1, padding=1)\n    def forward(self, x1, x2):\n        v1 = self.conv(x1)\n        v2 = self.conv_transpose16(x2)\n        v3 = torch.sigmoid(v1 + v2)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 512, 50, 50)\nx2 = torch.randn(1, 32, 50, 50)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose_4 = torch.nn.ConvTranspose2d(1536, 512, 1, stride=3, padding=0)\n    def forward(self, x1):\n        v1 = self.conv_transpose_4(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 1536, 35, 6)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose_49 = torch.nn.ConvTranspose1d(512, 125, 3, stride=1, groups=16, padding=1, output_padding=0)\n    def forward(self, x1):\n        v1 = self.conv_transpose_49(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 512, 128)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose_16 = torch.nn.ConvTranspose2d(8, 8, 15, stride=1, groups=2, padding=7)\n    def forward(self, x1):\n        v1 = self.conv_transpose_16(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 8, 5, 6)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose_31 = torch.nn.ConvTranspose2d(1, 1, 3, stride=1, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv_transpose_31(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 1, 10, 10)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose13 = torch.nn.ConvTranspose2d(1024, 1024, 1, stride=1, padding=0)\n    def forward(self, x1):\n        v1 = self.conv_transpose13(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 1024, 28, 28)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose_2 = torch.nn.ConvTranspose2d(648, 19, 1, stride=1, padding=0)\n    def forward(self, x1):\n        v1 = self.conv_transpose_2(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 648, 56, 39)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose57 = torch.nn.ConvTranspose2d(65, 56, 3, stride=1, groups=4, padding=1)\n    def forward(self, x1):\n        v1 = self.conv_transpose57(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 65, 100, 25)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transposed = torch.nn.ConvTranspose2d(640, 640, 1, stride=1, padding=0)\n    def forward(self, x1):\n        v1 = self.conv_transposed(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 640, 48, 48)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose_64 = torch.nn.ConvTranspose2d(8, 3, 5, stride=2, groups=1, padding=2)\n    def forward(self, x1):\n        v1 = self.conv_transpose_64(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 8, 110, 90)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(512, 1, 3, stride=1, padding=1, groups=2)\n        self.conv_transpose16 = torch.nn.ConvTranspose2d(32, 128, 3, stride=1, padding=1)\n    def forward(self, x1, x2):\n        v1 = self.conv(x1)\n        v2 = self.conv_transpose16(x2)\n        v3 = torch.sigmoid(v1 + v2)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 512, 50, 50)\nx2 = torch.randn(1, 32, 50, 50)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose_4 = torch.nn.ConvTranspose2d(1536, 512, 1, stride=3, padding=0)\n    def forward(self, x1):\n        v1 = self.conv_transpose_4(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 1536, 35, 6)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose_49 = torch.nn.ConvTranspose1d(512, 125, 3, stride=1, groups=16, padding=1, output_padding=0)\n    def forward(self, x1):\n        v1 = self.conv_transpose_49(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 512, 128)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose_16 = torch.nn.ConvTranspose2d(8, 8, 15, stride=1, groups=2, padding=7)\n    def forward(self, x1):\n        v1 = self.conv_transpose_16(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 8, 5, 6)\n"
            ],
            "g_time": 6.821823358535767
        }
    }
}
{
    "opt": "sfdp=3",
    "response": {
        "0": {
            "raw": [
                "\nclass SelfAttention(nn.Module):\n    def __init__(self, input_dim, num_heads, dropout_p):\n        super().__init__()\n\n        self.input_dim=input_dim   \n        self.num_heads = num_heads\n        self.dropout_p = dropout_p\n\n        self.Wq = nn.Linear(input_dim, input_dim)\n        self.Wk = nn.Linear(input_dim, input_dim)\n        self.Wv = nn.Linear(input_dim, input_dim)\n\n    def scaled_dot_product_attention(self, q, k, v, mask=None):\n        wk = torch.matmul(q, k.transpose(-2, -1))\n        scaled_qk = wk.mul(1/(self.input_dim)**0.5)\n\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.scale_factor = 1.0 / (math.sqrt(query.size(-1)) * key.size(-1))\n \n    def forward(self, query, key, value, **kwargs):\n        qk = torch.matmul(query, key.transpose(-2, -1))\n        scaled_qk = qk * self.scale_factor\n        softmax_qk = torch.nn.functional.softmax(scaled_qk, dim=-1)\n        dropout_qk = torch.nn.functional.dropout(softmax_qk, p=kwargs.get('dropout_p', 0.0))\n        output = dropout_qk.matmul(value)\n        return output\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nquery = torch.randn(1, 8, 64, 64)\nkey = torch.randn(1, 8, 64, 64)\nvalue = torch.randn(1, 8, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n \n    def forward(self, queries, keys, values, dropout=0.2, scale_factor=1/np.sqrt(512)):\n        qk = torch.matmul(queries, keys.transpose(-2, -1))\n        scaled_qk = qk.mul(scale_factor)\n        softmax_qk = scaled_qk.softmax(dim=-1)\n        dropout_qk = torch.nn.functional.dropout(softmax_qk, p=dropout)\n        return dropout_qk.matmul(values)\n \n# Initializing the model\nm = Model()\n\n# Inputs to the model\nqueries = torch.randn(1, 50, 48, 512)\nkeys = torch.randn(1, 35, 512, 48)\nvalues = torch.randn(1, 35, 48, 512)\ndropout_p = 0.1\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, input_size, hidden_size, num_heads, output_size=64, activation=nn.ReLU):\n        super().__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.output_size = output_size\n        self.num_heads = num_heads\n        self.query = nn.Linear(input_size, hidden_size)\n        self.key = nn.Linear(input_size, hidden_size)\n        self.value = nn.Linear(input_size, hidden_size)\n        self.attention = scaled_dot_product_attention(hidden_size)\n        self.dropout_layer = nn.Dropout(0.2)\n        self.fc = nn.Linear(hidden_size, output_size)\n        self.activation = activation\n \n    def forward(self, x):\n        x = rearrange(x, 'b n (h d) -> b h n d', n=self.num_heads, h=self.input_size)\n        query = self.query(x)\n        key = self.key(x)\n        value = self.value(x)\n        output = self.attention(query, key, value)\n        output = rearrange(output, 'b h t d -> b t (h d)')\n        output = self.dropout_layer(output)\n        return self.fc(output)\n\n# Initializing the model\nm = Model(2048, 1024, 48)\n\n# Inputs to the model\nx = torch.randn(16, 30, 2048)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n \n    def forward(self, x1, x2):\n        v1 = torch.matmul(x1, x2.transpose(-2, -1))\n        v2 = v1 * 1.7574051560137903\n        v3 = torch.nn.functional.tanh(v2)\n        v4 = torch.mul(v3, v3)\n        v5 = 0.205\n        v6 = v4.mul(v5)\n        return v6\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 4, 4)\nx2 = torch.randn(1, 4, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.dropout = torch.nn.Dropout(p=dropout_p)\n \n    def forward(self, query, key, value, scale_factor):\n        qk = torch.matmul(query, key.transpose(-2, -1))\n        scaled_qk = qk.mul(scale_factor)\n        softmax_qk = torch.nn.functional.softmax(scaled_qk, dim=-1)\n        dropout_qk = self.dropout(softmax_qk)\n        output = dropout_qk.matmul(value)\n        return output\n\n# Initializing the model\nmodel = Model()\n\n# Inputs to the model\nquery = torch.randn(4, 5, 15)\nkey = torch.randn(4, 5, 15)\nvalue = torch.randn(4, 6, 15)\nscale_factor = 1 / np.sqrt(value_head.size(1))\n \n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n \n    def forward(self, query, key, value, scale, dropout_p):\n        v1 = torch.matmul(query, key.transpose(-2, -1))\n        v2 = v1 * scale\n        v3 = F.softmax(v2, dim=-1)\n        v4 = F.dropout(v3, p=dropout_p)\n        v5 = torch.matmul(v4, value)\n        return v5\n\n# Initializing the model\nm = Model()\n\n# Generating random inputs for the model\nkey = torch.randn(1, 125, 15)\nquery = torch.randn(1, 100, 125)\nvalue = torch.randn(1, 1, 100)\nscale = np.power(self.d_input, -0.5)\ndropout_p = 0\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, dropout_p, kernel_size, groups, in_channels, out_channels):\n        super().__init__()\n        self.model = torch.nn.Sequential(\n            torch.nn.Conv2d(in_channels=in_channels,\n                            out_channels=out_channels,\n                            kernel_size=kernel_size,\n                            groups=groups),\n            torch.nn.ReLU(),\n            torch.nn.Dropout(p=dropout_p, inplace=True),\n            torch.nn.Conv2d(in_channels=out_channels,\n                            out_channels=out_channels,\n                            kernel_size=1,\n                            groups=groups),\n            torch.nn.ReLU(),\n            torch.nn.Dropout(p=dropout_p, inplace=True)\n        )\n        scale_factor = math.sqrt((1.0 / in_channels) * (1.0 / groups))\n        self.scale_factor = scale_factor\n\n    def forward(self, input_tensor):\n        result = self.model(input_tensor)\n        scaled_qk = result * self.scale_factor\n        softmax_qk = scaled_qk.softmax(dim=-1)\n        dropout_qk = torch.nn.functional.dropout(softmax_qk, p=dropout_p)\n        output = dropout_qk.matmul(v)\n        return output\n\n# Initializing the model\nm = Model(dropout_p=0.6, kernel_size=1, groups=8, in_channels=8, out_channels=4)\n\n# Inputs to the model\nx = torch.randn(1, 3, 12, 12)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, input1_size, input2_size, hidden_size, dropout_p):\n        super().__init__()\n        self.attention = torch.nn.Linear(input1_size * input2_size, hidden_size)\n        self.dropout_qk = torch.nn.Dropout(dropout_p)\n\n    def forward(self, q, k, v):\n        q_k = torch.matmul(q, k.transpose(-2, -1))\n        scale_factor = torch.sqrt(torch.tensor(float(v.size(-1))/float(q_k.size(-1))))\n        q_k_scaled = q_k.mul(scale_factor)\n        softmax_qk = q_k_scaled.softmax(dim=-1)\n        dropout_qk = self.dropout_qk(softmax_qk)\n        o = dropout_qk.matmul(v)\n        return o\n\n# Initializing the model\nm = Model(input1_size=3, input2_size=3, hidden_size=6, dropout_p=0.5)\n\n# Inputs to the model\nq = torch.randn(1, 3, 4)\nk = torch.randn(1, 3, 8)\nv = torch.randn(1, 8, 4)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, x1, x2):\n        v1 = torch.matmul(x1, x2.transpose(-2, -1))\n        v2 = v1 * self.scale_factor\n        v3 = v2.softmax(dim=-1)\n        v4 = torch.nn.functional.dropout(v3, p=self.dropout_p)\n        v5 = torch.matmul(v4, self.v)\n        return v5\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\nx2 = torch.randn(1, 25, 64, 64)\nscale_factor = torch.nn.Parameter(torch.rand(1, 1, 1, 1) * 50.0)\ndropout_p = torch.nn.Parameter(torch.rand(1, 1) * 0.8)\n"
            ],
            "code": [
                "\nclass SelfAttention(nn.Module):\n    def __init__(self, input_dim, num_heads, dropout_p):\n        super().__init__()\n\n        self.input_dim=input_dim   \n        self.num_heads = num_heads\n        self.dropout_p = dropout_p\n\n        self.Wq = nn.Linear(input_dim, input_dim)\n        self.Wk = nn.Linear(input_dim, input_dim)\n        self.Wv = nn.Linear(input_dim, input_dim)\n\n    def scaled_dot_product_attention(self, q, k, v, mask=None):\n        wk = torch.matmul(q, k.transpose(-2, -1))\n        scaled_qk = wk.mul(1/(self.input_dim)**0.5)\n\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.scale_factor = 1.0 / (math.sqrt(query.size(-1)) * key.size(-1))\n \n    def forward(self, query, key, value, **kwargs):\n        qk = torch.matmul(query, key.transpose(-2, -1))\n        scaled_qk = qk * self.scale_factor\n        softmax_qk = torch.nn.functional.softmax(scaled_qk, dim=-1)\n        dropout_qk = torch.nn.functional.dropout(softmax_qk, p=kwargs.get('dropout_p', 0.0))\n        output = dropout_qk.matmul(value)\n        return output\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nquery = torch.randn(1, 8, 64, 64)\nkey = torch.randn(1, 8, 64, 64)\nvalue = torch.randn(1, 8, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n \n    def forward(self, queries, keys, values, dropout=0.2, scale_factor=1/np.sqrt(512)):\n        qk = torch.matmul(queries, keys.transpose(-2, -1))\n        scaled_qk = qk.mul(scale_factor)\n        softmax_qk = scaled_qk.softmax(dim=-1)\n        dropout_qk = torch.nn.functional.dropout(softmax_qk, p=dropout)\n        return dropout_qk.matmul(values)\n \n# Initializing the model\nm = Model()\n\n# Inputs to the model\nqueries = torch.randn(1, 50, 48, 512)\nkeys = torch.randn(1, 35, 512, 48)\nvalues = torch.randn(1, 35, 48, 512)\ndropout_p = 0.1\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, input_size, hidden_size, num_heads, output_size=64, activation=nn.ReLU):\n        super().__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.output_size = output_size\n        self.num_heads = num_heads\n        self.query = nn.Linear(input_size, hidden_size)\n        self.key = nn.Linear(input_size, hidden_size)\n        self.value = nn.Linear(input_size, hidden_size)\n        self.attention = scaled_dot_product_attention(hidden_size)\n        self.dropout_layer = nn.Dropout(0.2)\n        self.fc = nn.Linear(hidden_size, output_size)\n        self.activation = activation\n \n    def forward(self, x):\n        x = rearrange(x, 'b n (h d) -> b h n d', n=self.num_heads, h=self.input_size)\n        query = self.query(x)\n        key = self.key(x)\n        value = self.value(x)\n        output = self.attention(query, key, value)\n        output = rearrange(output, 'b h t d -> b t (h d)')\n        output = self.dropout_layer(output)\n        return self.fc(output)\n\n# Initializing the model\nm = Model(2048, 1024, 48)\n\n# Inputs to the model\nx = torch.randn(16, 30, 2048)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n \n    def forward(self, x1, x2):\n        v1 = torch.matmul(x1, x2.transpose(-2, -1))\n        v2 = v1 * 1.7574051560137903\n        v3 = torch.nn.functional.tanh(v2)\n        v4 = torch.mul(v3, v3)\n        v5 = 0.205\n        v6 = v4.mul(v5)\n        return v6\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 4, 4)\nx2 = torch.randn(1, 4, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.dropout = torch.nn.Dropout(p=dropout_p)\n \n    def forward(self, query, key, value, scale_factor):\n        qk = torch.matmul(query, key.transpose(-2, -1))\n        scaled_qk = qk.mul(scale_factor)\n        softmax_qk = torch.nn.functional.softmax(scaled_qk, dim=-1)\n        dropout_qk = self.dropout(softmax_qk)\n        output = dropout_qk.matmul(value)\n        return output\n\n# Initializing the model\nmodel = Model()\n\n# Inputs to the model\nquery = torch.randn(4, 5, 15)\nkey = torch.randn(4, 5, 15)\nvalue = torch.randn(4, 6, 15)\nscale_factor = 1 / np.sqrt(value_head.size(1))\n \n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n \n    def forward(self, query, key, value, scale, dropout_p):\n        v1 = torch.matmul(query, key.transpose(-2, -1))\n        v2 = v1 * scale\n        v3 = F.softmax(v2, dim=-1)\n        v4 = F.dropout(v3, p=dropout_p)\n        v5 = torch.matmul(v4, value)\n        return v5\n\n# Initializing the model\nm = Model()\n\n# Generating random inputs for the model\nkey = torch.randn(1, 125, 15)\nquery = torch.randn(1, 100, 125)\nvalue = torch.randn(1, 1, 100)\nscale = np.power(self.d_input, -0.5)\ndropout_p = 0\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, dropout_p, kernel_size, groups, in_channels, out_channels):\n        super().__init__()\n        self.model = torch.nn.Sequential(\n            torch.nn.Conv2d(in_channels=in_channels,\n                            out_channels=out_channels,\n                            kernel_size=kernel_size,\n                            groups=groups),\n            torch.nn.ReLU(),\n            torch.nn.Dropout(p=dropout_p, inplace=True),\n            torch.nn.Conv2d(in_channels=out_channels,\n                            out_channels=out_channels,\n                            kernel_size=1,\n                            groups=groups),\n            torch.nn.ReLU(),\n            torch.nn.Dropout(p=dropout_p, inplace=True)\n        )\n        scale_factor = math.sqrt((1.0 / in_channels) * (1.0 / groups))\n        self.scale_factor = scale_factor\n\n    def forward(self, input_tensor):\n        result = self.model(input_tensor)\n        scaled_qk = result * self.scale_factor\n        softmax_qk = scaled_qk.softmax(dim=-1)\n        dropout_qk = torch.nn.functional.dropout(softmax_qk, p=dropout_p)\n        output = dropout_qk.matmul(v)\n        return output\n\n# Initializing the model\nm = Model(dropout_p=0.6, kernel_size=1, groups=8, in_channels=8, out_channels=4)\n\n# Inputs to the model\nx = torch.randn(1, 3, 12, 12)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, input1_size, input2_size, hidden_size, dropout_p):\n        super().__init__()\n        self.attention = torch.nn.Linear(input1_size * input2_size, hidden_size)\n        self.dropout_qk = torch.nn.Dropout(dropout_p)\n\n    def forward(self, q, k, v):\n        q_k = torch.matmul(q, k.transpose(-2, -1))\n        scale_factor = torch.sqrt(torch.tensor(float(v.size(-1))/float(q_k.size(-1))))\n        q_k_scaled = q_k.mul(scale_factor)\n        softmax_qk = q_k_scaled.softmax(dim=-1)\n        dropout_qk = self.dropout_qk(softmax_qk)\n        o = dropout_qk.matmul(v)\n        return o\n\n# Initializing the model\nm = Model(input1_size=3, input2_size=3, hidden_size=6, dropout_p=0.5)\n\n# Inputs to the model\nq = torch.randn(1, 3, 4)\nk = torch.randn(1, 3, 8)\nv = torch.randn(1, 8, 4)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, x1, x2):\n        v1 = torch.matmul(x1, x2.transpose(-2, -1))\n        v2 = v1 * self.scale_factor\n        v3 = v2.softmax(dim=-1)\n        v4 = torch.nn.functional.dropout(v3, p=self.dropout_p)\n        v5 = torch.matmul(v4, self.v)\n        return v5\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\nx2 = torch.randn(1, 25, 64, 64)\nscale_factor = torch.nn.Parameter(torch.rand(1, 1, 1, 1) * 50.0)\ndropout_p = torch.nn.Parameter(torch.rand(1, 1) * 0.8)\n"
            ],
            "g_time": 12.158740520477295
        }
    }
}
{
    "opt": "unary=27",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min, max):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(16, 11, 1, stride=1, padding=0)\n        self.min = min\n        self.max = max\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = torch.clamp_min(v1, self.min)\n        v3 = torch.clamp_max(v2, self.max)\n        return v3\nmin = 1\nmax = -1\n# Inputs to the model\nx1 = torch.randn(1, 16, 12, 24)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min, max):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(89, 51, 1, stride=1, padding=0)\n        self.min = min\n        self.max = max\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = torch.clamp_min(v1, self.min)\n        v3 = torch.clamp_max(v2, self.max)\n        return v3\nmin = 0.2204519060628913\nmax = -0.4016851171706073\n# Inputs to the model\nx1 = torch.randn(1, 89, 27, 27)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min, max):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(2, 2, 5, stride=1, padding=3)\n        self.min = min\n        self.max = max\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = torch.clamp_min(v1, self.min)\n        v3 = torch.clamp_max(v2, self.max)\n        return v3\nmin = 0.4622644447597122\nmax = 0.5410665342330933\n# Inputs to the model\nx1 = torch.randn(9, 2, 22, 26)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min, max):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(20, 17, 1, stride=1, padding=2)\n        self.min = min\n        self.max = max\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = torch.clamp_min(v1, self.min)\n        v3 = torch.clamp_max(v2, self.max)\n        return v3\nmin = 0.22351846999516057\nmax = 0.07\n# Inputs to the model\nx1 = torch.randn(1, 20, 224, 224)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min, max):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(83, 35, 5, stride=4, padding=3)\n        self.min = min\n        self.max = max\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = torch.clamp_min(v1, self.min)\n        v3 = torch.clamp_max(v2, self.max)\n        return v3\nmin = 0.2584912429654222\nmax = 1.18402120825\n# Inputs to the model\nx1 = torch.randn(1, 83, 7, 27)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min):\n        super().__init__()\n        self.conv = torch.nn.Conv3d(3, 5, 1, stride=(1, 2, 2), padding=1)\n        self.min = min\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = torch.clamp_min(v1, self.min)\n        return v2\nmin = 5.257919054398539\n# Inputs to the model\nx1 = torch.randn(1, 3, 17, 7, 9)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min, max):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(98, 3, 9, stride=2, padding=0)\n        self.min = min\n        self.max = max\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = torch.clamp_min(v1, self.min)\n        v3 = torch.clamp_max(v2, self.max)\n        return v3\nmin = 0.4978923360489526\nmax = -0.513869123042744\n# Inputs to the model\nx1 = torch.randn(1, 98, 48, 48)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min, max):\n        super().__init__()\n        self.convolution = torch.nn.Conv2d(1, 2, (3, 5), stride=1)\n        self.activation = torch.nn.ReLU(inplace=False)\n        self.min = min\n        self.max = max\n    def forward(self, x1):\n        v1 = self.convolution(x1) \n        v2 = self.activation(v1)\n        v3 = torch.clamp_min(v2, self.min)\n        v4 = torch.clamp_max(v3, self.max)\n        return v4\nmin = 0\nmax = 256\n# Inputs to the model\nx1 = torch.randn(8,1,32,64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min, max):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(672, 113, 5, stride=2, padding=0)\n        self.min = min\n        self.max = max\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = torch.clamp_min(v1, self.min)\n        v3 = torch.clamp_max(v2, self.max)\n        return v3\nmin = -332\nmax = 455\n# Inputs to the model\nx1 = torch.randn(1, 672, 16, 11)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min, max):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(1, 46, 3, stride=1, padding=4, dilation=5)\n        self.min = min\n        self.max = max\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = torch.clamp_min(v1, self.min)\n        v3 = torch.clamp_max(v2, self.max)\n        return v3\nmin = -68.25299224853516\nmax = -32.71911811828613\n# Inputs to the model\nx1 = torch.randn(1, 1, 10, 10)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min, max):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(16, 11, 1, stride=1, padding=0)\n        self.min = min\n        self.max = max\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = torch.clamp_min(v1, self.min)\n        v3 = torch.clamp_max(v2, self.max)\n        return v3\nmin = 1\nmax = -1\n# Inputs to the model\nx1 = torch.randn(1, 16, 12, 24)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min, max):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(89, 51, 1, stride=1, padding=0)\n        self.min = min\n        self.max = max\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = torch.clamp_min(v1, self.min)\n        v3 = torch.clamp_max(v2, self.max)\n        return v3\nmin = 0.2204519060628913\nmax = -0.4016851171706073\n# Inputs to the model\nx1 = torch.randn(1, 89, 27, 27)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min, max):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(2, 2, 5, stride=1, padding=3)\n        self.min = min\n        self.max = max\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = torch.clamp_min(v1, self.min)\n        v3 = torch.clamp_max(v2, self.max)\n        return v3\nmin = 0.4622644447597122\nmax = 0.5410665342330933\n# Inputs to the model\nx1 = torch.randn(9, 2, 22, 26)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min, max):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(20, 17, 1, stride=1, padding=2)\n        self.min = min\n        self.max = max\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = torch.clamp_min(v1, self.min)\n        v3 = torch.clamp_max(v2, self.max)\n        return v3\nmin = 0.22351846999516057\nmax = 0.07\n# Inputs to the model\nx1 = torch.randn(1, 20, 224, 224)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min, max):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(83, 35, 5, stride=4, padding=3)\n        self.min = min\n        self.max = max\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = torch.clamp_min(v1, self.min)\n        v3 = torch.clamp_max(v2, self.max)\n        return v3\nmin = 0.2584912429654222\nmax = 1.18402120825\n# Inputs to the model\nx1 = torch.randn(1, 83, 7, 27)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min):\n        super().__init__()\n        self.conv = torch.nn.Conv3d(3, 5, 1, stride=(1, 2, 2), padding=1)\n        self.min = min\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = torch.clamp_min(v1, self.min)\n        return v2\nmin = 5.257919054398539\n# Inputs to the model\nx1 = torch.randn(1, 3, 17, 7, 9)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min, max):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(98, 3, 9, stride=2, padding=0)\n        self.min = min\n        self.max = max\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = torch.clamp_min(v1, self.min)\n        v3 = torch.clamp_max(v2, self.max)\n        return v3\nmin = 0.4978923360489526\nmax = -0.513869123042744\n# Inputs to the model\nx1 = torch.randn(1, 98, 48, 48)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min, max):\n        super().__init__()\n        self.convolution = torch.nn.Conv2d(1, 2, (3, 5), stride=1)\n        self.activation = torch.nn.ReLU(inplace=False)\n        self.min = min\n        self.max = max\n    def forward(self, x1):\n        v1 = self.convolution(x1) \n        v2 = self.activation(v1)\n        v3 = torch.clamp_min(v2, self.min)\n        v4 = torch.clamp_max(v3, self.max)\n        return v4\nmin = 0\nmax = 256\n# Inputs to the model\nx1 = torch.randn(8,1,32,64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min, max):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(672, 113, 5, stride=2, padding=0)\n        self.min = min\n        self.max = max\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = torch.clamp_min(v1, self.min)\n        v3 = torch.clamp_max(v2, self.max)\n        return v3\nmin = -332\nmax = 455\n# Inputs to the model\nx1 = torch.randn(1, 672, 16, 11)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min, max):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(1, 46, 3, stride=1, padding=4, dilation=5)\n        self.min = min\n        self.max = max\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = torch.clamp_min(v1, self.min)\n        v3 = torch.clamp_max(v2, self.max)\n        return v3\nmin = -68.25299224853516\nmax = -32.71911811828613\n# Inputs to the model\nx1 = torch.randn(1, 1, 10, 10)\n"
            ],
            "g_time": 7.264060020446777
        }
    }
}
{
    "opt": "unary=11",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.Sequential(torch.nn.ConvTranspose2d(10, 12, 7, stride=2, padding=0), torch.nn.Conv2d(12, 15, 1, stride=1, padding=0), torch.nn.ConvTranspose2d(15, 12, 7, stride=2, padding=0), torch.nn.Conv2d(12, 15, 1, stride=1, padding=0))\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v4 / 6\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 10, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.jit.script(torch.nn.ConvTranspose2d(1, 2, 2, stride=1))\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v4 / 6\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 1, 32, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(3, 3, kernel_size=(1, 4), padding=3)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v4 / 6\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 3, 150, 130)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.Sequential(torch.nn.ConvTranspose3d(10, 3, 4, stride=2, padding=3, output_padding=1), torch.nn.ReLU(inplace=True))\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v4 / 6\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 10, 40, 20, 10)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.Sequential(torch.nn.ConvTranspose2d(3, 76, 3, stride=2, padding=1), torch.nn.ReLU(inplace=True), torch.nn.ConvTranspose2d(76, 80, 1, stride=1, padding=0), torch.nn.ReLU(inplace=True), torch.nn.ConvTranspose2d(80, 192, 3, stride=2, padding=1), torch.nn.ReLU(inplace=True))\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v4 / 6\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 3, 128, 128)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(3, 2, 1, stride=1, padding=0)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v4 / 6\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 3, 28, 28)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(3, 64, 3, stride=(1, 2), padding=1, groups=4, output_padding=1)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v4 / 6\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 3, 128, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.Sequential(torch.nn.ConvTranspose2d(13, 90, 18, stride=18, padding=0), torch.nn.ReLU(inplace=True))\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v4 / 6\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 13, 20, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.Sequential(torch.nn.ConvTranspose2d(18432, 9216, 1, stride=1, padding=0), torch.nn.ReLU(inplace=True), torch.nn.ConvTranspose2d(9216, 9216, 1, stride=1, padding=0), torch.nn.ReLU(inplace=True), torch.nn.ConvTranspose2d(9216, 9216, 1, stride=1, padding=0), torch.nn.ReLU(inplace=True), torch.nn.ConvTranspose2d(9216, 9216, 1, stride=1, padding=0), torch.nn.ReLU(inplace=True), torch.nn.ConvTranspose2d(9216, 9216, 1, stride=1, padding=0), torch.nn.ReLU(inplace=True), torch.nn.ConvTranspose2d(9216, 9216, 1, stride=1, padding=0), torch.nn.ReLU(inplace=True), torch.nn.ConvTranspose2d(9216, 4608, 1, stride=1, padding=0), torch.nn.ReLU(inplace=True), torch.nn.ConvTranspose2d(4608, 4608, 1, stride=1, padding=0), torch.nn.ReLU(inplace=True), torch.nn.ConvTranspose2d(4608, 4608, 1, stride=1, padding=0), torch.nn.ReLU(inplace=True), torch.nn.ConvTranspose2d(4608, 9216, 1, stride=1, padding=0), torch.nn.ReLU(inplace=True), torch.nn.ConvTranspose2d(9216, 4608, 1, stride=1, padding=0), torch.nn.ReLU(inplace=True), torch.nn.ConvTranspose2d(4608, 4608, 1, stride=1, padding=0), torch.nn.ReLU(inplace=True), torch.nn.ConvTranspose2d(4608, 4608, 1, stride=1, padding=0), torch.nn.ReLU(inplace=True), torch.nn.ConvTranspose2d(4608, 4608, 1, stride=1, padding=0), torch.nn.ReLU(inplace=True), torch.nn.ConvTranspose2d(4608, 4608, 1, stride=1, padding=0), torch.nn.ReLU(inplace=True), torch.nn.ConvTranspose2d(4608, 4608, 1, stride=1, padding=0), torch.nn.ReLU(inplace=True), torch.nn.ConvTranspose2d(4608, 4608, 1, stride=1, padding=0), torch.nn.ReLU(inplace=True), torch.nn.ConvTranspose2d(4608, 4608, 1, stride=1, padding=0), torch.nn.ReLU(inplace=True), torch.nn.ConvTranspose2d(4608, 4608, 1, stride=1, padding=0), torch.nn.ReLU(inplace=True), torch.nn.ConvTranspose2d(4608, 4608, 1, stride=1, padding=0), torch.nn.ReLU(inplace=True), torch.nn.ConvTranspose2d(4608, 4608, 1, stride=1, padding=0), torch.nn.ReLU(inplace=True))\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v4 / 6\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 18432, 4, 4)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.Sequential(torch.nn.ConvTranspose2d(22, 20, 1, stride=1, padding=0), torch.nn.ReLU(inplace=True), torch.nn.ConvTranspose2d(20, 20, 1, stride=1, padding=0), torch.nn.ReLU(inplace=True), torch.nn.ConvTranspose2d(20, 20, 1, stride=1, padding=0), torch.nn.ReLU(inplace=True), torch.nn.ConvTranspose2d(20, 20, 1, stride=1, padding=0), torch.nn.ReLU(inplace=True), torch.nn.ConvTranspose2d(20, 20, 1, stride=1, padding=0), torch.nn.ReLU(inplace=True), torch.nn.ConvTranspose2d(20, 20, 1, stride=1, padding=0), torch.nn.ReLU(inplace=True), torch.nn.ConvTranspose2d(20, 20, 1, stride=1, padding=0), torch.nn.ReLU(inplace=True), torch.nn.ConvTranspose2d(20, 20, 1, stride=1, padding=0), torch.nn.ReLU(inplace=True), torch.nn.ConvTranspose2d(20, 20, 1, stride=1, padding=0), torch.nn.ReLU(inplace=True), torch.nn.ConvTranspose2d(20, 20, 1, stride=1, padding=0), torch.nn.ReLU(inplace=True), torch.nn.ConvTranspose2d(20, 20, 1, stride=1, padding=0), torch.nn.ReLU(inplace=True), torch.nn.ConvTranspose2d(20, 20, 1, stride=1, padding=0), torch.nn.ReLU(inplace=True), torch.nn.ConvTranspose2d(20, 20, 1, stride=1, padding=0), torch.nn.ReLU(inplace=True), torch.nn.ConvTranspose2d(20, 20, 1, stride=1, padding=0), torch.nn.ReLU(inplace=True), torch.nn.ConvTranspose2d(20, 20, 1, stride=1, padding=0), torch.nn.ReLU(inplace=True), torch.nn.ConvTranspose2d(20, 20, 1, stride=1, padding=0), torch.nn.ReLU(inplace=True), torch.nn.ConvTranspose2d(20, 20, 1, stride=1, padding=0), torch.nn.ReLU(inplace=True), torch.nn.ConvTranspose2d(20, 6, 1, stride=1, padding=0))\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v4 / 6\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 22, 67, 61)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.Sequential(torch.nn.ConvTranspose2d(10, 12, 7, stride=2, padding=0), torch.nn.Conv2d(12, 15, 1, stride=1, padding=0), torch.nn.ConvTranspose2d(15, 12, 7, stride=2, padding=0), torch.nn.Conv2d(12, 15, 1, stride=1, padding=0))\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v4 / 6\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 10, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.jit.script(torch.nn.ConvTranspose2d(1, 2, 2, stride=1))\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v4 / 6\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 1, 32, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(3, 3, kernel_size=(1, 4), padding=3)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v4 / 6\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 3, 150, 130)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.Sequential(torch.nn.ConvTranspose3d(10, 3, 4, stride=2, padding=3, output_padding=1), torch.nn.ReLU(inplace=True))\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v4 / 6\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 10, 40, 20, 10)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.Sequential(torch.nn.ConvTranspose2d(3, 76, 3, stride=2, padding=1), torch.nn.ReLU(inplace=True), torch.nn.ConvTranspose2d(76, 80, 1, stride=1, padding=0), torch.nn.ReLU(inplace=True), torch.nn.ConvTranspose2d(80, 192, 3, stride=2, padding=1), torch.nn.ReLU(inplace=True))\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v4 / 6\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 3, 128, 128)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(3, 2, 1, stride=1, padding=0)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v4 / 6\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 3, 28, 28)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(3, 64, 3, stride=(1, 2), padding=1, groups=4, output_padding=1)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v4 / 6\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 3, 128, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.Sequential(torch.nn.ConvTranspose2d(13, 90, 18, stride=18, padding=0), torch.nn.ReLU(inplace=True))\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v4 / 6\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 13, 20, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.Sequential(torch.nn.ConvTranspose2d(18432, 9216, 1, stride=1, padding=0), torch.nn.ReLU(inplace=True), torch.nn.ConvTranspose2d(9216, 9216, 1, stride=1, padding=0), torch.nn.ReLU(inplace=True), torch.nn.ConvTranspose2d(9216, 9216, 1, stride=1, padding=0), torch.nn.ReLU(inplace=True), torch.nn.ConvTranspose2d(9216, 9216, 1, stride=1, padding=0), torch.nn.ReLU(inplace=True), torch.nn.ConvTranspose2d(9216, 9216, 1, stride=1, padding=0), torch.nn.ReLU(inplace=True), torch.nn.ConvTranspose2d(9216, 9216, 1, stride=1, padding=0), torch.nn.ReLU(inplace=True), torch.nn.ConvTranspose2d(9216, 4608, 1, stride=1, padding=0), torch.nn.ReLU(inplace=True), torch.nn.ConvTranspose2d(4608, 4608, 1, stride=1, padding=0), torch.nn.ReLU(inplace=True), torch.nn.ConvTranspose2d(4608, 4608, 1, stride=1, padding=0), torch.nn.ReLU(inplace=True), torch.nn.ConvTranspose2d(4608, 9216, 1, stride=1, padding=0), torch.nn.ReLU(inplace=True), torch.nn.ConvTranspose2d(9216, 4608, 1, stride=1, padding=0), torch.nn.ReLU(inplace=True), torch.nn.ConvTranspose2d(4608, 4608, 1, stride=1, padding=0), torch.nn.ReLU(inplace=True), torch.nn.ConvTranspose2d(4608, 4608, 1, stride=1, padding=0), torch.nn.ReLU(inplace=True), torch.nn.ConvTranspose2d(4608, 4608, 1, stride=1, padding=0), torch.nn.ReLU(inplace=True), torch.nn.ConvTranspose2d(4608, 4608, 1, stride=1, padding=0), torch.nn.ReLU(inplace=True), torch.nn.ConvTranspose2d(4608, 4608, 1, stride=1, padding=0), torch.nn.ReLU(inplace=True), torch.nn.ConvTranspose2d(4608, 4608, 1, stride=1, padding=0), torch.nn.ReLU(inplace=True), torch.nn.ConvTranspose2d(4608, 4608, 1, stride=1, padding=0), torch.nn.ReLU(inplace=True), torch.nn.ConvTranspose2d(4608, 4608, 1, stride=1, padding=0), torch.nn.ReLU(inplace=True), torch.nn.ConvTranspose2d(4608, 4608, 1, stride=1, padding=0), torch.nn.ReLU(inplace=True), torch.nn.ConvTranspose2d(4608, 4608, 1, stride=1, padding=0), torch.nn.ReLU(inplace=True))\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v4 / 6\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 18432, 4, 4)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.Sequential(torch.nn.ConvTranspose2d(22, 20, 1, stride=1, padding=0), torch.nn.ReLU(inplace=True), torch.nn.ConvTranspose2d(20, 20, 1, stride=1, padding=0), torch.nn.ReLU(inplace=True), torch.nn.ConvTranspose2d(20, 20, 1, stride=1, padding=0), torch.nn.ReLU(inplace=True), torch.nn.ConvTranspose2d(20, 20, 1, stride=1, padding=0), torch.nn.ReLU(inplace=True), torch.nn.ConvTranspose2d(20, 20, 1, stride=1, padding=0), torch.nn.ReLU(inplace=True), torch.nn.ConvTranspose2d(20, 20, 1, stride=1, padding=0), torch.nn.ReLU(inplace=True), torch.nn.ConvTranspose2d(20, 20, 1, stride=1, padding=0), torch.nn.ReLU(inplace=True), torch.nn.ConvTranspose2d(20, 20, 1, stride=1, padding=0), torch.nn.ReLU(inplace=True), torch.nn.ConvTranspose2d(20, 20, 1, stride=1, padding=0), torch.nn.ReLU(inplace=True), torch.nn.ConvTranspose2d(20, 20, 1, stride=1, padding=0), torch.nn.ReLU(inplace=True), torch.nn.ConvTranspose2d(20, 20, 1, stride=1, padding=0), torch.nn.ReLU(inplace=True), torch.nn.ConvTranspose2d(20, 20, 1, stride=1, padding=0), torch.nn.ReLU(inplace=True), torch.nn.ConvTranspose2d(20, 20, 1, stride=1, padding=0), torch.nn.ReLU(inplace=True), torch.nn.ConvTranspose2d(20, 20, 1, stride=1, padding=0), torch.nn.ReLU(inplace=True), torch.nn.ConvTranspose2d(20, 20, 1, stride=1, padding=0), torch.nn.ReLU(inplace=True), torch.nn.ConvTranspose2d(20, 20, 1, stride=1, padding=0), torch.nn.ReLU(inplace=True), torch.nn.ConvTranspose2d(20, 20, 1, stride=1, padding=0), torch.nn.ReLU(inplace=True), torch.nn.ConvTranspose2d(20, 6, 1, stride=1, padding=0))\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v4 / 6\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 22, 67, 61)\n"
            ],
            "g_time": 33.3577721118927
        }
    }
}
{
    "opt": "unary=6",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.relu = torch.nn.ReLU(inplace=True)\n    def forward(self, x1):\n        v1 = self.relu(x1)\n        v2 = 3 + v1\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v1 * v4\n        v6 = v5 / 6\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 3, 224, 224)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 3, 3, stride=1, padding=1)\n        self.bn = torch.nn.BatchNorm2d(3)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = 3 + v1\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v1 * v4\n        v6 = v5 / 6\n        v7 = self.bn(v6)\n        return v7\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(4, 16, 3, stride=1, padding=1)\n        self.conv2 = torch.nn.Conv2d(16, 16, 3, stride=1, padding=3)\n    def forward(self, x1):\n        x = self.conv1(x1)\n        x = self.conv2(x)\n        return x\n# Inputs to the model\nx1 = torch.randn(1, 4, 16, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 3, 3, stride=1, padding=1)\n        self.relu = torch.nn.ReLU(inplace=True)\n        self.mul_ = torch.Tensor([5])\n        self.tanh = torch.nn.Tanh()\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = 3 + v1\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v1 * v4\n        v6 = v5 / 3\n        v7 = self.relu(v6)\n        v8 = v7 * self.mul_\n        v9 = self.tanh(v8)\n        return v9\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.conv1 = torch.nn.Conv2d(3, 64, 3, stride=1, padding=1)\n        self.bn1 = torch.nn.BatchNorm2d(64)\n        self.gpool1 = torch.nn.AdaptiveAvgPool2d(1)\n        self.conv2 = torch.nn.Conv2d(64, 128, 1, stride=1, padding=0)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = self.bn1(v1)\n        v3 = self.gpool1(v2)\n        v4 = self.conv2(v3)\n        v5 = v4 * 3\n        v6 = v5 - 6\n        v7 = v6 / -6\n        return v7\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 3, 7, stride=1, padding=3)\n        self.relu = torch.nn.ReLU(inplace=True)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = 3 + v1\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v1 * v4\n        v6 = v5 / 6\n        v7 = self.relu(v6)\n        return v7\n# Inputs to the model\nx1 = torch.randn(1, 3, 32, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 3, 1, stride=1, padding=0)\n        self.mul = torch.nn.functional.mul\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = torch.clamp_min(v1, 6)\n        v3 = torch.clamp_max(v2, 0)\n        return self.mul(v1, v3)\n# Inputs to the model\nx1 = torch.randn(1, 3, 32, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.bn = torch.nn.BatchNorm2d(1)\n    def forward(self, x1):\n        t1 = x1.reshape(x1.size()[0], x1.size()[1] * 3, 28, 28)\n        t2 = self.bn(t1)\n        return  x1.reshape(x1.size()[0], x1.size()[1], 28, 28)\n# Inputs to the model\nx1 = torch.randn(1, 3, 112, 112)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.quant0 = torch.quantization.QuantStub()\n        self.quant1 = torch.quantization.QuantStub()\n        self.conv0 = torch.nn.Conv2d(3, 3, 5, stride=1, padding=2)\n        self.bias0 = torch.nn.Parameter(torch.zeros(3, 3, 5, 5))\n        self.sigmoid = torch.nn.Sigmoid()\n        self.relu = torch.nn.ReLU(inplace=True)\n        self.conv1 = torch.nn.Conv2d(3, 3, 5, stride=1, padding=2)\n        self.bias1 = torch.nn.Parameter(torch.zeros(3, 3, 5, 5))\n        self.sigmoid = torch.nn.Sigmoid()\n        self.add = torch.nn.quantized.FloatFunctional()\n        self.quant2 = torch.quantization.DeQuantStub()\n        self.quant3 = torch.quantization.DeQuantStub()\n    def forward(self, x0, x1):\n        v0 = self.quant0(x0)\n        v1 = self.quant1(x1)\n        v2 = self.conv0(v1)\n        v2 = torch.quantize_per_tensor(v2, 0.05153478597640991, 0, torch.quint8)\n        v2 = torch.add(v2, self.bias0)\n        v2 = self.sigmoid(v2)\n        v3 = self.relu(v2)\n        v4 = self.conv1(v3)\n        v4 = torch.quantize_per_tensor(v4, 0.012193959850027084, 0, torch.quint8)\n        v4 = torch.add(v4, self.bias1)\n        v4 = self.sigmoid(v4)\n        v5 = self.add.add_relu(v0, v4)\n        v6 = self.quant2(v1)\n        v7 = self.quant3(v5)\n        return v7\n# Inputs to the model\nx0 = torch.randn(1, 3, 32, 32)\nx1 = torch.randn(1, 3, 32, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        num_classes = 2622\n        self.num_classes = num_classes\n        self.conv0 = nn.Conv2d(3, self.num_classes*16, 3,\n                               stride=1, padding=1)\n        self.blocks = nn.Sequential()\n        self.conv1 = nn.Conv2d(3, 64, 3,\n                               stride=1, padding=1)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv2d(64, self.num_classes*16, 1,\n                               stride=1, padding=0)\n        self.blocks.add_module('block1', self._make_layer())\n        self.conv3 = nn.Conv2d(3, 64, 3,\n                               stride=1, padding=1)\n        self.bn2 = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv4 = nn.Conv2d(128, self.num_classes*16, 1,\n                               stride=1, padding=0)\n        self.blocks.add_module('block1', self._make_layer())\n        self.conv5 = nn.Conv2d(3, 64, 3,\n                               stride=1, padding=1)\n        self.bn3 = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv6 = nn.Conv2d(128, self.num_classes*16, 1,\n                               stride=1, padding=0)\n        self.blocks.add_module('block1', self._make_layer())\n    def _make_layer(self):\n        layers = []\n        layers.apdend(self._make_block())\n        return nn.Sequential(*layers)\n    def _make_block(self):\n        layer1 = self.conv1(3, 64, 1, 1, 0)\n        layer2 = self.bn1(layer1)\n        layer3 = self.relu(layer2)\n        layer4 = self.conv2(layer3)\n        return layer4\n    def _make_layer2(self):\n        layers = []\n        layers.apdend(self._make_block())\n        return nn.Sequential(*layers)\n    def forward(self, x):\n        v1 = self.conv0(x)\n        v2 = v1.reshape(x.shape[0], self.num_classes, 16, x.shape[3], x.shape[4])\n        v3 = v2.permute([0, 1, 3, 4, 2])\n        h1 = self.relu(self.bn1(self.conv1(x)))\n        h2 = self.relu(self.bn2(self.conv3(h1)) + \\\n            self.conv4(self.relu(self.bn2(self.conv3(h1)))))\n        h3 = self.relu(self.bn3(self.conv5(x)) + \\\n            self.conv6(self.relu(self.bn3(self.conv5(x)))))\n        v4 = torch.cat([v3, h2, h3], 4)\n        x = self.blocks(v4)\n        return self.relu(self.conv7(x))\n# Inputs to the model\nx1 = torch.randn(1, 3, 32, 32)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.relu = torch.nn.ReLU(inplace=True)\n    def forward(self, x1):\n        v1 = self.relu(x1)\n        v2 = 3 + v1\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v1 * v4\n        v6 = v5 / 6\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 3, 224, 224)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 3, 3, stride=1, padding=1)\n        self.bn = torch.nn.BatchNorm2d(3)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = 3 + v1\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v1 * v4\n        v6 = v5 / 6\n        v7 = self.bn(v6)\n        return v7\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(4, 16, 3, stride=1, padding=1)\n        self.conv2 = torch.nn.Conv2d(16, 16, 3, stride=1, padding=3)\n    def forward(self, x1):\n        x = self.conv1(x1)\n        x = self.conv2(x)\n        return x\n# Inputs to the model\nx1 = torch.randn(1, 4, 16, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 3, 3, stride=1, padding=1)\n        self.relu = torch.nn.ReLU(inplace=True)\n        self.mul_ = torch.Tensor([5])\n        self.tanh = torch.nn.Tanh()\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = 3 + v1\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v1 * v4\n        v6 = v5 / 3\n        v7 = self.relu(v6)\n        v8 = v7 * self.mul_\n        v9 = self.tanh(v8)\n        return v9\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.conv1 = torch.nn.Conv2d(3, 64, 3, stride=1, padding=1)\n        self.bn1 = torch.nn.BatchNorm2d(64)\n        self.gpool1 = torch.nn.AdaptiveAvgPool2d(1)\n        self.conv2 = torch.nn.Conv2d(64, 128, 1, stride=1, padding=0)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = self.bn1(v1)\n        v3 = self.gpool1(v2)\n        v4 = self.conv2(v3)\n        v5 = v4 * 3\n        v6 = v5 - 6\n        v7 = v6 / -6\n        return v7\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 3, 7, stride=1, padding=3)\n        self.relu = torch.nn.ReLU(inplace=True)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = 3 + v1\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v1 * v4\n        v6 = v5 / 6\n        v7 = self.relu(v6)\n        return v7\n# Inputs to the model\nx1 = torch.randn(1, 3, 32, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 3, 1, stride=1, padding=0)\n        self.mul = torch.nn.functional.mul\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = torch.clamp_min(v1, 6)\n        v3 = torch.clamp_max(v2, 0)\n        return self.mul(v1, v3)\n# Inputs to the model\nx1 = torch.randn(1, 3, 32, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.bn = torch.nn.BatchNorm2d(1)\n    def forward(self, x1):\n        t1 = x1.reshape(x1.size()[0], x1.size()[1] * 3, 28, 28)\n        t2 = self.bn(t1)\n        return  x1.reshape(x1.size()[0], x1.size()[1], 28, 28)\n# Inputs to the model\nx1 = torch.randn(1, 3, 112, 112)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.quant0 = torch.quantization.QuantStub()\n        self.quant1 = torch.quantization.QuantStub()\n        self.conv0 = torch.nn.Conv2d(3, 3, 5, stride=1, padding=2)\n        self.bias0 = torch.nn.Parameter(torch.zeros(3, 3, 5, 5))\n        self.sigmoid = torch.nn.Sigmoid()\n        self.relu = torch.nn.ReLU(inplace=True)\n        self.conv1 = torch.nn.Conv2d(3, 3, 5, stride=1, padding=2)\n        self.bias1 = torch.nn.Parameter(torch.zeros(3, 3, 5, 5))\n        self.sigmoid = torch.nn.Sigmoid()\n        self.add = torch.nn.quantized.FloatFunctional()\n        self.quant2 = torch.quantization.DeQuantStub()\n        self.quant3 = torch.quantization.DeQuantStub()\n    def forward(self, x0, x1):\n        v0 = self.quant0(x0)\n        v1 = self.quant1(x1)\n        v2 = self.conv0(v1)\n        v2 = torch.quantize_per_tensor(v2, 0.05153478597640991, 0, torch.quint8)\n        v2 = torch.add(v2, self.bias0)\n        v2 = self.sigmoid(v2)\n        v3 = self.relu(v2)\n        v4 = self.conv1(v3)\n        v4 = torch.quantize_per_tensor(v4, 0.012193959850027084, 0, torch.quint8)\n        v4 = torch.add(v4, self.bias1)\n        v4 = self.sigmoid(v4)\n        v5 = self.add.add_relu(v0, v4)\n        v6 = self.quant2(v1)\n        v7 = self.quant3(v5)\n        return v7\n# Inputs to the model\nx0 = torch.randn(1, 3, 32, 32)\nx1 = torch.randn(1, 3, 32, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        num_classes = 2622\n        self.num_classes = num_classes\n        self.conv0 = nn.Conv2d(3, self.num_classes*16, 3,\n                               stride=1, padding=1)\n        self.blocks = nn.Sequential()\n        self.conv1 = nn.Conv2d(3, 64, 3,\n                               stride=1, padding=1)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv2d(64, self.num_classes*16, 1,\n                               stride=1, padding=0)\n        self.blocks.add_module('block1', self._make_layer())\n        self.conv3 = nn.Conv2d(3, 64, 3,\n                               stride=1, padding=1)\n        self.bn2 = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv4 = nn.Conv2d(128, self.num_classes*16, 1,\n                               stride=1, padding=0)\n        self.blocks.add_module('block1', self._make_layer())\n        self.conv5 = nn.Conv2d(3, 64, 3,\n                               stride=1, padding=1)\n        self.bn3 = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv6 = nn.Conv2d(128, self.num_classes*16, 1,\n                               stride=1, padding=0)\n        self.blocks.add_module('block1', self._make_layer())\n    def _make_layer(self):\n        layers = []\n        layers.apdend(self._make_block())\n        return nn.Sequential(*layers)\n    def _make_block(self):\n        layer1 = self.conv1(3, 64, 1, 1, 0)\n        layer2 = self.bn1(layer1)\n        layer3 = self.relu(layer2)\n        layer4 = self.conv2(layer3)\n        return layer4\n    def _make_layer2(self):\n        layers = []\n        layers.apdend(self._make_block())\n        return nn.Sequential(*layers)\n    def forward(self, x):\n        v1 = self.conv0(x)\n        v2 = v1.reshape(x.shape[0], self.num_classes, 16, x.shape[3], x.shape[4])\n        v3 = v2.permute([0, 1, 3, 4, 2])\n        h1 = self.relu(self.bn1(self.conv1(x)))\n        h2 = self.relu(self.bn2(self.conv3(h1)) + \\\n            self.conv4(self.relu(self.bn2(self.conv3(h1)))))\n        h3 = self.relu(self.bn3(self.conv5(x)) + \\\n            self.conv6(self.relu(self.bn3(self.conv5(x)))))\n        v4 = torch.cat([v3, h2, h3], 4)\n        x = self.blocks(v4)\n        return self.relu(self.conv7(x))\n# Inputs to the model\nx1 = torch.randn(1, 3, 32, 32)\n"
            ],
            "g_time": 26.553632020950317
        }
    }
}
{
    "opt": "replace_fx",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1):\n        x2 = torch.nn.functional.dropout(x1)\n        x3 = torch.mean(x1)\n        x3 = (x3, x1)\n        x4 = torch.rand_like(x3[1])\n        x5 = torch.rand_like(x3).values() # Dict\n        x6 = torch.rand_like(x3).values()[1] # Tensor\n        return x5\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, x2):\n        h1 = torch.nn.functional.gelu(x1)\n        h2 = torch.nn.functional.gelu(x1)\n        h3 = torch.nn.functional.gelu(x2)\n        h4 = torch.nn.ReLU()(h3)\n        return (h1, h4)\n# Inputs to the model\nx1 = torch.randn(1)\nx2 = torch.randn(1)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1):\n        x2 = torch.randint(2, (1,), dtype=torch.int32)\n        x3 = torch.randint(0, 3, (1,), dtype=torch.int32)\n        x4 = F.dropout(x1, p=float(x2), training=bool(x3))\n        return x4\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        init = torch.rand_like(x1)\n    def forward(self, x1):\n        x2 = init\n        x2 = F.dropout(x2, p=0.5) * x1\n        return x2\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, x2):\n        t1 = torch.rand_like(x1, dtype=torch.float32)\n        t2 = t1 * x2\n        return t2\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\nx2 = 1\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, x2):\n        t1 = torch.nn.Dropout2d(p=0.5)\n        t2 = t1(x1) # Use instance method of dropout2d to trigger a subgraph that has a pattern node that has args and is thus replaced\n        t3 = torch.rand_like(x2)\n        t4 = torch.nn.functional.dropout(t3, p=0.3) # Dropout has p=0.5, however rand_like will generate random values with p=0\n        t5 = torch.nn.functional.dropout(t2, p=0.5) # Dropout has p=0.5, which is already in the subgraph of t1, and t3 does not affect subgraph of t1, so this will be replaced as well\n        return (t4, t5)\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\nx2 = torch.randn(1)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1):\n        x2 = torch.rand_like(x1)\n        x3 = torch.rand_like(x1)\n        return x3\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1):\n        x2 = torch.nn.functional.dropout(x1, inplace=False)\n        x3 = torch.nn.functional.dropout(x2, inplace=False)\n        x4 = torch.nn.functional.dropout(x3, inplace=True)\n        x5 = torch.nn.functional.dropout(x4, inplace=False)\n        x6 = torch.nn.functional.dropout(x5, inplace=False)\n        return (x6)\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x):\n        t1 = torch.rand_like(x)\n        t3 = torch.rand_like(t1, dtype=torch.int)\n        t4 = torch.rand_like(t1, dtype=torch.bool)\n        t5 = torch.rand_like(t1, dtype=torch.long)\n        t6 = torch.rand_like(t1, dtype=torch.bfloat16)\n        t7 = torch.rand_like(t1, dtype=torch.QInt8)\n        t8 = torch.rand_like(t1, dtype=torch.quint8)\n        t9 = torch.rand_like(t1, dtype=torch.qint32)\n\n        return t1 + t3 + t4 + t5 + t6 + t7 + t8 + t9\n# Input to the model\nx = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1):\n        x1 = F.dropout(x1, p=0.1)\n        x2 = F.dropout(x1, p=0.2)\n        x3 = F.dropout(x1, p=0.25)\n        x4 = F.dropout(x1, p=0.02)\n        return (x1, x2, x3, x4)\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\n"
            ],
            "code": [
                "\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1):\n        x2 = torch.nn.functional.dropout(x1)\n        x3 = torch.mean(x1)\n        x3 = (x3, x1)\n        x4 = torch.rand_like(x3[1])\n        x5 = torch.rand_like(x3).values() # Dict\n        x6 = torch.rand_like(x3).values()[1] # Tensor\n        return x5\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, x2):\n        h1 = torch.nn.functional.gelu(x1)\n        h2 = torch.nn.functional.gelu(x1)\n        h3 = torch.nn.functional.gelu(x2)\n        h4 = torch.nn.ReLU()(h3)\n        return (h1, h4)\n# Inputs to the model\nx1 = torch.randn(1)\nx2 = torch.randn(1)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1):\n        x2 = torch.randint(2, (1,), dtype=torch.int32)\n        x3 = torch.randint(0, 3, (1,), dtype=torch.int32)\n        x4 = F.dropout(x1, p=float(x2), training=bool(x3))\n        return x4\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        init = torch.rand_like(x1)\n    def forward(self, x1):\n        x2 = init\n        x2 = F.dropout(x2, p=0.5) * x1\n        return x2\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, x2):\n        t1 = torch.rand_like(x1, dtype=torch.float32)\n        t2 = t1 * x2\n        return t2\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\nx2 = 1\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, x2):\n        t1 = torch.nn.Dropout2d(p=0.5)\n        t2 = t1(x1) # Use instance method of dropout2d to trigger a subgraph that has a pattern node that has args and is thus replaced\n        t3 = torch.rand_like(x2)\n        t4 = torch.nn.functional.dropout(t3, p=0.3) # Dropout has p=0.5, however rand_like will generate random values with p=0\n        t5 = torch.nn.functional.dropout(t2, p=0.5) # Dropout has p=0.5, which is already in the subgraph of t1, and t3 does not affect subgraph of t1, so this will be replaced as well\n        return (t4, t5)\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\nx2 = torch.randn(1)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1):\n        x2 = torch.rand_like(x1)\n        x3 = torch.rand_like(x1)\n        return x3\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1):\n        x2 = torch.nn.functional.dropout(x1, inplace=False)\n        x3 = torch.nn.functional.dropout(x2, inplace=False)\n        x4 = torch.nn.functional.dropout(x3, inplace=True)\n        x5 = torch.nn.functional.dropout(x4, inplace=False)\n        x6 = torch.nn.functional.dropout(x5, inplace=False)\n        return (x6)\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x):\n        t1 = torch.rand_like(x)\n        t3 = torch.rand_like(t1, dtype=torch.int)\n        t4 = torch.rand_like(t1, dtype=torch.bool)\n        t5 = torch.rand_like(t1, dtype=torch.long)\n        t6 = torch.rand_like(t1, dtype=torch.bfloat16)\n        t7 = torch.rand_like(t1, dtype=torch.QInt8)\n        t8 = torch.rand_like(t1, dtype=torch.quint8)\n        t9 = torch.rand_like(t1, dtype=torch.qint32)\n\n        return t1 + t3 + t4 + t5 + t6 + t7 + t8 + t9\n# Input to the model\nx = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1):\n        x1 = F.dropout(x1, p=0.1)\n        x2 = F.dropout(x1, p=0.2)\n        x3 = F.dropout(x1, p=0.25)\n        x4 = F.dropout(x1, p=0.02)\n        return (x1, x2, x3, x4)\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\n"
            ],
            "g_time": 8.03386926651001
        }
    }
}
{
    "opt": "unary=19",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(4, 1)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.sigmoid(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 4)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    \n    def forward(self, x_in):\n        v1 = torch.matmul(x_in, torch.tensor(np.random.uniform(-1.0, 1.0, [128, 128])\n        v2 = torch.sigmoid(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx_in = torch.randn(128)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.sigmoid(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(10, 10)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.sigmoid(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 10)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 1)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.sigmoid(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(256, 768)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.sigmoid(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 256)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n \n    def forward(self, x2):\n        v7 = self.linear(x2)\n        v8 = torch.sigmoid(v7)\n        return v8\n\n# Initializing the model\nm = Model()\n# Inputs to the model\nx2 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8, bias=False)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.sigmoid(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc1 = torch.nn.Linear(10, 10)\n \n    def forward(self, x1):\n        v1 = self.fc1(x1)\n        v2 = torch.sigmoid(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 10)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.sigmoid(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(4, 1)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.sigmoid(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 4)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    \n    def forward(self, x_in):\n        v1 = torch.matmul(x_in, torch.tensor(np.random.uniform(-1.0, 1.0, [128, 128])\n        v2 = torch.sigmoid(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx_in = torch.randn(128)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.sigmoid(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(10, 10)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.sigmoid(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 10)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 1)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.sigmoid(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(256, 768)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.sigmoid(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 256)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n \n    def forward(self, x2):\n        v7 = self.linear(x2)\n        v8 = torch.sigmoid(v7)\n        return v8\n\n# Initializing the model\nm = Model()\n# Inputs to the model\nx2 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8, bias=False)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.sigmoid(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc1 = torch.nn.Linear(10, 10)\n \n    def forward(self, x1):\n        v1 = self.fc1(x1)\n        v2 = torch.sigmoid(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 10)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.sigmoid(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n"
            ],
            "g_time": 4.792409896850586
        }
    }
}
{
    "opt": "unary=20",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_t = torch.nn.ConvTranspose2d(2, 6, kernel_size=1, stride=1, bias=False)\n    def forward(self, x1):\n        v1 = self.conv_t(x1)\n        v2 = torch.sigmoid(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(3, 2, 7, 7)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_t = torch.nn.ConvTranspose2d(3, 14, kernel_size=5, stride=7, padding=0)\n    def forward(self, x1):\n        v1 = self.conv_t(x1)\n        v2 = torch.sigmoid(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(3, 3, 96, 48)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_t = torch.nn.ConvTranspose2d(3, 2, kernel_size=3)\n        self.conv_t1 = torch.nn.ConvTranspose2d(2, 2, kernel_size=3)\n    def forward(self, x1):\n        x2 = self.conv_t(x1)\n        x3 = self.conv_t1(x2)\n        v2 = torch.sigmoid(x3)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 3, 336, 384)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_t1 = torch.nn.ConvTranspose2d(4, 4, kernel_size=4, stride=4)\n        self.conv_t2 = torch.nn.ConvTranspose2d(3, 6, kernel_size=3, stride=2)\n        self.conv_t3 = torch.nn.ConvTranspose2d(1, 2, kernel_size=3, stride=1)\n    def forward(self, x1):\n        v1 = self.conv_t1(x1)\n        v2 = self.conv_t2(v1)\n        v3 = self.conv_t3(v1)\n        v4 = torch.sigmoid(v2)\n        return v4\n# Inputs to the model\nx1 = torch.randn(2, 4, 7, 5)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_t = torch.nn.ConvTranspose2d(3, 3, kernel_size=3, stride=1)\n    def forward(self, x1):\n        v1 = self.conv_t(x1)\n        v2 = torch.sigmoid(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 3, 192, 192)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_t = torch.nn.ConvTranspose2d(3, 8, kernel_size=4, stride=2, groups=2)\n    def forward(self, x1):\n        v1 = self.conv_t(x1)\n        v2 = torch.sigmoid(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 3, 48, 36)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_t = torch.nn.ConvTranspose2d(1, 1, kernel_size=6, stride=1, padding=0, output_padding=1)\n    def forward(self, x1):\n        v1 = self.conv_t(x1)\n        v2 = torch.sigmoid(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 4, 8, 1)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_t = torch.nn.ConvTranspose2d(9, 7, kernel_size=4, stride=(1, 1))\n    def forward(self, x1):\n        v1 = self.conv_t(x1)\n        v2 = torch.sigmoid(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 9, 16, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_t = torch.nn.ConvTranspose2d(3, 8, kernel_size=5, stride=(1, 4), padding=3)\n    def forward(self, x1):\n        v1 = self.conv_t(x1)\n        v2 = torch.sigmoid(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(2, 3, 24, 24)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_t = torch.nn.ConvTranspose2d(6, 3, kernel_size=2, stride=(1, 2))\n    def forward(self, x1):\n        v1 = self.conv_t(x1)\n        v2 = torch.sigmoid(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 6, 145, 145)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_t = torch.nn.ConvTranspose2d(2, 6, kernel_size=1, stride=1, bias=False)\n    def forward(self, x1):\n        v1 = self.conv_t(x1)\n        v2 = torch.sigmoid(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(3, 2, 7, 7)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_t = torch.nn.ConvTranspose2d(3, 14, kernel_size=5, stride=7, padding=0)\n    def forward(self, x1):\n        v1 = self.conv_t(x1)\n        v2 = torch.sigmoid(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(3, 3, 96, 48)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_t = torch.nn.ConvTranspose2d(3, 2, kernel_size=3)\n        self.conv_t1 = torch.nn.ConvTranspose2d(2, 2, kernel_size=3)\n    def forward(self, x1):\n        x2 = self.conv_t(x1)\n        x3 = self.conv_t1(x2)\n        v2 = torch.sigmoid(x3)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 3, 336, 384)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_t1 = torch.nn.ConvTranspose2d(4, 4, kernel_size=4, stride=4)\n        self.conv_t2 = torch.nn.ConvTranspose2d(3, 6, kernel_size=3, stride=2)\n        self.conv_t3 = torch.nn.ConvTranspose2d(1, 2, kernel_size=3, stride=1)\n    def forward(self, x1):\n        v1 = self.conv_t1(x1)\n        v2 = self.conv_t2(v1)\n        v3 = self.conv_t3(v1)\n        v4 = torch.sigmoid(v2)\n        return v4\n# Inputs to the model\nx1 = torch.randn(2, 4, 7, 5)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_t = torch.nn.ConvTranspose2d(3, 3, kernel_size=3, stride=1)\n    def forward(self, x1):\n        v1 = self.conv_t(x1)\n        v2 = torch.sigmoid(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 3, 192, 192)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_t = torch.nn.ConvTranspose2d(3, 8, kernel_size=4, stride=2, groups=2)\n    def forward(self, x1):\n        v1 = self.conv_t(x1)\n        v2 = torch.sigmoid(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 3, 48, 36)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_t = torch.nn.ConvTranspose2d(1, 1, kernel_size=6, stride=1, padding=0, output_padding=1)\n    def forward(self, x1):\n        v1 = self.conv_t(x1)\n        v2 = torch.sigmoid(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 4, 8, 1)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_t = torch.nn.ConvTranspose2d(9, 7, kernel_size=4, stride=(1, 1))\n    def forward(self, x1):\n        v1 = self.conv_t(x1)\n        v2 = torch.sigmoid(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 9, 16, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_t = torch.nn.ConvTranspose2d(3, 8, kernel_size=5, stride=(1, 4), padding=3)\n    def forward(self, x1):\n        v1 = self.conv_t(x1)\n        v2 = torch.sigmoid(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(2, 3, 24, 24)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_t = torch.nn.ConvTranspose2d(6, 3, kernel_size=2, stride=(1, 2))\n    def forward(self, x1):\n        v1 = self.conv_t(x1)\n        v2 = torch.sigmoid(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 6, 145, 145)\n"
            ],
            "g_time": 7.0418384075164795
        }
    }
}
{
    "opt": "linear_permute_fusion",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(2, 2)\n    def forward(self, x1):\n        v1 = torch.nn.functional.linear(x1, self.linear.weight, self.linear.bias)\n        v2 = v1.transpose(1, 2)\n        lstm1 = torch.nn.LSTMCell(2, 2)\n        v3 = lstm1(v2)\n        v4 = v3.transpose(1, 2)\n        linear1 = torch.nn.Linear(2, 2)\n        v5 = linear1(v4)\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(480, 640)\n    def forward(self, x1):\n        v1 = torch.nn.functional.linear(x1, self.linear.weight, self.linear.bias).permute(0, 2, 1)\n        v2 = v1.permute(0, 2, 1)\n        v3 = torch.ops.aten.addmm(v1, v2, self.linear.weight)\n        return torch.ops.aten.bmm(v3, v1)\n# Inputs to the model\nx1 = torch.randn(1, 100, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(2, 2)\n    def forward(self, x0, x1):\n        v0 = torch.nn.functional.linear(x0, self.linear.weight, self.linear.bias).permute(0, 2, 1)\n        lstm1 = torch.nn.LSTMCell(3, 2)\n        v1 = lstm1(v0)\n        v2 = v1.transpose(0, 1)\n        v3 = v2.transpose(0, 1)\n        v4 = torch.nn.functional.linear(x1, self.linear.weight, self.linear.bias).permute(0, 2, 1)\n        lstm2 = torch.nn.LSTMCell(3, 2)\n        v5 = lstm2(v4)\n        v6 = v5.transpose(0, 1)\n        v7 = torch.nn.functional.linear(v6, self.linear.weight, self.linear.bias).permute(0, 2, 1)\n        linear1 = torch.nn.Linear(2, 2)\n        v8 = linear1(v7)\n        return v8.permute(0, 2, 1)\n# Inputs to the model\nx0 = torch.randn(1, 3, 2)\nx1 = torch.randn(1, 3, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear1 = torch.nn.Linear(3, 3)\n        self.linear2 = torch.nn.Linear(3, 5)\n    def forward(self, x0):\n        v0 = torch.nn.functional.linear(x0, self.linear1.weight, self.linear1.bias)\n        linear2 = torch.nn.Linear(3, 5)\n        v1 = linear2(v0)\n        return v1\n# Inputs to the model\nx0 = torch.randn(1, 15, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(2, 2)\n    def forward(self, x2):\n        v2 = torch.nn.functional.linear(x2, self.linear.weight, self.linear.bias).permute(0, 2, 1)\n        lstm1 = torch.nn.LSTMCell(4, 2)\n        v3 = lstm1(v2)\n        linear1 = torch.nn.Linear(2, 2)\n        v3 = torch.nn.functional.linear(v3, linear1.weight, linear1.bias).permute(0, 2, 1)\n        lstm2 = torch.nn.LSTMCell(4, 2)\n        v4 = lstm2(v3)\n        linear2 = torch.nn.Linear(2, 2)\n        v5 =linear2(v4)\n        return v5.permute(0, 2, 1)\n# Inputs to the model\nx2 = torch.randn(1, 3, 2) \n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(2, 2)\n    def forward(self, x0):\n        v0 = torch.nn.functional.linear(x0, self.linear.weight, self.linear.bias).permute(0, 2, 1)\n        lstm1 = torch.nn.LSTMCell(2, 2)\n        v1 = lstm1(v0)\n        v2 = v1.permute(0, 2, 1)\n        lstm2 = torch.nn.LSTMCell(2, 2)\n        v3 = lstm2(v2)\n        return v3.permute(0, 2, 1).permute(0, 2, 1)\n# Inputs to the model\nx0 = torch.randn(1, 3, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(4, 10)\n\n    def forward(self, x0):\n        v0 = torch.nn.functional.linear(x0, self.linear.weight, self.linear.bias)\n        v1 = v0.permute(0, 2, 3, 1)\n        lstm1 = torch.nn.LSTMCell(10, 16)\n        v2 = lstm1(v1)\n        v6 = torch.nn.functional.linear(x0, self.linear.weight, self.linear.bias)\n        v7 = v6.permute(0, 2, 3, 1)\n        lstm2 = torch.nn.LSTMCell(10, 8)\n        v8 = lstm2(v7)\n        return v8.permute(0, 3, 2, 1) + v2 * 2\n# Inputs to the model\nx0 = torch.randn(1, 3, 10, 10)\nx1 = torch.randn(1, 4, 10, 10)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear1 = torch.nn.Linear(2, 2)\n        self.linear2 = torch.nn.Linear(2, 2)\n    def forward(self, x1):\n        v1 = torch.nn.functional.linear(x1, self.linear2.weight, self.linear2.bias).permute(0, 2, 1)\n        v2 = v1.permute(0, 2, 1)\n        return v2.permute(0, 2, 1)\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(2, 2)\n    def forward(self, x0):\n        v0 = torch.nn.functional.linear(x0, self.linear.weight, self.linear.bias).permute(0, 2, 1)\n        lstm1 = torch.nn.LSTMCell(4, 2)\n        v1 = lstm1(v0)\n        v2 = v1.permute(0, 2, 1)\n        linear1 = torch.nn.Linear(2, 2)\n        v3 = linear1(v2)\n        return v3\n# Inputs to the model\nx0 = torch.randn(1, 3, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(2, 2)\n    def forward(self, x1):\n        linear1 = torch.nn.Linear(2, 2)\n        v1 = torch.nn.functional.linear(x1, linear1.weight, linear1.bias)\n        v2 = torch.nn.functional.linear(v1, self.linear.weight, self.linear.bias)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(2, 2)\n    def forward(self, x1):\n        v1 = torch.nn.functional.linear(x1, self.linear.weight, self.linear.bias)\n        v2 = v1.transpose(1, 2)\n        lstm1 = torch.nn.LSTMCell(2, 2)\n        v3 = lstm1(v2)\n        v4 = v3.transpose(1, 2)\n        linear1 = torch.nn.Linear(2, 2)\n        v5 = linear1(v4)\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(480, 640)\n    def forward(self, x1):\n        v1 = torch.nn.functional.linear(x1, self.linear.weight, self.linear.bias).permute(0, 2, 1)\n        v2 = v1.permute(0, 2, 1)\n        v3 = torch.ops.aten.addmm(v1, v2, self.linear.weight)\n        return torch.ops.aten.bmm(v3, v1)\n# Inputs to the model\nx1 = torch.randn(1, 100, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(2, 2)\n    def forward(self, x0, x1):\n        v0 = torch.nn.functional.linear(x0, self.linear.weight, self.linear.bias).permute(0, 2, 1)\n        lstm1 = torch.nn.LSTMCell(3, 2)\n        v1 = lstm1(v0)\n        v2 = v1.transpose(0, 1)\n        v3 = v2.transpose(0, 1)\n        v4 = torch.nn.functional.linear(x1, self.linear.weight, self.linear.bias).permute(0, 2, 1)\n        lstm2 = torch.nn.LSTMCell(3, 2)\n        v5 = lstm2(v4)\n        v6 = v5.transpose(0, 1)\n        v7 = torch.nn.functional.linear(v6, self.linear.weight, self.linear.bias).permute(0, 2, 1)\n        linear1 = torch.nn.Linear(2, 2)\n        v8 = linear1(v7)\n        return v8.permute(0, 2, 1)\n# Inputs to the model\nx0 = torch.randn(1, 3, 2)\nx1 = torch.randn(1, 3, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear1 = torch.nn.Linear(3, 3)\n        self.linear2 = torch.nn.Linear(3, 5)\n    def forward(self, x0):\n        v0 = torch.nn.functional.linear(x0, self.linear1.weight, self.linear1.bias)\n        linear2 = torch.nn.Linear(3, 5)\n        v1 = linear2(v0)\n        return v1\n# Inputs to the model\nx0 = torch.randn(1, 15, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(2, 2)\n    def forward(self, x2):\n        v2 = torch.nn.functional.linear(x2, self.linear.weight, self.linear.bias).permute(0, 2, 1)\n        lstm1 = torch.nn.LSTMCell(4, 2)\n        v3 = lstm1(v2)\n        linear1 = torch.nn.Linear(2, 2)\n        v3 = torch.nn.functional.linear(v3, linear1.weight, linear1.bias).permute(0, 2, 1)\n        lstm2 = torch.nn.LSTMCell(4, 2)\n        v4 = lstm2(v3)\n        linear2 = torch.nn.Linear(2, 2)\n        v5 =linear2(v4)\n        return v5.permute(0, 2, 1)\n# Inputs to the model\nx2 = torch.randn(1, 3, 2) \n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(2, 2)\n    def forward(self, x0):\n        v0 = torch.nn.functional.linear(x0, self.linear.weight, self.linear.bias).permute(0, 2, 1)\n        lstm1 = torch.nn.LSTMCell(2, 2)\n        v1 = lstm1(v0)\n        v2 = v1.permute(0, 2, 1)\n        lstm2 = torch.nn.LSTMCell(2, 2)\n        v3 = lstm2(v2)\n        return v3.permute(0, 2, 1).permute(0, 2, 1)\n# Inputs to the model\nx0 = torch.randn(1, 3, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(4, 10)\n\n    def forward(self, x0):\n        v0 = torch.nn.functional.linear(x0, self.linear.weight, self.linear.bias)\n        v1 = v0.permute(0, 2, 3, 1)\n        lstm1 = torch.nn.LSTMCell(10, 16)\n        v2 = lstm1(v1)\n        v6 = torch.nn.functional.linear(x0, self.linear.weight, self.linear.bias)\n        v7 = v6.permute(0, 2, 3, 1)\n        lstm2 = torch.nn.LSTMCell(10, 8)\n        v8 = lstm2(v7)\n        return v8.permute(0, 3, 2, 1) + v2 * 2\n# Inputs to the model\nx0 = torch.randn(1, 3, 10, 10)\nx1 = torch.randn(1, 4, 10, 10)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear1 = torch.nn.Linear(2, 2)\n        self.linear2 = torch.nn.Linear(2, 2)\n    def forward(self, x1):\n        v1 = torch.nn.functional.linear(x1, self.linear2.weight, self.linear2.bias).permute(0, 2, 1)\n        v2 = v1.permute(0, 2, 1)\n        return v2.permute(0, 2, 1)\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(2, 2)\n    def forward(self, x0):\n        v0 = torch.nn.functional.linear(x0, self.linear.weight, self.linear.bias).permute(0, 2, 1)\n        lstm1 = torch.nn.LSTMCell(4, 2)\n        v1 = lstm1(v0)\n        v2 = v1.permute(0, 2, 1)\n        linear1 = torch.nn.Linear(2, 2)\n        v3 = linear1(v2)\n        return v3\n# Inputs to the model\nx0 = torch.randn(1, 3, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(2, 2)\n    def forward(self, x1):\n        linear1 = torch.nn.Linear(2, 2)\n        v1 = torch.nn.functional.linear(x1, linear1.weight, linear1.bias)\n        v2 = torch.nn.functional.linear(v1, self.linear.weight, self.linear.bias)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\n"
            ],
            "g_time": 10.848720073699951
        }
    }
}
{
    "opt": "unary=26",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.conv_t = torch.nn.ConvTranspose2d(90, 39, 8, stride=4, padding=2, bias=True)\n    def forward(self, x10):\n        m1 = self.conv_t(x10)\n        m2 = m1 > 0\n        m3 = m1 * -0.101\n        m4 = torch.where(m2, m1, m3)\n        return torch.nn.functional.pad(m4, (4, 3, 2, 33))\n# Inputs to the model\nx10 = torch.randn(8, 90, 16, 84)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_t = torch.nn.ConvTranspose2d(38, 112, 10, stride=1, padding=6, bias=False)\n        self.linear0 = torch.nn.Linear(13, 34)\n    def forward(self, x9):\n        m1 = self.conv_t(x9)\n        m2 = m1 > 0\n        m3 = m1 * 0.29893\n        m4 = torch.where(m2, m1, m3)\n        m5 = torch.nn.functional.adaptive_avg_pool2d(m4, (4, 4))\n        m6 = self.linear0(m5.flatten(1))\n        m7 = torch.nn.functional.softmax(m6, dim=-1)\n\n        return m6\n# Inputs to the model\nx9 = torch.randn(2, 38, 21, 24)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_t = torch.nn.ConvTranspose2d(151, 247, 5, stride=1, padding=1, groups=2, bias=False)\n    def forward(self, x23):\n        i1 = self.conv_t(x23)\n        return torch.nn.functional.adaptive_avg_pool2d(i1, (7, 2))\n# Inputs to the model\nx23 = torch.randn(19, 151, 12, 19)\n",
                "\nclass Network(nn.Module):\n    def __init__(self):\n        super(Network, self).__init__()\n        self.conv_t = nn.Sequential(\n        nn.ConvTranspose2d(480, 192, 3, stride=2, bias=False),\n        nn.BatchNorm2d(192), )\n    def forward(self, x49):\n        y1 = self.conv_t(x49)\n        y2 = torch.nn.functional.leaky_relu(y1)\n        y3 = torch.nn.functional.adaptive_avg_pool2d(y2, (1, 1))\n        return y3\n# Inputs to the model\nx49 = torch.randn(3, 480, 13, 24)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.conv_t = torch.nn.ConvTranspose1d(334, 418, 1, stride=1, padding=0, bias=True)\n    def forward(self, x19):\n        u1 = self.conv_t(x19)\n        u2 = u1 > 0\n        u3 = u1 * -0.162322\n        u4 = torch.where(u2, u1, u3)\n        return torch.nn.functional.adaptive_avg_pool2d(u4, (7, 5))\n# Inputs to the model\nx19 = torch.randn(4, 334, 24)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_t = torch.nn.ConvTranspose2d(986, 702, 5, stride=2, padding=2, output_padding=1, bias=True)\n    def forward(self, x18):\n        j1 = self.conv_t(x18)\n        j2 = j1 > 0\n        j3 = j1 * -8.199585\n        j4 = torch.where(j2, j1, j3)\n        return torch.nn.functional.adaptive_avg_pool2d(j4, (22, 41))\n# Inputs to the model\nx18 = torch.randn(7, 986, 13, 20)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_t = torch.nn.ConvTranspose2d(29, 6, 2, stride=1, padding=0, bias=False)\n    def forward(self, x49):\n        b1 = self.conv_t(x49)\n        b2 = b1 > 0\n        b3 = b1 * 0.000\n        b4 = torch.where(b2, b1, b3)\n        return b4\n# Inputs to the model\nx49 = torch.randn(1, 29, 30, 6)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.conv_t = torch.nn.ConvTranspose2d(19, 167, 3, stride=1, padding=1, bias=False)\n    def forward(self, x16):\n        t1 = self.conv_t(x16)\n        t2 = t1 > 0\n        t3 = t1 * -0.651877\n        t4 = torch.where(t2, t1, t3)\n        return torch.nn.functional.pad(t4, (4, 4, 2, 0))\n# Inputs to the model\nx16 = torch.randn(3, 19, 11, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.conv_t = torch.nn.ConvTranspose2d(51, 264, 1, stride=1, padding=0, bias=True)\n    def forward(self, x18):\n        l1 = self.conv_t(x18)\n        l2 = l1 > 0\n        l3 = l1 * -3.3682\n        l4 = torch.where(l2, l1, l3)\n        return torch.nn.functional.adaptive_avg_pool2d(l4, (1, 1))\n# Inputs to the model\nx18 = torch.randn(11, 51, 13, 24)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_t = torch.nn.ConvTranspose2d(7, 8, 6, stride=1, padding=3, bias=False)\n    def forward(self, x32):\n        z1 = self.conv_t(x32)\n        z2 = z1 > 0\n        z3 = z1 * 0.554\n        z4 = torch.where(z2, z1, z3)\n        return z4\n# Inputs to the model\nx32 = torch.randn(6, 7, 11, 14)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.conv_t = torch.nn.ConvTranspose2d(90, 39, 8, stride=4, padding=2, bias=True)\n    def forward(self, x10):\n        m1 = self.conv_t(x10)\n        m2 = m1 > 0\n        m3 = m1 * -0.101\n        m4 = torch.where(m2, m1, m3)\n        return torch.nn.functional.pad(m4, (4, 3, 2, 33))\n# Inputs to the model\nx10 = torch.randn(8, 90, 16, 84)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_t = torch.nn.ConvTranspose2d(38, 112, 10, stride=1, padding=6, bias=False)\n        self.linear0 = torch.nn.Linear(13, 34)\n    def forward(self, x9):\n        m1 = self.conv_t(x9)\n        m2 = m1 > 0\n        m3 = m1 * 0.29893\n        m4 = torch.where(m2, m1, m3)\n        m5 = torch.nn.functional.adaptive_avg_pool2d(m4, (4, 4))\n        m6 = self.linear0(m5.flatten(1))\n        m7 = torch.nn.functional.softmax(m6, dim=-1)\n\n        return m6\n# Inputs to the model\nx9 = torch.randn(2, 38, 21, 24)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_t = torch.nn.ConvTranspose2d(151, 247, 5, stride=1, padding=1, groups=2, bias=False)\n    def forward(self, x23):\n        i1 = self.conv_t(x23)\n        return torch.nn.functional.adaptive_avg_pool2d(i1, (7, 2))\n# Inputs to the model\nx23 = torch.randn(19, 151, 12, 19)\n",
                "\nclass Network(nn.Module):\n    def __init__(self):\n        super(Network, self).__init__()\n        self.conv_t = nn.Sequential(\n        nn.ConvTranspose2d(480, 192, 3, stride=2, bias=False),\n        nn.BatchNorm2d(192), )\n    def forward(self, x49):\n        y1 = self.conv_t(x49)\n        y2 = torch.nn.functional.leaky_relu(y1)\n        y3 = torch.nn.functional.adaptive_avg_pool2d(y2, (1, 1))\n        return y3\n# Inputs to the model\nx49 = torch.randn(3, 480, 13, 24)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.conv_t = torch.nn.ConvTranspose1d(334, 418, 1, stride=1, padding=0, bias=True)\n    def forward(self, x19):\n        u1 = self.conv_t(x19)\n        u2 = u1 > 0\n        u3 = u1 * -0.162322\n        u4 = torch.where(u2, u1, u3)\n        return torch.nn.functional.adaptive_avg_pool2d(u4, (7, 5))\n# Inputs to the model\nx19 = torch.randn(4, 334, 24)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_t = torch.nn.ConvTranspose2d(986, 702, 5, stride=2, padding=2, output_padding=1, bias=True)\n    def forward(self, x18):\n        j1 = self.conv_t(x18)\n        j2 = j1 > 0\n        j3 = j1 * -8.199585\n        j4 = torch.where(j2, j1, j3)\n        return torch.nn.functional.adaptive_avg_pool2d(j4, (22, 41))\n# Inputs to the model\nx18 = torch.randn(7, 986, 13, 20)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_t = torch.nn.ConvTranspose2d(29, 6, 2, stride=1, padding=0, bias=False)\n    def forward(self, x49):\n        b1 = self.conv_t(x49)\n        b2 = b1 > 0\n        b3 = b1 * 0.000\n        b4 = torch.where(b2, b1, b3)\n        return b4\n# Inputs to the model\nx49 = torch.randn(1, 29, 30, 6)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.conv_t = torch.nn.ConvTranspose2d(19, 167, 3, stride=1, padding=1, bias=False)\n    def forward(self, x16):\n        t1 = self.conv_t(x16)\n        t2 = t1 > 0\n        t3 = t1 * -0.651877\n        t4 = torch.where(t2, t1, t3)\n        return torch.nn.functional.pad(t4, (4, 4, 2, 0))\n# Inputs to the model\nx16 = torch.randn(3, 19, 11, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.conv_t = torch.nn.ConvTranspose2d(51, 264, 1, stride=1, padding=0, bias=True)\n    def forward(self, x18):\n        l1 = self.conv_t(x18)\n        l2 = l1 > 0\n        l3 = l1 * -3.3682\n        l4 = torch.where(l2, l1, l3)\n        return torch.nn.functional.adaptive_avg_pool2d(l4, (1, 1))\n# Inputs to the model\nx18 = torch.randn(11, 51, 13, 24)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_t = torch.nn.ConvTranspose2d(7, 8, 6, stride=1, padding=3, bias=False)\n    def forward(self, x32):\n        z1 = self.conv_t(x32)\n        z2 = z1 > 0\n        z3 = z1 * 0.554\n        z4 = torch.where(z2, z1, z3)\n        return z4\n# Inputs to the model\nx32 = torch.randn(6, 7, 11, 14)\n"
            ],
            "g_time": 8.823967695236206
        }
    }
}
{
    "opt": "permute_linear_fusion",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear1 = torch.nn.Linear(2, 2)\n        self.linear2 = torch.nn.Linear(2, 2)\n    def forward(self, x1):\n        v1 = torch.nn.functional.tanh(x1)\n        v2 = torch.nn.functional.relu(v1)\n        v2 = torch.nn.functional.linear(v2, self.linear1.weight, self.linear1.bias)\n        v2 = v2.permute(0, 2, 1)\n        v3 = torch.nn.functional.linear(v2, self.linear2.weight, self.linear2.bias)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(2, 2)\n    def forward(self, x1):\n        v1 = torch.nn.functional.tanh(x1).permute(0, 2, 1)\n        v2 = torch.nn.functional.linear(v1, self.linear.weight, self.linear.bias)\n        x2 = torch.nn.functional.relu(v2)\n        v3 = x2.detach()\n        v3 = torch.nn.functional.tanh(v3)\n        v4 = v3 * v1\n        v4 = v4 + v1\n        x2 = x2.permute(0, 2, 1)\n        return x2.permute(0, 2, 1).matmul(v4)\ndef func(x):\n    v1 = torch.nn.functional.tanh(x).permute(0, 2, 1)\n    v2 = torch.nn.functional.linear(v1, self.linear.weight, self.linear.bias)\n    x2 = torch.nn.functional.relu(v2)\n    v3 = x2.detach()\n    v3 = torch.nn.functional.tanh(v3)\n    v4 = v3 * v1\n    v4 = v4 + v1\n    x2 = x2.permute(0, 2, 1)\n    return x2.permute(0, 2, 1).matmul(v4)\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.linear = torch.nn.Linear(2, 2)\n        self.func = func\n    def forward(self, x1):\n        return self.func(x1)\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(2, 2)\n    def forward(self, x1):\n        v1 = torch.nn.functional.tanh(x1)\n        v2 = v1.permute(0, 2, 1)\n        v2 = torch.nn.functional.relu(v2)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(2, 2)\n    def forward(self, x1):\n        v1 = torch.nn.functional.tanh(x1)\n        v2 = torch.autograd.grad(v1, x1, v1)\n        v3 = torch.nn.functional.cos(self.linear.weight)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 2, 2, requires_grad=True)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(2, 2)\n    def forward(self, x1):\n        v1 = torch.nn.functional.tanh(x1).permute(0, 2, 1)\n        v2 = torch.nn.functional.linear(v1, self.linear.weight, self.linear.bias)\n        x2 = torch.nn.functional.relu(v2)\n        v3 = x2.detach()\n        v3 = torch.nn.functional.tanh(v3)\n        v4 = v3 * v1\n        v4 = v4 + v1\n        # This last block is a reshaping operation where the permute function is used\n        v5 = torch.nn.functional.linear(x1, v4.reshape_as(x1), self.linear.bias)\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(2, 2)\n    def forward(self, x1):\n        v1 = torch.nn.functional.tanh(x1)\n        v1 = v1.abs()\n        v1 = v1.ceil()\n        v1 = v1.detach()\n        v2 = torch.nn.functional.linear(v1, self.linear.weight, self.linear.bias)\n        x2 = torch.nn.functional.relu(v2)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.linear = torch.nn.Linear(2, 2)\n    def forward(self, x1):\n        v1 = torch.nn.functional.tanh(x1).permute(0, 1)\n        v2 = torch.nn.functional.linear(v1, self.linear.weight, self.linear.bias)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(2, 2)\n    def forward(self, x1):\n        v1 = x1.permute(0, 2, 1)\n        v2 = torch.nn.functional.linear(v1, self.linear.weight, self.linear.bias)\n        x2 = torch.nn.functional.relu(v2)\n        return x2.permute(1, 2, 0)\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.linear = torch.nn.Linear(8, 8)\n    def forward(self, x1):\n        v1 = torch.nn.functional.tanh(x1).permute(0, 2, 1)\n        v2 = torch.nn.functional.linear(v1, self.linear.weight, self.linear.bias)\n        x2 = torch.nn.functional.relu(v2)\n        return x2\n# Inputs to the model\nx1 = torch.randn(1, 8, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(2, 2)\n    def forward(self, x1):\n        v1 = x1.permute(0, 2, 1)\n        v2 = torch.nn.functional.linear(v1, self.linear.weight, self.linear.bias)\n        v3 = torch.nn.functional.sigmoid(v2)\n        x2 = v3 * v1\n        return x2\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear1 = torch.nn.Linear(2, 2)\n        self.linear2 = torch.nn.Linear(2, 2)\n    def forward(self, x1):\n        v1 = torch.nn.functional.tanh(x1)\n        v2 = torch.nn.functional.relu(v1)\n        v2 = torch.nn.functional.linear(v2, self.linear1.weight, self.linear1.bias)\n        v2 = v2.permute(0, 2, 1)\n        v3 = torch.nn.functional.linear(v2, self.linear2.weight, self.linear2.bias)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(2, 2)\n    def forward(self, x1):\n        v1 = torch.nn.functional.tanh(x1).permute(0, 2, 1)\n        v2 = torch.nn.functional.linear(v1, self.linear.weight, self.linear.bias)\n        x2 = torch.nn.functional.relu(v2)\n        v3 = x2.detach()\n        v3 = torch.nn.functional.tanh(v3)\n        v4 = v3 * v1\n        v4 = v4 + v1\n        x2 = x2.permute(0, 2, 1)\n        return x2.permute(0, 2, 1).matmul(v4)\ndef func(x):\n    v1 = torch.nn.functional.tanh(x).permute(0, 2, 1)\n    v2 = torch.nn.functional.linear(v1, self.linear.weight, self.linear.bias)\n    x2 = torch.nn.functional.relu(v2)\n    v3 = x2.detach()\n    v3 = torch.nn.functional.tanh(v3)\n    v4 = v3 * v1\n    v4 = v4 + v1\n    x2 = x2.permute(0, 2, 1)\n    return x2.permute(0, 2, 1).matmul(v4)\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.linear = torch.nn.Linear(2, 2)\n        self.func = func\n    def forward(self, x1):\n        return self.func(x1)\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(2, 2)\n    def forward(self, x1):\n        v1 = torch.nn.functional.tanh(x1)\n        v2 = v1.permute(0, 2, 1)\n        v2 = torch.nn.functional.relu(v2)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(2, 2)\n    def forward(self, x1):\n        v1 = torch.nn.functional.tanh(x1)\n        v2 = torch.autograd.grad(v1, x1, v1)\n        v3 = torch.nn.functional.cos(self.linear.weight)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 2, 2, requires_grad=True)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(2, 2)\n    def forward(self, x1):\n        v1 = torch.nn.functional.tanh(x1).permute(0, 2, 1)\n        v2 = torch.nn.functional.linear(v1, self.linear.weight, self.linear.bias)\n        x2 = torch.nn.functional.relu(v2)\n        v3 = x2.detach()\n        v3 = torch.nn.functional.tanh(v3)\n        v4 = v3 * v1\n        v4 = v4 + v1\n        # This last block is a reshaping operation where the permute function is used\n        v5 = torch.nn.functional.linear(x1, v4.reshape_as(x1), self.linear.bias)\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(2, 2)\n    def forward(self, x1):\n        v1 = torch.nn.functional.tanh(x1)\n        v1 = v1.abs()\n        v1 = v1.ceil()\n        v1 = v1.detach()\n        v2 = torch.nn.functional.linear(v1, self.linear.weight, self.linear.bias)\n        x2 = torch.nn.functional.relu(v2)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.linear = torch.nn.Linear(2, 2)\n    def forward(self, x1):\n        v1 = torch.nn.functional.tanh(x1).permute(0, 1)\n        v2 = torch.nn.functional.linear(v1, self.linear.weight, self.linear.bias)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(2, 2)\n    def forward(self, x1):\n        v1 = x1.permute(0, 2, 1)\n        v2 = torch.nn.functional.linear(v1, self.linear.weight, self.linear.bias)\n        x2 = torch.nn.functional.relu(v2)\n        return x2.permute(1, 2, 0)\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.linear = torch.nn.Linear(8, 8)\n    def forward(self, x1):\n        v1 = torch.nn.functional.tanh(x1).permute(0, 2, 1)\n        v2 = torch.nn.functional.linear(v1, self.linear.weight, self.linear.bias)\n        x2 = torch.nn.functional.relu(v2)\n        return x2\n# Inputs to the model\nx1 = torch.randn(1, 8, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(2, 2)\n    def forward(self, x1):\n        v1 = x1.permute(0, 2, 1)\n        v2 = torch.nn.functional.linear(v1, self.linear.weight, self.linear.bias)\n        v3 = torch.nn.functional.sigmoid(v2)\n        x2 = v3 * v1\n        return x2\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\n"
            ],
            "g_time": 14.679071187973022
        }
    }
}
{
    "opt": "unary=10",
    "response": {
        "0": {
            "raw": [
                "\nm = torch.nn.Sequential(torch.nn.Linear(12, 7), torch.nn.ReLU6(True), torch.nn.Linear(7, 5))\n\n# Inputs to the model\nx = torch.randn(20, 12)\n",
                " without nn.Module wrapping\nclass ModelWithoutModuleWrapping(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, x1):\n        v1 = x1 + 3\n        v2 = torch.clamp_min(v1, 0)\n        v3 = torch.clamp_max(v2, 6)\n        return v3 / 6\n\n# Initializing the model\nm1 = ModelWithoutModuleWrapping()\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(10, 3)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v4 / 6\n        return v5\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 10)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(128, 256)\n\n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v4 / 6\n        return v5\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 128, 128)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(128, 1)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v4 / 6\n        return v5\n\n# Initializing the model\nm = Model()\n\n# Input to the model\nx1 = torch.randn(1, 128)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(10, 20)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v4 / 6\n        return v5\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 10)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n \n    def forward(self, x1): # A sample linear model.\n        l1 = self.linear(x1)\n        l2 = l1 + 3\n        l3 = torch.clamp_min(l2, 0)\n        l4 = torch.clamp_max(l3, 6)\n        l5 = l4 / 6\n        return l5\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v4 / 6\n        return v5\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v4 / 6\n        return v5\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 16, bias=False)\n \n    def forward(self, x):\n        v1 = self.linear(x)\n        v2 = v1 + 3\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v4 / 6\n        return v5\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx = torch.randn(2, 3)\n"
            ],
            "code": [
                "\nm = torch.nn.Sequential(torch.nn.Linear(12, 7), torch.nn.ReLU6(True), torch.nn.Linear(7, 5))\n\n# Inputs to the model\nx = torch.randn(20, 12)\n",
                " without nn.Module wrapping\nclass ModelWithoutModuleWrapping(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, x1):\n        v1 = x1 + 3\n        v2 = torch.clamp_min(v1, 0)\n        v3 = torch.clamp_max(v2, 6)\n        return v3 / 6\n\n# Initializing the model\nm1 = ModelWithoutModuleWrapping()\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(10, 3)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v4 / 6\n        return v5\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 10)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(128, 256)\n\n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v4 / 6\n        return v5\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 128, 128)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(128, 1)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v4 / 6\n        return v5\n\n# Initializing the model\nm = Model()\n\n# Input to the model\nx1 = torch.randn(1, 128)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(10, 20)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v4 / 6\n        return v5\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 10)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n \n    def forward(self, x1): # A sample linear model.\n        l1 = self.linear(x1)\n        l2 = l1 + 3\n        l3 = torch.clamp_min(l2, 0)\n        l4 = torch.clamp_max(l3, 6)\n        l5 = l4 / 6\n        return l5\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v4 / 6\n        return v5\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v4 / 6\n        return v5\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 16, bias=False)\n \n    def forward(self, x):\n        v1 = self.linear(x)\n        v2 = v1 + 3\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v4 / 6\n        return v5\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx = torch.randn(2, 3)\n"
            ],
            "g_time": 6.237699031829834
        }
    }
}
{
    "opt": "unary=28",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(512, 512)\n  \n    def forward(self, x1, min_value=-1.0, max_value=1.0):\n        v1 = self.linear(x1)\n        v2 = torch.clamp_min(v1, min_value)\n        v3 = torch.clamp_max(v2, max_value)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 512)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, v1):\n        v2 = torch.nn.functional.linear(v1, 10)\n        v3 = torch.clamp_min(v2, 1.23)\n        v4 = torch.clamp_max(v3, 3.45)\n        return v4\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nv1 = torch.randn(4, 10)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(472, 477)\n        self.min_value = torch.tensor(-0.20202)\n        self.max_value = torch.tensor(0.002711)\n \n    def forward(self, x):\n        v1 = self.linear(x)\n        v2 = torch.clamp(v1, self.min_value, self.max_value)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx = torch.randn(100, 472)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(2, 4)\n \n    def forward(self, x):\n        return torch.clamp_max(torch.clamp_min(self.linear(x), 1.3), 0.5)\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx = torch.randn(1, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min_value=0, max_value=1):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.clamp_min(v1, min_value)\n        v3 = torch.clamp_max(v2, max_value)\n        return v3\n\n# Initializing the model\nm = Model(0, 1)\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(50, 100)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.clamp_min(v1, min=-10.0)\n        v3 = torch.clamp_max(v2, max=10.0)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 50)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        pass\n\n    def forward(self, input, **kwargs):\n        v1 = self.t1(input)\n        v2 = torch.clamp_min(v1, kwargs[\"min_value\"])\n        v3 = torch.clamp_max(v2, kwargs[\"max_value\"])\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\ninput = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min_value, max_value):\n        super().__init__()\n        self.linear = torch.nn.Linear(64, 64)\n        self.min_value = min_value\n        self.max_value = max_value\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.clamp_min(v1, self.min_value)\n        v3 = torch.clamp_max(v2, self.max_value)\n        return v3\n\n# Initializing the model\nmin_value = -1\nmax_value = 1\nm = Model(min_value, max_value)\n \n# Inputs to the model\nx1 = torch.randn(1, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(16, 16)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        return torch.clamp_min(torch.clamp_max(v1, min_value), max_value)\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n \n    def forward(self, x2):\n        v2 = torch.nn.functional.linear(x2, weight=__weight1__, bias=__bias1__)\n        v3 = torch.clamp_min(v2, min=__minvalue__)\n        v4 = torch.clamp_max(v3, max=__maxvalue__)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx2 = torch.randn(1, 32)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(512, 512)\n  \n    def forward(self, x1, min_value=-1.0, max_value=1.0):\n        v1 = self.linear(x1)\n        v2 = torch.clamp_min(v1, min_value)\n        v3 = torch.clamp_max(v2, max_value)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 512)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, v1):\n        v2 = torch.nn.functional.linear(v1, 10)\n        v3 = torch.clamp_min(v2, 1.23)\n        v4 = torch.clamp_max(v3, 3.45)\n        return v4\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nv1 = torch.randn(4, 10)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(472, 477)\n        self.min_value = torch.tensor(-0.20202)\n        self.max_value = torch.tensor(0.002711)\n \n    def forward(self, x):\n        v1 = self.linear(x)\n        v2 = torch.clamp(v1, self.min_value, self.max_value)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx = torch.randn(100, 472)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(2, 4)\n \n    def forward(self, x):\n        return torch.clamp_max(torch.clamp_min(self.linear(x), 1.3), 0.5)\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx = torch.randn(1, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min_value=0, max_value=1):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.clamp_min(v1, min_value)\n        v3 = torch.clamp_max(v2, max_value)\n        return v3\n\n# Initializing the model\nm = Model(0, 1)\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(50, 100)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.clamp_min(v1, min=-10.0)\n        v3 = torch.clamp_max(v2, max=10.0)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 50)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        pass\n\n    def forward(self, input, **kwargs):\n        v1 = self.t1(input)\n        v2 = torch.clamp_min(v1, kwargs[\"min_value\"])\n        v3 = torch.clamp_max(v2, kwargs[\"max_value\"])\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\ninput = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min_value, max_value):\n        super().__init__()\n        self.linear = torch.nn.Linear(64, 64)\n        self.min_value = min_value\n        self.max_value = max_value\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.clamp_min(v1, self.min_value)\n        v3 = torch.clamp_max(v2, self.max_value)\n        return v3\n\n# Initializing the model\nmin_value = -1\nmax_value = 1\nm = Model(min_value, max_value)\n \n# Inputs to the model\nx1 = torch.randn(1, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(16, 16)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        return torch.clamp_min(torch.clamp_max(v1, min_value), max_value)\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n \n    def forward(self, x2):\n        v2 = torch.nn.functional.linear(x2, weight=__weight1__, bias=__bias1__)\n        v3 = torch.clamp_min(v2, min=__minvalue__)\n        v4 = torch.clamp_max(v3, max=__maxvalue__)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx2 = torch.randn(1, 32)\n"
            ],
            "g_time": 6.786121129989624
        }
    }
}
{
    "opt": "binary=4",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n \n    def forward(self, x):\n        v1 = self.linear(x)\n        v2 = v1 + 10\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(8, 8)\n \n    def forward(self, x1, other=None):\n        v1 = self.linear(x1)\n        v2 = v1 + other if other is not None else v1\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 8)\nx2 = torch.randn(1, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(10, 5)\n \n    def forward(self, linear_input, add_tensor, other):\n        v1 = self.linear(linear_input)\n        v2 = v1 + add_tensor\n        v3 = v2 + other\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Input to the model\nlinear_input = torch.randn(1, 10)\n# One way to pass the model's input\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 4)\n        self.other = torch.randn(4, 4)\n\n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 + self.other\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(64, 32)\n \n    def forward(self, x1, x2):\n        v1 = self.linear(x1)\n        v2 = v1 + x2\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 64)\nx2 = torch.randn(1, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 + other\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 5, 5)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(8, 8)\n \n    def forward(self, x1, other):\n        v1 = self.linear(x1)\n        v2 = v1 + other\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 8)\nother = torch.randn(1, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc = torch.nn.Linear(10, 16)\n \n    def forward(self, x1, other):\n        v1 = self.fc(x1)\n        v2 = v1 + other\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(16, 10)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(1, 1)\n \n    def forward(self, x1, other):\n        v1 = self.linear(x1)\n        v2 = v1 + other\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 1, 1)\nother = torch.randn(1, 1, 1)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(8, 4)\n \n    def forward(self, x1, __other__):\n        v1 = self.linear(x1)\n        v2 = v1 + __other__\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(4, 8)\n__other__ = torch.rand(4) * 5\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n \n    def forward(self, x):\n        v1 = self.linear(x)\n        v2 = v1 + 10\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(8, 8)\n \n    def forward(self, x1, other=None):\n        v1 = self.linear(x1)\n        v2 = v1 + other if other is not None else v1\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 8)\nx2 = torch.randn(1, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(10, 5)\n \n    def forward(self, linear_input, add_tensor, other):\n        v1 = self.linear(linear_input)\n        v2 = v1 + add_tensor\n        v3 = v2 + other\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Input to the model\nlinear_input = torch.randn(1, 10)\n# One way to pass the model's input\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 4)\n        self.other = torch.randn(4, 4)\n\n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 + self.other\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(64, 32)\n \n    def forward(self, x1, x2):\n        v1 = self.linear(x1)\n        v2 = v1 + x2\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 64)\nx2 = torch.randn(1, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 + other\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 5, 5)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(8, 8)\n \n    def forward(self, x1, other):\n        v1 = self.linear(x1)\n        v2 = v1 + other\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 8)\nother = torch.randn(1, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc = torch.nn.Linear(10, 16)\n \n    def forward(self, x1, other):\n        v1 = self.fc(x1)\n        v2 = v1 + other\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(16, 10)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(1, 1)\n \n    def forward(self, x1, other):\n        v1 = self.linear(x1)\n        v2 = v1 + other\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 1, 1)\nother = torch.randn(1, 1, 1)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(8, 4)\n \n    def forward(self, x1, __other__):\n        v1 = self.linear(x1)\n        v2 = v1 + __other__\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(4, 8)\n__other__ = torch.rand(4) * 5\n"
            ],
            "g_time": 5.117187023162842
        }
    }
}
{
    "opt": "binary=10",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 16)\n\n    def forward(self, x1, x2):\n        v1 = self.linear(x1)\n        v2 = v1 + x2\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\nx2 = torch.randn(1, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(128, 256)\n \n    def forward(self, x1, x2):\n        v1 = self.linear(x1)\n        v2 = v1 + x2\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 128)\nx2 = torch.randn(1, 128)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear1 = torch.nn.Linear(1, 1, bias=True)\n        self.linear2 = torch.nn.Linear(1, 1, bias=True)\n \n    def forward(self, x1):\n        v1 = self.linear1(x1) # Linear transformation\n        v2 = v1 + self.linear2(x1) # Add the output of the first linear transformation and the output of the second linear transformation\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 1)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n \n    def forward(self, x1, v1=None):\n        v2 = self.linear(x1)\n        if v1 is not None:\n            v2 = v2 + v1\n        return v2\n \n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\nv1 = torch.randn(1, 8, 4, 12)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(8, 6)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 + other\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Other\nother = torch.Tensor(1, 6)\n\n# Inputs to the model\nx1 = torch.randn(1, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, other):\n        super().__init__()\n        self.linear = torch.nn.Linear(16, 16)\n        self.__other__ = other\n \n    def forward(self, x1):\n        v0 = self.__other__.view(1, 16)\n        v1 = self.linear(x1)\n        v2 = v1 + v0\n        return v2\n\n# Initializing the model\nm = Model(torch.randn(1, 16))\n\n# Inputs to the model\nx1 = torch.randn(1, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n \n    def forward(self, x1, other):\n        v1 = self.linear(x1)\n        v2 = v1 + other\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\nx2 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(64, 8)\n    \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 + x2\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 64)\nx2 = torch.randn(1, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(8, 8)\n \n    def forward(self, x1, other=None):\n        v1 = self.linear(x1)\n        v2 = v1 + other\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 8)\nx2 = torch.randn(8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 12)\n \n    def forward(self, x1, other):\n        v1 = self.linear(x1)\n        return v1 + other\n\n \nm = Model()\n\n# Inputs to the model\n## The \"other\" input is a tensor with shape [1, 3]\nx1 = torch.randn(1, 3, 64, 64)\nother = torch.randn(1, 3)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 16)\n\n    def forward(self, x1, x2):\n        v1 = self.linear(x1)\n        v2 = v1 + x2\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\nx2 = torch.randn(1, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(128, 256)\n \n    def forward(self, x1, x2):\n        v1 = self.linear(x1)\n        v2 = v1 + x2\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 128)\nx2 = torch.randn(1, 128)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear1 = torch.nn.Linear(1, 1, bias=True)\n        self.linear2 = torch.nn.Linear(1, 1, bias=True)\n \n    def forward(self, x1):\n        v1 = self.linear1(x1) # Linear transformation\n        v2 = v1 + self.linear2(x1) # Add the output of the first linear transformation and the output of the second linear transformation\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 1)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n \n    def forward(self, x1, v1=None):\n        v2 = self.linear(x1)\n        if v1 is not None:\n            v2 = v2 + v1\n        return v2\n \n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\nv1 = torch.randn(1, 8, 4, 12)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(8, 6)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 + other\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Other\nother = torch.Tensor(1, 6)\n\n# Inputs to the model\nx1 = torch.randn(1, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, other):\n        super().__init__()\n        self.linear = torch.nn.Linear(16, 16)\n        self.__other__ = other\n \n    def forward(self, x1):\n        v0 = self.__other__.view(1, 16)\n        v1 = self.linear(x1)\n        v2 = v1 + v0\n        return v2\n\n# Initializing the model\nm = Model(torch.randn(1, 16))\n\n# Inputs to the model\nx1 = torch.randn(1, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n \n    def forward(self, x1, other):\n        v1 = self.linear(x1)\n        v2 = v1 + other\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\nx2 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(64, 8)\n    \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 + x2\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 64)\nx2 = torch.randn(1, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(8, 8)\n \n    def forward(self, x1, other=None):\n        v1 = self.linear(x1)\n        v2 = v1 + other\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 8)\nx2 = torch.randn(8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 12)\n \n    def forward(self, x1, other):\n        v1 = self.linear(x1)\n        return v1 + other\n\n \nm = Model()\n\n# Inputs to the model\n## The \"other\" input is a tensor with shape [1, 3]\nx1 = torch.randn(1, 3, 64, 64)\nother = torch.randn(1, 3)\n"
            ],
            "g_time": 5.809179067611694
        }
    }
}
{
    "opt": "unary=3",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.weight = torch.nn.ParameterDict({\n            'w1': torch.nn.Parameter(torch.randn(54, 473, 3)),\n            'w2': torch.nn.Parameter(torch.randn(3, 1))\n        })\n        self.dropout = torch.nn.Dropout(p=0.1)\n        self.conv = torch.nn.Conv2d(3, 30, 3, stride=3, padding=1)\n        self.conv2 = torch.nn.Conv2d(30, 25, 5, stride=2, padding=0)\n        self.conv3 = torch.nn.Conv1d(840, 29, 10, stride=14, padding=0)\n        self.conv4 = torch.nn.Conv1d(29, 30, 20, stride=17, padding=0)\n        self.conv5 = torch.nn.Conv2d(30, 30, 1, stride=1, padding=0)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        v7 = self.conv2(v6)\n        v8 = v7 * 0.5\n        v9 = v7 * 0.7071067811865476\n        v10 = torch.erf(v9)\n        v11 = v10 + 1\n        v12 = v8 * v11\n        v13 = self.conv3(v12)\n        v14 = self.conv4(v12)\n        v15 = torch.flatten(v14, 1)\n        v16 = self.dropout(v15)\n        v17 = v16.matmul(self.weight['w1'])\n        v18 = v17.transpose(0, 1)\n        v19 = self.conv5(v18)\n        return v19\n# Inputs to the model\nx1 = torch.randn(1, 3, 199, 397)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(1, 7, 5, stride=2, padding=1)\n        self.conv2 = torch.nn.Conv2d(7, 15, 2, stride=1, padding=0)\n        self.conv3 = torch.nn.Conv2d(15, 5, 5, stride=2, padding=1)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        v7 = self.conv2(v6)\n        v8 = v7 * 0.5\n        v9 = v7 * 0.7071067811865476\n        v10 = torch.erf(v9)\n        v11 = v10 + 1\n        v12 = v8 * v11\n        v13 = self.conv3(v12)\n        return v13\n# Inputs to the model\nx1 = torch.randn(1, 1, 8, 37)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(1, 1000, 5, stride=1, padding=0)\n        self.conv2 = torch.nn.Conv2d(100, 160, 3, stride=2, padding=0)\n        self.conv3 = torch.nn.Conv2d(100, 160, 3, stride=2, padding=0)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        v7 = self.conv2(v6)\n        v8 = v7 * 0.5\n        v9 = v7 * 0.7071067811865476\n        v10 = torch.erf(v9)\n        v11 = v10 + 1\n        v12 = v8 * v11\n        v13 = self.conv3(v12)\n        return v13\n# Inputs to the model\nx1 = torch.randn(1, 1, 640, 377)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 3, 1, stride=1, padding=0)\n        self.conv2 = torch.nn.Conv2d(3, 3, 5, stride=2, padding=1)\n        self.conv3 = torch.nn.Conv2d(3, 3, 3, stride=2, padding=0)\n        self.conv4 = torch.nn.Conv2d(3, 3, 5, stride=4, padding=0)\n        self.conv5 = torch.nn.Conv2d(3, 3, 5, stride=1, padding=1)\n        self.conv6 = torch.nn.Conv2d(3, 3, 3, stride=2, padding=0)\n        self.conv7 = torch.nn.Conv2d(3, 3, 1, stride=1, padding=0)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        v7 = self.conv2(v6)\n        v8 = v7 * 0.5\n        v9 = v7 * 0.7071067811865476\n        v10 = torch.erf(v9)\n        v11 = v10 + 1\n        v12 = v8 * v11\n        v13 = self.conv3(v12)\n        v14 = v13 * 0.5\n        v15 = v13 * 0.7071067811865476\n        v16 = torch.erf(v15)\n        v17 = v16 + 1\n        v18 = v14 * v17\n        v19 = self.conv4(v18)\n        v20 = self.conv5(v19)\n        v21 = v20 * 0.5\n        v22 = v20 * 0.7071067811865476\n        v23 = torch.erf(v22)\n        v24 = v23 + 1\n        v25 = v21 * v24\n        v26 = self.conv6(v25)\n        v27 = v26 * 0.5\n        v28 = v26 * 0.7071067811865476\n        v29 = torch.erf(v28)\n        v30 = v29 + 1\n        v31 = v27 * v30\n        v32 = self.conv7(v31)\n        return v32\n# Inputs to the model\nx1 = torch.randn(1, 3, 22, 22)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(1, 3, 1, stride=1, padding=0)\n        self.conv2 = torch.nn.Conv2d(3, 5, 1, stride=2, padding=0)\n        self.conv3 = torch.nn.Conv2d(5, 2, 1, stride=1, padding=0)\n        self.conv4 = torch.nn.ConvTranspose2d(2, 2, 2, stride=1, padding=0)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        v7 = self.conv2(v6)\n        v8 = v7 * 0.5\n        v9 = v7 * 0.7071067811865476\n        v10 = torch.erf(v9)\n        v11 = v10 + 1\n        v12 = v8 * v11\n        v13 = self.conv3(v12)\n        v14 = v13 * 0.5\n        v15 = v13 * 0.7071067811865476\n        v16 = torch.erf(v15)\n        v17 = v16 + 1\n        v18 = v14 * v17\n        v19 = self.conv4(v18)\n        return v19\n# Inputs to the model\nx1 = torch.randn(1, 1, 106, 99)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(1, 1024, 1, stride=1, padding=0)\n        self.conv2 = torch.nn.Conv2d(1024, 1024, 1, stride=1, padding=0)\n        self.conv3 = torch.nn.Conv2d(1024, 1024, 1, stride=1, padding=0)\n        self.conv4 = torch.nn.Conv2d(1024, 1024, 1, stride=1, padding=0)\n        self.conv5 = torch.nn.Conv2d(1024, 1024, 1, stride=1, padding=0)\n        self.conv6 = torch.nn.Conv2d(1024, 1024, 1, stride=1, padding=0)\n        self.conv7 = torch.nn.Conv2d(1024, 1024, 1, stride=1, padding=0)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        v7 = self.conv2(v6)\n        v8 = v7 * 0.5\n        v9 = v7 * 0.7071067811865476\n        v10 = torch.erf(v9)\n        v11 = v10 + 1\n        v12 = v8 * v11\n        v13 = self.conv3(v12)\n        v14 = v13 * 0.5\n        v15 = v13 * 0.7071067811865476\n        v16 = torch.erf(v15)\n        v17 = v16 + 1\n        v18 = v14 * v17\n        v19 = self.conv4(v18)\n        v20 = v19 * 0.5\n        v21 = v19 * 0.7071067811865476\n        v22 = torch.erf(v21)\n        v23 = v22 + 1\n        v24 = v20 * v23\n        v25 = self.conv5(v24)\n        v26 = v25 * 0.5\n        v27 = v25 * 0.7071067811865476\n        v28 = torch.erf(v27)\n        v29 = v28 + 1\n        v30 = v26 * v29\n        v31 = self.conv6(v30)\n        v32 = v31 * 0.5\n        v33 = v31 * 0.7071067811865476\n        v34 = torch.erf(v33)\n        v35 = v34 + 1\n        v36 = v32 * v35\n        v37 = self.conv7(v36)\n        return v7\n# Inputs to the model\nx1 = torch.randn(1, 1, 1024, 7)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 5, 3, stride=2, padding=1)\n        self.conv2 = torch.nn.Conv2d(5, 3, 1, stride=1, padding=0)\n        self.conv3 = torch.nn.Conv2d(3, 3, 3, padding=2)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        v7 = self.conv2(v6)\n        v8 = v7 * 0.5\n        v9 = v7 * 0.7071067811865476\n        v10 = torch.erf(v9)\n        v11 = v10 + 1\n        v12 = v8 * v11\n        v13 = self.conv3(v12)\n        return v13\n# Inputs to the model\nx1 = torch.randn(1, 3, 67, 150)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv1d(24, 18, 5, stride=1, padding=0)\n        self.conv2 = torch.nn.Conv1d(18, 4, 5, stride=3, padding=0)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        v7 = self.conv2(v6)\n        return v7\n# Inputs to the model\nx1 = torch.randn(1, 24, 84)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(1, 3, 1, stride=1, padding=0)\n        self.conv2 = torch.nn.Conv2d(3, 4, 5, stride=2, padding=1)\n        self.conv3 = torch.nn.Conv2d(4, 1, 5, stride=3, padding=2)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        v7 = self.conv2(v6)\n        v8 = v7 * 0.5\n        v9 = v7 * 0.7071067811865476\n        v10 = torch.erf(v9)\n        v11 = v10 + 1\n        v12 = v8 * v11\n        v13 = self.conv3(v12)\n        return v13\n# Inputs to the model\nx1 = torch.randn(1, 1, 108, 89)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 3, 1, stride=1, padding=0)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        return v1\n# Inputs to the model\nx1 = torch.randn(1, 3, 106, 99)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.weight = torch.nn.ParameterDict({\n            'w1': torch.nn.Parameter(torch.randn(54, 473, 3)),\n            'w2': torch.nn.Parameter(torch.randn(3, 1))\n        })\n        self.dropout = torch.nn.Dropout(p=0.1)\n        self.conv = torch.nn.Conv2d(3, 30, 3, stride=3, padding=1)\n        self.conv2 = torch.nn.Conv2d(30, 25, 5, stride=2, padding=0)\n        self.conv3 = torch.nn.Conv1d(840, 29, 10, stride=14, padding=0)\n        self.conv4 = torch.nn.Conv1d(29, 30, 20, stride=17, padding=0)\n        self.conv5 = torch.nn.Conv2d(30, 30, 1, stride=1, padding=0)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        v7 = self.conv2(v6)\n        v8 = v7 * 0.5\n        v9 = v7 * 0.7071067811865476\n        v10 = torch.erf(v9)\n        v11 = v10 + 1\n        v12 = v8 * v11\n        v13 = self.conv3(v12)\n        v14 = self.conv4(v12)\n        v15 = torch.flatten(v14, 1)\n        v16 = self.dropout(v15)\n        v17 = v16.matmul(self.weight['w1'])\n        v18 = v17.transpose(0, 1)\n        v19 = self.conv5(v18)\n        return v19\n# Inputs to the model\nx1 = torch.randn(1, 3, 199, 397)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(1, 7, 5, stride=2, padding=1)\n        self.conv2 = torch.nn.Conv2d(7, 15, 2, stride=1, padding=0)\n        self.conv3 = torch.nn.Conv2d(15, 5, 5, stride=2, padding=1)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        v7 = self.conv2(v6)\n        v8 = v7 * 0.5\n        v9 = v7 * 0.7071067811865476\n        v10 = torch.erf(v9)\n        v11 = v10 + 1\n        v12 = v8 * v11\n        v13 = self.conv3(v12)\n        return v13\n# Inputs to the model\nx1 = torch.randn(1, 1, 8, 37)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(1, 1000, 5, stride=1, padding=0)\n        self.conv2 = torch.nn.Conv2d(100, 160, 3, stride=2, padding=0)\n        self.conv3 = torch.nn.Conv2d(100, 160, 3, stride=2, padding=0)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        v7 = self.conv2(v6)\n        v8 = v7 * 0.5\n        v9 = v7 * 0.7071067811865476\n        v10 = torch.erf(v9)\n        v11 = v10 + 1\n        v12 = v8 * v11\n        v13 = self.conv3(v12)\n        return v13\n# Inputs to the model\nx1 = torch.randn(1, 1, 640, 377)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 3, 1, stride=1, padding=0)\n        self.conv2 = torch.nn.Conv2d(3, 3, 5, stride=2, padding=1)\n        self.conv3 = torch.nn.Conv2d(3, 3, 3, stride=2, padding=0)\n        self.conv4 = torch.nn.Conv2d(3, 3, 5, stride=4, padding=0)\n        self.conv5 = torch.nn.Conv2d(3, 3, 5, stride=1, padding=1)\n        self.conv6 = torch.nn.Conv2d(3, 3, 3, stride=2, padding=0)\n        self.conv7 = torch.nn.Conv2d(3, 3, 1, stride=1, padding=0)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        v7 = self.conv2(v6)\n        v8 = v7 * 0.5\n        v9 = v7 * 0.7071067811865476\n        v10 = torch.erf(v9)\n        v11 = v10 + 1\n        v12 = v8 * v11\n        v13 = self.conv3(v12)\n        v14 = v13 * 0.5\n        v15 = v13 * 0.7071067811865476\n        v16 = torch.erf(v15)\n        v17 = v16 + 1\n        v18 = v14 * v17\n        v19 = self.conv4(v18)\n        v20 = self.conv5(v19)\n        v21 = v20 * 0.5\n        v22 = v20 * 0.7071067811865476\n        v23 = torch.erf(v22)\n        v24 = v23 + 1\n        v25 = v21 * v24\n        v26 = self.conv6(v25)\n        v27 = v26 * 0.5\n        v28 = v26 * 0.7071067811865476\n        v29 = torch.erf(v28)\n        v30 = v29 + 1\n        v31 = v27 * v30\n        v32 = self.conv7(v31)\n        return v32\n# Inputs to the model\nx1 = torch.randn(1, 3, 22, 22)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(1, 3, 1, stride=1, padding=0)\n        self.conv2 = torch.nn.Conv2d(3, 5, 1, stride=2, padding=0)\n        self.conv3 = torch.nn.Conv2d(5, 2, 1, stride=1, padding=0)\n        self.conv4 = torch.nn.ConvTranspose2d(2, 2, 2, stride=1, padding=0)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        v7 = self.conv2(v6)\n        v8 = v7 * 0.5\n        v9 = v7 * 0.7071067811865476\n        v10 = torch.erf(v9)\n        v11 = v10 + 1\n        v12 = v8 * v11\n        v13 = self.conv3(v12)\n        v14 = v13 * 0.5\n        v15 = v13 * 0.7071067811865476\n        v16 = torch.erf(v15)\n        v17 = v16 + 1\n        v18 = v14 * v17\n        v19 = self.conv4(v18)\n        return v19\n# Inputs to the model\nx1 = torch.randn(1, 1, 106, 99)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(1, 1024, 1, stride=1, padding=0)\n        self.conv2 = torch.nn.Conv2d(1024, 1024, 1, stride=1, padding=0)\n        self.conv3 = torch.nn.Conv2d(1024, 1024, 1, stride=1, padding=0)\n        self.conv4 = torch.nn.Conv2d(1024, 1024, 1, stride=1, padding=0)\n        self.conv5 = torch.nn.Conv2d(1024, 1024, 1, stride=1, padding=0)\n        self.conv6 = torch.nn.Conv2d(1024, 1024, 1, stride=1, padding=0)\n        self.conv7 = torch.nn.Conv2d(1024, 1024, 1, stride=1, padding=0)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        v7 = self.conv2(v6)\n        v8 = v7 * 0.5\n        v9 = v7 * 0.7071067811865476\n        v10 = torch.erf(v9)\n        v11 = v10 + 1\n        v12 = v8 * v11\n        v13 = self.conv3(v12)\n        v14 = v13 * 0.5\n        v15 = v13 * 0.7071067811865476\n        v16 = torch.erf(v15)\n        v17 = v16 + 1\n        v18 = v14 * v17\n        v19 = self.conv4(v18)\n        v20 = v19 * 0.5\n        v21 = v19 * 0.7071067811865476\n        v22 = torch.erf(v21)\n        v23 = v22 + 1\n        v24 = v20 * v23\n        v25 = self.conv5(v24)\n        v26 = v25 * 0.5\n        v27 = v25 * 0.7071067811865476\n        v28 = torch.erf(v27)\n        v29 = v28 + 1\n        v30 = v26 * v29\n        v31 = self.conv6(v30)\n        v32 = v31 * 0.5\n        v33 = v31 * 0.7071067811865476\n        v34 = torch.erf(v33)\n        v35 = v34 + 1\n        v36 = v32 * v35\n        v37 = self.conv7(v36)\n        return v7\n# Inputs to the model\nx1 = torch.randn(1, 1, 1024, 7)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 5, 3, stride=2, padding=1)\n        self.conv2 = torch.nn.Conv2d(5, 3, 1, stride=1, padding=0)\n        self.conv3 = torch.nn.Conv2d(3, 3, 3, padding=2)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        v7 = self.conv2(v6)\n        v8 = v7 * 0.5\n        v9 = v7 * 0.7071067811865476\n        v10 = torch.erf(v9)\n        v11 = v10 + 1\n        v12 = v8 * v11\n        v13 = self.conv3(v12)\n        return v13\n# Inputs to the model\nx1 = torch.randn(1, 3, 67, 150)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv1d(24, 18, 5, stride=1, padding=0)\n        self.conv2 = torch.nn.Conv1d(18, 4, 5, stride=3, padding=0)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        v7 = self.conv2(v6)\n        return v7\n# Inputs to the model\nx1 = torch.randn(1, 24, 84)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(1, 3, 1, stride=1, padding=0)\n        self.conv2 = torch.nn.Conv2d(3, 4, 5, stride=2, padding=1)\n        self.conv3 = torch.nn.Conv2d(4, 1, 5, stride=3, padding=2)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        v7 = self.conv2(v6)\n        v8 = v7 * 0.5\n        v9 = v7 * 0.7071067811865476\n        v10 = torch.erf(v9)\n        v11 = v10 + 1\n        v12 = v8 * v11\n        v13 = self.conv3(v12)\n        return v13\n# Inputs to the model\nx1 = torch.randn(1, 1, 108, 89)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 3, 1, stride=1, padding=0)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        return v1\n# Inputs to the model\nx1 = torch.randn(1, 3, 106, 99)\n"
            ],
            "g_time": 33.07858443260193
        }
    }
}
{
    "opt": "addmm",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, input):\n        x1.add_(input)\n        return x1\n\n# Inputs to the model\nx1 = torch.randn(10, 10)\nx = torch.randn(10, 10)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.v1 = torch.randn(10)\n        self.x1 = torch.randn(10)\n    def forward(self, x2):\n        y1 = torch.mm(x2, x2) + self.x1 + x2 + self.v1\n        return x2 + y1\n# Inputs to the model\nx2 = torch.randn(10, 10)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.x1 = torch.randn(10, 10, requires_grad=True)\n        with torch.no_grad():\n            self.x2 = torch.randn(10, 10)\n    def forward(self, x2):\n        q = torch.mm(x2, self.x1)\n        return q + self.x2\n# Inputs to the model\nx2 = torch.randn(5, 5, requires_grad=True)\n",
                "\nx = torch.randn(3, 10) + torch.randn(10)\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, y):\n        return y+x\n# Input to the model\ny = torch.randn(3, 10)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x, y, z):\n        z1= torch.mm(x, y)\n        z2= torch.mm(x, z)\n        z3= torch.mm(y, z)\n        t1 = z1 + z2\n        t2 = t1 + z3\n        return t2\n# Inputs to the model\nx = torch.randn(10, 10)\ny = torch.randn(10, 10)\nz = torch.randn(10, 10)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.inp = torch.randn(10)\n    def forward(self, x1, x2, y1):\n        z1 = torch.mm(x1, x2) + x1 + y1 + x2\n        return z1\n# Inputs to the model\nx1 = torch.randn(10, 10)\nx2 = torch.randn(10, 10)\ny1 = torch.randn(10, 10)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, inp, x2):\n        t1 = torch.mm(x1, inp) # t1 is a 256x1 tensor result of matrix multiplication\n        t2 = torch.mm(t1, x2) # t2 is a 256x1024 tensor result of another matrix multiplication\n        return t2 # return t2\n# Inputs to the model\nx1 = torch.randn(256, 1)\ninp = torch.randn(256, 256)\nx2 = torch.randn(256, 1024)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, x2, x3, x4, inp):\n        z1 = torch.mm(x1, inp)\n        z2 = x3 * self.add(x4, inp)\n        z3 = z1 + x4\n        z4 = (z1 * z2) + x4 + x3\n        return z3, z4\n    def add(self, x5, x6):\n        return x5 + x6\n# Inputs to the model\nx1 = torch.randn(3, 3)\nx2 = torch.randn(3, 3, requires_grad=True)\nx3 = torch.randn(3)\nx4 = torch.randn(3, requires_grad=True)\ninp = torch.randn(3, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, n):\n        super().__init__()\n        self.linear = torch.nn.Linear(n, n)\n\n    def forward(self, x):\n        y = torch.matmul(x, x.T)\n        z =  torch.matmul(x, x.T)\n        return self.linear(y) + z\n# Inputs to the model\nx = torch.randn(5, 10)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, x2, y1, inp):\n        v1 = torch.mm(x1, x2)\n        z = torch.mm(y1, v1) + inp\n        s = z.view(10)\n        v2 = torch.mm(x1, x2) + s\n        return v2\n# Inputs to the model\nx1 = torch.randn(3, 3)\nx2 = torch.randn(3, 3)\ny1 = torch.randn(3, 3)\ninp = torch.randn(10)\n\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, input):\n        x1.add_(input)\n        return x1\n\n# Inputs to the model\nx1 = torch.randn(10, 10)\nx = torch.randn(10, 10)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.v1 = torch.randn(10)\n        self.x1 = torch.randn(10)\n    def forward(self, x2):\n        y1 = torch.mm(x2, x2) + self.x1 + x2 + self.v1\n        return x2 + y1\n# Inputs to the model\nx2 = torch.randn(10, 10)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.x1 = torch.randn(10, 10, requires_grad=True)\n        with torch.no_grad():\n            self.x2 = torch.randn(10, 10)\n    def forward(self, x2):\n        q = torch.mm(x2, self.x1)\n        return q + self.x2\n# Inputs to the model\nx2 = torch.randn(5, 5, requires_grad=True)\n",
                "\nx = torch.randn(3, 10) + torch.randn(10)\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, y):\n        return y+x\n# Input to the model\ny = torch.randn(3, 10)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x, y, z):\n        z1= torch.mm(x, y)\n        z2= torch.mm(x, z)\n        z3= torch.mm(y, z)\n        t1 = z1 + z2\n        t2 = t1 + z3\n        return t2\n# Inputs to the model\nx = torch.randn(10, 10)\ny = torch.randn(10, 10)\nz = torch.randn(10, 10)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.inp = torch.randn(10)\n    def forward(self, x1, x2, y1):\n        z1 = torch.mm(x1, x2) + x1 + y1 + x2\n        return z1\n# Inputs to the model\nx1 = torch.randn(10, 10)\nx2 = torch.randn(10, 10)\ny1 = torch.randn(10, 10)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, inp, x2):\n        t1 = torch.mm(x1, inp) # t1 is a 256x1 tensor result of matrix multiplication\n        t2 = torch.mm(t1, x2) # t2 is a 256x1024 tensor result of another matrix multiplication\n        return t2 # return t2\n# Inputs to the model\nx1 = torch.randn(256, 1)\ninp = torch.randn(256, 256)\nx2 = torch.randn(256, 1024)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, x2, x3, x4, inp):\n        z1 = torch.mm(x1, inp)\n        z2 = x3 * self.add(x4, inp)\n        z3 = z1 + x4\n        z4 = (z1 * z2) + x4 + x3\n        return z3, z4\n    def add(self, x5, x6):\n        return x5 + x6\n# Inputs to the model\nx1 = torch.randn(3, 3)\nx2 = torch.randn(3, 3, requires_grad=True)\nx3 = torch.randn(3)\nx4 = torch.randn(3, requires_grad=True)\ninp = torch.randn(3, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, n):\n        super().__init__()\n        self.linear = torch.nn.Linear(n, n)\n\n    def forward(self, x):\n        y = torch.matmul(x, x.T)\n        z =  torch.matmul(x, x.T)\n        return self.linear(y) + z\n# Inputs to the model\nx = torch.randn(5, 10)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, x2, y1, inp):\n        v1 = torch.mm(x1, x2)\n        z = torch.mm(y1, v1) + inp\n        s = z.view(10)\n        v2 = torch.mm(x1, x2) + s\n        return v2\n# Inputs to the model\nx1 = torch.randn(3, 3)\nx2 = torch.randn(3, 3)\ny1 = torch.randn(3, 3)\ninp = torch.randn(10)\n\n"
            ],
            "g_time": 6.622371673583984
        }
    }
}
{
    "opt": "unary=12",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(128, 128, 3, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 128, 256, 256)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv0 = torch.nn.Conv2d(4, 1, 5, stride=2)\n        self.conv1 = torch.nn.Conv2d(4, 16, 3, stride=1, padding=1)\n        self.conv2 = torch.nn.Conv2d(16, 64, 1, stride=1)\n    def forward(self, x1):\n        v1 = self.conv0(x1)\n        v2 = v1.sum(dim=1, keepdim=True)\n        v3 = self.conv1(v2)\n        v4 = self.conv2(v3)\n        v5 = v4.sum(dim=1, keepdim=True)\n        v6 = F.sigmoid(v5)\n        v7 = v4 * v6\n        return v7\n# Inputs to the model\nx1 = torch.randn(1, 4, 32, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(64, 16, 3, stride=1, padding=1)\n        self.conv2 = torch.nn.Conv2d(32, 8, 3, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        v4 = self.conv2(v3)\n        v5 = torch.sigmoid(v4)\n        v6 = v4 * v5\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 64, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(4, 8, 3, stride=1)\n        self.conv2 = torch.nn.Conv2d(8, 16, 3, stride=1)\n    def forward(self, x1):\n        v1 = torch.sigmoid(self.conv(x1))\n        v2 = self.conv(x1) * v1\n        v3 = torch.sigmoid(self.conv2(v2))\n        v4 = self.conv2(v2) * v3\n        v5 = torch.sigmoid(torch.Conv2d(4, 16, 3, stride=1)(v4))\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 4, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 3, 1, stride=1, padding=1)\n        self.conv2 = torch.nn.Conv2d(3, 3, 1, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        v4 = self.conv2(v3)\n        v5 = F.sigmoid(v4)\n        v6 = v4 * v5\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n            nn.Sigmoid(),\n            nn.Dropout(p=0.5),\n            nn.BatchNorm2d(32, momentum=0.9),\n            nn.Conv2d(32, 8, kernel_size=3, stride=1, padding=1),\n            nn.Sigmoid()\n        )\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = torch.cat([v1, v1, v1], dim=1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 16, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 3, 1, stride=1, padding=1)\n        self.conv2 = torch.nn.Conv2d(3, 3, 1, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        v4 = self.conv2(v3)\n        v5 = torch.sigmoid(v4)\n        v6 = v5 * v4\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(1, 1, 1, stride=1, padding=1)\n        self.conv2 = torch.nn.Conv2d(1, 1, 1, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = self.conv2(v1)\n        v3 = torch.sigmoid(v2)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 1, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 3, 4, stride=4, padding=4)\n        self.conv2 = torch.nn.Conv2d(3, 3, 1, stride=1, padding=1)\n        self.conv3 = torch.nn.Conv2d(3, 3, 2, stride=2, padding=2)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        v4 = F.relu(v3)\n        v5 = F.max_pool2d(v4, 4, 4, 1, 1)\n        v6 = self.conv2(v5)\n        v7 = torch.sigmoid(v6)\n        v8 = v6 * v7\n        v9 = F.leaky_relu(v8)\n        v10 = F.avg_pool2d(v9, 1, 1, 2, 2)\n        v11 = self.conv3(v10)\n        v12 = torch.sigmoid(v11)\n        v13 = v11 * v12\n        return v13\n# Inputs to the model\nx1 = torch.randn(1, 3, 160, 160)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(64, 32, 1, stride=1, padding=1)\n        self.conv2 = torch.nn.Conv2d(32, 2, 1, stride=1, padding=1)\n    def forward(self, x1):\n        x1 = F.relu(self.conv2(F.relu(self.conv(x1))))\n        return x1\n# Inputs to the model\nx1 = torch.randn(1, 64, 64, 64)\n\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(128, 128, 3, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 128, 256, 256)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv0 = torch.nn.Conv2d(4, 1, 5, stride=2)\n        self.conv1 = torch.nn.Conv2d(4, 16, 3, stride=1, padding=1)\n        self.conv2 = torch.nn.Conv2d(16, 64, 1, stride=1)\n    def forward(self, x1):\n        v1 = self.conv0(x1)\n        v2 = v1.sum(dim=1, keepdim=True)\n        v3 = self.conv1(v2)\n        v4 = self.conv2(v3)\n        v5 = v4.sum(dim=1, keepdim=True)\n        v6 = F.sigmoid(v5)\n        v7 = v4 * v6\n        return v7\n# Inputs to the model\nx1 = torch.randn(1, 4, 32, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(64, 16, 3, stride=1, padding=1)\n        self.conv2 = torch.nn.Conv2d(32, 8, 3, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        v4 = self.conv2(v3)\n        v5 = torch.sigmoid(v4)\n        v6 = v4 * v5\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 64, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(4, 8, 3, stride=1)\n        self.conv2 = torch.nn.Conv2d(8, 16, 3, stride=1)\n    def forward(self, x1):\n        v1 = torch.sigmoid(self.conv(x1))\n        v2 = self.conv(x1) * v1\n        v3 = torch.sigmoid(self.conv2(v2))\n        v4 = self.conv2(v2) * v3\n        v5 = torch.sigmoid(torch.Conv2d(4, 16, 3, stride=1)(v4))\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 4, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 3, 1, stride=1, padding=1)\n        self.conv2 = torch.nn.Conv2d(3, 3, 1, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        v4 = self.conv2(v3)\n        v5 = F.sigmoid(v4)\n        v6 = v4 * v5\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n            nn.Sigmoid(),\n            nn.Dropout(p=0.5),\n            nn.BatchNorm2d(32, momentum=0.9),\n            nn.Conv2d(32, 8, kernel_size=3, stride=1, padding=1),\n            nn.Sigmoid()\n        )\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = torch.cat([v1, v1, v1], dim=1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 16, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 3, 1, stride=1, padding=1)\n        self.conv2 = torch.nn.Conv2d(3, 3, 1, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        v4 = self.conv2(v3)\n        v5 = torch.sigmoid(v4)\n        v6 = v5 * v4\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(1, 1, 1, stride=1, padding=1)\n        self.conv2 = torch.nn.Conv2d(1, 1, 1, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = self.conv2(v1)\n        v3 = torch.sigmoid(v2)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 1, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 3, 4, stride=4, padding=4)\n        self.conv2 = torch.nn.Conv2d(3, 3, 1, stride=1, padding=1)\n        self.conv3 = torch.nn.Conv2d(3, 3, 2, stride=2, padding=2)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        v4 = F.relu(v3)\n        v5 = F.max_pool2d(v4, 4, 4, 1, 1)\n        v6 = self.conv2(v5)\n        v7 = torch.sigmoid(v6)\n        v8 = v6 * v7\n        v9 = F.leaky_relu(v8)\n        v10 = F.avg_pool2d(v9, 1, 1, 2, 2)\n        v11 = self.conv3(v10)\n        v12 = torch.sigmoid(v11)\n        v13 = v11 * v12\n        return v13\n# Inputs to the model\nx1 = torch.randn(1, 3, 160, 160)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(64, 32, 1, stride=1, padding=1)\n        self.conv2 = torch.nn.Conv2d(32, 2, 1, stride=1, padding=1)\n    def forward(self, x1):\n        x1 = F.relu(self.conv2(F.relu(self.conv(x1))))\n        return x1\n# Inputs to the model\nx1 = torch.randn(1, 64, 64, 64)\n\n"
            ],
            "g_time": 10.71635389328003
        }
    }
}
{
    "opt": "mm_plus_mm",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, x2, x3, x4):\n        xx1 = torch.mm(x1, x2)\n        xx2 = torch.mm(x3, x4)\n        xx3 = xx1 + xx2\n        return xx3\n# Inputs to the model\nx1 = torch.randn(1, 65)\nx2 = torch.randn(65, 5)\nx3 = torch.randn(1, 65)\nx4 = torch.randn(65, 5)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, input1, input2, input3, input4):\n        t1 = input1 + input2\n        t2 = input3 + input4\n        return t1 + t2\n# Inputs to the model\ninput1 = torch.randn(32,64,3,24,24)\ninput2 = torch.randn(32,64,3,24,24)\ninput3 = torch.randn(32,64,3,24,24)\ninput4 = torch.randn(32,64,3,24,24)\n",
                "\nclass Model(nn.Module):\n    def forward(self, input1, input2, input3, input4):\n        t1 = torch.mm(input2, input3)\n        t2 = torch.mm(input1, input4)\n        t3 = t1 + t2\n        return t3\n# Inputs to the model\ninput1 = torch.randn(64, 65)\ninput2 = torch.randn(65, 64)\ninput3 = torch.randn(64, 65)\ninput4 = torch.randn(65, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, inputs1, inputs2, inputs3, inputs4):\n        layer_weights = {}\n        layer_weights[\"t1\"] = torch.mm(inputs1, inputs2)\n        layer_weights[\"t2\"] = torch.mm(inputs3, inputs4)\n        layer_weights[\"t3\"] = layer_weights[\"t1\"] + layer_weights[\"t2\"]\n        return layer_weights[\"t3\"]\n# Inputs to the model\ninputs1 = torch.randn(166, 320)\ninputs2 = torch.randn(320, 1024)\ninputs3 = torch.randn(166, 320)\ninputs4 = torch.randn(320, 1024)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 32, kernel_size=1)\n    def forward(self, input):\n        return self.conv(input) * 1000\n# Input to the model\ninput_dummy = torch.randn(1, 3, 32, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def forward(self, p1, p2):\n        y1 = torch.mm(p1, p2)\n        z1 = torch.rand(64, 64)\n        y2 = torch.mm(p1, p2)\n        z2 = torch.rand(64, 64)\n        y3 = y1 * y2\n        z3 = z1 * z2\n        y4 = torch.add(y3, y1)\n        z4 = torch.add(z3, z1)\n        y5 = (torch.add(y3, y4))\n        z5 = (torch.add(z3, z4))\n        y6 = torch.add(y5, y2)\n        z6 = torch.add(z5, z2)\n        y7 = torch.add(y6, torch.mm(p1, p2))\n        z7 = torch.add(z6, torch.mm(p1, p2))\n        y8 = torch.mm(y7, y8)\n        z8 = torch.mm(z7, z8)\n        y9 = 1 * 2 + 3 + 4\n        z9 = 1 * 2 + 3 + 4\n        y10 = x1y10z10 + x2y10z10 + x3y10z10 + 8 * 9 * 10\n        z10 = x1z10z10 + x2z10z10 + x3z10z10 + 8 * 9 * 10\n        y11 = x1 * y11 * y10y10z10 + x2 * z11 * z10z10\n        z11 = x1 * z11 * z10z10 + x2 * y11 * y10y10z10\n        q1 = 0 + 1\n        f1 = q1 * 2 + x\n        return y10z10y10z10 + z10z10z10y10\n# Inputs to the model\np1 = 1\np2 = 1\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, x2, x3, x4):\n        v1 = torch.mm(x1, x1)\n        v3 = v1\n        v4 = v1\n        v5 = torch.mm(x2, x2)\n        v6 = v5\n        v7 = v5\n        v8 = torch.mm(x3, x3)\n        v9 = v8\n        v10 = v8\n        v11 = torch.mm(x4, x4)\n        v12 = v11\n        v13 = v11\n        v2 = v3 + v4 + v5 + v6 + v7 + v8 + v9 + v10 + v11 + v12 + v13\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 65)\nx2 = torch.randn(65, 5)\nx3 = torch.randn(1, 65)\nx4 = torch.randn(65, 5)\n",
                "\nclass MyModel(nn.Module):\n    def __init__(self):\n        super(MyModel, self).__init__()\n        self.fc1 = nn.Linear(256, 128)\n        #self.fc2 = nn.Linear(256, 128)\n    def forward(self, x, x2):\n        x = F.relu(self.fc1(x))\n        x = self.fc1(x2)\n        return x\n# Inputs to the model\nx = torch.randn(1, 256)\nx2 = torch.randn(256, 256)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, input1, input2, input3, input4):\n        t1 = torch.mm(input1, input2)\n        t2 = torch.mm(input3, input4)\n        t3 = t1 + t2 # Addition of the results of the two matrix multiplications\n        output = torch.mm(t1, t2)\n        return output\n# Inputs to the model\ninput1 = torch.randn(2, 3, 32, 32)\ninput2 = torch.randn(2, 3, 32, 32)\ninput3 = torch.randn(2, 3, 32, 32)\ninput4 = torch.randn(2, 3, 32, 32)\n",
                "\nclass Model(nn.Module):\n    def forward(self, x1, x2, x3, x4):\n        m1 = torch.mm(x1, x2) # Matrix multiplication\n        m2 = torch.mm(x3, x4) # Matrix multiplication\n        m3 = m1 + m2 # Addition\n        return m3\n# Inputs to the model\nx1 = torch.randn(64,64)\nx2 = torch.randn(64,64)\nx3 = torch.randn(64,64)\nx4 = torch.randn(64,64)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, x2, x3, x4):\n        xx1 = torch.mm(x1, x2)\n        xx2 = torch.mm(x3, x4)\n        xx3 = xx1 + xx2\n        return xx3\n# Inputs to the model\nx1 = torch.randn(1, 65)\nx2 = torch.randn(65, 5)\nx3 = torch.randn(1, 65)\nx4 = torch.randn(65, 5)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, input1, input2, input3, input4):\n        t1 = input1 + input2\n        t2 = input3 + input4\n        return t1 + t2\n# Inputs to the model\ninput1 = torch.randn(32,64,3,24,24)\ninput2 = torch.randn(32,64,3,24,24)\ninput3 = torch.randn(32,64,3,24,24)\ninput4 = torch.randn(32,64,3,24,24)\n",
                "\nclass Model(nn.Module):\n    def forward(self, input1, input2, input3, input4):\n        t1 = torch.mm(input2, input3)\n        t2 = torch.mm(input1, input4)\n        t3 = t1 + t2\n        return t3\n# Inputs to the model\ninput1 = torch.randn(64, 65)\ninput2 = torch.randn(65, 64)\ninput3 = torch.randn(64, 65)\ninput4 = torch.randn(65, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, inputs1, inputs2, inputs3, inputs4):\n        layer_weights = {}\n        layer_weights[\"t1\"] = torch.mm(inputs1, inputs2)\n        layer_weights[\"t2\"] = torch.mm(inputs3, inputs4)\n        layer_weights[\"t3\"] = layer_weights[\"t1\"] + layer_weights[\"t2\"]\n        return layer_weights[\"t3\"]\n# Inputs to the model\ninputs1 = torch.randn(166, 320)\ninputs2 = torch.randn(320, 1024)\ninputs3 = torch.randn(166, 320)\ninputs4 = torch.randn(320, 1024)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 32, kernel_size=1)\n    def forward(self, input):\n        return self.conv(input) * 1000\n# Input to the model\ninput_dummy = torch.randn(1, 3, 32, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def forward(self, p1, p2):\n        y1 = torch.mm(p1, p2)\n        z1 = torch.rand(64, 64)\n        y2 = torch.mm(p1, p2)\n        z2 = torch.rand(64, 64)\n        y3 = y1 * y2\n        z3 = z1 * z2\n        y4 = torch.add(y3, y1)\n        z4 = torch.add(z3, z1)\n        y5 = (torch.add(y3, y4))\n        z5 = (torch.add(z3, z4))\n        y6 = torch.add(y5, y2)\n        z6 = torch.add(z5, z2)\n        y7 = torch.add(y6, torch.mm(p1, p2))\n        z7 = torch.add(z6, torch.mm(p1, p2))\n        y8 = torch.mm(y7, y8)\n        z8 = torch.mm(z7, z8)\n        y9 = 1 * 2 + 3 + 4\n        z9 = 1 * 2 + 3 + 4\n        y10 = x1y10z10 + x2y10z10 + x3y10z10 + 8 * 9 * 10\n        z10 = x1z10z10 + x2z10z10 + x3z10z10 + 8 * 9 * 10\n        y11 = x1 * y11 * y10y10z10 + x2 * z11 * z10z10\n        z11 = x1 * z11 * z10z10 + x2 * y11 * y10y10z10\n        q1 = 0 + 1\n        f1 = q1 * 2 + x\n        return y10z10y10z10 + z10z10z10y10\n# Inputs to the model\np1 = 1\np2 = 1\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, x2, x3, x4):\n        v1 = torch.mm(x1, x1)\n        v3 = v1\n        v4 = v1\n        v5 = torch.mm(x2, x2)\n        v6 = v5\n        v7 = v5\n        v8 = torch.mm(x3, x3)\n        v9 = v8\n        v10 = v8\n        v11 = torch.mm(x4, x4)\n        v12 = v11\n        v13 = v11\n        v2 = v3 + v4 + v5 + v6 + v7 + v8 + v9 + v10 + v11 + v12 + v13\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 65)\nx2 = torch.randn(65, 5)\nx3 = torch.randn(1, 65)\nx4 = torch.randn(65, 5)\n",
                "\nclass MyModel(nn.Module):\n    def __init__(self):\n        super(MyModel, self).__init__()\n        self.fc1 = nn.Linear(256, 128)\n        #self.fc2 = nn.Linear(256, 128)\n    def forward(self, x, x2):\n        x = F.relu(self.fc1(x))\n        x = self.fc1(x2)\n        return x\n# Inputs to the model\nx = torch.randn(1, 256)\nx2 = torch.randn(256, 256)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, input1, input2, input3, input4):\n        t1 = torch.mm(input1, input2)\n        t2 = torch.mm(input3, input4)\n        t3 = t1 + t2 # Addition of the results of the two matrix multiplications\n        output = torch.mm(t1, t2)\n        return output\n# Inputs to the model\ninput1 = torch.randn(2, 3, 32, 32)\ninput2 = torch.randn(2, 3, 32, 32)\ninput3 = torch.randn(2, 3, 32, 32)\ninput4 = torch.randn(2, 3, 32, 32)\n",
                "\nclass Model(nn.Module):\n    def forward(self, x1, x2, x3, x4):\n        m1 = torch.mm(x1, x2) # Matrix multiplication\n        m2 = torch.mm(x3, x4) # Matrix multiplication\n        m3 = m1 + m2 # Addition\n        return m3\n# Inputs to the model\nx1 = torch.randn(64,64)\nx2 = torch.randn(64,64)\nx3 = torch.randn(64,64)\nx4 = torch.randn(64,64)\n"
            ],
            "g_time": 15.500064373016357
        }
    }
}
{
    "opt": "unary=29",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min_value=0, max_value=1):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose3d(1, 1, 1, stride=1, padding=0)\n        self.min_value = min_value\n        self.max_value = max_value\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = torch.clamp_min(v1, self.min_value)\n        v3 = torch.clamp_max(v2, self.max_value)\n        return v3\n# Inputs to the model\nx1 = torch.randn(2, 1, 1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min_value=-17.1238, max_value=-6.29):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(1, 1, 2, stride=1, padding=1)\n        self.min_value = min_value\n        self.max_value = max_value\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = torch.clamp_min(v1, self.min_value)\n        v3 = torch.clamp_max(v2, self.max_value)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 1, 13, 5)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min_value=5, max_value=-4.464):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose3d(8, 8, (1, 2, 2), stride=(2, 1, 1), padding=(2, 1, 1))\n        self.min_value = min_value\n        self.max_value = max_value\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = torch.clamp_min(v1, self.min_value)\n        v3 = torch.clamp_max(v2, self.max_value)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 8, 20, 11, 4)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min_value=1, max_value=2.2):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose3d(6, 9, 1, stride=1, padding=1)\n        self.min_value = min_value\n        self.max_value = max_value\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = torch.clamp_min(v1, self.min_value)\n        v3 = torch.clamp_max(v2, self.max_value)\n        return v3\n# Inputs to the model\nx1 = torch.randn(4, 6, 18, 16, 9)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min_value=1.8608, max_value=1.0570):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(1, 1, 1, stride=1, padding=0)\n        self.min_value = min_value\n        self.max_value = max_value\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = torch.clamp_min(v1, self.min_value)\n        v3 = torch.clamp_max(v2, self.max_value)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 1, 19, 7)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min_value=-2.1, max_value=36.4):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(2, 2, 1, stride=1, padding=1)\n        self.min_value = min_value\n        self.max_value = max_value\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = torch.clamp_min(v1, self.min_value)\n        v3 = torch.clamp_max(v2, self.max_value)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 2, 5, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min_value=5.19, max_value=7.11):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose1d(15, 6, 3, stride=1, padding=3)\n        self.min_value = min_value\n        self.max_value = max_value\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = torch.clamp_min(v1, self.min_value)\n        v3 = torch.clamp_max(v2, self.max_value)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 15, 5)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min_value=0.6768, max_value=4.2349):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(3, 2, 4, stride=1, padding=0)\n        self.min_value = min_value\n        self.max_value = max_value\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = torch.clamp_min(v1, self.min_value)\n        v3 = torch.clamp_max(v2, self.max_value)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 3, 4, 2) # 1, 3, 4, 2\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min_value=-6.28, max_value=-2.039):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(3699, 77, 1479, stride=15, padding=13)\n        self.min_value = min_value\n        self.max_value = max_value\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = torch.clamp_min(v1, self.min_value)\n        v3 = torch.clamp_max(v2, self.max_value)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 3699, 12, 15)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min_value={0, 0, 0, 0}, max_value=1.9502):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(6, 2, 1, stride=1, padding=1)\n        self.min_value = min_value\n        self.max_value = max_value\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = torch.clamp_min(v1, self.min_value)\n        v3 = torch.clamp_max(v2, self.max_value)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 6, 5, 2)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min_value=0, max_value=1):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose3d(1, 1, 1, stride=1, padding=0)\n        self.min_value = min_value\n        self.max_value = max_value\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = torch.clamp_min(v1, self.min_value)\n        v3 = torch.clamp_max(v2, self.max_value)\n        return v3\n# Inputs to the model\nx1 = torch.randn(2, 1, 1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min_value=-17.1238, max_value=-6.29):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(1, 1, 2, stride=1, padding=1)\n        self.min_value = min_value\n        self.max_value = max_value\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = torch.clamp_min(v1, self.min_value)\n        v3 = torch.clamp_max(v2, self.max_value)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 1, 13, 5)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min_value=5, max_value=-4.464):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose3d(8, 8, (1, 2, 2), stride=(2, 1, 1), padding=(2, 1, 1))\n        self.min_value = min_value\n        self.max_value = max_value\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = torch.clamp_min(v1, self.min_value)\n        v3 = torch.clamp_max(v2, self.max_value)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 8, 20, 11, 4)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min_value=1, max_value=2.2):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose3d(6, 9, 1, stride=1, padding=1)\n        self.min_value = min_value\n        self.max_value = max_value\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = torch.clamp_min(v1, self.min_value)\n        v3 = torch.clamp_max(v2, self.max_value)\n        return v3\n# Inputs to the model\nx1 = torch.randn(4, 6, 18, 16, 9)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min_value=1.8608, max_value=1.0570):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(1, 1, 1, stride=1, padding=0)\n        self.min_value = min_value\n        self.max_value = max_value\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = torch.clamp_min(v1, self.min_value)\n        v3 = torch.clamp_max(v2, self.max_value)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 1, 19, 7)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min_value=-2.1, max_value=36.4):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(2, 2, 1, stride=1, padding=1)\n        self.min_value = min_value\n        self.max_value = max_value\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = torch.clamp_min(v1, self.min_value)\n        v3 = torch.clamp_max(v2, self.max_value)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 2, 5, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min_value=5.19, max_value=7.11):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose1d(15, 6, 3, stride=1, padding=3)\n        self.min_value = min_value\n        self.max_value = max_value\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = torch.clamp_min(v1, self.min_value)\n        v3 = torch.clamp_max(v2, self.max_value)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 15, 5)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min_value=0.6768, max_value=4.2349):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(3, 2, 4, stride=1, padding=0)\n        self.min_value = min_value\n        self.max_value = max_value\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = torch.clamp_min(v1, self.min_value)\n        v3 = torch.clamp_max(v2, self.max_value)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 3, 4, 2) # 1, 3, 4, 2\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min_value=-6.28, max_value=-2.039):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(3699, 77, 1479, stride=15, padding=13)\n        self.min_value = min_value\n        self.max_value = max_value\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = torch.clamp_min(v1, self.min_value)\n        v3 = torch.clamp_max(v2, self.max_value)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 3699, 12, 15)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min_value={0, 0, 0, 0}, max_value=1.9502):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(6, 2, 1, stride=1, padding=1)\n        self.min_value = min_value\n        self.max_value = max_value\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = torch.clamp_min(v1, self.min_value)\n        v3 = torch.clamp_max(v2, self.max_value)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 6, 5, 2)\n"
            ],
            "g_time": 7.444153547286987
        }
    }
}
{
    "opt": "unary=25",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self, negative_slope=0.02):\n        super().__init__()\n        self.linear = torch.nn.Linear(8, 8, bias=True)\n        self._negative_slope = negative_slope\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 > 0\n        v3 = v1 * self._negative_slope\n        v4 = torch.where(v2, v1, v3)\n        return v4\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 8)\n",
                "\nnegative_slope = 0.01\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 12)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = (v1 > 0).float()\n        v3 = v1 * -negative_slope\n        v4 = torch.where(v2 > 0, v1, v3)\n        return v4\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, negative_slope):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n        self.negative_slope = negative_slope\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 > 0\n        v3 = v2 * self.negative_slope\n        v4 = torch.where(v2, v1, v3)\n        return v4\n\n# Initializing the model\nm = Model(-0.5)\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n",
                "\n\nclass Model(torch.nn.Module):\n    def __init__(self, negative_slope=0.01):\n        super().__init__()\n        self.linear = torch.nn.Linear(64*64, 64*64)\n        self.negative_slope = negative_slope\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 > 0\n        v3 = v1 * negative_slope\n        v4 = torch.where(v2, v1, v3)\n        return v4\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 64*64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(8, 8)\n \n    def forward(self, x1, ):\n        v1 = self.linear(x1)\n        v2 = v1 > 0\n        v3 = v1 * 0.01\n        v4 = torch.where(v2, v1, v3)\n        return v4\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, negative_slope):\n        super().__init__()\n        self.linear = torch.nn.Linear(10, 128)\n        self.negative_slope = negative_slope\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 > 0\n        v3 = v1 * self.negative_slope\n        return torch.where(v2, v1, v3)\n\n# Initializing the model\nnegative_slope = 0.01\nm = Model(negative_slope)\n\n# Inputs to the model\nx1 = torch.randn(10, 10)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(1024, 512)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 > 0\n        v3 = v1 * negative_slope\n        v4 = torch.where(v2, v1, v3)\n        return v4\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 1024)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(64, 512)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 > 0\n        v3 = v1 * 0.01\n        v4 = torch.where(v2, v1, v3)\n        return v4\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(10, 5)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = 1.0 - x1.abs()\n        v3 = self.linear(v2) - 0.1\n        v4 = torch.where(x1 > 0, x1, v3)\n        return v4\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 10)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(8, 64)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 > 0\n        v3 = v1 * 0.01\n        v4 = torch.where(v2, v1, v3)\n        return v4\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 8)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self, negative_slope=0.02):\n        super().__init__()\n        self.linear = torch.nn.Linear(8, 8, bias=True)\n        self._negative_slope = negative_slope\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 > 0\n        v3 = v1 * self._negative_slope\n        v4 = torch.where(v2, v1, v3)\n        return v4\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 8)\n",
                "\nnegative_slope = 0.01\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 12)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = (v1 > 0).float()\n        v3 = v1 * -negative_slope\n        v4 = torch.where(v2 > 0, v1, v3)\n        return v4\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, negative_slope):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n        self.negative_slope = negative_slope\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 > 0\n        v3 = v2 * self.negative_slope\n        v4 = torch.where(v2, v1, v3)\n        return v4\n\n# Initializing the model\nm = Model(-0.5)\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n",
                "\n\nclass Model(torch.nn.Module):\n    def __init__(self, negative_slope=0.01):\n        super().__init__()\n        self.linear = torch.nn.Linear(64*64, 64*64)\n        self.negative_slope = negative_slope\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 > 0\n        v3 = v1 * negative_slope\n        v4 = torch.where(v2, v1, v3)\n        return v4\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 64*64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(8, 8)\n \n    def forward(self, x1, ):\n        v1 = self.linear(x1)\n        v2 = v1 > 0\n        v3 = v1 * 0.01\n        v4 = torch.where(v2, v1, v3)\n        return v4\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, negative_slope):\n        super().__init__()\n        self.linear = torch.nn.Linear(10, 128)\n        self.negative_slope = negative_slope\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 > 0\n        v3 = v1 * self.negative_slope\n        return torch.where(v2, v1, v3)\n\n# Initializing the model\nnegative_slope = 0.01\nm = Model(negative_slope)\n\n# Inputs to the model\nx1 = torch.randn(10, 10)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(1024, 512)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 > 0\n        v3 = v1 * negative_slope\n        v4 = torch.where(v2, v1, v3)\n        return v4\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 1024)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(64, 512)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 > 0\n        v3 = v1 * 0.01\n        v4 = torch.where(v2, v1, v3)\n        return v4\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(10, 5)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = 1.0 - x1.abs()\n        v3 = self.linear(v2) - 0.1\n        v4 = torch.where(x1 > 0, x1, v3)\n        return v4\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 10)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(8, 64)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 > 0\n        v3 = v1 * 0.01\n        v4 = torch.where(v2, v1, v3)\n        return v4\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 8)\n"
            ],
            "g_time": 6.596294403076172
        }
    }
}
{
    "opt": "sfdp=1",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self, query_size, key_size, value_size, dropout_p, inv_scale_factor):\n        super().__init__()\n        self.query_dim = query_size\n        self.key_dim = key_size\n        self.value_dim = value_size\n        self.dropout_p = dropout_p\n        self.inv_scale_factor = inv_scale_factor\n    \n    def forward(self, query, key, value):\n        qk = torch.matmul(query, key.transpose(-2, -1)) # Compute the dot product of the query and key tensors\n        scaled_qk = qk.div(self.inv_scale_factor) # Scale the dot product by the inverse scale factor\n        softmax_qk = scaled_qk.softmax(dim=-1) # Apply softmax to the scaled dot product\n        dropout_qk = torch.nn.functional.dropout(softmax_qk, p=self.dropout_p) # Apply dropout to the softmax ouput\n        output = dropout_qk.matmul(value) # Compute the dot product of the dropout output and the value tensor\n        return output\n\n# Initializing the model\nm = Model(query_size=512, key_size=512, value_size=512, dropout_p=0.1, inv_scale_factor=math.sqrt(0.2))\n\n# Inputs to the model\nquery = torch.randn(1, 512, 16)\nkey = torch.randn(1, 512, 320)\nvalue = torch.randn(1, 512, 320)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, query_dim, key_dim, value_dim):\n        super().__init__()\n        self.query_scaling = torch.nn.Parameter(torch.ones(1, 1, query_dim))\n \n    def forward(self, q, k, v, dropout_p):\n        q = q * self.query_scaling\n        qk = torch.matmul(q, k.transpose(-2, -1))\n        inv_scale_factor = torch.rsqrt(torch.tensor(query_dim)).float()\n        scaled_qk = qk.div(inv_scale_factor)\n        softmax_qk = scaled_qk.softmax(dim=-1)\n        dropout_qk = torch.nn.functional.dropout(softmax_qk, p=dropout_p)\n        output = dropout_qk.matmul(v)\n        return output\n\n# Initializing the model\nm = Model(query_dim, key_dim, value_dim)\n\n# Inputs to the model\nq = torch.randn(1, 1, query_dim)\nk = torch.randn(1, key_dim, query_dim)\nv = torch.randn(1, value_dim, query_dim)\n",
                "\nclass Model(torch.nn.Module):\n    def forward(self, query, key, value):\n        qk = torch.matmul(query, key.transpose(-2, -1))\n        inv_scale_factor = 1 / math.sqrt(query.size(-1))\n        scaled_qk = qk * inv_scale_factor\n        softmax_qk = scaled_qk.softmax(dim=-1)\n        dropout_qk = torch.nn.functional.dropout(softmax_qk,\n                                                 p=0.4)\n        output = dropout_qk.matmul(value)\n        return output\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nquery = torch.randn(1, 1, 10, 10)\nkey = torch.randn(1, 1, 10, 10)\nvalue = torch.randn(1, 1, 10, 10)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n \n    def forward(self, query, key, value):\n        qk = torch.matmul(query, key.transpose(-2, -1))\n        inv_scale_factor = math.pow(query.size(-1)).float()\n        scaled_qk = qk.div(inv_scale_factor)\n        softmax_qk = torch.nn.Softmax(dim=-1)(scaled_qk)\n        dropout_qk = torch.nn.functional.dropout(softmax_qk, p=0.00249854953108063)\n        output = torch.matmul(dropout_qk, value)\n        return output\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nquery = torch.randn(1, 512, 64)\nkey = torch.randn(1, 512, 512)\nvalue = torch.randn(1, 512, 512)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.q = torch.nn.Linear(1024, 1024)\n        self.k = torch.nn.Linear(1024, 1024)\n        self.v = torch.nn.Linear(1024, 1024)\n        self.dropout = torch.nn.Dropout(p=0.3)\n \n    def forward(self, query, key, value):\n        q = self.q(query)\n        k = self.k(key)\n        v = self.v(value)\n        qk = torch.matmul(q, k.transpose(-1, -2)) # Compute the dot product of the query and key tensors\n        inv_scale_factor = 1 / math.sqrt(self.k.in_features) # Specify the inverse scale factor using an approximation\n        softmax_qk = qk.div(inv_scale_factor).softmax(dim=-1) # Scale the dot product by the inverse scale factor, then apply softmax to the scaled dot product tensor\n        dropout_qk = self.dropout(softmax_qk) # Apply dropout to the softmax output  \n        output = torch.matmul(dropout_qk, v) # Compute the dot product of the dropout output and the value tensor\n        return output\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 1024)\nx2 = torch.randn(1, 1024)\nx3 = torch.randn(1, 1024)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n \n    def forward(self, query, key, value, mask, dropout_p=0.3):\n        qk = torch.matmul(query, key.transpose(-2, -1)) # Compute the dot product of the query and key tensors\n        inv_scale_factor = 1.0 / np.sqrt(np.prod((key.shape[-1])))\n        scaled_qk = qk.div(inv_scale_factor) # Scale the dot product by the inverse scale factor\n        softmax_qk = scaled_qk.softmax(dim=-1) # Apply softmax to the scaled dot product\n        dropout_qk = torch.nn.functional.dropout(softmax_qk, p=dropout_p) # Apply dropout to the softmax output\n        output = dropout_qk.matmul(value) # Compute the dot product of the dropout output and the value tensor\n \n        if (mask is not None):\n            output = output.masked_fill(mask, -1e9)\n \n        return output\n \n# Initializing the model\nm = Model()\n\n# Inputs to the model\nquery = torch.randn(1, 12, 100)\nkey = torch.randn(1, 24, 100)\nvalue = torch.randn(1, 24, 100)\nmask = torch.ones(1, 12, 24).bool()\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(16, 8)\n \n    def forward(self, x1, x2):\n        q = self.linear(x1)\n        k = self.linear(x1)\n        v = self.linear(x2)\n  \n        qk = torch.matmul(q, k.transpose(-2, -1))\n        rands = torch.randint(0, 1, shape=qk.shape, device=x1.device)\n        mask = torch.logical_not(torch.equal(rands, 0))\n        inv_scale_factor = torch.div(1.0, mask.float().sum(axis=-1).unsqueeze(1))\n        scaled_qk = torch.div(qk, inv_scale_factor)\n        softmax_qk = scaled_qk.softmax(dim=-1)\n        dropout_qk = torch.nn.functional.dropout(softmax_qk, p=0.5)\n        dropout_qk = dropout_qk * mask\n        output = torch.matmul(dropout_qk, v)\n        return output\n\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 4, 10, 16)\nx2 = torch.randn(1, 4, 30, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.query = torch.nn.Parameter(torch.randn(2, 4, 2))\n        self.key = torch.nn.Parameter(torch.randn(2, 3, 2))\n        self.value = torch.nn.Parameter(torch.randn(2, 3, 2))\n        self.softmax_dropout = torch.nn.functional.softmax + torch.nn.functional.dropout\n \n    def forward(self, x1):\n        v1 = torch.matmul(x1, self.query.transpose(-2, -1))\n        v2 = v1.div(0.1)\n        v3 = self.softmax_dropout(v2, p=0.5)\n        v4 = torch.matmul(v3, self.value)\n        return v4\n\n# Initializing the model\nm = Model()\n\n# Input to the model\nx1 = torch.randn(1, 2, 4)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.query = torch.nn.Linear(80, 100)\n        self.key = torch.nn.Linear(80, 100)\n        self.value = torch.nn.Linear(80, 100)\n        self.softmax = torch.nn.Softmax(dim=-1)\n \n    def forward(self, query, key):\n        q = self.query(query)\n        k = self.key(key)\n        v = self.value(key)\n        qk = torch.matmul(q, k.transpose(-2, -1))\n        scaled_qk = qk.div(10)\n        softmax_qk = self.softmax(scaled_qk)\n        dropout_qk = torch.nn.functional.dropout(softmax_qk, p=0.05)\n        output = torch.matmul(dropout_qk, v)\n        return output\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 80)\nx2 = torch.randn(1, 80)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, query, key, value, inv_scale_factor, dropout_p = 0.):\n        qk = torch.matmul(query, key.transpose(-2, -1))\n        weighted_qk = qk.div(inv_scale_factor)\n        softmax_qk = torch.nn.Softmax(dim=-1)(weighted_qk)\n        dropout_qk = torch.nn.Droupout(dropout_p)(softmax_qk)\n        output = torch.matmul(dropout_qk, value)\n        return output\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nquery = torch.randn(1, 8, 64)\nkey = torch.randn(1, 8, 64)\nvalue = torch.randn(1, 8, 64)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self, query_size, key_size, value_size, dropout_p, inv_scale_factor):\n        super().__init__()\n        self.query_dim = query_size\n        self.key_dim = key_size\n        self.value_dim = value_size\n        self.dropout_p = dropout_p\n        self.inv_scale_factor = inv_scale_factor\n    \n    def forward(self, query, key, value):\n        qk = torch.matmul(query, key.transpose(-2, -1)) # Compute the dot product of the query and key tensors\n        scaled_qk = qk.div(self.inv_scale_factor) # Scale the dot product by the inverse scale factor\n        softmax_qk = scaled_qk.softmax(dim=-1) # Apply softmax to the scaled dot product\n        dropout_qk = torch.nn.functional.dropout(softmax_qk, p=self.dropout_p) # Apply dropout to the softmax ouput\n        output = dropout_qk.matmul(value) # Compute the dot product of the dropout output and the value tensor\n        return output\n\n# Initializing the model\nm = Model(query_size=512, key_size=512, value_size=512, dropout_p=0.1, inv_scale_factor=math.sqrt(0.2))\n\n# Inputs to the model\nquery = torch.randn(1, 512, 16)\nkey = torch.randn(1, 512, 320)\nvalue = torch.randn(1, 512, 320)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, query_dim, key_dim, value_dim):\n        super().__init__()\n        self.query_scaling = torch.nn.Parameter(torch.ones(1, 1, query_dim))\n \n    def forward(self, q, k, v, dropout_p):\n        q = q * self.query_scaling\n        qk = torch.matmul(q, k.transpose(-2, -1))\n        inv_scale_factor = torch.rsqrt(torch.tensor(query_dim)).float()\n        scaled_qk = qk.div(inv_scale_factor)\n        softmax_qk = scaled_qk.softmax(dim=-1)\n        dropout_qk = torch.nn.functional.dropout(softmax_qk, p=dropout_p)\n        output = dropout_qk.matmul(v)\n        return output\n\n# Initializing the model\nm = Model(query_dim, key_dim, value_dim)\n\n# Inputs to the model\nq = torch.randn(1, 1, query_dim)\nk = torch.randn(1, key_dim, query_dim)\nv = torch.randn(1, value_dim, query_dim)\n",
                "\nclass Model(torch.nn.Module):\n    def forward(self, query, key, value):\n        qk = torch.matmul(query, key.transpose(-2, -1))\n        inv_scale_factor = 1 / math.sqrt(query.size(-1))\n        scaled_qk = qk * inv_scale_factor\n        softmax_qk = scaled_qk.softmax(dim=-1)\n        dropout_qk = torch.nn.functional.dropout(softmax_qk,\n                                                 p=0.4)\n        output = dropout_qk.matmul(value)\n        return output\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nquery = torch.randn(1, 1, 10, 10)\nkey = torch.randn(1, 1, 10, 10)\nvalue = torch.randn(1, 1, 10, 10)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n \n    def forward(self, query, key, value):\n        qk = torch.matmul(query, key.transpose(-2, -1))\n        inv_scale_factor = math.pow(query.size(-1)).float()\n        scaled_qk = qk.div(inv_scale_factor)\n        softmax_qk = torch.nn.Softmax(dim=-1)(scaled_qk)\n        dropout_qk = torch.nn.functional.dropout(softmax_qk, p=0.00249854953108063)\n        output = torch.matmul(dropout_qk, value)\n        return output\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nquery = torch.randn(1, 512, 64)\nkey = torch.randn(1, 512, 512)\nvalue = torch.randn(1, 512, 512)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.q = torch.nn.Linear(1024, 1024)\n        self.k = torch.nn.Linear(1024, 1024)\n        self.v = torch.nn.Linear(1024, 1024)\n        self.dropout = torch.nn.Dropout(p=0.3)\n \n    def forward(self, query, key, value):\n        q = self.q(query)\n        k = self.k(key)\n        v = self.v(value)\n        qk = torch.matmul(q, k.transpose(-1, -2)) # Compute the dot product of the query and key tensors\n        inv_scale_factor = 1 / math.sqrt(self.k.in_features) # Specify the inverse scale factor using an approximation\n        softmax_qk = qk.div(inv_scale_factor).softmax(dim=-1) # Scale the dot product by the inverse scale factor, then apply softmax to the scaled dot product tensor\n        dropout_qk = self.dropout(softmax_qk) # Apply dropout to the softmax output  \n        output = torch.matmul(dropout_qk, v) # Compute the dot product of the dropout output and the value tensor\n        return output\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 1024)\nx2 = torch.randn(1, 1024)\nx3 = torch.randn(1, 1024)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n \n    def forward(self, query, key, value, mask, dropout_p=0.3):\n        qk = torch.matmul(query, key.transpose(-2, -1)) # Compute the dot product of the query and key tensors\n        inv_scale_factor = 1.0 / np.sqrt(np.prod((key.shape[-1])))\n        scaled_qk = qk.div(inv_scale_factor) # Scale the dot product by the inverse scale factor\n        softmax_qk = scaled_qk.softmax(dim=-1) # Apply softmax to the scaled dot product\n        dropout_qk = torch.nn.functional.dropout(softmax_qk, p=dropout_p) # Apply dropout to the softmax output\n        output = dropout_qk.matmul(value) # Compute the dot product of the dropout output and the value tensor\n \n        if (mask is not None):\n            output = output.masked_fill(mask, -1e9)\n \n        return output\n \n# Initializing the model\nm = Model()\n\n# Inputs to the model\nquery = torch.randn(1, 12, 100)\nkey = torch.randn(1, 24, 100)\nvalue = torch.randn(1, 24, 100)\nmask = torch.ones(1, 12, 24).bool()\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(16, 8)\n \n    def forward(self, x1, x2):\n        q = self.linear(x1)\n        k = self.linear(x1)\n        v = self.linear(x2)\n  \n        qk = torch.matmul(q, k.transpose(-2, -1))\n        rands = torch.randint(0, 1, shape=qk.shape, device=x1.device)\n        mask = torch.logical_not(torch.equal(rands, 0))\n        inv_scale_factor = torch.div(1.0, mask.float().sum(axis=-1).unsqueeze(1))\n        scaled_qk = torch.div(qk, inv_scale_factor)\n        softmax_qk = scaled_qk.softmax(dim=-1)\n        dropout_qk = torch.nn.functional.dropout(softmax_qk, p=0.5)\n        dropout_qk = dropout_qk * mask\n        output = torch.matmul(dropout_qk, v)\n        return output\n\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 4, 10, 16)\nx2 = torch.randn(1, 4, 30, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.query = torch.nn.Parameter(torch.randn(2, 4, 2))\n        self.key = torch.nn.Parameter(torch.randn(2, 3, 2))\n        self.value = torch.nn.Parameter(torch.randn(2, 3, 2))\n        self.softmax_dropout = torch.nn.functional.softmax + torch.nn.functional.dropout\n \n    def forward(self, x1):\n        v1 = torch.matmul(x1, self.query.transpose(-2, -1))\n        v2 = v1.div(0.1)\n        v3 = self.softmax_dropout(v2, p=0.5)\n        v4 = torch.matmul(v3, self.value)\n        return v4\n\n# Initializing the model\nm = Model()\n\n# Input to the model\nx1 = torch.randn(1, 2, 4)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.query = torch.nn.Linear(80, 100)\n        self.key = torch.nn.Linear(80, 100)\n        self.value = torch.nn.Linear(80, 100)\n        self.softmax = torch.nn.Softmax(dim=-1)\n \n    def forward(self, query, key):\n        q = self.query(query)\n        k = self.key(key)\n        v = self.value(key)\n        qk = torch.matmul(q, k.transpose(-2, -1))\n        scaled_qk = qk.div(10)\n        softmax_qk = self.softmax(scaled_qk)\n        dropout_qk = torch.nn.functional.dropout(softmax_qk, p=0.05)\n        output = torch.matmul(dropout_qk, v)\n        return output\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 80)\nx2 = torch.randn(1, 80)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, query, key, value, inv_scale_factor, dropout_p = 0.):\n        qk = torch.matmul(query, key.transpose(-2, -1))\n        weighted_qk = qk.div(inv_scale_factor)\n        softmax_qk = torch.nn.Softmax(dim=-1)(weighted_qk)\n        dropout_qk = torch.nn.Droupout(dropout_p)(softmax_qk)\n        output = torch.matmul(dropout_qk, value)\n        return output\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nquery = torch.randn(1, 8, 64)\nkey = torch.randn(1, 8, 64)\nvalue = torch.randn(1, 8, 64)\n"
            ],
            "g_time": 12.453157186508179
        }
    }
}
{
    "opt": "unary=0",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(133, 120, 3, stride=1, padding=99)\n    def forward(self, x387):\n        v1 = self.conv(x387)\n        v2 = v1 * 0.5\n        v3 = v1 * v1\n        v4 = v3 * v1\n        v5 = v4 * 0.044715\n        v6 = v1 + v5\n        v7 = v6 * 0.7978845608028654\n        v8 = torch.tanh(v7)\n        v9 = v8 + 1\n        v10 = v2 * v9\n        return v10\n# Inputs to the model\nx387 = torch.randn(1, 133, 36, 127)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(7, 11, 1, stride=1, padding=3)\n    def forward(self, x29):\n        v1 = self.conv(x29)\n        v2 = v1 * 0.5\n        v3 = v1 * v1\n        v4 = v3 * v1\n        v5 = v4 * 0.044715\n        v6 = v1 + v5\n        v7 = v6 * 0.7978845608028654\n        v8 = torch.tanh(v7)\n        v9 = v8 + 1\n        v10 = v2 * v9\n        return v10\n# Inputs to the model\nx29 = torch.randn(1, 7, 25, 36)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.ConvTranspose1d(10, 18, 2, stride=5, padding=15)\n    def forward(self, x161):\n        v1 = self.conv(x161)\n        v2 = v1 * 0.5\n        v3 = v1 * v1\n        v4 = v3 * v1\n        v5 = v4 * 0.044715\n        v6 = v1 + v5\n        v7 = v6 * 0.7978845608028654\n        v8 = torch.tanh(v7)\n        v9 = v8 + 1\n        v10 = v2 * v9\n        return v10\n# Inputs to the model\nx161 = torch.randn(1, 10, 50)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(33, 1, 1, stride=1, padding=33)\n    def forward(self, x39):\n        v1 = self.conv(x39)\n        v2 = v1 * 0.5\n        v3 = v1 * v1\n        v4 = v3 * v1\n        v5 = v4 * 0.044715\n        v6 = v1 + v5\n        v7 = v6 * 0.7978845608028654\n        v8 = torch.tanh(v7)\n        v9 = v8 + 1\n        v10 = v2 * v9\n        return v10\n# Inputs to the model\nx39 = torch.randn(1, 33, 16, 1)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(5, 7, 1, stride=2, padding=1)\n    def forward(self, x59):\n        v1 = self.conv(x59)\n        v2 = v1 * 0.5\n        v3 = v1 * v1\n        v4 = v3 * v1\n        v5 = v4 * 0.044715\n        v6 = v1 + v5\n        v7 = v6 * 0.7978845608028654\n        v8 = torch.tanh(v7)\n        v9 = v8 + 1\n        v10 = v2 * v9\n        return v10\n# Inputs to the model\nx59 = torch.randn(1, 5, 16, 62)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(4, 12, 14, stride=2, padding=6)\n    def forward(self, x175):\n        v1 = self.conv(x175)\n        v2 = v1 * 0.5\n        v3 = v1 * v1\n        v4 = v3 * v1\n        v5 = v4 * 0.044715\n        v6 = v1 + v5\n        v7 = v6 * 0.7978845608028654\n        v8 = torch.tanh(v7)\n        v9 = v8 + 1\n        v10 = v2 * v9\n        return v10\n# Inputs to the model\nx175 = torch.randn(1, 4, 51, 15)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(21, 11, 3, stride=1, padding=9)\n    def forward(self, x85):\n        v1 = self.conv(x85)\n        v2 = v1 * 0.5\n        v3 = v1 * v1\n        v4 = v3 * v1\n        v5 = v4 * 0.044715\n        v6 = v1 + v5\n        v7 = v6 * 0.7978845608028654\n        v8 = torch.tanh(v7)\n        v9 = v8 + 1\n        v10 = v2 * v9\n        return v10\n# Inputs to the model\nx85 = torch.randn(1, 21, 29, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(32, 24, 1, stride=1, padding=7)\n    def forward(self, x226):\n        v1 = self.conv(x226)\n        v2 = v1 * 0.5\n        v3 = v1 * v1\n        v4 = v3 * v1\n        v5 = v4 * 0.044715\n        v6 = v1 + v5\n        v7 = v6 * 0.7978845608028654\n        v8 = torch.tanh(v7)\n        v9 = v8 + 1\n        v10 = v2 * v9\n        return v10\n# Inputs to the model\nx226 = torch.randn(1, 32, 79, 60)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(35, 18, 1, stride=2, padding=35)\n    def forward(self, x32):\n        v1 = self.conv(x32)\n        v2 = v1 * 0.5\n        v3 = v1 * v1\n        v4 = v3 * v1\n        v5 = v4 * 0.044715\n        v6 = v1 + v5\n        v7 = v6 * 0.7978845608028654\n        v8 = torch.tanh(v7)\n        v9 = v8 + 1\n        v10 = v2 * v9\n        return v10\n# Inputs to the model\nx32 = torch.randn(1, 35, 21, 60)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(14, 12, 1, stride=14, padding=12)\n    def forward(self, x20):\n        v1 = self.conv(x20)\n        v2 = v1 * 0.5\n        v3 = v1 * v1\n        v4 = v3 * v1\n        v5 = v4 * 0.044715\n        v6 = v1 + v5\n        v7 = v6 * 0.7978845608028654\n        v8 = torch.tanh(v7)\n        v9 = v8 + 1\n        v10 = v2 * v9\n        return v10\n# Inputs to the model\nx20 = torch.randn(1, 14, 44, 57)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(133, 120, 3, stride=1, padding=99)\n    def forward(self, x387):\n        v1 = self.conv(x387)\n        v2 = v1 * 0.5\n        v3 = v1 * v1\n        v4 = v3 * v1\n        v5 = v4 * 0.044715\n        v6 = v1 + v5\n        v7 = v6 * 0.7978845608028654\n        v8 = torch.tanh(v7)\n        v9 = v8 + 1\n        v10 = v2 * v9\n        return v10\n# Inputs to the model\nx387 = torch.randn(1, 133, 36, 127)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(7, 11, 1, stride=1, padding=3)\n    def forward(self, x29):\n        v1 = self.conv(x29)\n        v2 = v1 * 0.5\n        v3 = v1 * v1\n        v4 = v3 * v1\n        v5 = v4 * 0.044715\n        v6 = v1 + v5\n        v7 = v6 * 0.7978845608028654\n        v8 = torch.tanh(v7)\n        v9 = v8 + 1\n        v10 = v2 * v9\n        return v10\n# Inputs to the model\nx29 = torch.randn(1, 7, 25, 36)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.ConvTranspose1d(10, 18, 2, stride=5, padding=15)\n    def forward(self, x161):\n        v1 = self.conv(x161)\n        v2 = v1 * 0.5\n        v3 = v1 * v1\n        v4 = v3 * v1\n        v5 = v4 * 0.044715\n        v6 = v1 + v5\n        v7 = v6 * 0.7978845608028654\n        v8 = torch.tanh(v7)\n        v9 = v8 + 1\n        v10 = v2 * v9\n        return v10\n# Inputs to the model\nx161 = torch.randn(1, 10, 50)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(33, 1, 1, stride=1, padding=33)\n    def forward(self, x39):\n        v1 = self.conv(x39)\n        v2 = v1 * 0.5\n        v3 = v1 * v1\n        v4 = v3 * v1\n        v5 = v4 * 0.044715\n        v6 = v1 + v5\n        v7 = v6 * 0.7978845608028654\n        v8 = torch.tanh(v7)\n        v9 = v8 + 1\n        v10 = v2 * v9\n        return v10\n# Inputs to the model\nx39 = torch.randn(1, 33, 16, 1)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(5, 7, 1, stride=2, padding=1)\n    def forward(self, x59):\n        v1 = self.conv(x59)\n        v2 = v1 * 0.5\n        v3 = v1 * v1\n        v4 = v3 * v1\n        v5 = v4 * 0.044715\n        v6 = v1 + v5\n        v7 = v6 * 0.7978845608028654\n        v8 = torch.tanh(v7)\n        v9 = v8 + 1\n        v10 = v2 * v9\n        return v10\n# Inputs to the model\nx59 = torch.randn(1, 5, 16, 62)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(4, 12, 14, stride=2, padding=6)\n    def forward(self, x175):\n        v1 = self.conv(x175)\n        v2 = v1 * 0.5\n        v3 = v1 * v1\n        v4 = v3 * v1\n        v5 = v4 * 0.044715\n        v6 = v1 + v5\n        v7 = v6 * 0.7978845608028654\n        v8 = torch.tanh(v7)\n        v9 = v8 + 1\n        v10 = v2 * v9\n        return v10\n# Inputs to the model\nx175 = torch.randn(1, 4, 51, 15)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(21, 11, 3, stride=1, padding=9)\n    def forward(self, x85):\n        v1 = self.conv(x85)\n        v2 = v1 * 0.5\n        v3 = v1 * v1\n        v4 = v3 * v1\n        v5 = v4 * 0.044715\n        v6 = v1 + v5\n        v7 = v6 * 0.7978845608028654\n        v8 = torch.tanh(v7)\n        v9 = v8 + 1\n        v10 = v2 * v9\n        return v10\n# Inputs to the model\nx85 = torch.randn(1, 21, 29, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(32, 24, 1, stride=1, padding=7)\n    def forward(self, x226):\n        v1 = self.conv(x226)\n        v2 = v1 * 0.5\n        v3 = v1 * v1\n        v4 = v3 * v1\n        v5 = v4 * 0.044715\n        v6 = v1 + v5\n        v7 = v6 * 0.7978845608028654\n        v8 = torch.tanh(v7)\n        v9 = v8 + 1\n        v10 = v2 * v9\n        return v10\n# Inputs to the model\nx226 = torch.randn(1, 32, 79, 60)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(35, 18, 1, stride=2, padding=35)\n    def forward(self, x32):\n        v1 = self.conv(x32)\n        v2 = v1 * 0.5\n        v3 = v1 * v1\n        v4 = v3 * v1\n        v5 = v4 * 0.044715\n        v6 = v1 + v5\n        v7 = v6 * 0.7978845608028654\n        v8 = torch.tanh(v7)\n        v9 = v8 + 1\n        v10 = v2 * v9\n        return v10\n# Inputs to the model\nx32 = torch.randn(1, 35, 21, 60)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(14, 12, 1, stride=14, padding=12)\n    def forward(self, x20):\n        v1 = self.conv(x20)\n        v2 = v1 * 0.5\n        v3 = v1 * v1\n        v4 = v3 * v1\n        v5 = v4 * 0.044715\n        v6 = v1 + v5\n        v7 = v6 * 0.7978845608028654\n        v8 = torch.tanh(v7)\n        v9 = v8 + 1\n        v10 = v2 * v9\n        return v10\n# Inputs to the model\nx20 = torch.randn(1, 14, 44, 57)\n"
            ],
            "g_time": 9.371485710144043
        }
    }
}
{
    "opt": "binary=6",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(5,1)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 - 1\n        return v1\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(2, 5)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, x1, x2):\n        v1 = x1 * x2\n        v2 = v1 - x2\n        return v2\n \n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 2, 3)\nx2 = torch.randn(1, 3, 4)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(8, 2)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 - other\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        m = torch.nn.Linear(10, 10)\n        m.weight.data.fill_(1.0)\n        self.linear = torch.nn.utils.weight_norm(m, dim=0)\n \n    def forward(self, x1, other):\n        v2 = self.linear(x1)\n        v3 = v2 - other\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(20, 10)\nother = torch.randn(1)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(64, 64)\n \n    def forward(self, x1, other):\n        v1 = self.linear(x1)\n        v2 = v1 - other\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 64)\n_other = torch.tensor([1.0])\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(5, 5)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 - 1.25\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 5)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(5, 3)\n \n    def forward(self, x1, x2):\n        v1 = self.linear(x1)\n        v2 = v1 - x2\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model, 'x2' (referred to as 'other' in the model) should be a constant tensor\nx1 = torch.rand(2, 5, requires_grad=True)\nx2 = torch.tensor([4.7, 7.9, -8.1])\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(128, 64)\n \n    def forward(self, x1, x2):\n        t1 = self.linear(x1)\n        t2 = t1 - x2\n        return t2\n\n# Initializing the model\nm = Model()\n\n# 'other'\nx2 = torch.randn(1, 64)\n# Inputs to the model\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(10, 10)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 - torch.randn(10)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(10)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc = torch.nn.Linear(1, 8)\n        self.other = 0.1\n \n    def forward(self, x1):\n        v1 = self.fc(x1)\n        v2 = v1 - self.other\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 1)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(5,1)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 - 1\n        return v1\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(2, 5)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, x1, x2):\n        v1 = x1 * x2\n        v2 = v1 - x2\n        return v2\n \n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 2, 3)\nx2 = torch.randn(1, 3, 4)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(8, 2)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 - other\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        m = torch.nn.Linear(10, 10)\n        m.weight.data.fill_(1.0)\n        self.linear = torch.nn.utils.weight_norm(m, dim=0)\n \n    def forward(self, x1, other):\n        v2 = self.linear(x1)\n        v3 = v2 - other\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(20, 10)\nother = torch.randn(1)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(64, 64)\n \n    def forward(self, x1, other):\n        v1 = self.linear(x1)\n        v2 = v1 - other\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 64)\n_other = torch.tensor([1.0])\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(5, 5)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 - 1.25\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 5)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(5, 3)\n \n    def forward(self, x1, x2):\n        v1 = self.linear(x1)\n        v2 = v1 - x2\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model, 'x2' (referred to as 'other' in the model) should be a constant tensor\nx1 = torch.rand(2, 5, requires_grad=True)\nx2 = torch.tensor([4.7, 7.9, -8.1])\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(128, 64)\n \n    def forward(self, x1, x2):\n        t1 = self.linear(x1)\n        t2 = t1 - x2\n        return t2\n\n# Initializing the model\nm = Model()\n\n# 'other'\nx2 = torch.randn(1, 64)\n# Inputs to the model\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(10, 10)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 - torch.randn(10)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(10)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc = torch.nn.Linear(1, 8)\n        self.other = 0.1\n \n    def forward(self, x1):\n        v1 = self.fc(x1)\n        v2 = v1 - self.other\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 1)\n"
            ],
            "g_time": 5.764892339706421
        }
    }
}
{
    "opt": "unary=9",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 8, (64, 1), stride=64, padding=0)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp(v2, min=0, max=6)\n        v4 = v3 / 6\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 8, (1, 6), stride=1)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 + 3\n        v3 = v2 / 6\n        v4 = torch.clamp(v3, min=0, max=6)\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 + 3\n        v3 = v2\n        v4 = torch.clamp(v3, min=0, max=6)\n        v5 = v4 / 6\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v3 = torch.clamp(v1, min=0, max=6)\n        v5 = v3 / 6\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = torch.max(x1, torch.randn(1, 1, 4, 1, 1))\n        v2 = self.conv(v1)\n        v3 = v2\n        v4 = torch.clamp(v3, min=0, max=6)\n        v5 = torch.div(v4, 6)\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 + 3\n        v3 = v2\n        v4 = torch.clamp(v3, min=0, max=6)\n        v5 = v4 / 6\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 8, (1, 6), stride=1)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp(v2, min=0, max=6)\n        v4 = v3 / 6\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc = torch.nn.Linear(8, 10)\n    def forward(self, x1):\n        v1 = self.fc(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp(v2, min=0, max=6)\n        v4 = v3 / 6\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 8, 1, stride=1, padding=(0, 1))\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp(v2, min=0, max=6)\n        v4 = v3 / 6\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 + 3\n        v3 = v2 + 0\n        v4 = v3 + 0\n        v5 = v4 + 0\n        v6 = v5 + 0\n        v7 = v6 + 0\n        v8 = v7 + 0\n        v9 = v8 + 0\n        v10 = v2 + 6\n        v11 = torch.clamp(v10, min=0)\n        v12 = v3 + 6\n        v13 = torch.clamp(v12, min=0)\n        v14 = v4 + 6\n        v15 = torch.clamp(v14, min=0)\n        v16 = v5 + 6\n        v17 = torch.clamp(v16, min=0)\n        v18 = v6 + 6\n        v19 = torch.clamp(v18, min=0)\n        v20 = v7 + 6\n        v21 = torch.clamp(v20, min=0)\n        v22 = v8 + 6\n        v23 = torch.clamp(v22, min=0)\n        v24 = v9 + 6\n        v25 = torch.clamp(v24, min=0)\n        v26 = v11 + v13\n        v27 = v15 + v19\n        v28 = v21 + v17\n        v29 = v25 + v15\n        v30 = v29 + v23\n        v31 = v30 / 6\n        return v31\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 8, (64, 1), stride=64, padding=0)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp(v2, min=0, max=6)\n        v4 = v3 / 6\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 8, (1, 6), stride=1)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 + 3\n        v3 = v2 / 6\n        v4 = torch.clamp(v3, min=0, max=6)\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 + 3\n        v3 = v2\n        v4 = torch.clamp(v3, min=0, max=6)\n        v5 = v4 / 6\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v3 = torch.clamp(v1, min=0, max=6)\n        v5 = v3 / 6\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = torch.max(x1, torch.randn(1, 1, 4, 1, 1))\n        v2 = self.conv(v1)\n        v3 = v2\n        v4 = torch.clamp(v3, min=0, max=6)\n        v5 = torch.div(v4, 6)\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 + 3\n        v3 = v2\n        v4 = torch.clamp(v3, min=0, max=6)\n        v5 = v4 / 6\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 8, (1, 6), stride=1)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp(v2, min=0, max=6)\n        v4 = v3 / 6\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc = torch.nn.Linear(8, 10)\n    def forward(self, x1):\n        v1 = self.fc(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp(v2, min=0, max=6)\n        v4 = v3 / 6\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 8, 1, stride=1, padding=(0, 1))\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp(v2, min=0, max=6)\n        v4 = v3 / 6\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 + 3\n        v3 = v2 + 0\n        v4 = v3 + 0\n        v5 = v4 + 0\n        v6 = v5 + 0\n        v7 = v6 + 0\n        v8 = v7 + 0\n        v9 = v8 + 0\n        v10 = v2 + 6\n        v11 = torch.clamp(v10, min=0)\n        v12 = v3 + 6\n        v13 = torch.clamp(v12, min=0)\n        v14 = v4 + 6\n        v15 = torch.clamp(v14, min=0)\n        v16 = v5 + 6\n        v17 = torch.clamp(v16, min=0)\n        v18 = v6 + 6\n        v19 = torch.clamp(v18, min=0)\n        v20 = v7 + 6\n        v21 = torch.clamp(v20, min=0)\n        v22 = v8 + 6\n        v23 = torch.clamp(v22, min=0)\n        v24 = v9 + 6\n        v25 = torch.clamp(v24, min=0)\n        v26 = v11 + v13\n        v27 = v15 + v19\n        v28 = v21 + v17\n        v29 = v25 + v15\n        v30 = v29 + v23\n        v31 = v30 / 6\n        return v31\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n"
            ],
            "g_time": 14.605634212493896
        }
    }
}
{
    "opt": "unary=8",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(246, 53, 10, stride=2, padding=6, output_padding=1)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp(v2, min=0)\n        v4 = torch.clamp(v3, max=6)\n        v5 = v1 * v4\n        v6 = v5 / 6\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 246, 13, 27)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(1, 1, 5, stride=2, bias=True, padding=2)\n    def forward(self, x1):\n        y1 = self.conv(x1)\n        y2 = torch.floor(y1)\n        return y2\n# Inputs to the model\nx1 = torch.randn(1, 1, 54, 94)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(512, 4, 5, stride=2, padding=2, output_padding=1)\n        self.t1 = torch.randn(1, 512, 8, 8)\n    def forward(self, x1):\n        y1 = self.t1 / 4.0\n        r1 = torch.clamp(y1, min=0)\n        v1 = self.conv_transpose(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp(v2, min=0)\n        v4 = torch.clamp(v3, max=6)\n        v5 = v1 * v4\n        v6 = v5 / 6\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 512, 8, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(6, 6, 3, stride=1, padding=1, dilation=3, output_padding=1)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp(v2, min=0)\n        v4 = torch.clamp(v3, max=6)\n        v5 = v1 * v4\n        v6 = v5 / 6\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 6, 22, 22)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(2, 2, 2)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp(v2, min=0)\n        v4 = torch.clamp(v3, max=6)\n        v5 = v1 * v4\n        v6 = v5 / 6\n        return v6\n# Inputs to the model\nx1 = torch.randn(2, 2, 15, 15)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(1, 1, 3, stride=1)\n        self.avgpool = torch.nn.AvgPool2d(3, stride=1, padding=1)\n        self.t1 = torch.randn(1, 1, 3, 3)\n        self.tanh = torch.nn.Tanh()\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        y1 = self.avgpool(v1)\n        v2 = self.t1 / 9.0\n        z1 = torch.clamp(v2, min=0)\n        y2 = self.tanh(y1)\n        v3 = self.t1 / 9.0\n        r1 = torch.clamp(v3, min=0)\n        v4 = z1 * r1\n        v5 = y2 * v4\n        v6 = v5 / 9.0\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 1, 3, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(512, 512, 1, stride=1)\n        self.conv_transpose = torch.nn.ConvTranspose2d(512, 512, 3, stride=1)\n        self.t1 = torch.randn(1, 512, 8, 8)\n        self.t2 = torch.randn(1, 512, 8, 8)\n    def forward(self, x1, x2):\n        y1 = self.t1 * 0.66\n        u1 = self.t2 * 2\n        z1 = self.conv(y1)\n        t1 = torch.clamp(z1, min=0)\n        y2 = self.t1 + u1\n        u2 = self.t2 / 9.0\n        z2 = self.conv(y2)\n        t2 = torch.clamp(z2, min=0)\n        v1 = self.conv_transpose(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp(v2, min=0 + t1 + t2)\n        v4 = torch.clamp(v3, max=6)\n        v5 = v1 * v4\n        v6 = v5 / 6\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 512, 8, 8)\nx2 = torch.randn(1, 512, 8, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(1, 1, 1, stride=1)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp(v2, min=0)\n        v4 = torch.clamp(v3, max=6)\n        v5 = v1 * v4\n        v6 = v5 / 6\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 1, 31, 39)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(7, 6, 5, stride=3, padding=2, groups=7)\n        self.t1 = torch.randn(1, 7, 15, 49)\n    def forward(self, x1):\n        c1 = 0.6483789937019348\n        y1 = self.t1 * c1\n        y2 = torch.clamp(y1, min=-1)\n        y3 = torch.clamp(y2, max=1)\n        o1 = y3.sign()\n        y4 = o1.float() * 2 - 1\n        r1 = torch.clamp(y4, min=0)\n        o2 = o1 == 0\n        z1 = o2.float() * r1\n        b1 = torch.clamp(z1, min=-1, max=1)\n        v1 = self.conv_transpose(b1)\n        v2 = v1 + 0.805849418683916\n        v3 = torch.clamp(v2, min=0)\n        v4 = torch.clamp(v3, max=10)\n        v5 = v1 * v4\n        v6 = v5 / 8.9792871\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 7, 15, 49)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(134, 96, 5, stride=1, padding=2)\n        self.t1 = torch.randn(1, 134, 68, 30)\n    def forward(self, x1):\n        y1 = self.t1 + 0.3\n        y2 = torch.abs(y1)\n        r1 = torch.clamp(y2, min=0)\n        v1 = self.conv_transpose(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp(v2, min=0)\n        v4 = torch.clamp(v3, max=6)\n        v5 = v1 * v4\n        v6 = v5 / 6\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 134, 68, 30)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(246, 53, 10, stride=2, padding=6, output_padding=1)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp(v2, min=0)\n        v4 = torch.clamp(v3, max=6)\n        v5 = v1 * v4\n        v6 = v5 / 6\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 246, 13, 27)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(1, 1, 5, stride=2, bias=True, padding=2)\n    def forward(self, x1):\n        y1 = self.conv(x1)\n        y2 = torch.floor(y1)\n        return y2\n# Inputs to the model\nx1 = torch.randn(1, 1, 54, 94)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(512, 4, 5, stride=2, padding=2, output_padding=1)\n        self.t1 = torch.randn(1, 512, 8, 8)\n    def forward(self, x1):\n        y1 = self.t1 / 4.0\n        r1 = torch.clamp(y1, min=0)\n        v1 = self.conv_transpose(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp(v2, min=0)\n        v4 = torch.clamp(v3, max=6)\n        v5 = v1 * v4\n        v6 = v5 / 6\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 512, 8, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(6, 6, 3, stride=1, padding=1, dilation=3, output_padding=1)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp(v2, min=0)\n        v4 = torch.clamp(v3, max=6)\n        v5 = v1 * v4\n        v6 = v5 / 6\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 6, 22, 22)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(2, 2, 2)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp(v2, min=0)\n        v4 = torch.clamp(v3, max=6)\n        v5 = v1 * v4\n        v6 = v5 / 6\n        return v6\n# Inputs to the model\nx1 = torch.randn(2, 2, 15, 15)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(1, 1, 3, stride=1)\n        self.avgpool = torch.nn.AvgPool2d(3, stride=1, padding=1)\n        self.t1 = torch.randn(1, 1, 3, 3)\n        self.tanh = torch.nn.Tanh()\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        y1 = self.avgpool(v1)\n        v2 = self.t1 / 9.0\n        z1 = torch.clamp(v2, min=0)\n        y2 = self.tanh(y1)\n        v3 = self.t1 / 9.0\n        r1 = torch.clamp(v3, min=0)\n        v4 = z1 * r1\n        v5 = y2 * v4\n        v6 = v5 / 9.0\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 1, 3, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(512, 512, 1, stride=1)\n        self.conv_transpose = torch.nn.ConvTranspose2d(512, 512, 3, stride=1)\n        self.t1 = torch.randn(1, 512, 8, 8)\n        self.t2 = torch.randn(1, 512, 8, 8)\n    def forward(self, x1, x2):\n        y1 = self.t1 * 0.66\n        u1 = self.t2 * 2\n        z1 = self.conv(y1)\n        t1 = torch.clamp(z1, min=0)\n        y2 = self.t1 + u1\n        u2 = self.t2 / 9.0\n        z2 = self.conv(y2)\n        t2 = torch.clamp(z2, min=0)\n        v1 = self.conv_transpose(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp(v2, min=0 + t1 + t2)\n        v4 = torch.clamp(v3, max=6)\n        v5 = v1 * v4\n        v6 = v5 / 6\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 512, 8, 8)\nx2 = torch.randn(1, 512, 8, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(1, 1, 1, stride=1)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp(v2, min=0)\n        v4 = torch.clamp(v3, max=6)\n        v5 = v1 * v4\n        v6 = v5 / 6\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 1, 31, 39)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(7, 6, 5, stride=3, padding=2, groups=7)\n        self.t1 = torch.randn(1, 7, 15, 49)\n    def forward(self, x1):\n        c1 = 0.6483789937019348\n        y1 = self.t1 * c1\n        y2 = torch.clamp(y1, min=-1)\n        y3 = torch.clamp(y2, max=1)\n        o1 = y3.sign()\n        y4 = o1.float() * 2 - 1\n        r1 = torch.clamp(y4, min=0)\n        o2 = o1 == 0\n        z1 = o2.float() * r1\n        b1 = torch.clamp(z1, min=-1, max=1)\n        v1 = self.conv_transpose(b1)\n        v2 = v1 + 0.805849418683916\n        v3 = torch.clamp(v2, min=0)\n        v4 = torch.clamp(v3, max=10)\n        v5 = v1 * v4\n        v6 = v5 / 8.9792871\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 7, 15, 49)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(134, 96, 5, stride=1, padding=2)\n        self.t1 = torch.randn(1, 134, 68, 30)\n    def forward(self, x1):\n        y1 = self.t1 + 0.3\n        y2 = torch.abs(y1)\n        r1 = torch.clamp(y2, min=0)\n        v1 = self.conv_transpose(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp(v2, min=0)\n        v4 = torch.clamp(v3, max=6)\n        v5 = v1 * v4\n        v6 = v5 / 6\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 134, 68, 30)\n"
            ],
            "g_time": 12.54234766960144
        }
    }
}
{
    "opt": "unary=7",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(16, 6)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * torch.clamp(v1 + 3, 0, 6)\n        v3 = v2 / 6\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * v1.clamp(min=0, max=6) + 3\n        v3 = v2 / 6\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(16, 32)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * torch.max(torch.min(v1 + 3, 6), 0) / 6\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(2, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(128, 128)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * torch.clamp(v1 + 3, min=0, max=6)\n        v3 = v2 / 6\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(4, 128)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(64, 32)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * torch.clamp(v1 + 3, 0, 6)\n        v3 = v2 / 6\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 32, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.clamp(v1 + 3, 0, 6)\n        v3 = v2 / 6\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(8, 3)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * torch.clamp(v1 + 3, 0, 6)\n        v3 = v2 / 6\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(10, 20)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * torch.clamp(torch.min(v1) + 3, 0, 6)\n        v3 = v2 / 6\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 10)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(4, 4)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * torch.clamp(v1 + 3, min=0, max=6)\n        v3 = v2 / 6\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 4)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self,):\n        super().__init__()\n        self.linear = torch.nn.Linear(100, 100)\n\n    def forward(self, x2):\n        v1 = self.linear(x2)\n        v2 = v1 * torch.clamp(v1 + 3, min=0, max=6)\n        v3 = v2 / 6\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx2 = torch.randn(1,100)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(16, 6)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * torch.clamp(v1 + 3, 0, 6)\n        v3 = v2 / 6\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * v1.clamp(min=0, max=6) + 3\n        v3 = v2 / 6\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(16, 32)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * torch.max(torch.min(v1 + 3, 6), 0) / 6\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(2, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(128, 128)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * torch.clamp(v1 + 3, min=0, max=6)\n        v3 = v2 / 6\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(4, 128)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(64, 32)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * torch.clamp(v1 + 3, 0, 6)\n        v3 = v2 / 6\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 32, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.clamp(v1 + 3, 0, 6)\n        v3 = v2 / 6\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(8, 3)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * torch.clamp(v1 + 3, 0, 6)\n        v3 = v2 / 6\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(10, 20)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * torch.clamp(torch.min(v1) + 3, 0, 6)\n        v3 = v2 / 6\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 10)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(4, 4)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * torch.clamp(v1 + 3, min=0, max=6)\n        v3 = v2 / 6\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 4)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self,):\n        super().__init__()\n        self.linear = torch.nn.Linear(100, 100)\n\n    def forward(self, x2):\n        v1 = self.linear(x2)\n        v2 = v1 * torch.clamp(v1 + 3, min=0, max=6)\n        v3 = v2 / 6\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx2 = torch.randn(1,100)\n"
            ],
            "g_time": 5.373092412948608
        }
    }
}
{
    "opt": "binary_unary=4",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    \n    def forward(self, x1, other=None):\n        v1 = self.linear(x1)\n        v2 = v1 + other\n        v3 = torch.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 4)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n \n    def forward(self, x1, other=torch.ones(8)):\n        v1 = self.linear(x1)\n        v2 = v1 + other\n        v3 = torch.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(16, 2, bias=False)\n        self.other = torch.randn(2, 16)\n \n    def forward(self, x1, **kargs):\n        v1 = self.linear(x1)\n        v2 = self.other + v1\n        v3 = F.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 1, 32, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(1, 10)\n \n    def forward(self, x1, other):\n        v1 = self.linear(x1)\n        v2 = v1 + other\n        v3 = torch.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 1)\nother = torch.randn(10)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, other):\n        super().__init__()\n        self.l1 = torch.nn.Linear(64, 32)\n        self.other = other\n\n    def forward(self, x1):\n        v1 = self.l1(x1)\n        v2 = v1 + self.other\n        v3 = F.relu(v2)\n        return v3\n\n# Initializing the model\nother = torch.randn(32)\nm = Model(other)\n\n# Inputs to the model\nx1 = torch.randn(1, 64)\n",
                "\nclass LinearModel(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n \n    def forward(self, x1, other):\n        v1 = self.linear(x1)\n        v2 = v1 + other\n        v3 = F.relu(v2)\n        return v3\n\n# Initializing the model\nm = LinearModel()\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(10, 10)\n \n    def forward(self, x1, x2=None):\n        v1 = self.linear(x1)\n        v2 = v1 + x2\n        v3 = F.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 10)\nx2 = torch.randn(1, 10)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(32, 64)\n \n    def forward(self, x1, x2):\n        v1 = self.linear(x1)\n        v2 = v1 + x2\n        v3 = torch.nn.functional.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Model inputs\nx1 = torch.randn(1, 32)\nx2 = torch.randn(1, 32)\n\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(64, 64)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 + other\n        v3 = torch.nn.functional.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 64)\nother = torch.randn(1, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, other):\n        super().__init__()\n        self.linear = torch.nn.Linear(6144, 256)\n        self.other = other\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 + self.other\n        v3 = torch.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model(torch.nn.Parameter(torch.rand(256)))\n\n# Inputs to the model\nx1 = torch.randn(1, 6144)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    \n    def forward(self, x1, other=None):\n        v1 = self.linear(x1)\n        v2 = v1 + other\n        v3 = torch.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 4)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n \n    def forward(self, x1, other=torch.ones(8)):\n        v1 = self.linear(x1)\n        v2 = v1 + other\n        v3 = torch.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(16, 2, bias=False)\n        self.other = torch.randn(2, 16)\n \n    def forward(self, x1, **kargs):\n        v1 = self.linear(x1)\n        v2 = self.other + v1\n        v3 = F.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 1, 32, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(1, 10)\n \n    def forward(self, x1, other):\n        v1 = self.linear(x1)\n        v2 = v1 + other\n        v3 = torch.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 1)\nother = torch.randn(10)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, other):\n        super().__init__()\n        self.l1 = torch.nn.Linear(64, 32)\n        self.other = other\n\n    def forward(self, x1):\n        v1 = self.l1(x1)\n        v2 = v1 + self.other\n        v3 = F.relu(v2)\n        return v3\n\n# Initializing the model\nother = torch.randn(32)\nm = Model(other)\n\n# Inputs to the model\nx1 = torch.randn(1, 64)\n",
                "\nclass LinearModel(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n \n    def forward(self, x1, other):\n        v1 = self.linear(x1)\n        v2 = v1 + other\n        v3 = F.relu(v2)\n        return v3\n\n# Initializing the model\nm = LinearModel()\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(10, 10)\n \n    def forward(self, x1, x2=None):\n        v1 = self.linear(x1)\n        v2 = v1 + x2\n        v3 = F.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 10)\nx2 = torch.randn(1, 10)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(32, 64)\n \n    def forward(self, x1, x2):\n        v1 = self.linear(x1)\n        v2 = v1 + x2\n        v3 = torch.nn.functional.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Model inputs\nx1 = torch.randn(1, 32)\nx2 = torch.randn(1, 32)\n\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(64, 64)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 + other\n        v3 = torch.nn.functional.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 64)\nother = torch.randn(1, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, other):\n        super().__init__()\n        self.linear = torch.nn.Linear(6144, 256)\n        self.other = other\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 + self.other\n        v3 = torch.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model(torch.nn.Parameter(torch.rand(256)))\n\n# Inputs to the model\nx1 = torch.randn(1, 6144)\n"
            ],
            "g_time": 5.503919363021851
        }
    }
}
{
    "opt": "unary=1",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc = torch.nn.Linear(16, 64)\n \n    def forward(self, x1):\n        v1 = self.fc(x1)\n        v2 = v1 * 0.5\n        v3 = v1 + (v1 * v1 * v1) * 0.044715\n        v4 = v3 * 0.7978845608028654\n        v5 = torch.tanh(v4)\n        v6 = v5 + 1\n        v7 = v2 * v6\n        return v7\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(128, 100)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * 0.5\n        v3 = v1 + (v1 * v1 * v1) * 0.044715\n        v4 = v3 * 0.7978845608028654\n        v5 = torch.tanh(v4)\n        v6 = v5 + 1\n        v7 = v2 * v6\n        return v7\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 128)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc1 = torch.nn.Linear(3, 1)\n \n    def forward(self, x1):\n        v1 = self.fc1(x1)\n        v2 = v1 * 0.5\n        v3 = v1 + (v1 * v1 * v1) * 0.044715\n        v4 = v3 * 0.7978845608028654\n        v5 = torch.tanh(v4)\n        v6 = v5 + 1\n        v7 = v2 * v6\n        return v7\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(8, 16)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * 0.5\n        v3 = v1 + (v1 * v1 * v1) * 0.044715\n        v4 = v3 * 0.7978845608028654\n        v5 = torch.tanh(v4)\n        v6 = v5 + 1\n        v7 = v2 * v6\n        return v7\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(64, 1000)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * 0.5\n        v3 = v1 + (v1 * v1 * v1) * 0.044715\n        v4 = v3 * 0.7978845608028654\n        v5 = torch.tanh(v4)\n        v6 = v5 + 1\n        v7 = v2 * v6\n        return v7\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * 0.5\n        v3 = v1 + (v1 * v1 * v1) * 0.044715\n        v4 = v3 * 0.7978845608028654\n        v5 = torch.tanh(v4)\n        v6 = v5 + 1\n        v7 = v2 * v6\n        return v7\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(8, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * 0.5\n        v3 = v1 + (v1 * v1 * v1) * 0.044715\n        v4 = v3 * 0.7978845608028654\n        v5 = torch.tanh(v4)\n        v6 = v5 + 1\n        v7 = v2 * v6\n        return v7\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(5, 1)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * 0.5\n        v3 = v1 + (v1 * v1 * v1) * 0.044715\n        v4 = v3 * 0.7978845608028654\n        v5 = torch.tanh(v4)\n        v6 = v5 + 1\n        v7 = v2 * v6\n        return v7\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 5)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(64, 128)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * 0.5\n        v3 = v1 + (v1 * v1 * v1) * 0.044715\n        v4 = v3 * 0.7978845608028654\n        v5 = torch.tanh(v4)\n        v6 = v5 + 1\n        v7 = v2 * v6\n        return v7\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * 0.5\n        v3 = v1 + (v1 * v1 * v1) * 0.044715\n        v4 = v3 * 0.7978845608028654\n        v5 = torch.tanh(v4)\n        v6 = v5 + 1\n        v7 = v2 * v6\n        return v7\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc = torch.nn.Linear(16, 64)\n \n    def forward(self, x1):\n        v1 = self.fc(x1)\n        v2 = v1 * 0.5\n        v3 = v1 + (v1 * v1 * v1) * 0.044715\n        v4 = v3 * 0.7978845608028654\n        v5 = torch.tanh(v4)\n        v6 = v5 + 1\n        v7 = v2 * v6\n        return v7\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(128, 100)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * 0.5\n        v3 = v1 + (v1 * v1 * v1) * 0.044715\n        v4 = v3 * 0.7978845608028654\n        v5 = torch.tanh(v4)\n        v6 = v5 + 1\n        v7 = v2 * v6\n        return v7\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 128)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc1 = torch.nn.Linear(3, 1)\n \n    def forward(self, x1):\n        v1 = self.fc1(x1)\n        v2 = v1 * 0.5\n        v3 = v1 + (v1 * v1 * v1) * 0.044715\n        v4 = v3 * 0.7978845608028654\n        v5 = torch.tanh(v4)\n        v6 = v5 + 1\n        v7 = v2 * v6\n        return v7\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(8, 16)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * 0.5\n        v3 = v1 + (v1 * v1 * v1) * 0.044715\n        v4 = v3 * 0.7978845608028654\n        v5 = torch.tanh(v4)\n        v6 = v5 + 1\n        v7 = v2 * v6\n        return v7\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(64, 1000)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * 0.5\n        v3 = v1 + (v1 * v1 * v1) * 0.044715\n        v4 = v3 * 0.7978845608028654\n        v5 = torch.tanh(v4)\n        v6 = v5 + 1\n        v7 = v2 * v6\n        return v7\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * 0.5\n        v3 = v1 + (v1 * v1 * v1) * 0.044715\n        v4 = v3 * 0.7978845608028654\n        v5 = torch.tanh(v4)\n        v6 = v5 + 1\n        v7 = v2 * v6\n        return v7\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(8, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * 0.5\n        v3 = v1 + (v1 * v1 * v1) * 0.044715\n        v4 = v3 * 0.7978845608028654\n        v5 = torch.tanh(v4)\n        v6 = v5 + 1\n        v7 = v2 * v6\n        return v7\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(5, 1)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * 0.5\n        v3 = v1 + (v1 * v1 * v1) * 0.044715\n        v4 = v3 * 0.7978845608028654\n        v5 = torch.tanh(v4)\n        v6 = v5 + 1\n        v7 = v2 * v6\n        return v7\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 5)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(64, 128)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * 0.5\n        v3 = v1 + (v1 * v1 * v1) * 0.044715\n        v4 = v3 * 0.7978845608028654\n        v5 = torch.tanh(v4)\n        v6 = v5 + 1\n        v7 = v2 * v6\n        return v7\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * 0.5\n        v3 = v1 + (v1 * v1 * v1) * 0.044715\n        v4 = v3 * 0.7978845608028654\n        v5 = torch.tanh(v4)\n        v6 = v5 + 1\n        v7 = v2 * v6\n        return v7\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n"
            ],
            "g_time": 7.992325782775879
        }
    }
}
{
    "opt": "cat_mm",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, x2):\n        v = torch.mm(x1, x2)\n        return torch.cat([v, v.abs(), v.add(100), 0 - v, -100 - v.abs(), v.add(200), -200 - v])\n# Inputs to the model\nx1 = torch.randn(2, 4)\nx2 = torch.randn(4, 4)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, x2):\n        v = torch.mm(x1, x2)\n        return torch.cat([v] * 20, 2)\n# Inputs to the model\nx1 = torch.randn(4, 4)\nx2 = torch.randn(3, 4)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, x2, x3, x4):\n        t2 = torch.mm(x2, x3)\n        t1 = torch.mm(x1, x2)\n        t = torch.cat([t1, t1, t1, t1, t1, t1, t1, t1, t1, t1, t1, t1], 1)\n        return torch.cat([t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t1], 1)\n# Inputs to the model\nx1 = torch.randn(2, 3)\nx2 = torch.randn(2, 3)\nx3 = torch.randn(2, 3)\nx4 = torch.randn(2, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, x2):\n        t1 = torch.mm(x1, x2)\n        t2 = torch.mm(x1, x2)\n        t = torch.cat([t1, t2])\n        return torch.cat([t] * 20, 1)\n# Inputs to the model\nx1 = torch.randn(2, 2)\nx2 = torch.randn(2, 2)\n",
                "\nclass Module1(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc1 = torch.nn.Linear(16, 32)\n    def forward(self, x):\n        return self.fc1(x)\nclass Module2(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc1 = torch.nn.Linear(32, 64)\n    def forward(self, x):\n        return self.fc1(x)\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc1 = torch.nn.Linear(16, 64)\n        self.module1 = Module1()\n        self.module2 = Module2()\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.module1(x)\n        x = self.module2(x)\n        return x\n# Inputs to the model\nx = torch.randn(2, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, x2):\n        v1 = torch.mm(x1, x2)\n        v2 = torch.mm(x1, x2)\n        v3 = torch.mm(x1, x2)\n        v4 = torch.mm(x1, x2)\n        v5 = torch.mm(x1, x2)\n        v6 = torch.mm(x1, x2)\n        v7 = torch.mm(x1, x2)\n        v8 = torch.mm(x1, x2)\n        v9 = torch.mm(x1, x2)\n        v10 = torch.mm(x1, x2)\n        v11 = torch.mm(x1, x2)\n        v12 = torch.mm(x1, x2)\n        v13 = torch.mm(x1, x2)\n        v14 = torch.mm(x1, x2)\n        v15 = torch.mm(x1, x2)\n        v16 = torch.mm(x1, x2)\n        v17 = torch.mm(x1, x2)\n        v18 = torch.mm(x1, x2)\n        v19 = torch.mm(x1, x2)\n        v20 = torch.mm(x1, x2)\n        v21 = torch.mm(x1, x2)\n        v22 = torch.mm(x1, x2)\n        v23 = torch.mm(x1, x2)\n        v24 = torch.mm(x1, x2)\n        v25 = torch.mm(x1, x2)\n        v26 = torch.mm(x1, x2)\n        v27 = torch.mm(x1, x2)\n        v28 = torch.mm(x1, x2)\n        v29 = torch.mm(x1, x2)\n        v30 = torch.mm(x1, x2)\n        return torch.cat([v1, v2, v3, v4, v5, v6, v7, v8, v9, v10, v11, v12, v13, v14, v15, v16, v17, v18, v19, v20, v21, v22, v23, v24, v25, v26, v27, v28, v29, v30], 1) # Concatenation of the result tensor along a specified dimension\n\n# Inputs to the model\nx1 = torch.randn(2, 4)\nx2 = torch.randn(4, 4)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, x2):\n        t = torch.mm(x1, x2)\n        return torch.cat([t, t, t, t], 1)\n# Inputs to the model\nx1 = torch.randn(3, 3)\nx2 = torch.randn(3, 5)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, x2):\n        t1 = torch.mm(x1, x2)\n        t1 = torch.mm(x1, x2)\n        t1 = torch.nn.functional.dropout(t1, p=0.1, training=False)\n        t2 = torch.cat([t1, t1, t1, t1], 1)\n        t2 = torch.cat([t2, t2, t2, t2], 1)\n        t3 = torch.cat([t2, t2, t2, t2], 1)\n        return torch.cat([t3, t3, t3, t3], 1)\n# Inputs to the model\nx1 = torch.randn(1, 4)\nx2 = torch.randn(4, 4)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, x2):\n        v = torch.mm(x1, x2)\n        v = torch.cat(2, [v] * 20)\n        return torch.cat([v] * 20, 1)\n# Inputs to the model\nx1 = torch.randn(2, 4)\nx2 = torch.randn(4, 4)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, x2):\n        t1 = torch.mm(x1, x2)\n        t2 = torch.cat([t1, t1, t1], 1)\n        t3 = torch.cat([t2, t2, t2], 1)\n        t4 = torch.cat([t3, t3, t3], 1)\n        t5 = torch.cat([t4, t4, t4], 1)\n        return torch.cat([t5, t5], 0)\n# Inputs to the model\nx1 = torch.randn(3, 3)\nx2 = torch.randn(3, 5)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, x2):\n        v = torch.mm(x1, x2)\n        return torch.cat([v, v.abs(), v.add(100), 0 - v, -100 - v.abs(), v.add(200), -200 - v])\n# Inputs to the model\nx1 = torch.randn(2, 4)\nx2 = torch.randn(4, 4)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, x2):\n        v = torch.mm(x1, x2)\n        return torch.cat([v] * 20, 2)\n# Inputs to the model\nx1 = torch.randn(4, 4)\nx2 = torch.randn(3, 4)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, x2, x3, x4):\n        t2 = torch.mm(x2, x3)\n        t1 = torch.mm(x1, x2)\n        t = torch.cat([t1, t1, t1, t1, t1, t1, t1, t1, t1, t1, t1, t1], 1)\n        return torch.cat([t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t1], 1)\n# Inputs to the model\nx1 = torch.randn(2, 3)\nx2 = torch.randn(2, 3)\nx3 = torch.randn(2, 3)\nx4 = torch.randn(2, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, x2):\n        t1 = torch.mm(x1, x2)\n        t2 = torch.mm(x1, x2)\n        t = torch.cat([t1, t2])\n        return torch.cat([t] * 20, 1)\n# Inputs to the model\nx1 = torch.randn(2, 2)\nx2 = torch.randn(2, 2)\n",
                "\nclass Module1(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc1 = torch.nn.Linear(16, 32)\n    def forward(self, x):\n        return self.fc1(x)\nclass Module2(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc1 = torch.nn.Linear(32, 64)\n    def forward(self, x):\n        return self.fc1(x)\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc1 = torch.nn.Linear(16, 64)\n        self.module1 = Module1()\n        self.module2 = Module2()\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.module1(x)\n        x = self.module2(x)\n        return x\n# Inputs to the model\nx = torch.randn(2, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, x2):\n        v1 = torch.mm(x1, x2)\n        v2 = torch.mm(x1, x2)\n        v3 = torch.mm(x1, x2)\n        v4 = torch.mm(x1, x2)\n        v5 = torch.mm(x1, x2)\n        v6 = torch.mm(x1, x2)\n        v7 = torch.mm(x1, x2)\n        v8 = torch.mm(x1, x2)\n        v9 = torch.mm(x1, x2)\n        v10 = torch.mm(x1, x2)\n        v11 = torch.mm(x1, x2)\n        v12 = torch.mm(x1, x2)\n        v13 = torch.mm(x1, x2)\n        v14 = torch.mm(x1, x2)\n        v15 = torch.mm(x1, x2)\n        v16 = torch.mm(x1, x2)\n        v17 = torch.mm(x1, x2)\n        v18 = torch.mm(x1, x2)\n        v19 = torch.mm(x1, x2)\n        v20 = torch.mm(x1, x2)\n        v21 = torch.mm(x1, x2)\n        v22 = torch.mm(x1, x2)\n        v23 = torch.mm(x1, x2)\n        v24 = torch.mm(x1, x2)\n        v25 = torch.mm(x1, x2)\n        v26 = torch.mm(x1, x2)\n        v27 = torch.mm(x1, x2)\n        v28 = torch.mm(x1, x2)\n        v29 = torch.mm(x1, x2)\n        v30 = torch.mm(x1, x2)\n        return torch.cat([v1, v2, v3, v4, v5, v6, v7, v8, v9, v10, v11, v12, v13, v14, v15, v16, v17, v18, v19, v20, v21, v22, v23, v24, v25, v26, v27, v28, v29, v30], 1) # Concatenation of the result tensor along a specified dimension\n\n# Inputs to the model\nx1 = torch.randn(2, 4)\nx2 = torch.randn(4, 4)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, x2):\n        t = torch.mm(x1, x2)\n        return torch.cat([t, t, t, t], 1)\n# Inputs to the model\nx1 = torch.randn(3, 3)\nx2 = torch.randn(3, 5)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, x2):\n        t1 = torch.mm(x1, x2)\n        t1 = torch.mm(x1, x2)\n        t1 = torch.nn.functional.dropout(t1, p=0.1, training=False)\n        t2 = torch.cat([t1, t1, t1, t1], 1)\n        t2 = torch.cat([t2, t2, t2, t2], 1)\n        t3 = torch.cat([t2, t2, t2, t2], 1)\n        return torch.cat([t3, t3, t3, t3], 1)\n# Inputs to the model\nx1 = torch.randn(1, 4)\nx2 = torch.randn(4, 4)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, x2):\n        v = torch.mm(x1, x2)\n        v = torch.cat(2, [v] * 20)\n        return torch.cat([v] * 20, 1)\n# Inputs to the model\nx1 = torch.randn(2, 4)\nx2 = torch.randn(4, 4)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, x2):\n        t1 = torch.mm(x1, x2)\n        t2 = torch.cat([t1, t1, t1], 1)\n        t3 = torch.cat([t2, t2, t2], 1)\n        t4 = torch.cat([t3, t3, t3], 1)\n        t5 = torch.cat([t4, t4, t4], 1)\n        return torch.cat([t5, t5], 0)\n# Inputs to the model\nx1 = torch.randn(3, 3)\nx2 = torch.randn(3, 5)\n"
            ],
            "g_time": 19.1188006401062
        }
    }
}
{
    "opt": "sink_cat_after_pointwise",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x):\n        y = torch.cat((x.tanh(), x.tanh(), x.tanh()), dim=1)\n        return y.view(-1) if y.shape!= (1, 3) else y.view(1, 1)\n# Inputs to the model\nx = torch.randn(2, 4)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x, y):\n        v1 = torch.cat((x.tanh(), x.tanh(), x.tanh()), dim=1)\n        v2 = torch.cat((y.tanh(), y.tanh(), y.tanh()), dim=1)\n        z = v1 + v2\n        return z.view(z.shape[0], -1).relu() if z.shape!= (1, 3) else z.view(z.shape[0], -1).tanh()\n# Inputs to the model\nx = torch.randn(2, 3, 4)\ny = torch.randn(2, 3, 4)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x):\n        y = x + torch.randn_like(x) + x + torch.randn_like(x)\n        return y\n# Inputs to the model\nx = torch.randn(2, 2, 3, 4)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x):\n        y = x.view(2, 4)\n        return y.view(4).tanh()\n# Inputs to the model\nx = torch.randn(2, 4)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.mod1 = torch.nn.ReLU()\n        self.mod2 = torch.nn.Tanh()\n    def forward(self, x):\n        y = torch.cat((x.tanh(), x.tanh(), x.tanh()), dim=1)\n        return y.view(y.shape[0], -1).relu() if y.shape!= (1, 3) else y.view(y.shape[0], -1).tanh()\n# Inputs to the model\nx = torch.randn(2, 3, 4)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x):\n        y = x.unsqueeze(dim=1).unsqueeze(dim=1)\n        y1 = y.tanh().sigmoid()\n        y2 = y.tanh().relu()\n        return torch.cat((y1, y2), dim=1).tanh() if y.shape!= (2, 3, 4, 4) else torch.cat((y1, y2), dim=1).relu()\n# Inputs to the model\nx = torch.randn(2, 3, 4)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x):\n        y = torch.cat((x.tanh(), x, x.tanh()), dim=-1) # y is non-overlapping\n        return y\n# Inputs to the model\nx = torch.randn(2, 3, 4)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x):\n        y = torch.tanh(x * x)\n        b = torch.relu(x + y)\n        c = torch.sigmoid(y * y)\n        d = torch.tanh(b + c)\n        e = torch.relu(-b + d)\n        f = torch.sigmoid(d * d)\n        return torch.cat((y, b, c * c), dim=1)\n# Inputs to the model\nx = torch.randn(1, 2, 4)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x):\n        y = torch.cat((x, x, x), dim=1)\n        y = y.view(y.shape[0], y.shape[1], 3)\n        y = y.relu()\n        if y.shape!= (1, 3):\n            y = y.tanh()\n        return y\n# Inputs to the model\nx = torch.randn(2, 3, 4)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x):\n        y = torch.cat((x, x), dim=0).view(-1, x.shape[:1])\n        return y\n# Inputs to the model\nx = torch.randn(2, 3, 4)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x):\n        y = torch.cat((x.tanh(), x.tanh(), x.tanh()), dim=1)\n        return y.view(-1) if y.shape!= (1, 3) else y.view(1, 1)\n# Inputs to the model\nx = torch.randn(2, 4)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x, y):\n        v1 = torch.cat((x.tanh(), x.tanh(), x.tanh()), dim=1)\n        v2 = torch.cat((y.tanh(), y.tanh(), y.tanh()), dim=1)\n        z = v1 + v2\n        return z.view(z.shape[0], -1).relu() if z.shape!= (1, 3) else z.view(z.shape[0], -1).tanh()\n# Inputs to the model\nx = torch.randn(2, 3, 4)\ny = torch.randn(2, 3, 4)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x):\n        y = x + torch.randn_like(x) + x + torch.randn_like(x)\n        return y\n# Inputs to the model\nx = torch.randn(2, 2, 3, 4)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x):\n        y = x.view(2, 4)\n        return y.view(4).tanh()\n# Inputs to the model\nx = torch.randn(2, 4)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.mod1 = torch.nn.ReLU()\n        self.mod2 = torch.nn.Tanh()\n    def forward(self, x):\n        y = torch.cat((x.tanh(), x.tanh(), x.tanh()), dim=1)\n        return y.view(y.shape[0], -1).relu() if y.shape!= (1, 3) else y.view(y.shape[0], -1).tanh()\n# Inputs to the model\nx = torch.randn(2, 3, 4)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x):\n        y = x.unsqueeze(dim=1).unsqueeze(dim=1)\n        y1 = y.tanh().sigmoid()\n        y2 = y.tanh().relu()\n        return torch.cat((y1, y2), dim=1).tanh() if y.shape!= (2, 3, 4, 4) else torch.cat((y1, y2), dim=1).relu()\n# Inputs to the model\nx = torch.randn(2, 3, 4)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x):\n        y = torch.cat((x.tanh(), x, x.tanh()), dim=-1) # y is non-overlapping\n        return y\n# Inputs to the model\nx = torch.randn(2, 3, 4)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x):\n        y = torch.tanh(x * x)\n        b = torch.relu(x + y)\n        c = torch.sigmoid(y * y)\n        d = torch.tanh(b + c)\n        e = torch.relu(-b + d)\n        f = torch.sigmoid(d * d)\n        return torch.cat((y, b, c * c), dim=1)\n# Inputs to the model\nx = torch.randn(1, 2, 4)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x):\n        y = torch.cat((x, x, x), dim=1)\n        y = y.view(y.shape[0], y.shape[1], 3)\n        y = y.relu()\n        if y.shape!= (1, 3):\n            y = y.tanh()\n        return y\n# Inputs to the model\nx = torch.randn(2, 3, 4)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x):\n        y = torch.cat((x, x), dim=0).view(-1, x.shape[:1])\n        return y\n# Inputs to the model\nx = torch.randn(2, 3, 4)\n"
            ],
            "g_time": 5.889155149459839
        }
    }
}
{
    "opt": "binary=2",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.conv = torch.nn.Conv2d(1, 4, 1, stride=1, padding=0)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 - True\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 1, 256, 256)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.conv = torch.nn.Conv2d(3, 6, 1, stride=1, padding=0)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 - True\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 3, 352, 256)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.conv = torch.nn.Conv2d(3, 8, 3)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 - False\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 8, 28, 28)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.quant = torch.quantization.QuantStub()\n        self.conv1 = torch.nn.Conv2d(1, 4, 7)\n        self.conv2 = torch.nn.Conv2d(4, 2, 5)\n        self.maxpool = torch.nn.MaxPool2d(3, stride=2, padding=1)\n        self.fc = torch.nn.Linear(84, 256)\n        self.relu = torch.nn.ReLU()\n        self.dequant = torch.quantization.DeQuantStub()\n        self.conv3 = torch.nn.Conv2d(2, 1, 5)\n        self.sigmoid = torch.nn.Sigmoid()\n    def forward(self, x):\n        x = self.quant(x)\n        x = self.conv1(x)\n        # TODO (soto) - add more layers here\n        x = self.conv3(x)\n        return self.sigmoid(self.dequant(x))\n# Inputs to the model\nx = torch.randn(1, 1, 28, 28)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(1, 2, 1)\n    def forward(self, x):\n        v1 = self.conv(x)\n        v2 = v1 - 1\n        return v2\n# Inputs to the model\nx = torch.randn(1, 1, 32, 32)\n",
                "\nclass Model(nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.conv = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1)\n        torch.nn.init.zeros_(self.conv.weight) # Set the weight tensor such that the result of the pointwise convolution is all zeros.\n\n    def forward(self, x):\n        v1 = self.conv(x) # Run the convolution operator\n        return torch.sigmoid(v1) # Apply sigmoid operator to the previous result\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.conv = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1, groups=2, dilation=2)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 - False\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 8, 1)\n    def forward(self, x2, x3, x4, x5):\n        v1 = self.conv(x2)\n        v2 = v1 - 1.0\n        v3 = torch.sinh(x3)\n        v4 = v2 * v3\n        v5 = torch.nn.functional.hardtanh(x4)\n        v6 = v4 - v5\n        v7 = v6 + x5\n        return v7\n# Inputs to the model\nx2 = torch.randn(1, 3, 64, 64)\nx3 = torch.randn(1)\nx4 = torch.randn(1)\nx5 = torch.randn(1, 8, 63, 63)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.conv = torch.nn.Conv2d(3, 8, 1, stride=1, padding=0, dilation=1)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v3 = v1 - 2.0\n        t1 = v1\n        v2 = t1 + 24\n        t2 = v3\n        v4 = t2 - [1.0, 1.0]\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 3, 32, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.conv = torch.nn.Conv2d(3, 9, 3, stride=2, padding=1)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 - 2\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.conv = torch.nn.Conv2d(1, 4, 1, stride=1, padding=0)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 - True\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 1, 256, 256)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.conv = torch.nn.Conv2d(3, 6, 1, stride=1, padding=0)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 - True\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 3, 352, 256)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.conv = torch.nn.Conv2d(3, 8, 3)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 - False\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 8, 28, 28)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.quant = torch.quantization.QuantStub()\n        self.conv1 = torch.nn.Conv2d(1, 4, 7)\n        self.conv2 = torch.nn.Conv2d(4, 2, 5)\n        self.maxpool = torch.nn.MaxPool2d(3, stride=2, padding=1)\n        self.fc = torch.nn.Linear(84, 256)\n        self.relu = torch.nn.ReLU()\n        self.dequant = torch.quantization.DeQuantStub()\n        self.conv3 = torch.nn.Conv2d(2, 1, 5)\n        self.sigmoid = torch.nn.Sigmoid()\n    def forward(self, x):\n        x = self.quant(x)\n        x = self.conv1(x)\n        # TODO (soto) - add more layers here\n        x = self.conv3(x)\n        return self.sigmoid(self.dequant(x))\n# Inputs to the model\nx = torch.randn(1, 1, 28, 28)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(1, 2, 1)\n    def forward(self, x):\n        v1 = self.conv(x)\n        v2 = v1 - 1\n        return v2\n# Inputs to the model\nx = torch.randn(1, 1, 32, 32)\n",
                "\nclass Model(nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.conv = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1)\n        torch.nn.init.zeros_(self.conv.weight) # Set the weight tensor such that the result of the pointwise convolution is all zeros.\n\n    def forward(self, x):\n        v1 = self.conv(x) # Run the convolution operator\n        return torch.sigmoid(v1) # Apply sigmoid operator to the previous result\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.conv = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1, groups=2, dilation=2)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 - False\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 8, 1)\n    def forward(self, x2, x3, x4, x5):\n        v1 = self.conv(x2)\n        v2 = v1 - 1.0\n        v3 = torch.sinh(x3)\n        v4 = v2 * v3\n        v5 = torch.nn.functional.hardtanh(x4)\n        v6 = v4 - v5\n        v7 = v6 + x5\n        return v7\n# Inputs to the model\nx2 = torch.randn(1, 3, 64, 64)\nx3 = torch.randn(1)\nx4 = torch.randn(1)\nx5 = torch.randn(1, 8, 63, 63)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.conv = torch.nn.Conv2d(3, 8, 1, stride=1, padding=0, dilation=1)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v3 = v1 - 2.0\n        t1 = v1\n        v2 = t1 + 24\n        t2 = v3\n        v4 = t2 - [1.0, 1.0]\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 3, 32, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.conv = torch.nn.Conv2d(3, 9, 3, stride=2, padding=1)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 - 2\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n"
            ],
            "g_time": 8.545833110809326
        }
    }
}
{
    "opt": "unary=18",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(9, 9, 3, stride=0, padding=1)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = torch.sigmoid(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 9, 512, 512)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(1, 4, 1, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = torch.sigmoid(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 1, 16, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(6, 4, 2, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = torch.sigmoid(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 6, 256, 256)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(4, 2, 5, stride=2, padding=1)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = torch.sigmoid(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 4, 513, 513)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 8, 3, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = torch.sigmoid(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 3, 512, 512)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 4, 3, stride=1, padding=1)\n        self.conv1 = torch.nn.Conv2d(9, 5, 5, stride=1, padding=0)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = self.conv1(v1)\n        v4 = torch.sigmoid(v3)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 3, 224, 224)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(32, 32, 3, stride=1, padding=1, bias=True)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = torch.sigmoid(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 32, 56, 56)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(31, 70, 2, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = torch.sigmoid(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 31, 65, 65)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(129, 53, 1, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = torch.sigmoid(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 129, 56, 56)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(64, 64, 1, stride=1, padding=0, bias=False)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = torch.sigmoid(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 64, 32, 32)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(9, 9, 3, stride=0, padding=1)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = torch.sigmoid(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 9, 512, 512)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(1, 4, 1, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = torch.sigmoid(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 1, 16, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(6, 4, 2, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = torch.sigmoid(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 6, 256, 256)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(4, 2, 5, stride=2, padding=1)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = torch.sigmoid(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 4, 513, 513)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 8, 3, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = torch.sigmoid(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 3, 512, 512)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 4, 3, stride=1, padding=1)\n        self.conv1 = torch.nn.Conv2d(9, 5, 5, stride=1, padding=0)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = self.conv1(v1)\n        v4 = torch.sigmoid(v3)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 3, 224, 224)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(32, 32, 3, stride=1, padding=1, bias=True)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = torch.sigmoid(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 32, 56, 56)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(31, 70, 2, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = torch.sigmoid(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 31, 65, 65)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(129, 53, 1, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = torch.sigmoid(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 129, 56, 56)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(64, 64, 1, stride=1, padding=0, bias=False)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = torch.sigmoid(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 64, 32, 32)\n"
            ],
            "g_time": 5.739354610443115
        }
    }
}
{
    "opt": "cat_slice_cat",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n \n    def forward(self, x1, x2):\n        v1 = torch.cat([x1, x2], dim=1)\n        v2 = v1[:, 0:9223372036854775807]\n        v3 = v2[:, 0:224]\n        v4 = torch.cat([v1, v3], dim=1)\n        return v4\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 224, 64, 64)\nx2 = torch.randn(1, 224, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def forward(self, x0, x1, x2):\n        v0 = torch.cat([x0, x1, x2], -2)\n        v1 = v0[:, :, :224]*0.5\n        v2 = v0[:, :, 224:]*0.114\n        v3 = (v1 - 0.5) + (v2 + 1.14)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx0 = torch.randn(1, 3, 256, 256)\nx1 = torch.rand(1, 1, 224, 224)\nx2 = torch.rand(1, 150, 80, 80)\n",
                "\nclass Model(torch.nn.Module):\n    pass\n \n# Initializing the model\nm = Model()\n \n# Inputs to the model\nx1 = torch.randn(1, 64)\nx2 = torch.randn(1, 64)\nx3 = torch.randn(1, 5)\nx4 = torch.randn(1, 4)\n__input_tensor__ = [\n    x1,\n    x2,\n    x3,\n    x4\n]\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n \n    def forward(self, x1, x2):\n        v1 = torch.cat((x1, x2), dim=1)\n        v2 = v1[:, 0:9223372036854775807]\n        v3 = v2[:, 0:35]\n        v4 = torch.cat((v1, v3), dim=1)\n        return v4\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 50, 100)\nx2 = torch.randn(1, 3, 65, 100)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n \n \n    def forward(self, x1, x2, x3):\n        v1 = torch.cat([x1, x2, x3], dim=1)\n        v2 = v1[:, 0:9223372036854775807]\n        v3 = v2[:, 0:128]\n        v4 = torch.cat([v1, v3], dim=1)\n        return v4\n\n# Initializing the model\nm = Model()\n \n# Inputs to the model\nx1 = torch.randn(1, 64, 56, 56)\nx2 = torch.randn(1, 32, 28, 28)\nx3 = torch.randn(1, 16, 14, 14)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n \n    def forward(self, x3):\n        v1 = torch.cat(x3, dim=1)\n        v2 = v1[:, 0:9223372036854775807]\n        v3 = v2[:, 0:10]\n        v4 = torch.cat([v1, v3], dim=1)\n        return v4\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx3 = [torch.randn(1, 32, 64, 64), torch.randn(1, 3, 64, 64), torch.randn(1, 3, 64, 64)]\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, x1):\n        t1 = torch.cat([x1, x2], dim=1)\n        t2 = t1[:,0:9223372036854775807]\n        t3 = t2[:,0:9223372036854775807]\n        t4 = torch.cat([t1, t3], dim=1)\n        return t4\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 256, 256)\nx2 = torch.randn(1, 3, 256, 256)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n \n    def forward(self, x1, x2):\n        v1 = torch.cat([x1, x2], dim=1)\n        v2 = v1[:, 0:9223372036854775807]\n        v3 = v2[:, 0:3]\n        v4 = torch.cat([v1, v3], dim=1)\n        return v4\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\nx2 = torch.randn(1, 7, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def forward(self, x1):\n        v1 = torch.cat([x1], dim=1)\n        v2 = v1[:, 0:9223372036854775807]\n        v3 = v2[:, 0::1000]\n        v4 = torch.cat([v1, v3], dim=1)\n        return v4\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, x1, x2, x3, x4, x5):\n        v1 = torch.cat([x1, x2, x3, x4, x5], dim=1)\n        v2 = v1[:, 0:9223372036854775807]\n        v3 = v2[:, 0:512]\n        v4 = torch.cat([v1, v3], dim=1)\n        return v4\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 32, 64, 64)\nx2 = torch.randn(1, 64, 64, 64)\nx3 = torch.randn(1, 128, 64, 64)\nx4 = torch.randn(1, 256, 64, 64)\nx5 = torch.randn(1, 512, 64, 64)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n \n    def forward(self, x1, x2):\n        v1 = torch.cat([x1, x2], dim=1)\n        v2 = v1[:, 0:9223372036854775807]\n        v3 = v2[:, 0:224]\n        v4 = torch.cat([v1, v3], dim=1)\n        return v4\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 224, 64, 64)\nx2 = torch.randn(1, 224, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def forward(self, x0, x1, x2):\n        v0 = torch.cat([x0, x1, x2], -2)\n        v1 = v0[:, :, :224]*0.5\n        v2 = v0[:, :, 224:]*0.114\n        v3 = (v1 - 0.5) + (v2 + 1.14)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx0 = torch.randn(1, 3, 256, 256)\nx1 = torch.rand(1, 1, 224, 224)\nx2 = torch.rand(1, 150, 80, 80)\n",
                "\nclass Model(torch.nn.Module):\n    pass\n \n# Initializing the model\nm = Model()\n \n# Inputs to the model\nx1 = torch.randn(1, 64)\nx2 = torch.randn(1, 64)\nx3 = torch.randn(1, 5)\nx4 = torch.randn(1, 4)\n__input_tensor__ = [\n    x1,\n    x2,\n    x3,\n    x4\n]\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n \n    def forward(self, x1, x2):\n        v1 = torch.cat((x1, x2), dim=1)\n        v2 = v1[:, 0:9223372036854775807]\n        v3 = v2[:, 0:35]\n        v4 = torch.cat((v1, v3), dim=1)\n        return v4\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 50, 100)\nx2 = torch.randn(1, 3, 65, 100)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n \n \n    def forward(self, x1, x2, x3):\n        v1 = torch.cat([x1, x2, x3], dim=1)\n        v2 = v1[:, 0:9223372036854775807]\n        v3 = v2[:, 0:128]\n        v4 = torch.cat([v1, v3], dim=1)\n        return v4\n\n# Initializing the model\nm = Model()\n \n# Inputs to the model\nx1 = torch.randn(1, 64, 56, 56)\nx2 = torch.randn(1, 32, 28, 28)\nx3 = torch.randn(1, 16, 14, 14)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n \n    def forward(self, x3):\n        v1 = torch.cat(x3, dim=1)\n        v2 = v1[:, 0:9223372036854775807]\n        v3 = v2[:, 0:10]\n        v4 = torch.cat([v1, v3], dim=1)\n        return v4\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx3 = [torch.randn(1, 32, 64, 64), torch.randn(1, 3, 64, 64), torch.randn(1, 3, 64, 64)]\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, x1):\n        t1 = torch.cat([x1, x2], dim=1)\n        t2 = t1[:,0:9223372036854775807]\n        t3 = t2[:,0:9223372036854775807]\n        t4 = torch.cat([t1, t3], dim=1)\n        return t4\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 256, 256)\nx2 = torch.randn(1, 3, 256, 256)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n \n    def forward(self, x1, x2):\n        v1 = torch.cat([x1, x2], dim=1)\n        v2 = v1[:, 0:9223372036854775807]\n        v3 = v2[:, 0:3]\n        v4 = torch.cat([v1, v3], dim=1)\n        return v4\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\nx2 = torch.randn(1, 7, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def forward(self, x1):\n        v1 = torch.cat([x1], dim=1)\n        v2 = v1[:, 0:9223372036854775807]\n        v3 = v2[:, 0::1000]\n        v4 = torch.cat([v1, v3], dim=1)\n        return v4\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, x1, x2, x3, x4, x5):\n        v1 = torch.cat([x1, x2, x3, x4, x5], dim=1)\n        v2 = v1[:, 0:9223372036854775807]\n        v3 = v2[:, 0:512]\n        v4 = torch.cat([v1, v3], dim=1)\n        return v4\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 32, 64, 64)\nx2 = torch.randn(1, 64, 64, 64)\nx3 = torch.randn(1, 128, 64, 64)\nx4 = torch.randn(1, 256, 64, 64)\nx5 = torch.randn(1, 512, 64, 64)\n"
            ],
            "g_time": 8.69526195526123
        }
    }
}
{
    "opt": "permute_matmul_fusion",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, x2):\n        x1 = x1.type(torch.int)\n        x2 = x2.type(torch.int)\n        x1_permute = x1.permute(0, 2, 1)\n        x2 = x2.permute(0, 2, 1)\n        v3 = torch.bmm(x1_permute, x2)\n        v2 = torch.add(v3, v3)\n        v1 = torch.reshape(v2, (1, -1))\n        return v1\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\nx2 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, x2):\n        x1_permute = x1.permute(0, 2, 1)\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\nx2 = torch.randn(1, 2, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, x2):\n        v0 = x1.permute(0, 2, 1)\n        v1 = x2.permute(0, 2, 1)\n        v2 = torch.bmm(v0, v1)\n        v3 = v2.permute(0, 2, 1)\n        v4 = v3.permute(0, 2, 1)\n        v5 = x1 * v4\n        v6 = x2 * v5\n        return x1 * v6\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\nx2 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, x2):\n        x2_permute = x2.permute(0, 2, 1)\n        x1 = x1.permute(0, 2, 1)\n        a = torch.bmm(x1, x2_permute)\n        b = a[0][0]\n        return b\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\nx2 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, x2):\n        x1_permute = x2.permute(0, 2, 1)\n        x2 = x1.permute(0, 2, 1)\n        a = torch.bmm(x1_permute, x2)\n        b = a[0][0]\n        return b\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\nx2 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, x2):\n        x1 = x1.permute(0, 2, 1)\n        v1 = torch.bmm(x1, x2.permute(0, 2, 1))\n        v2 = torch.reshape(v1, (1,1,-1))\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\nx2 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, x2):\n        x1_permute = x1.permute(2, 0, 1)\n        x1_permute = x1_permute.permute(2, 0, 1).mul(x1_permute)\n        x1_permute = x1_permute.permute(2, 0, 1)\n        x2_permute = x2.permute(0, 2, 1)\n        x2_permute.div(x2_permute)\n        x2_permute = x2_permute.permute(2, 0, 1)\n        x2_permute = x2_permute.permute(2, 0, 1).div(x2_permute)\n        x2_permute = x2_permute.permute(0, 2, 1)\n        res = torch.bmm(x1_permute, x2_permute)\n        return res\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\nx2 = torch.randn(2, 2, 2)\nx3 = torch.randn(1, 2, 1)\nx4 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, x2):\n        x1 = torch.randn(1, 2, 2)\n        a = torch.bmm(x1, x2)\n        b = a[0][0]\n        return b\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\nx2 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, x2):\n        a1 = x1.permute(0, 2, 1)[0]\n        a2 = x2.permute(0, 2, 1)[0]\n        b = a1 + a2\n        c = b.unsqueeze(0)\n        d = c.unsqueeze(0)\n        e0 = d[0][0]\n        return e0\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\nx2 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, x2):\n        x1_permute = x1.permute(0, 2, 1)\n        x2 = x2.permute(0, 2, 1)\n        a = torch.matmul(x1_permute, x2)\n        b = a[0][0]\n        return b\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\nx2 = torch.randn(1, 2, 2)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, x2):\n        x1 = x1.type(torch.int)\n        x2 = x2.type(torch.int)\n        x1_permute = x1.permute(0, 2, 1)\n        x2 = x2.permute(0, 2, 1)\n        v3 = torch.bmm(x1_permute, x2)\n        v2 = torch.add(v3, v3)\n        v1 = torch.reshape(v2, (1, -1))\n        return v1\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\nx2 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, x2):\n        x1_permute = x1.permute(0, 2, 1)\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\nx2 = torch.randn(1, 2, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, x2):\n        v0 = x1.permute(0, 2, 1)\n        v1 = x2.permute(0, 2, 1)\n        v2 = torch.bmm(v0, v1)\n        v3 = v2.permute(0, 2, 1)\n        v4 = v3.permute(0, 2, 1)\n        v5 = x1 * v4\n        v6 = x2 * v5\n        return x1 * v6\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\nx2 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, x2):\n        x2_permute = x2.permute(0, 2, 1)\n        x1 = x1.permute(0, 2, 1)\n        a = torch.bmm(x1, x2_permute)\n        b = a[0][0]\n        return b\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\nx2 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, x2):\n        x1_permute = x2.permute(0, 2, 1)\n        x2 = x1.permute(0, 2, 1)\n        a = torch.bmm(x1_permute, x2)\n        b = a[0][0]\n        return b\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\nx2 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, x2):\n        x1 = x1.permute(0, 2, 1)\n        v1 = torch.bmm(x1, x2.permute(0, 2, 1))\n        v2 = torch.reshape(v1, (1,1,-1))\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\nx2 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, x2):\n        x1_permute = x1.permute(2, 0, 1)\n        x1_permute = x1_permute.permute(2, 0, 1).mul(x1_permute)\n        x1_permute = x1_permute.permute(2, 0, 1)\n        x2_permute = x2.permute(0, 2, 1)\n        x2_permute.div(x2_permute)\n        x2_permute = x2_permute.permute(2, 0, 1)\n        x2_permute = x2_permute.permute(2, 0, 1).div(x2_permute)\n        x2_permute = x2_permute.permute(0, 2, 1)\n        res = torch.bmm(x1_permute, x2_permute)\n        return res\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\nx2 = torch.randn(2, 2, 2)\nx3 = torch.randn(1, 2, 1)\nx4 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, x2):\n        x1 = torch.randn(1, 2, 2)\n        a = torch.bmm(x1, x2)\n        b = a[0][0]\n        return b\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\nx2 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, x2):\n        a1 = x1.permute(0, 2, 1)[0]\n        a2 = x2.permute(0, 2, 1)[0]\n        b = a1 + a2\n        c = b.unsqueeze(0)\n        d = c.unsqueeze(0)\n        e0 = d[0][0]\n        return e0\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\nx2 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, x2):\n        x1_permute = x1.permute(0, 2, 1)\n        x2 = x2.permute(0, 2, 1)\n        a = torch.matmul(x1_permute, x2)\n        b = a[0][0]\n        return b\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\nx2 = torch.randn(1, 2, 2)\n"
            ],
            "g_time": 10.789353609085083
        }
    }
}
{
    "opt": "binary_unary=10",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(5, 1)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 + x1\n        v3 = torch.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 5)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(64, 16)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 + torch.randn_like(v1)\n        v3 = F.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, w_init):\n        super(Model, self).__init__()\n        self.linear = torch.nn.Linear(2, 2, bias=True)\n \n        # The weight is initialized with the input tensor\n        self.linear.weight.data = torch.tensor(w_init, dtype=torch.float32) \n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        return v1 + x\n  \n# Inputs to the model\nx1 = torch.randn(1, 2)\nw_init = np.array([\n    [0, 1],\n    [1, 0]])\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n         super().__init__()\n         self.linear = torch.nn.Linear(3, 3)\n\n    def forward(self, x1, x2):\n        t1 = self.linear(x1)\n        v1 = t1 + x2\n        v2 = torch.relu(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\nx2 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 1)\n \n    def forward(self, x1, x2):\n        v1 = self.linear(x1)\n        v2 = v1 + x2\n        v3 = torch.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\nx2 = torch.randn(1, 1)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        self.linear = torch.nn.Linear(3, 2)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 + x1\n        v3 = torch.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\nx2 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 4, bias=True)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 + torch.tensor([1., 2., 3., 4.])\n        v3 = F.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(in_features=1, out_features=4, bias=True)\n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 + 1.1\n        v3 = torch.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Input to the model\nx1 = torch.randn(1, 1)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(100, 100)\n\n    def forward(self, x1, x2):\n        v1 = self.linear(x1)\n        v2 = v1 + x2\n        v3 = torch.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 100)\nx2 = torch.randn(1, 100)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc = torch.nn.Linear(10, 2)\n \n    def forward(self, x1):\n        v1 = self.fc(x1)\n        v2 = v1 + x1\n        v3 = torch.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 10)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(5, 1)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 + x1\n        v3 = torch.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 5)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(64, 16)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 + torch.randn_like(v1)\n        v3 = F.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, w_init):\n        super(Model, self).__init__()\n        self.linear = torch.nn.Linear(2, 2, bias=True)\n \n        # The weight is initialized with the input tensor\n        self.linear.weight.data = torch.tensor(w_init, dtype=torch.float32) \n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        return v1 + x\n  \n# Inputs to the model\nx1 = torch.randn(1, 2)\nw_init = np.array([\n    [0, 1],\n    [1, 0]])\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n         super().__init__()\n         self.linear = torch.nn.Linear(3, 3)\n\n    def forward(self, x1, x2):\n        t1 = self.linear(x1)\n        v1 = t1 + x2\n        v2 = torch.relu(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\nx2 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 1)\n \n    def forward(self, x1, x2):\n        v1 = self.linear(x1)\n        v2 = v1 + x2\n        v3 = torch.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\nx2 = torch.randn(1, 1)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        self.linear = torch.nn.Linear(3, 2)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 + x1\n        v3 = torch.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\nx2 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 4, bias=True)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 + torch.tensor([1., 2., 3., 4.])\n        v3 = F.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(in_features=1, out_features=4, bias=True)\n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 + 1.1\n        v3 = torch.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Input to the model\nx1 = torch.randn(1, 1)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(100, 100)\n\n    def forward(self, x1, x2):\n        v1 = self.linear(x1)\n        v2 = v1 + x2\n        v3 = torch.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 100)\nx2 = torch.randn(1, 100)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc = torch.nn.Linear(10, 2)\n \n    def forward(self, x1):\n        v1 = self.fc(x1)\n        v2 = v1 + x1\n        v3 = torch.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 10)\n"
            ],
            "g_time": 5.675154447555542
        }
    }
}
{
    "opt": "unary=23",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(7, 8, (7, 5), stride=(2, 8), padding=(1, 5), dilation=(1, 1))\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = torch.tanh(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 7, 12, 37)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(7, 1, (7, 5), stride=(2, 4), padding=(1, 3), groups=1, output_padding=(1, 1))\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = torch.tanh(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 7, 12, 10)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(2, 9, 9, stride=2, groups=2)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = torch.tanh(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 2, 5, 5)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(27, 27, (7, 5), stride=(2, 1), padding=(1, 2), output_padding=(1, 3), dilation=(2, 4))\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = torch.tanh(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 27, 2, 4)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(1, 8, 7, stride=1, padding=7, dilation=4)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = torch.tanh(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 1, 16, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(1, 1, 6, stride=2, padding=3, outp`ut_padding=2)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = torch.tanh(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 1, 8, 9)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(3, 7, (4, 4), stride=(1, 5), padding=(1, 1), dilation=(5, 4))\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = torch.tanh(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 3, 17, 18)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(10, 9, 11, stride=9, padding=15, dilation=19)\n    def forward(self, x1):\n        v1 = (self).conv_transpose(x1)\n        v2 = torch.tanh(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(6, 10, 6, 6)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(2, 6, kernel_size=24, stride=(2, 3), padding=17, dilation=20, groups=2)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = torch.tanh(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 2, 10, 10)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(8, 3, 3, stride=2, dilation=2, groups=3)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = torch.tanh(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(2, 8, 6, 7)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(7, 8, (7, 5), stride=(2, 8), padding=(1, 5), dilation=(1, 1))\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = torch.tanh(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 7, 12, 37)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(7, 1, (7, 5), stride=(2, 4), padding=(1, 3), groups=1, output_padding=(1, 1))\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = torch.tanh(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 7, 12, 10)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(2, 9, 9, stride=2, groups=2)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = torch.tanh(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 2, 5, 5)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(27, 27, (7, 5), stride=(2, 1), padding=(1, 2), output_padding=(1, 3), dilation=(2, 4))\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = torch.tanh(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 27, 2, 4)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(1, 8, 7, stride=1, padding=7, dilation=4)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = torch.tanh(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 1, 16, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(1, 1, 6, stride=2, padding=3, outp`ut_padding=2)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = torch.tanh(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 1, 8, 9)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(3, 7, (4, 4), stride=(1, 5), padding=(1, 1), dilation=(5, 4))\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = torch.tanh(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 3, 17, 18)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(10, 9, 11, stride=9, padding=15, dilation=19)\n    def forward(self, x1):\n        v1 = (self).conv_transpose(x1)\n        v2 = torch.tanh(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(6, 10, 6, 6)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(2, 6, kernel_size=24, stride=(2, 3), padding=17, dilation=20, groups=2)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = torch.tanh(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 2, 10, 10)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(8, 3, 3, stride=2, dilation=2, groups=3)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = torch.tanh(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(2, 8, 6, 7)\n"
            ],
            "g_time": 5.199918985366821
        }
    }
}
{
    "opt": "fuse_conv_bn",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(2, 2, 2)\n        self.conv2 = torch.nn.Conv2d(2, 2, 2)\n        self.bn = torch.nn.BatchNorm2d(2, affine=True)\n    def forward(self, x7):\n        conv1_res = self.conv1(x7)\n        conv2_res = self.conv2(conv1_res)\n        bn_res = self.bn(conv2_res)\n        return bn_res\n# Inputs to the model\nx7 = torch.randn(1, 2, 8, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 64, 1, 1)\n        self.bn = torch.nn.BatchNorm2d(64, momentum=None)\n    def forward(self, x6):\n        x6 = self.conv(x6)\n        x6 = self.bn(x6)\n        return x6\n# Inputs to the model\nx6 = torch.randn(1, 3, 224, 224)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv1d(2, 4, 2)\n        self.conv2 = torch.nn.Conv1d(4, 2, 2)\n        self.bn1 = torch.nn.BatchNorm1d(2)\n        self.bn2 = torch.nn.BatchNorm1d(4)\n        self.bn3 = torch.nn.BatchNorm1d(2)\n    def forward(self, x6):\n        x6 = self.conv(x6)\n        x6 = self.bn1(x6)\n        x6 = self.conv2(x6)\n        x6 = self.bn2(x6)\n        x6 = self.bn3(x6)\n        y7 = x6\n        return y7\n# Inputs to the model\nx6 = torch.randn(1, 2, 4)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(8, 8, 3, 2)\n        self.bn = torch.nn.BatchNorm2d(8, affine=False)\n    def forward(self, x6):\n        x6 = self.bn(x6)\n        x6 = self.conv(x6)\n        return x6\n# Inputs to the model\nx6 = torch.randn(1, 8, 5, 5)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(2, 4, 3, 1, 2)\n        self.bn = torch.nn.BatchNorm2d(4, track_running_stats=False)\n        self.pool = torch.nn.MaxPool2d(2)\n    def forward(self, x0):\n        x0 = self.conv(x0)\n        x0 = self.bn(x0)\n        x0 = self.pool(x0)\n        return x0\n# Inputs to the model\nx0 = torch.randn(1, 2, 5, 6)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(1, 1, 2, groups=1, bias=True)\n        self.bn = torch.nn.BatchNorm2d(1, affine=False, track_running_stats=False)\n    # Note that the bias is required so that the model contains a conv and bn node.\n    def forward(self, x7):\n        x7 = self.conv(x7)\n        x7 = self.bn(x7)\n        return x7\n# Inputs to the model\nx7 = torch.randn(1, 1, 32, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(2, 4, 6, name='conv')\n        self.bn = torch.nn.BatchNorm2d(4, affine=False)\n    def forward(self, x7):\n        x7 = self.conv(x7)\n        x7 = self.bn(x7)\n        return x7\n# Inputs to the model\nx7 = torch.randn(2, 2, 6, 6)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(16, 32, (3,5), 1, padding=(1,2), dilation=(2,1), groups=4)\n        self.bn1 = torch.nn.BatchNorm2d(32)\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        return x\n# Inputs to the model\nx = torch.randn(5, 16, 55, 20)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(1, 1, 1)\n    def forward(self, x):\n        return self.conv(x)\n# Inputs to the model\nx = torch.randn(1, 10, 11, 11)\n",
                "\nclass A(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.bn = torch.nn.BatchNorm2d(16)\n        self.conv = torch.nn.Conv2d(16, 32, 2, 1)\n    def forward(self, x8):\n        x8 = self.bn(x8)\n        x8 = self.conv(x8)\n        return x8\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(8, 16, 1, 2)\n        self.mod = A()\n    def forward(self, x6):\n        x6 = self.conv(x6)\n        x6 = self.mod(x6)\n        return x6\n# Inputs to the model\nx6 = torch.randn(1, 8, 4, 4)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(2, 2, 2)\n        self.conv2 = torch.nn.Conv2d(2, 2, 2)\n        self.bn = torch.nn.BatchNorm2d(2, affine=True)\n    def forward(self, x7):\n        conv1_res = self.conv1(x7)\n        conv2_res = self.conv2(conv1_res)\n        bn_res = self.bn(conv2_res)\n        return bn_res\n# Inputs to the model\nx7 = torch.randn(1, 2, 8, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 64, 1, 1)\n        self.bn = torch.nn.BatchNorm2d(64, momentum=None)\n    def forward(self, x6):\n        x6 = self.conv(x6)\n        x6 = self.bn(x6)\n        return x6\n# Inputs to the model\nx6 = torch.randn(1, 3, 224, 224)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv1d(2, 4, 2)\n        self.conv2 = torch.nn.Conv1d(4, 2, 2)\n        self.bn1 = torch.nn.BatchNorm1d(2)\n        self.bn2 = torch.nn.BatchNorm1d(4)\n        self.bn3 = torch.nn.BatchNorm1d(2)\n    def forward(self, x6):\n        x6 = self.conv(x6)\n        x6 = self.bn1(x6)\n        x6 = self.conv2(x6)\n        x6 = self.bn2(x6)\n        x6 = self.bn3(x6)\n        y7 = x6\n        return y7\n# Inputs to the model\nx6 = torch.randn(1, 2, 4)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(8, 8, 3, 2)\n        self.bn = torch.nn.BatchNorm2d(8, affine=False)\n    def forward(self, x6):\n        x6 = self.bn(x6)\n        x6 = self.conv(x6)\n        return x6\n# Inputs to the model\nx6 = torch.randn(1, 8, 5, 5)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(2, 4, 3, 1, 2)\n        self.bn = torch.nn.BatchNorm2d(4, track_running_stats=False)\n        self.pool = torch.nn.MaxPool2d(2)\n    def forward(self, x0):\n        x0 = self.conv(x0)\n        x0 = self.bn(x0)\n        x0 = self.pool(x0)\n        return x0\n# Inputs to the model\nx0 = torch.randn(1, 2, 5, 6)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(1, 1, 2, groups=1, bias=True)\n        self.bn = torch.nn.BatchNorm2d(1, affine=False, track_running_stats=False)\n    # Note that the bias is required so that the model contains a conv and bn node.\n    def forward(self, x7):\n        x7 = self.conv(x7)\n        x7 = self.bn(x7)\n        return x7\n# Inputs to the model\nx7 = torch.randn(1, 1, 32, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(2, 4, 6, name='conv')\n        self.bn = torch.nn.BatchNorm2d(4, affine=False)\n    def forward(self, x7):\n        x7 = self.conv(x7)\n        x7 = self.bn(x7)\n        return x7\n# Inputs to the model\nx7 = torch.randn(2, 2, 6, 6)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(16, 32, (3,5), 1, padding=(1,2), dilation=(2,1), groups=4)\n        self.bn1 = torch.nn.BatchNorm2d(32)\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        return x\n# Inputs to the model\nx = torch.randn(5, 16, 55, 20)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(1, 1, 1)\n    def forward(self, x):\n        return self.conv(x)\n# Inputs to the model\nx = torch.randn(1, 10, 11, 11)\n",
                "\nclass A(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.bn = torch.nn.BatchNorm2d(16)\n        self.conv = torch.nn.Conv2d(16, 32, 2, 1)\n    def forward(self, x8):\n        x8 = self.bn(x8)\n        x8 = self.conv(x8)\n        return x8\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(8, 16, 1, 2)\n        self.mod = A()\n    def forward(self, x6):\n        x6 = self.conv(x6)\n        x6 = self.mod(x6)\n        return x6\n# Inputs to the model\nx6 = torch.randn(1, 8, 4, 4)\n"
            ],
            "g_time": 7.774274110794067
        }
    }
}
{
    "opt": "unary=13",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = nn.Linear(3, 8)\n\n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = nn.Sigmoid()(v1)\n        v3 = v1 * v2\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.lin = torch.nn.Linear(16, 8)\n \n    def forward(self, x1):\n        v1 = self.lin(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(32, 16, bias=True)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear1 = torch.nn.Linear(3, 8)\n        self.linear2 = torch.nn.Linear(8, 1)\n        self.sigmoid = torch.nn.Sigmoid()\n\n    def forward(self, input):\n        return torch.max(self.sigmoid(self.linear2(self.linear1(input))), dim=[2]).values\n\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\ninput = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.sigmoid = torch.nn.Sigmoid()\n        self.linear = torch.nn.Linear(1024, 1024)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = self.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 1024)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(75, 150)\n \n    def forward(self, x):\n        v1 = self.linear(x)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx = torch.randn(25, 75)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Input to the model\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(10, 10)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 10)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = nn.Linear(3, 8)\n\n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = nn.Sigmoid()(v1)\n        v3 = v1 * v2\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.lin = torch.nn.Linear(16, 8)\n \n    def forward(self, x1):\n        v1 = self.lin(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(32, 16, bias=True)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear1 = torch.nn.Linear(3, 8)\n        self.linear2 = torch.nn.Linear(8, 1)\n        self.sigmoid = torch.nn.Sigmoid()\n\n    def forward(self, input):\n        return torch.max(self.sigmoid(self.linear2(self.linear1(input))), dim=[2]).values\n\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\ninput = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.sigmoid = torch.nn.Sigmoid()\n        self.linear = torch.nn.Linear(1024, 1024)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = self.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 1024)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(75, 150)\n \n    def forward(self, x):\n        v1 = self.linear(x)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx = torch.randn(25, 75)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Input to the model\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(10, 10)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 10)\n"
            ],
            "g_time": 5.6159279346466064
        }
    }
}
{
    "opt": "unary=5",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(8, 8, 7, stride=3, padding=0)\n    def forward(self, x1):\n        v1 = v6 = self.conv_transpose(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 8, 23, 23)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(1, 1, 1, stride=1, padding=0)\n    def forward(self, x3):\n        v1 = v6 = self.conv_transpose(x3)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n# Inputs to the model\nx3 = torch.randn(1, 1, 1, 1)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(7, 7, (2, 3), stride=(3, 4), padding=0)\n    def forward(self, x1):\n        v1 = v6 = self.conv_transpose(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 7, 16, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose1d(2, 1, 1, stride=5, padding=3)\n    def forward(self, x1):\n        v1 = v6 = self.conv_transpose(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 2, 40)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(1, 9, 7, stride=5, padding=0)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 1, 150, 150)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(2, 1, 1, stride=5, padding=3)\n    def forward(self, x1):\n        v1 = v6 = self.conv_transpose(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.exp(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 2, 40, 40)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose1d(4, 4, 5, stride=2, padding=2, output_padding=1)\n    def forward(self, x1):\n        v1 = v6 = self.conv_transpose(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n# Inputs to the model\nx1 = torch.randn(25, 4, 70)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(1, 1, 1, stride=3, padding=0)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 1, 8, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(5, 43, kernel_size=(2,))\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 5, 4, 4)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(9, 3, 7, stride=1, padding=3, output_padding=2)\n    def forward(self, x1):\n        v1 = v6 = self.conv_transpose(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 9, 16, 16)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(8, 8, 7, stride=3, padding=0)\n    def forward(self, x1):\n        v1 = v6 = self.conv_transpose(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 8, 23, 23)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(1, 1, 1, stride=1, padding=0)\n    def forward(self, x3):\n        v1 = v6 = self.conv_transpose(x3)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n# Inputs to the model\nx3 = torch.randn(1, 1, 1, 1)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(7, 7, (2, 3), stride=(3, 4), padding=0)\n    def forward(self, x1):\n        v1 = v6 = self.conv_transpose(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 7, 16, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose1d(2, 1, 1, stride=5, padding=3)\n    def forward(self, x1):\n        v1 = v6 = self.conv_transpose(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 2, 40)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(1, 9, 7, stride=5, padding=0)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 1, 150, 150)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(2, 1, 1, stride=5, padding=3)\n    def forward(self, x1):\n        v1 = v6 = self.conv_transpose(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.exp(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 2, 40, 40)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose1d(4, 4, 5, stride=2, padding=2, output_padding=1)\n    def forward(self, x1):\n        v1 = v6 = self.conv_transpose(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n# Inputs to the model\nx1 = torch.randn(25, 4, 70)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(1, 1, 1, stride=3, padding=0)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 1, 8, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(5, 43, kernel_size=(2,))\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 5, 4, 4)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(9, 3, 7, stride=1, padding=3, output_padding=2)\n    def forward(self, x1):\n        v1 = v6 = self.conv_transpose(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 9, 16, 16)\n"
            ],
            "g_time": 7.328933477401733
        }
    }
}
{
    "opt": "cat_addmm",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layers = nn.Linear(2, 2)\n        self.conv = nn.Conv2d(2, 2, 2, 1, 0)\n    def forward(self, x):\n        x = self.layers(x)\n        x = self.conv(x)\n        x = x.transpose(2, 3).flatten(1)\n        return x\n# Inputs to the model\nx = torch.randn(1, 2, 2, 2)\n",
                "\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layers = nn.Linear(4, 4)\n        self.concat = nn.Linear(2, 2)\n    def forward(self, x):\n        x = x[::3, :]\n        x = self.layers(x)\n        x = self.concat(x).flatten(start_dim=1)\n        return x\n# Inputs to the model\nx = torch.randn(25, 4)\n",
                "\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layers = nn.Linear(2, 2)\n    def forward(self, x):\n        l1 = self.layers(x)\n        x = x.unsqueeze(dim=1)\n        l2 = self.layers(x)\n        x = x.unsqueeze(dim=1)\n        l3 = self.layers(x)\n        x = x.flatten(start_dim=1)\n        l4 = self.layers(x)\n        x = x.flatten(end_dim=1)\n        return torch.cat((l1, l2, l3, l4), dim=1)\n# Inputs to the model\nx = torch.randn(2, 2)\n",
                "\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layers = nn.Linear(2, 2)\n    def forward(self, x):\n        x = self.layers(x)\n        x = torch.cat((x, x), dim=1)\n        x = torch.cat((x, x), dim=1)\n        return x\n# Inputs to the model\nx = torch.randn(2, 2)\n",
                "\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layers = nn.Linear(2, 1)\n    def forward(self, x):\n        x = torch.cat((x, x), dim=1).flatten(start_dim=1)\n        x = self.layers(x)\n        return x\n# Inputs to the model\nx = torch.randn(1, 2)\n",
                "\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layers = nn.Linear(4, 2)\n    def forward(self, x):\n        x = self.layers(x)\n        x = torch.stack((x, x, torch.cat((x, x), dim=1), torch.cat((x, x), dim=1)), dim=0).flatten(1).flatten(1)\n        return x\n# Inputs to the model\nx = torch.randn(4, 2)\n",
                "\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layers = nn.Linear(2, 2)\n    def forward(self, x):\n        x = self.layers(x)\n        x = x*x\n        x = torch.stack((x, x), dim=1).flatten(end_dim=1)\n        return x\n# Inputs to the model\nx = torch.randn(2, 2)\n",
                "\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layers = nn.Linear(2, 8)\n    def forward(self, x):\n        x = self.layers(x)\n        x = x.narrow(start=1, length=3, dim=1)\n        x = torch.cat((x, x), dim=1)\n        return x\n# Inputs to the model\nx = torch.randn(1, 8)\n",
                "\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layers = nn.Linear(2, 4)\n    def forward(self, x):\n        x = self.layers(x)\n        x = torch.stack((x, x, x, x), dim=1).flatten(0, 1)\n        return x\n# Inputs to the model\nx = torch.randn(2, 2)\n",
                "\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layers = nn.Linear(2, 2)\n    def forward(self, x):\n        x = self.layers(x)\n        x = x.view(1, 1, 2, 2)\n        x = torch.flatten(x, start_dim=1, end_dim=2)\n        return x\n# Inputs to the model\nx = torch.randn(2, 2)\n"
            ],
            "code": [
                "\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layers = nn.Linear(2, 2)\n        self.conv = nn.Conv2d(2, 2, 2, 1, 0)\n    def forward(self, x):\n        x = self.layers(x)\n        x = self.conv(x)\n        x = x.transpose(2, 3).flatten(1)\n        return x\n# Inputs to the model\nx = torch.randn(1, 2, 2, 2)\n",
                "\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layers = nn.Linear(4, 4)\n        self.concat = nn.Linear(2, 2)\n    def forward(self, x):\n        x = x[::3, :]\n        x = self.layers(x)\n        x = self.concat(x).flatten(start_dim=1)\n        return x\n# Inputs to the model\nx = torch.randn(25, 4)\n",
                "\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layers = nn.Linear(2, 2)\n    def forward(self, x):\n        l1 = self.layers(x)\n        x = x.unsqueeze(dim=1)\n        l2 = self.layers(x)\n        x = x.unsqueeze(dim=1)\n        l3 = self.layers(x)\n        x = x.flatten(start_dim=1)\n        l4 = self.layers(x)\n        x = x.flatten(end_dim=1)\n        return torch.cat((l1, l2, l3, l4), dim=1)\n# Inputs to the model\nx = torch.randn(2, 2)\n",
                "\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layers = nn.Linear(2, 2)\n    def forward(self, x):\n        x = self.layers(x)\n        x = torch.cat((x, x), dim=1)\n        x = torch.cat((x, x), dim=1)\n        return x\n# Inputs to the model\nx = torch.randn(2, 2)\n",
                "\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layers = nn.Linear(2, 1)\n    def forward(self, x):\n        x = torch.cat((x, x), dim=1).flatten(start_dim=1)\n        x = self.layers(x)\n        return x\n# Inputs to the model\nx = torch.randn(1, 2)\n",
                "\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layers = nn.Linear(4, 2)\n    def forward(self, x):\n        x = self.layers(x)\n        x = torch.stack((x, x, torch.cat((x, x), dim=1), torch.cat((x, x), dim=1)), dim=0).flatten(1).flatten(1)\n        return x\n# Inputs to the model\nx = torch.randn(4, 2)\n",
                "\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layers = nn.Linear(2, 2)\n    def forward(self, x):\n        x = self.layers(x)\n        x = x*x\n        x = torch.stack((x, x), dim=1).flatten(end_dim=1)\n        return x\n# Inputs to the model\nx = torch.randn(2, 2)\n",
                "\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layers = nn.Linear(2, 8)\n    def forward(self, x):\n        x = self.layers(x)\n        x = x.narrow(start=1, length=3, dim=1)\n        x = torch.cat((x, x), dim=1)\n        return x\n# Inputs to the model\nx = torch.randn(1, 8)\n",
                "\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layers = nn.Linear(2, 4)\n    def forward(self, x):\n        x = self.layers(x)\n        x = torch.stack((x, x, x, x), dim=1).flatten(0, 1)\n        return x\n# Inputs to the model\nx = torch.randn(2, 2)\n",
                "\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layers = nn.Linear(2, 2)\n    def forward(self, x):\n        x = self.layers(x)\n        x = x.view(1, 1, 2, 2)\n        x = torch.flatten(x, start_dim=1, end_dim=2)\n        return x\n# Inputs to the model\nx = torch.randn(2, 2)\n"
            ],
            "g_time": 5.694913148880005
        }
    }
}

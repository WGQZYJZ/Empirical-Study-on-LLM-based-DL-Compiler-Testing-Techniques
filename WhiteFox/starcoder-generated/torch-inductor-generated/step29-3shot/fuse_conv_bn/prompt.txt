### Please generate different valid PyTorch models with public PyTorch APIs that meets the specified requirements. Plus, please also generate the input tensor for the newly generated model. Be creative when generating new models to explore different possibilities that trigger the pattern. Feel free to leverage various PyTorch APIs, uncommon arguments, and input tensors with different shapes and data types.

# Description of requirements:
The model should contain the following pattern:
```
conv = torch.nn.ConvXd(...) # X can be 1, 2, or 3 representing the dimension
bn = torch.nn.BatchNormXd(...) # X should match with ConvXd
output = bn(conv(input_tensor))
```
This pattern characterizes scenarios where a convolution layer (`torch.nn.ConvXd`) is followed by a batch normalization layer (`torch.nn.BatchNormXd`). The output of the convolution layer is used as the input to the batch normalization layer. 

The `fuse_conv_bn` optimization is triggered when the convolution and batch normalization layers are in evaluation mode (not in training mode), and the batch normalization layer is tracking running statistics. 

After the optimization, the convolution and batch normalization layers are fused into a single convolution layer, and the batch normalization layer is removed from the graph. If the output of the convolution layer is used by other nodes, the optimization will not be performed. 

The optimization also applies to the functional API equivalent of the above pattern, where `torch.nn.functional.convXd` and `torch.nn.functional.batch_norm` are used instead of the module API. The constraints for the functional API pattern are similar to the module API pattern.

# Model begins
class Model(torch.nn.Module):
    def __init__(self):
        super().__init__()
        c = torch.nn.Conv2d(2, 4, 3)
        torch.manual_seed(3)
        c.weight = torch.nn.Parameter(torch.randn(c.weight.shape))
        torch.manual_seed(4)
        c.bias = torch.nn.Parameter(torch.randn(c.bias.shape))
        bn = torch.nn.BatchNorm2d(4)
        bn.running_mean = torch.arange(4, dtype=torch.float)
        bn.running_var = torch.arange(4, dtype=torch.float) * 2 + 1
        bn.affine = False
        self.layer = torch.nn.Sequential(c, bn)
    def forward(self, x1):
        v1 = self.layer(x1)
        return v1
# Inputs to the model
x1 = torch.randn(2, 2, 4, 4)
# Model ends

# Model begins
class Model(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = torch.nn.Conv2d(5, 5, 2)
        self.bn1 = torch.nn.BatchNorm2d(5)
        self.conv2 = torch.nn.Conv2d(5, 3, 1)
    def forward(self, x2):
        x1 = self.conv1(x2)
        x3 = self.bn1(x1)
        x4 = self.conv2(x3)
        return x4
# Inputs to the model
x2 = torch.randn(1, 5, 4, 4)
# Model ends

# Model begins
class Model(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.layer = torch.nn.Sequential(torch.nn.Conv2d(5, 5, 2), torch.nn.BatchNorm2d(5))
    def forward(self, x1, x2):
        out = [self.layer(x1), self.layer(x2)]
        return (out[0], out[1])
# Inputs to the model
x1 = torch.randn(1, 5, 4, 4)
x2 = torch.randn(2, 5, 4, 4)
# Model ends

# Model begins
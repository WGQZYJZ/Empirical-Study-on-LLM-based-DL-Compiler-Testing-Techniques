
class Model(torch.nn.Module):
    def __init__(self):
        super().__init__()
    def forward(self):
        return torch.cat(torch.cat([torch.cat([torch.cat([torch.nn.Conv2d(in_channels=7 * 32, out_channels=16, kernel_size=(1, 1), stride=(1, 1), padding=(0, 0), dilation=(1, 1), groups=1, bias=False), torch.nn.Conv2d(in_channels=7 * 32, out_channels=32, kernel_size=(1, 1), stride=(1, 1), padding=(0, 0), dilation=(1, 1), groups=1, bias=False), torch.nn.Conv2d(in_channels=7 * 32, out_channels=32, kernel_size=(1, 1), stride=(1, 1), padding=(0, 0), dilation=(1, 1), groups=1, bias=False)], 3), torch.cat([torch.cat([torch.cat([torch.nn.Conv2d(in_channels=7 * 32, out_channels=16, kernel_size=(1, 1), stride=(1, 1), padding=(0, 0), dilation=(1, 1), groups=1, bias=False), torch.nn.Conv2d(in_channels=7 * 32, out_channels=32, kernel_size=(1, 1), stride=(1, 1), padding=(0, 0), dilation=(1, 1), groups=1, bias=False), torch.nn.Conv2d(in_channels=7 * 32, out_channels=32, kernel_size=(1, 1), stride=(1, 1), padding=(0, 0), dilation=(1, 1), groups=1, bias=False)], 3), torch.cat([torch.cat([torch.nn.Conv2d(in_channels=7 * 32, out_channels=16, kernel_size=(1, 1), stride=(1, 1), padding=(0, 0), dilation=(1, 1), groups=1, bias=False), torch.nn.Conv2d(in_channels=7 * 32, out_channels=32, kernel_size=(1, 1), stride=(1, 1), padding=(0, 0), dilation=(1, 1), groups=1, bias=False), torch.nn.Conv2d(in_channels=7 * 32, out_channels=32, kernel_size=(1, 1), stride=(1, 1), padding=(0, 0), dilation=(1, 1), groups=1, bias=False)], 3), torch.cat([torch.cat([torch.nn.Conv2d(in_channels=7 * 32, out_channels=16, kernel_size=(1, 1), stride=(1, 1), padding=(0, 0), dilation=(1, 1), groups=1, bias=False), torch.nn.Conv2d(in_channels=7 * 32, out_channels=32, kernel_size=(1, 1), stride=(1, 1), padding=(0, 0), dilation=(1, 1), groups=1, bias=False), torch.nn.Conv2d(in_channels=7 * 32, out_channels=32, kernel_size=(1, 1), stride=(1, 1), padding=(0, 0), dilation=(1, 1), groups=1, bias=False)], 3), torch.cat([torch.cat([torch.nn.Conv2d(in_channels=7 * 32, out_channels=16, kernel_size=(1, 1), stride=(1, 1), padding=(0, 0), dilation=(1, 1), groups=1, bias=False), torch.nn.Conv2d(in_channels=7 * 32, out_channels=32, kernel_size=(1, 1), stride=(1, 1), padding=(0, 0), dilation=(1, 1), groups=1, bias=False), torch.nn.Conv2d(in_channels=7 * 32, out_channels=32, kernel_size=(1, 1), stride=(1, 1), padding=(0, 0), dilation=(1, 1), groups=1, bias=False)], 3)], 4)], 4)], 4)], 4)], 4)], 4), None, None))
# Inputs to the model

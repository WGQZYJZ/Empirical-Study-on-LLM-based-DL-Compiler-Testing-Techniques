{
    "opt": "unary=10",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(10, 10)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v4 / 6\n        return v5\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 10)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(64, 1)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v4 / 6\n        return v5\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(256, 10)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v4 / 6\n        return v5\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 256)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(784, 300)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v4 / 6\n        return v5\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 784)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(100, 100)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v4 / 6\n        return v5\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(10, 100)\nx2 = torch.randn(10, 100)\nx = torch.cat((x1, x2), 0)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(32, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 + 3\n        v3 = v2.clamp(0, 6)\n        v4 = v3 / 6\n        return v4\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(2, 32, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v4 / 6\n        return v5\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(5, 5)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v4 / 6\n        return v5\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 5)\na1 = torch.Tensor.clone(x1)\na1.detach_()\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n \n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v4 / 6\n        return v5\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(10, 10)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v4 / 6\n        return v5\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 10)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(64, 1)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v4 / 6\n        return v5\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(256, 10)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v4 / 6\n        return v5\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 256)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(784, 300)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v4 / 6\n        return v5\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 784)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(100, 100)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v4 / 6\n        return v5\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(10, 100)\nx2 = torch.randn(10, 100)\nx = torch.cat((x1, x2), 0)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(32, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 + 3\n        v3 = v2.clamp(0, 6)\n        v4 = v3 / 6\n        return v4\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(2, 32, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v4 / 6\n        return v5\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(5, 5)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v4 / 6\n        return v5\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 5)\na1 = torch.Tensor.clone(x1)\na1.detach_()\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n \n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v4 / 6\n        return v5\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n"
            ],
            "g_time": 6.820054531097412
        }
    }
}
{
    "opt": "unary=28",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min_value, max_value):\n        super().__init__()\n        self.linear = torch.nn.Linear(32, 16)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.clamp_min(v1, min_value)\n        v3 = torch.clamp_max(v2, max_value)\n        return v2\n\n# Initializing the model\nm = Model(min_value, max_value)\n\n# Inputs to the model\nx1 = torch.randn(1, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(32, 64)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.clamp_min(v1, min_value=-4)\n        v3 = torch.clamp_max(v2, max_value=4)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(10, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min_value=0.0, max_value=1.0):\n        super().__init__()\n        self.linear = torch.nn.Linear(in_features=3, out_features=8)\n        self.min_value = min_value\n        self.max_value = max_value\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.clamp_min(v1, self.min_value)\n        v3 = torch.clamp_max(v2, self.max_value)\n        return v3\n\n# Initializing the model\nm = Model(min_value=-128, max_value=128)\n\n# Inputs to the model\nx1 = 10.0*torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min_value, max_value):\n        super().__init__()\n        self.linear = torch.nn.Linear(64, 32)\n        self.min_value = min_value\n        self.max_value = max_value\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.clamp_min(v1, self.min_value)\n        v3 = torch.clamp_max(v2, self.max_value)\n        return v3\n\n# Initializing the model\nm = Model(-1.0, 0.75)\n\n# Inputs to the model\nx1 = torch.randn(1, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(784, 32)\n \n    def forward(self, x1, min_value, max_value):\n        v1 = self.linear.forward(x1)\n        v2 = torch.clamp_min(v1, min_value)\n        v3 = torch.clamp_max(v2, max_value)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(16, 784)\nmin_value = 0.0\nmax_value = 0.1\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(8, 8)\n \n    def forward(self, x1, min_value=0, max_value=0.9):\n        v1 = self.linear(x1)\n        v2 = torch.clamp_min(v1, min_value)\n        v3 = torch.clamp_max(v2, max_value)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Input to the model\nx1 = torch.randn(1, 8)\nprint(m(x1))\n\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(2, 4)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.clamp_min(v1, 0.0)\n        v3 = torch.clamp_max(v2, 3.5)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 4)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.clamp_min(v1, min=0.0)\n        v3 = torch.clamp_max(v2, max=6.0)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min_value=0, max_value=5):\n        super().__init__()\n        self.min_value = min_value\n        self.max_value = max_value\n\n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.clamp_min(v1, self.min_value)\n        v3 = torch.clamp_max(v2, self.max_value)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 32, 1024, 256)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 6)\n \n    def forward(self, x1, min_value=0):\n        v1 = self.linear(x1)\n        v2 = torch.clamp_min(v1, min_value)\n        v3 = torch.clamp_max(v2, 6)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min_value, max_value):\n        super().__init__()\n        self.linear = torch.nn.Linear(32, 16)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.clamp_min(v1, min_value)\n        v3 = torch.clamp_max(v2, max_value)\n        return v2\n\n# Initializing the model\nm = Model(min_value, max_value)\n\n# Inputs to the model\nx1 = torch.randn(1, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(32, 64)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.clamp_min(v1, min_value=-4)\n        v3 = torch.clamp_max(v2, max_value=4)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(10, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min_value=0.0, max_value=1.0):\n        super().__init__()\n        self.linear = torch.nn.Linear(in_features=3, out_features=8)\n        self.min_value = min_value\n        self.max_value = max_value\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.clamp_min(v1, self.min_value)\n        v3 = torch.clamp_max(v2, self.max_value)\n        return v3\n\n# Initializing the model\nm = Model(min_value=-128, max_value=128)\n\n# Inputs to the model\nx1 = 10.0*torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min_value, max_value):\n        super().__init__()\n        self.linear = torch.nn.Linear(64, 32)\n        self.min_value = min_value\n        self.max_value = max_value\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.clamp_min(v1, self.min_value)\n        v3 = torch.clamp_max(v2, self.max_value)\n        return v3\n\n# Initializing the model\nm = Model(-1.0, 0.75)\n\n# Inputs to the model\nx1 = torch.randn(1, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(784, 32)\n \n    def forward(self, x1, min_value, max_value):\n        v1 = self.linear.forward(x1)\n        v2 = torch.clamp_min(v1, min_value)\n        v3 = torch.clamp_max(v2, max_value)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(16, 784)\nmin_value = 0.0\nmax_value = 0.1\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(8, 8)\n \n    def forward(self, x1, min_value=0, max_value=0.9):\n        v1 = self.linear(x1)\n        v2 = torch.clamp_min(v1, min_value)\n        v3 = torch.clamp_max(v2, max_value)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Input to the model\nx1 = torch.randn(1, 8)\nprint(m(x1))\n\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(2, 4)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.clamp_min(v1, 0.0)\n        v3 = torch.clamp_max(v2, 3.5)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 4)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.clamp_min(v1, min=0.0)\n        v3 = torch.clamp_max(v2, max=6.0)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min_value=0, max_value=5):\n        super().__init__()\n        self.min_value = min_value\n        self.max_value = max_value\n\n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.clamp_min(v1, self.min_value)\n        v3 = torch.clamp_max(v2, self.max_value)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 32, 1024, 256)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 6)\n \n    def forward(self, x1, min_value=0):\n        v1 = self.linear(x1)\n        v2 = torch.clamp_min(v1, min_value)\n        v3 = torch.clamp_max(v2, 6)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n"
            ],
            "g_time": 7.039150714874268
        }
    }
}
{
    "opt": "binary=4",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(5, 1)\n \n    def forward(self, x1, x2):\n        v1 = self.linear(x1)\n        v2 = v1 + x2\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 5)\nx2 = torch.randn(1, 1)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 5)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 + other\n        return v2\n \n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear1 = torch.nn.Linear(3, 8)\n        self.linear2 = torch.nn.Linear(8, 4)\n \n    def forward(self, x):\n        v1 = self.linear1(x)\n        v2 = v1 + self.linear2.weight\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(16, 32)\n \n    def forward(self, x1, x2):\n        v1 = self.linear(x1)\n        v2 = v1 + x2\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 16)\nx2 = torch.randn(1, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(8, 8)\n \n    def forward(self, x1, x2):\n        v1 = self.linear(x1)\n        v2 = v1 + x2\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 8)\nx2 = torch.randn(1, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(8, 1)\n \n    def forward(self, x1, x2):\n        v1 = self.linear(x1)\n        v2 = v1 + x2\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 8)\nx2 = torch.randn(1, 1)\n",
                "\nt = torch.rand(1, 5) # Create a tensor representing the values of a certain constant distribution\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(5, 3)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 + t\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 5)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear1 = torch.nn.Linear(5, 5)\n        self.linear2 = torch.nn.Linear(5, 5)\n \n    def forward(self, x):\n        v1 = self.linear1(x)\n        v2 = self.linear2(x)\n        v3 = v1 + v2\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx = torch.randn(5)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(8, 8)\n \n    def forward(self, x1, x2=torch.ones(1, 8)):\n        v1 = self.linear(x1)\n        v2 = v1 + x2\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 8)\nx2 = torch.randn(1, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 3)\n\n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 + other\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(5, 1)\n \n    def forward(self, x1, x2):\n        v1 = self.linear(x1)\n        v2 = v1 + x2\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 5)\nx2 = torch.randn(1, 1)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 5)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 + other\n        return v2\n \n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear1 = torch.nn.Linear(3, 8)\n        self.linear2 = torch.nn.Linear(8, 4)\n \n    def forward(self, x):\n        v1 = self.linear1(x)\n        v2 = v1 + self.linear2.weight\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(16, 32)\n \n    def forward(self, x1, x2):\n        v1 = self.linear(x1)\n        v2 = v1 + x2\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 16)\nx2 = torch.randn(1, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(8, 8)\n \n    def forward(self, x1, x2):\n        v1 = self.linear(x1)\n        v2 = v1 + x2\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 8)\nx2 = torch.randn(1, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(8, 1)\n \n    def forward(self, x1, x2):\n        v1 = self.linear(x1)\n        v2 = v1 + x2\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 8)\nx2 = torch.randn(1, 1)\n",
                "\nt = torch.rand(1, 5) # Create a tensor representing the values of a certain constant distribution\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(5, 3)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 + t\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 5)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear1 = torch.nn.Linear(5, 5)\n        self.linear2 = torch.nn.Linear(5, 5)\n \n    def forward(self, x):\n        v1 = self.linear1(x)\n        v2 = self.linear2(x)\n        v3 = v1 + v2\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx = torch.randn(5)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(8, 8)\n \n    def forward(self, x1, x2=torch.ones(1, 8)):\n        v1 = self.linear(x1)\n        v2 = v1 + x2\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 8)\nx2 = torch.randn(1, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 3)\n\n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 + other\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n"
            ],
            "g_time": 4.917060375213623
        }
    }
}
{
    "opt": "binary=10",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(8, 5)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 + other\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(8, 8)\n \n    def forward(self, x1, x2):\n        y = self.linear(x1)\n        y = y + x2\n        return y\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 8)\nx2 = torch.randn(1, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, other):\n        super().__init__()\n        self.linear = torch.nn.Linear(6, 10)\n \n        self.other = other\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 + self.other\n        return v2\n\n# Initializing the model\nm = Model(other=torch.randn(1, 10))\n\n# Inputs to the model\nx1 = torch.randn(1, 6)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(1, 10)\n \n    def forward(self, x1, other=None):\n        if other is None:\n            other = torch.zeros_like(self.linear.bias)\n        v1 = self.linear(x1)\n        v2 = v1 + other\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 1)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(8, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 + x2\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the models\nx1 = torch.randn(1, 8)\nx2 = torch.randn(1, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(10,10)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 + another\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 10)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Linear(13, 8)\n \n    def forward(self, x1, other=None):\n        v1 = self.conv(x1)\n        if other is not None:\n            v1 = v1 + other\n        return v1\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(3, 13)\nx2 = torch.randn(3, 13)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(32, 64)\n \n    def forward(self, x1, other):\n        v1 = self.linear(x1)\n        v2 = other + v1\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        # Specify the linear transformation weights\n        self.linear = torch.nn.Linear(3, 15)\n \n    def forward(self, x1, other):\n        v0 = self.linear(x1)\n        v1 = v0 + other\n        return v1\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n",
                " (same as https://github.com/pytorch/benchmark/blob/239c969c5288fc28b846894440ef1d154c40ba04/models/BERT/bert_pytorch.py)\n\nclass BERTLayerNorm(torch.nn.Module):\n    def __init__(self, hidden_size, eps=1e-12):\n        "
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(8, 5)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 + other\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(8, 8)\n \n    def forward(self, x1, x2):\n        y = self.linear(x1)\n        y = y + x2\n        return y\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 8)\nx2 = torch.randn(1, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, other):\n        super().__init__()\n        self.linear = torch.nn.Linear(6, 10)\n \n        self.other = other\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 + self.other\n        return v2\n\n# Initializing the model\nm = Model(other=torch.randn(1, 10))\n\n# Inputs to the model\nx1 = torch.randn(1, 6)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(1, 10)\n \n    def forward(self, x1, other=None):\n        if other is None:\n            other = torch.zeros_like(self.linear.bias)\n        v1 = self.linear(x1)\n        v2 = v1 + other\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 1)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(8, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 + x2\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the models\nx1 = torch.randn(1, 8)\nx2 = torch.randn(1, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(10,10)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 + another\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 10)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Linear(13, 8)\n \n    def forward(self, x1, other=None):\n        v1 = self.conv(x1)\n        if other is not None:\n            v1 = v1 + other\n        return v1\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(3, 13)\nx2 = torch.randn(3, 13)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(32, 64)\n \n    def forward(self, x1, other):\n        v1 = self.linear(x1)\n        v2 = other + v1\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        # Specify the linear transformation weights\n        self.linear = torch.nn.Linear(3, 15)\n \n    def forward(self, x1, other):\n        v0 = self.linear(x1)\n        v1 = v0 + other\n        return v1\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n",
                " (same as https://github.com/pytorch/benchmark/blob/239c969c5288fc28b846894440ef1d154c40ba04/models/BERT/bert_pytorch.py)\n\nclass BERTLayerNorm(torch.nn.Module):\n    def __init__(self, hidden_size, eps=1e-12):\n        "
            ],
            "g_time": 5.0187742710113525
        }
    }
}
{
    "opt": "unary=3",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv3 = torch.nn.ConvTranspose2d(1, 1, 16, stride=1, padding=0)\n        self.conv4 = torch.nn.ConvTranspose2d(1, 2, 3, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv3(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        v7 = self.conv4(v6)\n        return v7\n# Inputs to the model\nx1 = torch.randn(1, 1, 16, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(1, 3, 3, stride=1, padding=0)\n        self.conv2 = torch.nn.Conv2d(3, 3, 2, stride=2, padding=0)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        v7 = self.conv2(v6)\n        return v7\n# Inputs to the model\nx1 = torch.randn(1, 1, 21, 23)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(32, 1, 1, stride=1, padding=0)\n        self.conv2 = torch.nn.Conv2d(1, 4, 3, stride=1, padding=1)\n    def forward(self, x1, x2):\n        v1 = self.conv1(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        v7 = self.conv2(v6)\n        return v7\n# Inputs to the model\nx1 = torch.randn(1, 32, 14, 10)\nx2 = torch.randn(1, 1, 3, 5)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(1, 32, 3, stride=1, padding=0)\n        self.conv1 = torch.nn.Conv2d(32, 64, 3, stride=2, padding=0)\n        self.conv2 = torch.nn.ConvTranspose2d(64, 32, 3, stride=2, padding=1)\n        self.conv3 = torch.nn.Conv2d(32, 3, 3, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        v7 = self.conv1(v6)\n        v8 = v7 * 0.5\n        v9 = v7 * 0.7071067811865476\n        v10 = torch.erf(v9)\n        v11 = v10 + 1\n        v12 = v8 * v11\n        v13 = self.conv2(v12)\n        v14 = v13 * 0.5\n        v15 = v13 * 0.7071067811865476\n        v16 = torch.erf(v15)\n        v17 = v16 + 1\n        v18 = v14 * v17\n        v19 = self.conv3(v18)\n        return v19\n# Inputs to the model\nx1 = torch.randn(1, 1, 80, 80)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.ConvTranspose2d(25, 2, 5, stride=5, padding=0)\n        self.conv2 = torch.nn.ConvTranspose2d(2, 5, 1, stride=1, padding=0)\n        self.conv3 = torch.nn.Conv2d(5, 1, 3, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = self.conv2(v1)\n        v3 = v2 * 0.5\n        v4 = v2 * 0.7071067811865476\n        v5 = torch.erf(v4)\n        v6 = v5 + 1\n        v7 = v3 * v6\n        v8 = self.conv3(v7)\n        return v8\n# Inputs to the model\nx1 = torch.randn(1, 25, 68, 26)\n",
                "",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(1, 9, 1, stride=1, padding=0)\n        self.conv2 = torch.nn.Conv2d(9, 9, 3, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        v7 = self.conv2(v6)\n        return v7\n# Inputs to the model\nx1 = torch.randn(1, 1, 40, 40)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(1, 8, 3, stride=1, padding=0)\n    def forward(self, x1):\n        return self.conv(x1) + 0.57118615\n# Inputs to the model\nx1 = torch.randn(1, 1, 17, 19)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(1, 9, 4, stride=1, padding=2)\n        self.conv2 = torch.nn.Conv2d(9, 3, 3, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = 0.24299560465349413 * x1\n        v2 = self.conv1(v1)\n        v3 = v2 * 0.5\n        v4 = v2 * 0.7071067811865476\n        v5 = torch.erf(v4)\n        v6 = v5 + 1\n        v7 = v3 * v6\n        v8 = self.conv2(v7)\n        return v8\n# Inputs to the model\nx1 = torch.randn(1, 1, 27, 33)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(5, 3, 3, stride=1, padding=1)\n        self.conv2 = torch.nn.Conv2d(3, 10, 3, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        v7 = self.conv2(v6)\n        return v7\n# Inputs to the model\nx1 = torch.randn(1, 5, 100, 100)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv3 = torch.nn.ConvTranspose2d(1, 1, 16, stride=1, padding=0)\n        self.conv4 = torch.nn.ConvTranspose2d(1, 2, 3, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv3(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        v7 = self.conv4(v6)\n        return v7\n# Inputs to the model\nx1 = torch.randn(1, 1, 16, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(1, 3, 3, stride=1, padding=0)\n        self.conv2 = torch.nn.Conv2d(3, 3, 2, stride=2, padding=0)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        v7 = self.conv2(v6)\n        return v7\n# Inputs to the model\nx1 = torch.randn(1, 1, 21, 23)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(32, 1, 1, stride=1, padding=0)\n        self.conv2 = torch.nn.Conv2d(1, 4, 3, stride=1, padding=1)\n    def forward(self, x1, x2):\n        v1 = self.conv1(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        v7 = self.conv2(v6)\n        return v7\n# Inputs to the model\nx1 = torch.randn(1, 32, 14, 10)\nx2 = torch.randn(1, 1, 3, 5)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(1, 32, 3, stride=1, padding=0)\n        self.conv1 = torch.nn.Conv2d(32, 64, 3, stride=2, padding=0)\n        self.conv2 = torch.nn.ConvTranspose2d(64, 32, 3, stride=2, padding=1)\n        self.conv3 = torch.nn.Conv2d(32, 3, 3, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        v7 = self.conv1(v6)\n        v8 = v7 * 0.5\n        v9 = v7 * 0.7071067811865476\n        v10 = torch.erf(v9)\n        v11 = v10 + 1\n        v12 = v8 * v11\n        v13 = self.conv2(v12)\n        v14 = v13 * 0.5\n        v15 = v13 * 0.7071067811865476\n        v16 = torch.erf(v15)\n        v17 = v16 + 1\n        v18 = v14 * v17\n        v19 = self.conv3(v18)\n        return v19\n# Inputs to the model\nx1 = torch.randn(1, 1, 80, 80)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.ConvTranspose2d(25, 2, 5, stride=5, padding=0)\n        self.conv2 = torch.nn.ConvTranspose2d(2, 5, 1, stride=1, padding=0)\n        self.conv3 = torch.nn.Conv2d(5, 1, 3, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = self.conv2(v1)\n        v3 = v2 * 0.5\n        v4 = v2 * 0.7071067811865476\n        v5 = torch.erf(v4)\n        v6 = v5 + 1\n        v7 = v3 * v6\n        v8 = self.conv3(v7)\n        return v8\n# Inputs to the model\nx1 = torch.randn(1, 25, 68, 26)\n",
                "",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(1, 9, 1, stride=1, padding=0)\n        self.conv2 = torch.nn.Conv2d(9, 9, 3, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        v7 = self.conv2(v6)\n        return v7\n# Inputs to the model\nx1 = torch.randn(1, 1, 40, 40)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(1, 8, 3, stride=1, padding=0)\n    def forward(self, x1):\n        return self.conv(x1) + 0.57118615\n# Inputs to the model\nx1 = torch.randn(1, 1, 17, 19)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(1, 9, 4, stride=1, padding=2)\n        self.conv2 = torch.nn.Conv2d(9, 3, 3, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = 0.24299560465349413 * x1\n        v2 = self.conv1(v1)\n        v3 = v2 * 0.5\n        v4 = v2 * 0.7071067811865476\n        v5 = torch.erf(v4)\n        v6 = v5 + 1\n        v7 = v3 * v6\n        v8 = self.conv2(v7)\n        return v8\n# Inputs to the model\nx1 = torch.randn(1, 1, 27, 33)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(5, 3, 3, stride=1, padding=1)\n        self.conv2 = torch.nn.Conv2d(3, 10, 3, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        v7 = self.conv2(v6)\n        return v7\n# Inputs to the model\nx1 = torch.randn(1, 5, 100, 100)\n"
            ],
            "g_time": 16.310201168060303
        }
    }
}
{
    "opt": "addmm",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, inp):\n        x2 = torch.mm(x1, x1)\n        v1 = torch.mm(x1, x2) + inp\n        return v1\n# Inputs to the model\nx1 = torch.randn(3, 3, requires_grad=True)\ninp = torch.randn(3, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1):\n        v1 = torch.mm(x1, x1)\n        v2 = torch.mm(x1, x1)\n        return v1 + v2\n# Inputs to the model\nx1 = torch.randn(3, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.t = torch.randn(3, 3, requires_grad=True)\n    def forward(self, x1, x2, inp):\n        v1 = torch.mm(x1, x2)\n        v2 = v1 - inp\n        v3 = torch.mm(v2, x1)\n        return v3\n# Inputs to the model\nx1 = torch.randn(3, 3, requires_grad=True)\nx2 = torch.randn(3, 3)\ninp = torch.randn(3, 3, requires_grad=True)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        torch.nn.init.zeros_(self.t1)\n    def forward(self, x1, x2, inp):\n        v1 = torch.mm(x1, x1)\n        v2 = v1 + x2\n        return v2 ** 2\nx1 = torch.randn(3, 3, requires_grad=True)\nx2 = torch.randn(3, 3, requires_grad=True)\ninp = torch.randn(3, 3, requires_grad=True)\n",
                "\nclass Model(torch.nn.Module):\n    def forward(self, t1, t2, x1, x2):\n        v1 = torch.mm(x1, x1)\n        out = v1 + v1\n        t1 = t1 + t2\n        v1 = torch.mm(t2, t2)\n        v1 = torch.mm(v1, t1)\n        out = v1 + v1\n        v1 = torch.mm(x1, x2) + out\n        return v1\n# Inputs to the model\nt1 = torch.randn(3, 3, requires_grad=True)\nt2 = torch.randn(3, 3)\nx1 = torch.randn(3, 3)\nx2 = torch.randn(3, 3)\nmm1 = torch.randn(3, 3, requires_grad=True)\nmm2 = torch.randn(3, 3, requires_grad=True)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, x2, inp):\n        v1 = torch.mm(inp, x1)\n        v2 = torch.mm(x2, x1)\n        return v2 + v1 + inp\n# Inputs to the model\nx1 = torch.randn(3, 3)\nx2 = torch.randn(3, 3, requires_grad=True)\ninp = torch.randn(3, 3, requires_grad=True)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.t = torch.randn(3, 3, requires_grad=True)\n    def forward(self, x1, x2, inp):\n        v1 = torch.mm(x1, x2)\n        v2 = v1 + inp \n        v2 = v2 * self.t\n        return v2\n# Inputs to the model\nx1 = torch.randn(3, 3)\nx2 = torch.randn(3, 3)\ninp = torch.randn(3, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.t1 = torch.randn(3, 3, requires_grad=True)\n    def forward(self, x1, x2, inp):\n        v1 = torch.mm(x1, x2)\n        v1 = self.t1 * v1\n        return v1 - inp\n# Inputs to the model\nx1 = torch.randn(3, 3)\nx2 = torch.randn(3, 3)\ninp = torch.randn(3, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.weight = torch.nn.Parameter(torch.Tensor(200, 200))\n        self.weight.data.normal_()\n    def forward(self, x1):\n        return torch.mm(x1, torch.mm(x1, x1))\n# Inputs to the model\nx = torch.randn(200, 200, requires_grad=True)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.t1 = torch.randn(3, 3, requires_grad=True)\n    def forward(self, x1, inp):\n        v1 = torch.mm(x1, x1) + self.t1\n        return v1 * inp\n# Inputs to the model\nx1 = torch.randn(1, 3)\ninp = torch.randn(3, 3)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, inp):\n        x2 = torch.mm(x1, x1)\n        v1 = torch.mm(x1, x2) + inp\n        return v1\n# Inputs to the model\nx1 = torch.randn(3, 3, requires_grad=True)\ninp = torch.randn(3, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1):\n        v1 = torch.mm(x1, x1)\n        v2 = torch.mm(x1, x1)\n        return v1 + v2\n# Inputs to the model\nx1 = torch.randn(3, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.t = torch.randn(3, 3, requires_grad=True)\n    def forward(self, x1, x2, inp):\n        v1 = torch.mm(x1, x2)\n        v2 = v1 - inp\n        v3 = torch.mm(v2, x1)\n        return v3\n# Inputs to the model\nx1 = torch.randn(3, 3, requires_grad=True)\nx2 = torch.randn(3, 3)\ninp = torch.randn(3, 3, requires_grad=True)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        torch.nn.init.zeros_(self.t1)\n    def forward(self, x1, x2, inp):\n        v1 = torch.mm(x1, x1)\n        v2 = v1 + x2\n        return v2 ** 2\nx1 = torch.randn(3, 3, requires_grad=True)\nx2 = torch.randn(3, 3, requires_grad=True)\ninp = torch.randn(3, 3, requires_grad=True)\n",
                "\nclass Model(torch.nn.Module):\n    def forward(self, t1, t2, x1, x2):\n        v1 = torch.mm(x1, x1)\n        out = v1 + v1\n        t1 = t1 + t2\n        v1 = torch.mm(t2, t2)\n        v1 = torch.mm(v1, t1)\n        out = v1 + v1\n        v1 = torch.mm(x1, x2) + out\n        return v1\n# Inputs to the model\nt1 = torch.randn(3, 3, requires_grad=True)\nt2 = torch.randn(3, 3)\nx1 = torch.randn(3, 3)\nx2 = torch.randn(3, 3)\nmm1 = torch.randn(3, 3, requires_grad=True)\nmm2 = torch.randn(3, 3, requires_grad=True)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, x2, inp):\n        v1 = torch.mm(inp, x1)\n        v2 = torch.mm(x2, x1)\n        return v2 + v1 + inp\n# Inputs to the model\nx1 = torch.randn(3, 3)\nx2 = torch.randn(3, 3, requires_grad=True)\ninp = torch.randn(3, 3, requires_grad=True)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.t = torch.randn(3, 3, requires_grad=True)\n    def forward(self, x1, x2, inp):\n        v1 = torch.mm(x1, x2)\n        v2 = v1 + inp \n        v2 = v2 * self.t\n        return v2\n# Inputs to the model\nx1 = torch.randn(3, 3)\nx2 = torch.randn(3, 3)\ninp = torch.randn(3, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.t1 = torch.randn(3, 3, requires_grad=True)\n    def forward(self, x1, x2, inp):\n        v1 = torch.mm(x1, x2)\n        v1 = self.t1 * v1\n        return v1 - inp\n# Inputs to the model\nx1 = torch.randn(3, 3)\nx2 = torch.randn(3, 3)\ninp = torch.randn(3, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.weight = torch.nn.Parameter(torch.Tensor(200, 200))\n        self.weight.data.normal_()\n    def forward(self, x1):\n        return torch.mm(x1, torch.mm(x1, x1))\n# Inputs to the model\nx = torch.randn(200, 200, requires_grad=True)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.t1 = torch.randn(3, 3, requires_grad=True)\n    def forward(self, x1, inp):\n        v1 = torch.mm(x1, x1) + self.t1\n        return v1 * inp\n# Inputs to the model\nx1 = torch.randn(1, 3)\ninp = torch.randn(3, 3)\n"
            ],
            "g_time": 7.1222851276397705
        }
    }
}
{
    "opt": "unary=12",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(1, 8, 1, stride=1, dilation=1)\n        self.sigmoid = torch.nn.Sigmoid()\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = self.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 1, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 6, 1, stride=1, padding=1, dilation=2)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = torch.mul(v1, v2)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 3, 224, 224)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv1d(3, 3, 3, stride=1, padding=0)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = torch.mul(v1, v2)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 3, 128)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 8, 1, stride=1, dilation=2, groups=2)\n        self.sigmoid = torch.nn.Sigmoid()\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = self.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 8, 1, stride=1, dilation=2)\n        self.sigmoid = torch.nn.Sigmoid()\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = self.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 16, 1, stride=1, dilation=7)\n        self.sigmoid = torch.nn.Sigmoid()\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = self.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(5, 8, 1, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = torch.mul(v2, v1)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 5, 32, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 4, 1, stride=1, padding=1)\n        self.conv2 = torch.nn.Conv2d(4, 3, 1, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = torch.mul(v1, v2)\n        v4 = self.conv2(v3)\n        return v1\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 3, 1, stride=1, padding=1, dilation=1)\n        self.sigmoid = torch.nn.Sigmoid()\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = self.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 16, 1, stride=1, dilation=2)\n        self.sigmoid = torch.nn.Sigmoid()\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = self.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(1, 8, 1, stride=1, dilation=1)\n        self.sigmoid = torch.nn.Sigmoid()\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = self.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 1, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 6, 1, stride=1, padding=1, dilation=2)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = torch.mul(v1, v2)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 3, 224, 224)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv1d(3, 3, 3, stride=1, padding=0)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = torch.mul(v1, v2)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 3, 128)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 8, 1, stride=1, dilation=2, groups=2)\n        self.sigmoid = torch.nn.Sigmoid()\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = self.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 8, 1, stride=1, dilation=2)\n        self.sigmoid = torch.nn.Sigmoid()\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = self.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 16, 1, stride=1, dilation=7)\n        self.sigmoid = torch.nn.Sigmoid()\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = self.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(5, 8, 1, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = torch.mul(v2, v1)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 5, 32, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 4, 1, stride=1, padding=1)\n        self.conv2 = torch.nn.Conv2d(4, 3, 1, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = torch.mul(v1, v2)\n        v4 = self.conv2(v3)\n        return v1\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 3, 1, stride=1, padding=1, dilation=1)\n        self.sigmoid = torch.nn.Sigmoid()\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = self.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 16, 1, stride=1, dilation=2)\n        self.sigmoid = torch.nn.Sigmoid()\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = self.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n"
            ],
            "g_time": 6.001601219177246
        }
    }
}
{
    "opt": "mm_plus_mm",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def forward(self, x1, x2, x3, x4):\n        y = torch.mm(x1, x2)\n        v = torch.mm(y, z)\n        return v\n# Inputs to the model\nx1 = torch.randn(5, 5)\nx2 = torch.randn(5, 5)\nx3 = torch.randn(5, 5)\nx4 = torch.randn(5, 5)\n",
                "\nclass Model(torch.nn.Module):\n    def forward(self, input1, input2, input3, input4):\n        t0 = torch.mm(input1, input3)\n        t1 = torch.mm(input1, input1)\n        out = torch.mm(input3, input4)\n        out = t1 + t0 + out\n        return out\n# Inputs to the model\ninput1 = torch.randn(5, 5)\ninput2 = torch.randn(5, 5)\ninput3 = torch.randn(5, 5)\ninput4 = torch.randn(5, 5)\n",
                "\nclass Model(torch.nn.Module):\n    def forward(self, x1, x2, x3, x4):\n        h1 = torch.mm(x1, x2)\n        h2 = torch.mm(x2, x1)\n        h3 = torch.mm(x3, x2)\n        h4 = torch.mm(x4, x4)\n        return h1 + h2 + h3 + h4\n# Inputs to the model\nx1 = torch.randn(6, 6)\nx2 = torch.randn(6, 6)\nx3 = torch.randn(6, 6)\nx4 = torch.randn(6, 6)\n",
                "\nclass Model(nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n\n    def forward(self, input1, input2, input3):\n        t1 = torch.mm(input1, input2)\n        t2 = torch.mm(input3, input1)\n        return t1 + t2\n# Inputs to the model\ninput1 = torch.randn(16, 16)\ninput2 = torch.randn(16, 16)\ninput3 = torch.randn(16, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def forward(self, input1, input2, input3, input4):\n        t1 = torch.mm(input1, input3)\n        t2 = torch.mm(input2, input1)\n        t3 = torch.mm(input2, input4)\n        t4 = torch.mm(input3, input3)\n        t5 = torch.mm(input4, input4)\n        t6 = torch.mm(input4, input4)\n        return t1 + t2 + t3 + t4 + t5 + t6\n# Inputs to the model\ninput1 = torch.randn(16, 16)\ninput2 = torch.randn(16, 16)\ninput3 = torch.randn(16, 16)\ninput4 = torch.randn(16, 16)\n",
                "\nclass Model(nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n    def forward(self, input1, input2, input3, input4):\n        t1 = torch.mm(input1, input2)\n        t2 = torch.mm(input2, input1)\n        t4 = torch.mm(input4, input4)\n        t5 = torch.mm(input3, input4)\n        t3 = t1 + t2 + t4 + t5\n        return t3\n# Inputs to the model\ninput1 = torch.randn(16, 16)\ninput2 = torch.randn(16, 16)\ninput3 = torch.randn(16, 16)\ninput4 = torch.randn(16, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def forward(self, input1, input2, input3, input4):\n        t1 = torch.mm(input1, input2)\n        t2 = torch.mm(input1, input2)\n        t3 = torch.mm(input1, input2)\n        t4 = torch.mm(input1, input2)\n        return t1 + t2 + t3 + t4\n# Inputs to the model\ninput1 = torch.randn(16, 16)\ninput2 = torch.randn(16, 16)\ninput3 = torch.randn(16, 16)\ninput4 = torch.randn(16, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def forward(self, x1, x2, x3, x4):\n        t1 = torch.mm(x1, x2)\n        t2 = torch.mm(x2, x3)\n        t3 = torch.mm(x4, x1)\n        t4 = torch.mm(x1, x3)\n        return t1 + t2 + t3 + t4\n# Inputs to the model\nx1 = torch.randn(16, 16)\nx2 = torch.randn(16, 16)\nx3 = torch.randn(16, 16)\nx4 = torch.randn(16, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def forward(self, x1, x2, x3, x4):\n        v1 = torch.mm(x1, x3)\n        v2 = torch.mm(x1, x3)\n        v3 = torch.mm(x1, x3)\n        v4 = torch.mm(x1, x3)\n        return v1 + v2 + v3 + v4\n# Inputs to the model\nx1 = torch.randn(7, 7)\nx2 = torch.randn(7, 7)\nx3 = torch.randn(7, 7)\nx4 = torch.randn(7, 7)\n",
                "\nclass Model(torch.nn.Module):\n    def forward(self,x1,x2,x3,x4):\n        result = torch.mm(x2,x1)\n        result = torch.mm(x3,x1)\n        result = torch.mm(x4,x1)\n        result = torch.mm(x4,x1)\n        result = torch.mm(x2,x1)\n        result = torch.mm(x2,x1)\n        result = torch.mm(x3,x1)\n        result = torch.mm(x4,x1)\n        result = torch.mm(x4,x1)\n        result = torch.mm(x2,x1)\n        for i in range(10):\n            result = torch.mm(x2,x1)\n        return result\n# Inputs to the model\nx1 = torch.randn(3,3)\nx2 = torch.randn(3,3)\nx3 = torch.randn(3,3)\nx4 = torch.randn(3,3)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def forward(self, x1, x2, x3, x4):\n        y = torch.mm(x1, x2)\n        v = torch.mm(y, z)\n        return v\n# Inputs to the model\nx1 = torch.randn(5, 5)\nx2 = torch.randn(5, 5)\nx3 = torch.randn(5, 5)\nx4 = torch.randn(5, 5)\n",
                "\nclass Model(torch.nn.Module):\n    def forward(self, input1, input2, input3, input4):\n        t0 = torch.mm(input1, input3)\n        t1 = torch.mm(input1, input1)\n        out = torch.mm(input3, input4)\n        out = t1 + t0 + out\n        return out\n# Inputs to the model\ninput1 = torch.randn(5, 5)\ninput2 = torch.randn(5, 5)\ninput3 = torch.randn(5, 5)\ninput4 = torch.randn(5, 5)\n",
                "\nclass Model(torch.nn.Module):\n    def forward(self, x1, x2, x3, x4):\n        h1 = torch.mm(x1, x2)\n        h2 = torch.mm(x2, x1)\n        h3 = torch.mm(x3, x2)\n        h4 = torch.mm(x4, x4)\n        return h1 + h2 + h3 + h4\n# Inputs to the model\nx1 = torch.randn(6, 6)\nx2 = torch.randn(6, 6)\nx3 = torch.randn(6, 6)\nx4 = torch.randn(6, 6)\n",
                "\nclass Model(nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n\n    def forward(self, input1, input2, input3):\n        t1 = torch.mm(input1, input2)\n        t2 = torch.mm(input3, input1)\n        return t1 + t2\n# Inputs to the model\ninput1 = torch.randn(16, 16)\ninput2 = torch.randn(16, 16)\ninput3 = torch.randn(16, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def forward(self, input1, input2, input3, input4):\n        t1 = torch.mm(input1, input3)\n        t2 = torch.mm(input2, input1)\n        t3 = torch.mm(input2, input4)\n        t4 = torch.mm(input3, input3)\n        t5 = torch.mm(input4, input4)\n        t6 = torch.mm(input4, input4)\n        return t1 + t2 + t3 + t4 + t5 + t6\n# Inputs to the model\ninput1 = torch.randn(16, 16)\ninput2 = torch.randn(16, 16)\ninput3 = torch.randn(16, 16)\ninput4 = torch.randn(16, 16)\n",
                "\nclass Model(nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n    def forward(self, input1, input2, input3, input4):\n        t1 = torch.mm(input1, input2)\n        t2 = torch.mm(input2, input1)\n        t4 = torch.mm(input4, input4)\n        t5 = torch.mm(input3, input4)\n        t3 = t1 + t2 + t4 + t5\n        return t3\n# Inputs to the model\ninput1 = torch.randn(16, 16)\ninput2 = torch.randn(16, 16)\ninput3 = torch.randn(16, 16)\ninput4 = torch.randn(16, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def forward(self, input1, input2, input3, input4):\n        t1 = torch.mm(input1, input2)\n        t2 = torch.mm(input1, input2)\n        t3 = torch.mm(input1, input2)\n        t4 = torch.mm(input1, input2)\n        return t1 + t2 + t3 + t4\n# Inputs to the model\ninput1 = torch.randn(16, 16)\ninput2 = torch.randn(16, 16)\ninput3 = torch.randn(16, 16)\ninput4 = torch.randn(16, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def forward(self, x1, x2, x3, x4):\n        t1 = torch.mm(x1, x2)\n        t2 = torch.mm(x2, x3)\n        t3 = torch.mm(x4, x1)\n        t4 = torch.mm(x1, x3)\n        return t1 + t2 + t3 + t4\n# Inputs to the model\nx1 = torch.randn(16, 16)\nx2 = torch.randn(16, 16)\nx3 = torch.randn(16, 16)\nx4 = torch.randn(16, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def forward(self, x1, x2, x3, x4):\n        v1 = torch.mm(x1, x3)\n        v2 = torch.mm(x1, x3)\n        v3 = torch.mm(x1, x3)\n        v4 = torch.mm(x1, x3)\n        return v1 + v2 + v3 + v4\n# Inputs to the model\nx1 = torch.randn(7, 7)\nx2 = torch.randn(7, 7)\nx3 = torch.randn(7, 7)\nx4 = torch.randn(7, 7)\n",
                "\nclass Model(torch.nn.Module):\n    def forward(self,x1,x2,x3,x4):\n        result = torch.mm(x2,x1)\n        result = torch.mm(x3,x1)\n        result = torch.mm(x4,x1)\n        result = torch.mm(x4,x1)\n        result = torch.mm(x2,x1)\n        result = torch.mm(x2,x1)\n        result = torch.mm(x3,x1)\n        result = torch.mm(x4,x1)\n        result = torch.mm(x4,x1)\n        result = torch.mm(x2,x1)\n        for i in range(10):\n            result = torch.mm(x2,x1)\n        return result\n# Inputs to the model\nx1 = torch.randn(3,3)\nx2 = torch.randn(3,3)\nx3 = torch.randn(3,3)\nx4 = torch.randn(3,3)\n"
            ],
            "g_time": 8.142249345779419
        }
    }
}
{
    "opt": "unary=25",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self, negative_slope):\n        super().__init__()\n        self.linear = torch.nn.Linear(64, 16)\n        self.negative_slope = negative_slope\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 > 0\n        v3 = v1 * self.negative_slope\n        v4 = torch.where(v2, v1, v3)\n        return v4\n\n# Initializing the model\nm = Model(negative_slope)\n\n# Inputs to the model\nx1 = torch.randn(16, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(16, 32)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 > 0\n        v3 = v1 * 0.01\n        v4 = torch.where(v2, v1, v3)\n        return v4\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 64, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, negative_slope):\n        super().__init__()\n        self.linear = torch.nn.Linear(8, 8)\n        self.negative_slope = negative_slope\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 > 0\n        v3 = v1 * self.negative_slope\n        v4 = torch.where(v2, input=v1, other=v3)\n        return v4\n\n# Initializing the model\nm = Model(negative_slope=0.01)\n\n# Inputs to the model\nx1 = torch.randn(1, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc = torch.nn.Linear(3, 3)\n \n    def forward(self, x1):\n        v1 = self.fc(x1)\n        v2 = v1 > 0\n        v3 = v1 * 0.01\n        v4 = torch.where(v2, v1, v3)\n        return v4\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(3, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, negative_slope=0.2):\n        super().__init__()\n        self.m = torch.nn.Linear(5, 3)\n        self.negative_slope = negative_slope\n \n    def forward(self, x1):\n        v1 = self.m(x1)\n        v2 = v1 > 0\n        v3 = v1 * self.negative_slope\n        v4 = torch.where(v2, v1, v3)\n        return v4\n\n# Initializing the model\nm = Model(0.2)\n\n# Inputs to the model\nx1 = torch.randn(1, 5)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, negative_slope = 0.2):\n        super().__init__()\n        self.linear = torch.nn.Linear(4, 4)\n        self.negative_slope = negative_slope\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 > 0\n        v3 = v1 * self.negative_slope\n        v4 = torch.where(v2, v1, v3)\n        return v4\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 4)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, negative_slope):\n        super().__init__()\n        self.linear = torch.nn.Linear(6, 4)\n        self.negative_slope = negative_slope\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 > 0\n        v3 = v1 * self.negative_slope\n        v4 = torch.where(v2, v1, v3)\n        return v4\n\n# Initializing the model\nm = Model(0.2)\n\n# Inputs to the model\nx1 = torch.randn(2, 6)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, negative_slope):\n        super().__init__()\n        self.linear = torch.nn.Linear(8, 8, bias=True)\n        self.negative_slope = negative_slope\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 > 0\n        v3 = torch.where(v2, self.linear(x1), self.negative_slope * v1)\n        return v3\n\n# Initializing the model\nm1 = Model(-0.5)\nm2 = Model(-2)\n\n# Inputs to the model\nx1 = torch.randn(1, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, negative_slope=0.02):\n        super().__init__()\n        self.linear = torch.nn.Linear(6, 1)\n        self.negative_slope = negative_slope\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 > 0\n        v3 = v1 * self.negative_slope\n        v4 = torch.where(v2, v1, v3)\n        return v4\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(2, 6)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 > 0\n        v3 = v1 * 0.01\n        v4 = torch.where(v2, v1, v3)\n        return v4\n\n# Initializing the model\nm = Model()\n# Inputs to the model\nx1 = torch.randn(1, 3)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self, negative_slope):\n        super().__init__()\n        self.linear = torch.nn.Linear(64, 16)\n        self.negative_slope = negative_slope\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 > 0\n        v3 = v1 * self.negative_slope\n        v4 = torch.where(v2, v1, v3)\n        return v4\n\n# Initializing the model\nm = Model(negative_slope)\n\n# Inputs to the model\nx1 = torch.randn(16, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(16, 32)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 > 0\n        v3 = v1 * 0.01\n        v4 = torch.where(v2, v1, v3)\n        return v4\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 64, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, negative_slope):\n        super().__init__()\n        self.linear = torch.nn.Linear(8, 8)\n        self.negative_slope = negative_slope\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 > 0\n        v3 = v1 * self.negative_slope\n        v4 = torch.where(v2, input=v1, other=v3)\n        return v4\n\n# Initializing the model\nm = Model(negative_slope=0.01)\n\n# Inputs to the model\nx1 = torch.randn(1, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc = torch.nn.Linear(3, 3)\n \n    def forward(self, x1):\n        v1 = self.fc(x1)\n        v2 = v1 > 0\n        v3 = v1 * 0.01\n        v4 = torch.where(v2, v1, v3)\n        return v4\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(3, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, negative_slope=0.2):\n        super().__init__()\n        self.m = torch.nn.Linear(5, 3)\n        self.negative_slope = negative_slope\n \n    def forward(self, x1):\n        v1 = self.m(x1)\n        v2 = v1 > 0\n        v3 = v1 * self.negative_slope\n        v4 = torch.where(v2, v1, v3)\n        return v4\n\n# Initializing the model\nm = Model(0.2)\n\n# Inputs to the model\nx1 = torch.randn(1, 5)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, negative_slope = 0.2):\n        super().__init__()\n        self.linear = torch.nn.Linear(4, 4)\n        self.negative_slope = negative_slope\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 > 0\n        v3 = v1 * self.negative_slope\n        v4 = torch.where(v2, v1, v3)\n        return v4\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 4)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, negative_slope):\n        super().__init__()\n        self.linear = torch.nn.Linear(6, 4)\n        self.negative_slope = negative_slope\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 > 0\n        v3 = v1 * self.negative_slope\n        v4 = torch.where(v2, v1, v3)\n        return v4\n\n# Initializing the model\nm = Model(0.2)\n\n# Inputs to the model\nx1 = torch.randn(2, 6)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, negative_slope):\n        super().__init__()\n        self.linear = torch.nn.Linear(8, 8, bias=True)\n        self.negative_slope = negative_slope\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 > 0\n        v3 = torch.where(v2, self.linear(x1), self.negative_slope * v1)\n        return v3\n\n# Initializing the model\nm1 = Model(-0.5)\nm2 = Model(-2)\n\n# Inputs to the model\nx1 = torch.randn(1, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, negative_slope=0.02):\n        super().__init__()\n        self.linear = torch.nn.Linear(6, 1)\n        self.negative_slope = negative_slope\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 > 0\n        v3 = v1 * self.negative_slope\n        v4 = torch.where(v2, v1, v3)\n        return v4\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(2, 6)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 > 0\n        v3 = v1 * 0.01\n        v4 = torch.where(v2, v1, v3)\n        return v4\n\n# Initializing the model\nm = Model()\n# Inputs to the model\nx1 = torch.randn(1, 3)\n"
            ],
            "g_time": 6.513689994812012
        }
    }
}
{
    "opt": "sfdp=1",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self, query_dim, key_dim, value_dim):\n        super().__init__()\n        self.query_dim = query_dim\n        self.key_dim = key_dim\n        self.value_dim = value_dim\n        self.inv_scale_factor = np.sqrt(key_dim)\n        \n        self.dropout = torch.nn.Dropout(0.1)\n        self.softmax = torch.nn.Softmax(dim=-1)\n        self.matmul_1 = torch.nn.Linear(query_dim, key_dim, bias=False)\n        self.matmul_2 = torch.nn.Linear(key_dim, value_dim, bias=False)\n\n    def forward(self, q, k, v):\n        q = q.unsqueeze(dim=1)\n        q = self.matmul_1(q)\n        k = self.matmul_1(k)\n        qk = torch.matmul(q, k.transpose(-2, -1))\n        qk = qk.div(self.inv_scale_factor)\n        \n        softmax_qk = self.softmax(qk)\n        dropout_qk = self.dropout(softmax_qk)\n        output = dropout_qk.matmul(v)\n        return output\n\n# Initializing the model\nm = Model(64, 64, 64)\n\n# Inputs to the model\n__query__ = torch.randn(1, 5, 64)\n__key__ = torch.randn(5, 15, 64)\n__value__ = torch.randn(5, 15, 64)\n",
                "\nclass ScaledDotProductAttention(nn.Module):\n    def forward(self, query, key, value, mask=None, dropout=None):\n        scale_factor = query.size(-1) ** 0.5 # Use this formula to compute the inverse scale factor\n\n        # To calculate the scaled dot product, uncomment the following line\n        # qk = torch.matmul(query, key.transpose(-2, -1))\n\n        qk = query * key.sum(dim=1, keepdim=True)\n        if scale_factor!= 1:\n            # Use this formula to scale the dot product\n            # scaled_qk = qk.div(scale_factor)\n\n            scaled_qk = qk / scale_factor\n        \n        # To apply softmax, uncomment the following line\n        # softmax_qk = scaled_qk.softmax(dim=-1)\n\n        softmax_qk = torch.softmax(scaled_qk, dim=-1)\n        if dropout is not None:\n            # To apply dropout, uncomment the following line\n            # dropout_qk = torch.nn.functional.dropout(softmax_qk, p=dropout)\n\n            dropout_qk = dropout(softmax_qk)\n        else:\n            dropout_qk = softmax_qk\n        output = dropout_qk * value\n        return output\n\n\nclass Model(torch.nn.Module):\n    def __init__(self, dropout=0.5, dim=10):\n        super().__init__()\n        self.dropout = dropout\n        self.dim = dim\n        self.attentions = list(ScaledDotProductAttention() for _ in range(dim))\n        self.convs = list(nn.Conv3d() for _ in range(dim))\n        self.norms = list(nn.LayerNorm() for _ in range(dim))\n\n    def forward(self, x):\n        v = x\n        for norm, conv, attention in zip(self.norms, self.convs, self.attentions):\n            v = norm(v)\n            v = conv(v)\n            v = attention(v)\n        return v\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx = torch.randn(1, 10, 32, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, query, key, value, dropout_p):\n        super().__init__()\n        self.dropout_p = dropout_p\n \n    def forward(self, query, key, value):\n        qk = torch.matmul(query, key.transpose(-2, -1))\n        scaled_qk = qk.div(inv_scale_factor)\n        softmax_qk = scaled_qk.softmax(dim=-1)\n        dropout_qk = torch.nn.functional.dropout(softmax_qk, p=dropout_p)\n        output = dropout_qk.matmul(value)\n\n# Initializing the model\nm = Model(query, key, value)\n\n# Inputs to the model\nquery = torch.randn(1, 8, 64, 64)\nkey = torch.randn(1, 32, 8, 64)\nvalue = torch.randn(1, 8, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n \n    def forward(self, query, key, value, inv_scale_factor, dropout_p):\n        kq = torch.matmul(query, key.transpose(-2, -1))\n        drop_kq = torch.nn.functional.dropout(kq, p=dropout_p)\n        scaled_drop_kq = drop_kq.div(inv_scale_factor)\n        output = scaled_drop_kq.matmul(value)\n        return output\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nquery = torch.randn(1, 8, 3136)\nkey = torch.randn(1, 16, 3136)\nvalue = torch.randn(1, 16, 3136)\ninv_scale_factor = torch.tensor(1.0, dtype=torch.float)\ndropout_p = 0.2\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, batch, embedding, num_head):\n        super().__init__()\n        self.num_head = num_head\n        self.batch = batch\n        self.embedding = embedding\n        self.q_linear = torch.nn.Linear(self.embedding, self.embedding)\n        self.k_linear = torch.nn.Linear(self.embedding, self.embedding)\n        self.v_linear = torch.nn.Linear(self.embedding, self.embedding)\n \n    def forward(self, q, k, v, pos, training=True):\n        r_qk = self.mha(q, k, v, pos)\n        r_qk = r_qk.transpose(-2, -1)\n        a = r_qk / math.sqrt(self.embedding / self.num_head)\n        a = a.softmax(dim=-1)\n        #a_dropout = torch.nn.functional.dropout(a)\n        r = torch.matmul(a, v)\n        return r\n \nclass mha(torch.nn.Module):   \n    def __init__(self):\n        super().__init__()\n        self.transpose = torch.nn.functional.linear(A=q, B=k, bias=None)\n    def forward(self, q, k, v, pos):\n        q_key = torch.matmul(q, k.transpose(-2, -1))\n        q_key = q_key.div(math.sqrt(self.embedding / self.num_head))\n        q_key = q_key.softmax(dim=-1)\n        q_key_dropout = torch.nn.functional.dropout(q_key)\n        output = torch.matmul(q_key_dropout, v)\n        return output\n\n# Initializing the model\nm = Model()\n\n# Query, Key and Value\n__batch__ = 4\n__embedding__ = 128\n__num_head__ = 2\n\nq = torch.randn(self.batch*self.num_head, __num_head__, __embedding__)\nk = torch.randn(self.q_len*self.num_head, __num_head__, __embedding__)\nv = torch.randn(self.q_len*self.num_head, __num_head__, __embedding__)\n\n# Pos\n__max_len__ = 41\n__pos_len__ = 41\nself.pos = torch.tensor([[pos_i-pos_i%self.num_head for pos_i in pos_j] for pos_j in pos])\nself.pos = self.pos.view(-1).to(m.device)\npos_encoding_tensor = torch.zeros(self.len, self.depth)\npos = torch.arange(self.len).unsqueeze(1).expand(self.len, self.depth).to(m.device)\n__pos_encoding__ = torch.tensor([pos_i % self.depth for pos_i in pos])\npos_encoding_tensor[...] = torch.nn.functional.embedding(input=__pos_encoding__, weight=self.position_encoding_table)\npos_encoding_tensor[::self.num_head,...] = torch.nn.functional.embedding(input=__pos_encoding__, weight=self.position_encoding_table)\n__pos_encoding__ = pos_encoding_tensor[range(self.len)]\n\n# Inputs to the model\n__query__ = q\n__key__ = k\n__value__ = v\n__pos__ = __pos_encoding__\n__training__ = True\nr = m(query=__query__, key=__key__, value=__value__, pos=__pos__, training=__training__)\n\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, dropout_p, softmax_input_dim, softmax_dim):\n        super().__init__()\n        self.linear_qk = torch.nn.Linear(softmax_input_dim, softmax_input_dim)\n        self.dropout_qk = torch.nn.Dropout(p=dropout_p)\n        self.linear_output = torch.nn.Linear(softmax_input_dim, softmax_input_dim)\n\n    def forward(self, query, key, value):\n        qk = self.linear_qk(query) + self.linear_qk(key).transpose(-2, -1)\n        qk = qk.div(math.sqrt(query.shape[-1]))\n        dropout_qk = self.dropout_qk(torch.nn.functional.softmax(qk, dim=-1))\n        output = self.linear_output(dropout_qk.matmul(value))\n        return output\n\n# Initializing the model\nm = Model(0, softmax_input_dim, softmax_dim)\n\n# Inputs to the model\nx1 = torch.randn(1, 30, softmax_input_dim)\nx2 = torch.randn(1, 40, softmax_input_dim)\nx3 = torch.randn(1, 40, softmax_input_dim)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n \n    def forward(self, query, key, value, inv_scale_factor, dropout_p, dropout_i):\n        v1 = torch.matmul(query, key.transpose(-2, -1))\n        v2 = v1 / inv_scale_factor\n        s_qk = torch.nn.functional.softmax(v2, dim=-1)\n        v3 = torch.nn.functional.dropout(s_qk, p=p, iid=d)\n        o = i.matmul(v3)\n        return o\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nquery = torch.randn(3, 512, 50)\nkey = torch.randn(3, 512, 18)\nvalue = torch.randn(3, 512, 18)\ninv_scale_factor = 1.0 / 0.5\ndropout_p = 0.1\ndropout_i = True\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.dropout_p = 0.2\n \n    def forward(self, query, key, value):\n        qk = torch.matmul(query, key.transpose(-2, -1))\n        inv_scale_factor = torch.full_like(qk[:, :, 0], 1e4)\n        scaled_qk = qk.div(inv_scale_factor)\n        softmax_qk = scaled_qk.softmax(dim=-1)\n        dropout_qk = torch.nn.functional.dropout(softmax_qk, p=self.dropout_p)\n        output = dropout_qk.matmul(value)\n        return output\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nquery = torch.randn(1, 8, 5)\nkey = torch.randn(1, 8, 10)\nvalue = torch.randn(1, 8, 10)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n \n    def forward(self, query, key, value, inv_scale_factor, dropout_p):\n        qk = torch.matmul(query, key.transpose(-2, -1))\n        scaled_qk = qk.div(inv_scale_factor)\n        softmax_qk = scaled_qk.softmax(dim=-1)\n        dropout_qk = torch.nn.functional.dropout(softmax_qk, p=dropout_p)\n        output = dropout_qk.matmul(value)\n        return output\n\n# Initializing the model\nm = Model().eval()\n\n# Inputs to the model\nquery = torch.randn(2, 5, 512, 64)\nkey = torch.randn(2, 5, 64, 64)\nvalue = torch.randn(2, 5, 64, 64)\ninv_scale_factor = torch.randn(2, 5, 1, 1)\ndropout_p = torch.randn([])\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n \n    def forward(self, query, key, value):\n        qk = query @ key.transpose(-2, -1)\n        inv_scale = 1.0/(len(key) ** 0.5)\n        scaled_qk = qk * inv_scale\n        softmax_qk = scaled_qk.softmax(dim=-1)\n        dropout_qk = torch.nn.functional.dropout(softmax_qk, p=0.3)\n        output = dropout_qk @ value\n        return output\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nquery = torch.randn(1, 16, 64)\nkey   = torch.randn(1, 32, 64)\nvalue = torch.randn(1, 32, 64)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self, query_dim, key_dim, value_dim):\n        super().__init__()\n        self.query_dim = query_dim\n        self.key_dim = key_dim\n        self.value_dim = value_dim\n        self.inv_scale_factor = np.sqrt(key_dim)\n        \n        self.dropout = torch.nn.Dropout(0.1)\n        self.softmax = torch.nn.Softmax(dim=-1)\n        self.matmul_1 = torch.nn.Linear(query_dim, key_dim, bias=False)\n        self.matmul_2 = torch.nn.Linear(key_dim, value_dim, bias=False)\n\n    def forward(self, q, k, v):\n        q = q.unsqueeze(dim=1)\n        q = self.matmul_1(q)\n        k = self.matmul_1(k)\n        qk = torch.matmul(q, k.transpose(-2, -1))\n        qk = qk.div(self.inv_scale_factor)\n        \n        softmax_qk = self.softmax(qk)\n        dropout_qk = self.dropout(softmax_qk)\n        output = dropout_qk.matmul(v)\n        return output\n\n# Initializing the model\nm = Model(64, 64, 64)\n\n# Inputs to the model\n__query__ = torch.randn(1, 5, 64)\n__key__ = torch.randn(5, 15, 64)\n__value__ = torch.randn(5, 15, 64)\n",
                "\nclass ScaledDotProductAttention(nn.Module):\n    def forward(self, query, key, value, mask=None, dropout=None):\n        scale_factor = query.size(-1) ** 0.5 # Use this formula to compute the inverse scale factor\n\n        # To calculate the scaled dot product, uncomment the following line\n        # qk = torch.matmul(query, key.transpose(-2, -1))\n\n        qk = query * key.sum(dim=1, keepdim=True)\n        if scale_factor!= 1:\n            # Use this formula to scale the dot product\n            # scaled_qk = qk.div(scale_factor)\n\n            scaled_qk = qk / scale_factor\n        \n        # To apply softmax, uncomment the following line\n        # softmax_qk = scaled_qk.softmax(dim=-1)\n\n        softmax_qk = torch.softmax(scaled_qk, dim=-1)\n        if dropout is not None:\n            # To apply dropout, uncomment the following line\n            # dropout_qk = torch.nn.functional.dropout(softmax_qk, p=dropout)\n\n            dropout_qk = dropout(softmax_qk)\n        else:\n            dropout_qk = softmax_qk\n        output = dropout_qk * value\n        return output\n\n\nclass Model(torch.nn.Module):\n    def __init__(self, dropout=0.5, dim=10):\n        super().__init__()\n        self.dropout = dropout\n        self.dim = dim\n        self.attentions = list(ScaledDotProductAttention() for _ in range(dim))\n        self.convs = list(nn.Conv3d() for _ in range(dim))\n        self.norms = list(nn.LayerNorm() for _ in range(dim))\n\n    def forward(self, x):\n        v = x\n        for norm, conv, attention in zip(self.norms, self.convs, self.attentions):\n            v = norm(v)\n            v = conv(v)\n            v = attention(v)\n        return v\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx = torch.randn(1, 10, 32, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, query, key, value, dropout_p):\n        super().__init__()\n        self.dropout_p = dropout_p\n \n    def forward(self, query, key, value):\n        qk = torch.matmul(query, key.transpose(-2, -1))\n        scaled_qk = qk.div(inv_scale_factor)\n        softmax_qk = scaled_qk.softmax(dim=-1)\n        dropout_qk = torch.nn.functional.dropout(softmax_qk, p=dropout_p)\n        output = dropout_qk.matmul(value)\n\n# Initializing the model\nm = Model(query, key, value)\n\n# Inputs to the model\nquery = torch.randn(1, 8, 64, 64)\nkey = torch.randn(1, 32, 8, 64)\nvalue = torch.randn(1, 8, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n \n    def forward(self, query, key, value, inv_scale_factor, dropout_p):\n        kq = torch.matmul(query, key.transpose(-2, -1))\n        drop_kq = torch.nn.functional.dropout(kq, p=dropout_p)\n        scaled_drop_kq = drop_kq.div(inv_scale_factor)\n        output = scaled_drop_kq.matmul(value)\n        return output\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nquery = torch.randn(1, 8, 3136)\nkey = torch.randn(1, 16, 3136)\nvalue = torch.randn(1, 16, 3136)\ninv_scale_factor = torch.tensor(1.0, dtype=torch.float)\ndropout_p = 0.2\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, batch, embedding, num_head):\n        super().__init__()\n        self.num_head = num_head\n        self.batch = batch\n        self.embedding = embedding\n        self.q_linear = torch.nn.Linear(self.embedding, self.embedding)\n        self.k_linear = torch.nn.Linear(self.embedding, self.embedding)\n        self.v_linear = torch.nn.Linear(self.embedding, self.embedding)\n \n    def forward(self, q, k, v, pos, training=True):\n        r_qk = self.mha(q, k, v, pos)\n        r_qk = r_qk.transpose(-2, -1)\n        a = r_qk / math.sqrt(self.embedding / self.num_head)\n        a = a.softmax(dim=-1)\n        #a_dropout = torch.nn.functional.dropout(a)\n        r = torch.matmul(a, v)\n        return r\n \nclass mha(torch.nn.Module):   \n    def __init__(self):\n        super().__init__()\n        self.transpose = torch.nn.functional.linear(A=q, B=k, bias=None)\n    def forward(self, q, k, v, pos):\n        q_key = torch.matmul(q, k.transpose(-2, -1))\n        q_key = q_key.div(math.sqrt(self.embedding / self.num_head))\n        q_key = q_key.softmax(dim=-1)\n        q_key_dropout = torch.nn.functional.dropout(q_key)\n        output = torch.matmul(q_key_dropout, v)\n        return output\n\n# Initializing the model\nm = Model()\n\n# Query, Key and Value\n__batch__ = 4\n__embedding__ = 128\n__num_head__ = 2\n\nq = torch.randn(self.batch*self.num_head, __num_head__, __embedding__)\nk = torch.randn(self.q_len*self.num_head, __num_head__, __embedding__)\nv = torch.randn(self.q_len*self.num_head, __num_head__, __embedding__)\n\n# Pos\n__max_len__ = 41\n__pos_len__ = 41\nself.pos = torch.tensor([[pos_i-pos_i%self.num_head for pos_i in pos_j] for pos_j in pos])\nself.pos = self.pos.view(-1).to(m.device)\npos_encoding_tensor = torch.zeros(self.len, self.depth)\npos = torch.arange(self.len).unsqueeze(1).expand(self.len, self.depth).to(m.device)\n__pos_encoding__ = torch.tensor([pos_i % self.depth for pos_i in pos])\npos_encoding_tensor[...] = torch.nn.functional.embedding(input=__pos_encoding__, weight=self.position_encoding_table)\npos_encoding_tensor[::self.num_head,...] = torch.nn.functional.embedding(input=__pos_encoding__, weight=self.position_encoding_table)\n__pos_encoding__ = pos_encoding_tensor[range(self.len)]\n\n# Inputs to the model\n__query__ = q\n__key__ = k\n__value__ = v\n__pos__ = __pos_encoding__\n__training__ = True\nr = m(query=__query__, key=__key__, value=__value__, pos=__pos__, training=__training__)\n\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, dropout_p, softmax_input_dim, softmax_dim):\n        super().__init__()\n        self.linear_qk = torch.nn.Linear(softmax_input_dim, softmax_input_dim)\n        self.dropout_qk = torch.nn.Dropout(p=dropout_p)\n        self.linear_output = torch.nn.Linear(softmax_input_dim, softmax_input_dim)\n\n    def forward(self, query, key, value):\n        qk = self.linear_qk(query) + self.linear_qk(key).transpose(-2, -1)\n        qk = qk.div(math.sqrt(query.shape[-1]))\n        dropout_qk = self.dropout_qk(torch.nn.functional.softmax(qk, dim=-1))\n        output = self.linear_output(dropout_qk.matmul(value))\n        return output\n\n# Initializing the model\nm = Model(0, softmax_input_dim, softmax_dim)\n\n# Inputs to the model\nx1 = torch.randn(1, 30, softmax_input_dim)\nx2 = torch.randn(1, 40, softmax_input_dim)\nx3 = torch.randn(1, 40, softmax_input_dim)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n \n    def forward(self, query, key, value, inv_scale_factor, dropout_p, dropout_i):\n        v1 = torch.matmul(query, key.transpose(-2, -1))\n        v2 = v1 / inv_scale_factor\n        s_qk = torch.nn.functional.softmax(v2, dim=-1)\n        v3 = torch.nn.functional.dropout(s_qk, p=p, iid=d)\n        o = i.matmul(v3)\n        return o\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nquery = torch.randn(3, 512, 50)\nkey = torch.randn(3, 512, 18)\nvalue = torch.randn(3, 512, 18)\ninv_scale_factor = 1.0 / 0.5\ndropout_p = 0.1\ndropout_i = True\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.dropout_p = 0.2\n \n    def forward(self, query, key, value):\n        qk = torch.matmul(query, key.transpose(-2, -1))\n        inv_scale_factor = torch.full_like(qk[:, :, 0], 1e4)\n        scaled_qk = qk.div(inv_scale_factor)\n        softmax_qk = scaled_qk.softmax(dim=-1)\n        dropout_qk = torch.nn.functional.dropout(softmax_qk, p=self.dropout_p)\n        output = dropout_qk.matmul(value)\n        return output\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nquery = torch.randn(1, 8, 5)\nkey = torch.randn(1, 8, 10)\nvalue = torch.randn(1, 8, 10)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n \n    def forward(self, query, key, value, inv_scale_factor, dropout_p):\n        qk = torch.matmul(query, key.transpose(-2, -1))\n        scaled_qk = qk.div(inv_scale_factor)\n        softmax_qk = scaled_qk.softmax(dim=-1)\n        dropout_qk = torch.nn.functional.dropout(softmax_qk, p=dropout_p)\n        output = dropout_qk.matmul(value)\n        return output\n\n# Initializing the model\nm = Model().eval()\n\n# Inputs to the model\nquery = torch.randn(2, 5, 512, 64)\nkey = torch.randn(2, 5, 64, 64)\nvalue = torch.randn(2, 5, 64, 64)\ninv_scale_factor = torch.randn(2, 5, 1, 1)\ndropout_p = torch.randn([])\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n \n    def forward(self, query, key, value):\n        qk = query @ key.transpose(-2, -1)\n        inv_scale = 1.0/(len(key) ** 0.5)\n        scaled_qk = qk * inv_scale\n        softmax_qk = scaled_qk.softmax(dim=-1)\n        dropout_qk = torch.nn.functional.dropout(softmax_qk, p=0.3)\n        output = dropout_qk @ value\n        return output\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nquery = torch.randn(1, 16, 64)\nkey   = torch.randn(1, 32, 64)\nvalue = torch.randn(1, 32, 64)\n"
            ],
            "g_time": 28.802151918411255
        }
    }
}
{
    "opt": "unary=0",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def forward(self, x15):\n        v1 = x15 * 0.5\n        v2 = x15 * x15\n        v3 = v2 * x15\n        v4 = v3 * 0.044715\n        v5 = x15 + v4\n        v6 = v5 * 0.7978845608028654\n        v7 = torch.tanh(v6)\n        v8 = v7 + 1\n        v9 = v1 * v8\n        return v9\n# Inputs to the model\nx15 = torch.randn(1, 4, 128, 28)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(130, 128, 1, stride=1, padding=0)\n    def forward(self, x7):\n        v1 = self.conv(x7)\n        v2 = v1 * 0.5\n        v3 = v1 * v1\n        v4 = v3 * v1\n        v5 = v4 * 0.044715\n        v6 = v1 + v5\n        v7 = v6 * 0.7978845608028654\n        v8 = torch.tanh(v7)\n        v9 = v8 + 1\n        v10 = v2 * v9\n        return v10\n# Inputs to the model\nx7 = torch.randn(1, 130, 72, 128)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(13, 2, 3, stride=1, padding=2, dilation=2)\n    def forward(self, x):\n        v1 = self.conv(x)\n        v2 = v1 * 0.5\n        v3 = v1 * v1\n        v4 = v3 * v1\n        v5 = v4 * 0.044715\n        v6 = v1 + v5\n        v7 = v6 * 0.7978845608028654\n        v8 = torch.tanh(v7)\n        v9 = v8 + 1\n        v10 = v2 * v9\n        return v10\n# Inputs to the model\nx = torch.randn(3, 13, 147, 165)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(4, 4, 1, stride=1, padding=0, dilation=2)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * v1\n        v4 = v3 * v1\n        v5 = v4 * 0.044715\n        v6 = v1 + v5\n        v7 = v6 * 0.7978845608028654\n        v8 = torch.tanh(v7)\n        v9 = v8 + 1\n        v10 = v2 * v9\n        return v10\n# Inputs to the model\nx1 = torch.randn(3, 4, 128, 28)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(18, 7, 1, stride=2, padding=0)\n        self.conv1 = torch.nn.Conv2d(18, 7, 3, stride=1, padding=3)\n    def forward(self, x6):\n        v1 = self.conv(x6)\n        v2 = self.conv1(v1)\n        v3 = v2 * 0.5\n        v4 = v2 * v2\n        v5 = v4 * v2\n        v6 = v5 * 0.044715\n        v7 = v2 + v6\n        v8 = v7 * 0.7978845608028654\n        v9 = torch.tanh(v8)\n        v10 = v9 + 1\n        v11 = v3 * v10\n        return v11\n# Inputs to the model\nx6 = torch.randn(3, 18, 32, 18)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(1, 16, 7, stride=1, padding=3, dilation=1)\n        self.conv2 = torch.nn.Conv2d(16, 8, 6, stride=2, padding=2, dilation=1)\n    def forward(self, x7):\n        v1 = self.conv1(x7)\n        v2 = self.conv2(v1)\n        v3 = v2 * 0.5\n        v4 = v2 * v2\n        v5 = v4 * v2\n        v6 = v5 * 0.044715\n        v7 = v2 + v6\n        v8 = v7 * 0.7978845608028654\n        v9 = torch.tanh(v8)\n        v10 = v9 + 1\n        v11 = v3 * v10\n        return v11\n# Inputs to the model\nx7 = torch.randn(1, 1, 161, 231)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(33, 68, 33, stride=2, padding=1)\n        self.conv1 = torch.nn.Conv2d(68, 31, 1, stride=1, padding=0)\n    def forward(self, x16):\n        v1 = self.conv(x16)\n        v2 = self.conv1(v1)\n        v3 = v2 * 0.5\n        v4 = v2 * v2\n        v5 = v4 * v2\n        v6 = v5 * 0.044715\n        v7 = v2 + v6\n        v8 = v7 * 0.7978845608028654\n        v9 = torch.tanh(v8)\n        v10 = v9 + 1\n        v11 = v3 * v10\n        return v11\n# Inputs to the model\nx16 = torch.randn(1, 33, 133, 34)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(1, 2, 3, stride=1, padding=0, dilation=2)\n    def forward(self, x16):\n        v1 = self.conv(x16)\n        v2 = v1 * 0.5\n        v3 = v1 * v1\n        v4 = v3 * v1\n        v5 = v4 * 0.044715\n        v6 = v1 + v5\n        v7 = v6 * 0.7978845608028654\n        v8 = torch.tanh(v7)\n        v9 = v8 + 1\n        v10 = v2 * v9\n        return v10\n# Inputs to the model\nx16 = torch.randn(3, 1, 208, 203)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(6, 14, 3, stride=1, padding=1)\n        self.conv1 = torch.nn.Conv2d(14, 4, 5, stride=1, padding=0, dilation=2)\n    def forward(self, x3):\n        v1 = self.conv(x3)\n        v2 = self.conv1(v1)\n        v3 = v2 * 0.5\n        v4 = v2 * v2\n        v5 = v4 * v2\n        v6 = v5 * 0.044715\n        v7 = v2 + v6\n        v8 = v7 * 0.7978845608028654\n        v9 = torch.tanh(v8)\n        v10 = v9 + 1\n        v11 = v3 * v10\n        return v11\n# Inputs to the model\nx3 = torch.randn(1, 6, 24, 24)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv1d(4, 5, kernel_size=3, stride=2, padding=2, dilation=2, transposed=True)\n        self.conv1 = torch.nn.Conv1d(5, 3, kernel_size=1, stride=1, padding=0, dilation=2, transposed=False)\n    def forward(self, x5):\n        v1 = self.conv(x5)\n        v2 = v1 * 0.5\n        v3 = v1 * v1\n        v4 = v3 * v1\n        v5 = v4 * 0.044715\n        v6 = v1 + v5\n        v7 = v6 * 0.7978845608028654\n        v8 = torch.tanh(v7)\n        v9 = v8 + 1\n        v10 = v2 * v9\n        v11 = self.conv1(v10)\n        return v11\n# Inputs to the model\nx5 = torch.randn(5, 4, 67)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def forward(self, x15):\n        v1 = x15 * 0.5\n        v2 = x15 * x15\n        v3 = v2 * x15\n        v4 = v3 * 0.044715\n        v5 = x15 + v4\n        v6 = v5 * 0.7978845608028654\n        v7 = torch.tanh(v6)\n        v8 = v7 + 1\n        v9 = v1 * v8\n        return v9\n# Inputs to the model\nx15 = torch.randn(1, 4, 128, 28)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(130, 128, 1, stride=1, padding=0)\n    def forward(self, x7):\n        v1 = self.conv(x7)\n        v2 = v1 * 0.5\n        v3 = v1 * v1\n        v4 = v3 * v1\n        v5 = v4 * 0.044715\n        v6 = v1 + v5\n        v7 = v6 * 0.7978845608028654\n        v8 = torch.tanh(v7)\n        v9 = v8 + 1\n        v10 = v2 * v9\n        return v10\n# Inputs to the model\nx7 = torch.randn(1, 130, 72, 128)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(13, 2, 3, stride=1, padding=2, dilation=2)\n    def forward(self, x):\n        v1 = self.conv(x)\n        v2 = v1 * 0.5\n        v3 = v1 * v1\n        v4 = v3 * v1\n        v5 = v4 * 0.044715\n        v6 = v1 + v5\n        v7 = v6 * 0.7978845608028654\n        v8 = torch.tanh(v7)\n        v9 = v8 + 1\n        v10 = v2 * v9\n        return v10\n# Inputs to the model\nx = torch.randn(3, 13, 147, 165)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(4, 4, 1, stride=1, padding=0, dilation=2)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * v1\n        v4 = v3 * v1\n        v5 = v4 * 0.044715\n        v6 = v1 + v5\n        v7 = v6 * 0.7978845608028654\n        v8 = torch.tanh(v7)\n        v9 = v8 + 1\n        v10 = v2 * v9\n        return v10\n# Inputs to the model\nx1 = torch.randn(3, 4, 128, 28)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(18, 7, 1, stride=2, padding=0)\n        self.conv1 = torch.nn.Conv2d(18, 7, 3, stride=1, padding=3)\n    def forward(self, x6):\n        v1 = self.conv(x6)\n        v2 = self.conv1(v1)\n        v3 = v2 * 0.5\n        v4 = v2 * v2\n        v5 = v4 * v2\n        v6 = v5 * 0.044715\n        v7 = v2 + v6\n        v8 = v7 * 0.7978845608028654\n        v9 = torch.tanh(v8)\n        v10 = v9 + 1\n        v11 = v3 * v10\n        return v11\n# Inputs to the model\nx6 = torch.randn(3, 18, 32, 18)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(1, 16, 7, stride=1, padding=3, dilation=1)\n        self.conv2 = torch.nn.Conv2d(16, 8, 6, stride=2, padding=2, dilation=1)\n    def forward(self, x7):\n        v1 = self.conv1(x7)\n        v2 = self.conv2(v1)\n        v3 = v2 * 0.5\n        v4 = v2 * v2\n        v5 = v4 * v2\n        v6 = v5 * 0.044715\n        v7 = v2 + v6\n        v8 = v7 * 0.7978845608028654\n        v9 = torch.tanh(v8)\n        v10 = v9 + 1\n        v11 = v3 * v10\n        return v11\n# Inputs to the model\nx7 = torch.randn(1, 1, 161, 231)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(33, 68, 33, stride=2, padding=1)\n        self.conv1 = torch.nn.Conv2d(68, 31, 1, stride=1, padding=0)\n    def forward(self, x16):\n        v1 = self.conv(x16)\n        v2 = self.conv1(v1)\n        v3 = v2 * 0.5\n        v4 = v2 * v2\n        v5 = v4 * v2\n        v6 = v5 * 0.044715\n        v7 = v2 + v6\n        v8 = v7 * 0.7978845608028654\n        v9 = torch.tanh(v8)\n        v10 = v9 + 1\n        v11 = v3 * v10\n        return v11\n# Inputs to the model\nx16 = torch.randn(1, 33, 133, 34)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(1, 2, 3, stride=1, padding=0, dilation=2)\n    def forward(self, x16):\n        v1 = self.conv(x16)\n        v2 = v1 * 0.5\n        v3 = v1 * v1\n        v4 = v3 * v1\n        v5 = v4 * 0.044715\n        v6 = v1 + v5\n        v7 = v6 * 0.7978845608028654\n        v8 = torch.tanh(v7)\n        v9 = v8 + 1\n        v10 = v2 * v9\n        return v10\n# Inputs to the model\nx16 = torch.randn(3, 1, 208, 203)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(6, 14, 3, stride=1, padding=1)\n        self.conv1 = torch.nn.Conv2d(14, 4, 5, stride=1, padding=0, dilation=2)\n    def forward(self, x3):\n        v1 = self.conv(x3)\n        v2 = self.conv1(v1)\n        v3 = v2 * 0.5\n        v4 = v2 * v2\n        v5 = v4 * v2\n        v6 = v5 * 0.044715\n        v7 = v2 + v6\n        v8 = v7 * 0.7978845608028654\n        v9 = torch.tanh(v8)\n        v10 = v9 + 1\n        v11 = v3 * v10\n        return v11\n# Inputs to the model\nx3 = torch.randn(1, 6, 24, 24)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv1d(4, 5, kernel_size=3, stride=2, padding=2, dilation=2, transposed=True)\n        self.conv1 = torch.nn.Conv1d(5, 3, kernel_size=1, stride=1, padding=0, dilation=2, transposed=False)\n    def forward(self, x5):\n        v1 = self.conv(x5)\n        v2 = v1 * 0.5\n        v3 = v1 * v1\n        v4 = v3 * v1\n        v5 = v4 * 0.044715\n        v6 = v1 + v5\n        v7 = v6 * 0.7978845608028654\n        v8 = torch.tanh(v7)\n        v9 = v8 + 1\n        v10 = v2 * v9\n        v11 = self.conv1(v10)\n        return v11\n# Inputs to the model\nx5 = torch.randn(5, 4, 67)\n"
            ],
            "g_time": 12.107379674911499
        }
    }
}
{
    "opt": "binary=6",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(10, 20)\n \n    def forward(self, x1):\n        v2 = self.linear(x1)\n        v1 = v2 - 5\n        return v1\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 10)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, p1):\n        super().__init__()\n        self.lin = torch.nn.Linear(6, 3)\n        self.p1 = p1\n\n    def forward(self, x1):\n        v1 = self.lin(x1)\n        v2 = v1 - self.p1\n        return v2\n\n# Initializing the model\nm = Model(torch.empty(3))\n\n# Inputs to the model\nx1 = torch.randn(8, 6)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(8, 4)\n \n    def forward(self, other, x1):\n        v1 = self.linear(x1)\n        v2 = v1 - other\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 8)\nother = torch.randn(1)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(16, 4)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 - other\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(2, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(8, 8)\n \n    def forward(self, x1, x2):\n        v1 = self.linear(x1)\n        v2 = v1 - x2\n        v3 = v1 / x2\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 8)\nx2 = torch.randn(1, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(5, 3)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 - other\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 5)\nx2 = torch.randn(64, 64, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(10, 20)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 - 1\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 10)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 - 1\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(512, 256)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 - other\n        return v2\n\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(8, 3)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 - other\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 8)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(10, 20)\n \n    def forward(self, x1):\n        v2 = self.linear(x1)\n        v1 = v2 - 5\n        return v1\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 10)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, p1):\n        super().__init__()\n        self.lin = torch.nn.Linear(6, 3)\n        self.p1 = p1\n\n    def forward(self, x1):\n        v1 = self.lin(x1)\n        v2 = v1 - self.p1\n        return v2\n\n# Initializing the model\nm = Model(torch.empty(3))\n\n# Inputs to the model\nx1 = torch.randn(8, 6)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(8, 4)\n \n    def forward(self, other, x1):\n        v1 = self.linear(x1)\n        v2 = v1 - other\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 8)\nother = torch.randn(1)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(16, 4)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 - other\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(2, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(8, 8)\n \n    def forward(self, x1, x2):\n        v1 = self.linear(x1)\n        v2 = v1 - x2\n        v3 = v1 / x2\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 8)\nx2 = torch.randn(1, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(5, 3)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 - other\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 5)\nx2 = torch.randn(64, 64, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(10, 20)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 - 1\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 10)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 - 1\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(512, 256)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 - other\n        return v2\n\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(8, 3)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 - other\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 8)\n"
            ],
            "g_time": 5.1634907722473145
        }
    }
}
{
    "opt": "unary=9",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(1, 1, 3, stride=3)\n    def forward(self, x1):\n        v1 = 3 * self.conv(x1)\n        v2 = 7 * v1\n        v3 = v2.clamp(0, 6)\n        v4 = v3 / 6\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 1, 224, 224)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Linear(64, 8)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 + 3\n        v3 = v2.clamp(0, 6)\n        v4 = v3.div(6)\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 8, (1, 6), stride=1)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1.add(3)\n        v3 = v2.clamp(min=0, max=6)\n        v4 = v3 / 6\n        return v4\n# Inputs to the model\nx1 = torch.randn(5, 3, 64, 64)\n",
                "\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    def __init__(self, w, b):\n        super().__init__()\n        self.conv = nn.Conv2d(3, 8, (1, 6), stride=1, padding=(0, 3), bias=False)\n        self.bn = nn.BatchNorm2d(8, affine=True)\n        self.weights = nn.ParameterList([\n            nn.Parameter(torch.randn(8, 3, 1, 6)),\n            nn.Parameter(torch.randn(8))\n        ])\n        for idx, p in enumerate(self.weights):\n            self.register_parameter('weights'+f'{idx}', p)\n        self.bias = b\n    def forward(self, x1):\n        self.conv.weight = self.weights[0]\n        self.bn.weight = self.weights[1]\n        self.bn.bias = self.bias\n        v2 = x1.flatten(2).permute(2, 0, 1)\n        v1 = self.conv(v2)\n        v2 = v1 + 3\n        v3 = v2.clamp(0, 6)\n        v4 = v3.div(6)\n        return v4.view(-1, 8, 14, 14)\nparams = [\n    # conv weight\n    torch.randn(8, 3, 1, 6),\n    # bn weight\n    torch.randn(8),\n    # bn bias\n    torch.tensor(0.0),\n]\nx1 = torch.randn(4, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 8, (1, 6), stride=1)\n    def forward(self, x1):\n        v1 = self.conv(x1) # Apply pointwise convolution with kernel size 1 to the input tensor\n        v2 = v1.add(3) # Add 3 to the output of the convolution\n        v3 = v2.clamp(min=0, max=6) # Clamp the output of the addition operation to a minimum of 0 and a maximum of 6\n        v4 = v3.div(6) # Divide the output of the previous operation by 6\n        return v4\n# Inputs to the model\nx1 = torch.randn(5, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 8, (1, 6), stride=1)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1.add(3)\n        v3 = v2.clamp(min=0, max=6)\n        v4 = v3 / 6\n        return v4\n# Inputs to the model\nx1 = torch.randn(5, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1, groups=3)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1.add(3)\n        v3 = v2.clamp(min=0, max=6)\n        v4 = v3 / 6\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 6, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 8, 1, stride=2, padding=0)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = 3 + v1\n        v3 = torch.clamp(torch.clamp(v2, min=0), max=6)\n        v4 = v3 / 6\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 8, (1, 6), stride=1)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1.add(3.)\n        v3 = v2.clamp(min=0., max=6.)\n        v4 = v3 / 6.\n        return v4\n# Inputs to the model\nx1 = torch.randn(5, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 8, (1, 6), stride=1)\n    def forward(self, x1):\n        v1 = 3 + self.conv(x1)\n        v2 = torch.clamp(v1, min=0, max=6)\n        v3 = v2.div(6)\n        return v3\n# Inputs to the model\nx1 = torch.randn(5, 3, 64, 64)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(1, 1, 3, stride=3)\n    def forward(self, x1):\n        v1 = 3 * self.conv(x1)\n        v2 = 7 * v1\n        v3 = v2.clamp(0, 6)\n        v4 = v3 / 6\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 1, 224, 224)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Linear(64, 8)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 + 3\n        v3 = v2.clamp(0, 6)\n        v4 = v3.div(6)\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 8, (1, 6), stride=1)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1.add(3)\n        v3 = v2.clamp(min=0, max=6)\n        v4 = v3 / 6\n        return v4\n# Inputs to the model\nx1 = torch.randn(5, 3, 64, 64)\n",
                "\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    def __init__(self, w, b):\n        super().__init__()\n        self.conv = nn.Conv2d(3, 8, (1, 6), stride=1, padding=(0, 3), bias=False)\n        self.bn = nn.BatchNorm2d(8, affine=True)\n        self.weights = nn.ParameterList([\n            nn.Parameter(torch.randn(8, 3, 1, 6)),\n            nn.Parameter(torch.randn(8))\n        ])\n        for idx, p in enumerate(self.weights):\n            self.register_parameter('weights'+f'{idx}', p)\n        self.bias = b\n    def forward(self, x1):\n        self.conv.weight = self.weights[0]\n        self.bn.weight = self.weights[1]\n        self.bn.bias = self.bias\n        v2 = x1.flatten(2).permute(2, 0, 1)\n        v1 = self.conv(v2)\n        v2 = v1 + 3\n        v3 = v2.clamp(0, 6)\n        v4 = v3.div(6)\n        return v4.view(-1, 8, 14, 14)\nparams = [\n    # conv weight\n    torch.randn(8, 3, 1, 6),\n    # bn weight\n    torch.randn(8),\n    # bn bias\n    torch.tensor(0.0),\n]\nx1 = torch.randn(4, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 8, (1, 6), stride=1)\n    def forward(self, x1):\n        v1 = self.conv(x1) # Apply pointwise convolution with kernel size 1 to the input tensor\n        v2 = v1.add(3) # Add 3 to the output of the convolution\n        v3 = v2.clamp(min=0, max=6) # Clamp the output of the addition operation to a minimum of 0 and a maximum of 6\n        v4 = v3.div(6) # Divide the output of the previous operation by 6\n        return v4\n# Inputs to the model\nx1 = torch.randn(5, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 8, (1, 6), stride=1)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1.add(3)\n        v3 = v2.clamp(min=0, max=6)\n        v4 = v3 / 6\n        return v4\n# Inputs to the model\nx1 = torch.randn(5, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1, groups=3)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1.add(3)\n        v3 = v2.clamp(min=0, max=6)\n        v4 = v3 / 6\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 6, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 8, 1, stride=2, padding=0)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = 3 + v1\n        v3 = torch.clamp(torch.clamp(v2, min=0), max=6)\n        v4 = v3 / 6\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 8, (1, 6), stride=1)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1.add(3.)\n        v3 = v2.clamp(min=0., max=6.)\n        v4 = v3 / 6.\n        return v4\n# Inputs to the model\nx1 = torch.randn(5, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 8, (1, 6), stride=1)\n    def forward(self, x1):\n        v1 = 3 + self.conv(x1)\n        v2 = torch.clamp(v1, min=0, max=6)\n        v3 = v2.div(6)\n        return v3\n# Inputs to the model\nx1 = torch.randn(5, 3, 64, 64)\n"
            ],
            "g_time": 13.368400573730469
        }
    }
}
{
    "opt": "unary=8",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose1 = torch.nn.ConvTranspose2d(4, 16, 3, stride=1, padding=0, output_padding=0)\n        self.conv_transpose2 = torch.nn.ConvTranspose2d(16, 32, 3, stride=1, padding=0, output_padding=0)\n        self.conv_transpose3 = torch.nn.ConvTranspose2d(32, 32, 3, stride=1, padding=0, output_padding=0)\n    def forward(self, x1):\n        v1 = self.conv_transpose1(x1)\n        v2 = self.conv_transpose2(v1)\n        v3 = self.conv_transpose3(v2)\n        v4 = v3 + 3\n        v5 = torch.clamp(v4, min=0)\n        v6 = torch.clamp(v5, max=6)\n        v7 = v3 * v6\n        v8 = v7 / 6\n        return v8\n# Inputs to the model\nx1 = torch.randn(1, 4, 4, 4)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose1 = torch.nn.ConvTranspose2d(1, 2, 3, stride=1, padding=0, output_padding=0)\n        self.conv_transpose2 = torch.nn.ConvTranspose2d(2, 3, 5, stride=2, padding=0, output_padding=0)\n    def forward(self, x1):\n        v1 = self.conv_transpose1(x1)\n        v1 = self.conv_transpose2(v1)\n        v2 = v1 + 3\n        v3 = torch.clamp(v2, min=0)\n        v4 = torch.clamp(v3, max=6)\n        v5 = v1 * v4\n        v6 = v5 / 6\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 1, 2, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(1, 16, 5, stride=2, padding=2, dilation=1, output_padding=1)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp(v2, min=0)\n        v4 = torch.clamp(v3, max=6)\n        v5 = v1 * v4\n        v6 = v5 / 6\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 1, 10, 10)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose1 = torch.nn.ConvTranspose2d(3, 64, 3, stride=1, padding=0, dilation=1)\n        self.conv_transpose2 = torch.nn.ConvTranspose2d(64, 64, 3, stride=1, padding=0, dilation=1)\n    def forward(self, x1):\n        v1 = self.conv_transpose1(x1)\n        v2 = self.conv_transpose2(v1)\n        v3 = v1 + 3\n        v4 = torch.clamp(v3, min=0)\n        v5 = torch.clamp(v4, max=6)\n        v6 = v1 * v5\n        v7 = v6 / 6\n        return v7\n# Inputs to the model\nx1 = torch.randn(1, 3, 20, 20)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(16, 32, 3, stride=1, padding=1, output_padding=0)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp(v2, min=0)\n        v4 = torch.clamp(v3, max=6)\n        v5 = v1 * v4\n        v6 = v5 / 6\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 16, 5, 5)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(1, 5, kernel_size=(10, 10), stride=2, padding=5, output_padding=3)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp(v2, min=0)\n        v4 = torch.clamp(v3, max=6)\n        v5 = v1 * v4\n        v6 = v5 / 6\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 1, 200, 300)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.identity = torch.nn.Identity()\n    def forward(self, x1):\n        v1 = x1\n        v1 = self.identity(v1)\n        return v1\n# Inputs to the model\nx1 = torch.randn(1, 2, 3, 4)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose1 = torch.nn.ConvTranspose2d(1, 1, 7, stride=1, padding=0, output_padding=0)\n    def forward(self, x1):\n        v1 = self.conv_transpose1(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp(v2, min=0)\n        v4 = torch.clamp(v3, max=6)\n        v5 = v1 * v4\n        v6 = v5 / 6\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 1, 28, 28)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(1, 4, 3, stride=1, padding=0, output_padding=0)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 + 3\n        v3 = torch.sigmoid(v2)\n        v4 = torch.clamp(v3, min=0)\n        v5 = v1 * v4\n        v6 = v1 / 6\n        return v6\n# Inputs to the model\nx1 = torch.randn(8, 1, 2, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose1 = torch.nn.ConvTranspose2d(32, 32, 3, stride=2, padding=1, output_padding=0)\n        self.conv_transpose2 = torch.nn.ConvTranspose2d(32, 32, 3, stride=1, padding=1, output_padding=0)\n    def forward(self, x1):\n        v1 = self.conv_transpose1(x1)\n        v1 = self.conv_transpose2(v1)\n        v2 = v1 + 3\n        v3 = torch.clamp(v2, min=0)\n        v4 = torch.clamp(v3, max=6)\n        v5 = v1 * v4\n        v6 = v5 / 6\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 32, 24, 24)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose1 = torch.nn.ConvTranspose2d(4, 16, 3, stride=1, padding=0, output_padding=0)\n        self.conv_transpose2 = torch.nn.ConvTranspose2d(16, 32, 3, stride=1, padding=0, output_padding=0)\n        self.conv_transpose3 = torch.nn.ConvTranspose2d(32, 32, 3, stride=1, padding=0, output_padding=0)\n    def forward(self, x1):\n        v1 = self.conv_transpose1(x1)\n        v2 = self.conv_transpose2(v1)\n        v3 = self.conv_transpose3(v2)\n        v4 = v3 + 3\n        v5 = torch.clamp(v4, min=0)\n        v6 = torch.clamp(v5, max=6)\n        v7 = v3 * v6\n        v8 = v7 / 6\n        return v8\n# Inputs to the model\nx1 = torch.randn(1, 4, 4, 4)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose1 = torch.nn.ConvTranspose2d(1, 2, 3, stride=1, padding=0, output_padding=0)\n        self.conv_transpose2 = torch.nn.ConvTranspose2d(2, 3, 5, stride=2, padding=0, output_padding=0)\n    def forward(self, x1):\n        v1 = self.conv_transpose1(x1)\n        v1 = self.conv_transpose2(v1)\n        v2 = v1 + 3\n        v3 = torch.clamp(v2, min=0)\n        v4 = torch.clamp(v3, max=6)\n        v5 = v1 * v4\n        v6 = v5 / 6\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 1, 2, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(1, 16, 5, stride=2, padding=2, dilation=1, output_padding=1)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp(v2, min=0)\n        v4 = torch.clamp(v3, max=6)\n        v5 = v1 * v4\n        v6 = v5 / 6\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 1, 10, 10)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose1 = torch.nn.ConvTranspose2d(3, 64, 3, stride=1, padding=0, dilation=1)\n        self.conv_transpose2 = torch.nn.ConvTranspose2d(64, 64, 3, stride=1, padding=0, dilation=1)\n    def forward(self, x1):\n        v1 = self.conv_transpose1(x1)\n        v2 = self.conv_transpose2(v1)\n        v3 = v1 + 3\n        v4 = torch.clamp(v3, min=0)\n        v5 = torch.clamp(v4, max=6)\n        v6 = v1 * v5\n        v7 = v6 / 6\n        return v7\n# Inputs to the model\nx1 = torch.randn(1, 3, 20, 20)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(16, 32, 3, stride=1, padding=1, output_padding=0)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp(v2, min=0)\n        v4 = torch.clamp(v3, max=6)\n        v5 = v1 * v4\n        v6 = v5 / 6\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 16, 5, 5)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(1, 5, kernel_size=(10, 10), stride=2, padding=5, output_padding=3)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp(v2, min=0)\n        v4 = torch.clamp(v3, max=6)\n        v5 = v1 * v4\n        v6 = v5 / 6\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 1, 200, 300)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.identity = torch.nn.Identity()\n    def forward(self, x1):\n        v1 = x1\n        v1 = self.identity(v1)\n        return v1\n# Inputs to the model\nx1 = torch.randn(1, 2, 3, 4)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose1 = torch.nn.ConvTranspose2d(1, 1, 7, stride=1, padding=0, output_padding=0)\n    def forward(self, x1):\n        v1 = self.conv_transpose1(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp(v2, min=0)\n        v4 = torch.clamp(v3, max=6)\n        v5 = v1 * v4\n        v6 = v5 / 6\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 1, 28, 28)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(1, 4, 3, stride=1, padding=0, output_padding=0)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 + 3\n        v3 = torch.sigmoid(v2)\n        v4 = torch.clamp(v3, min=0)\n        v5 = v1 * v4\n        v6 = v1 / 6\n        return v6\n# Inputs to the model\nx1 = torch.randn(8, 1, 2, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose1 = torch.nn.ConvTranspose2d(32, 32, 3, stride=2, padding=1, output_padding=0)\n        self.conv_transpose2 = torch.nn.ConvTranspose2d(32, 32, 3, stride=1, padding=1, output_padding=0)\n    def forward(self, x1):\n        v1 = self.conv_transpose1(x1)\n        v1 = self.conv_transpose2(v1)\n        v2 = v1 + 3\n        v3 = torch.clamp(v2, min=0)\n        v4 = torch.clamp(v3, max=6)\n        v5 = v1 * v4\n        v6 = v5 / 6\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 32, 24, 24)\n"
            ],
            "g_time": 11.7236909866333
        }
    }
}
{
    "opt": "unary=7",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(256, 96)\n \n    def forward(self, x1):\n        l1 = self.linear(x1)\n        l2 = l1 * (l1.clamp(min=0, max=6) + 3)\n        l3 = l2 / 6\n        return l3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 256)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(16, 24)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * torch.clamp(v1 + 3, min=0, max=6)\n        v3 = v2 / 6\n        return v3\n\n# Initializing the model\nm = Model()\n \n# Inputs to the model\nx1 = torch.randn(1, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(16 * 5 * 5, 120)\n \n    def forward(self, x1):\n        c1 = x1.reshape((-1, 16 * 5 * 5)).to(device=device)\n        v1 = self.linear(c1)\n        v2 = torch.clamp(torch.add(v1, 3), min=0, max=6) * 6\n        return v2\n\n# Initializing the model\nx = torch.randn(1, 16 * 5 * 5)\nm = Model()\n\n# Inputs to the model\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n \n    def forward(self, x1):\n        l1 = self.linear(x1)\n        l2 = l1 * torch.clamp(l1 + 3, min=0, max=6)\n        l3 = l2 / 6\n        return l3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(8, 4)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * torch.clamp(torch.min(v1) + 3, min=0, max=6)\n        v3 = v2 / 6\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(8, 64)\n \n    def forward(self, v1):\n        l1 = self.linear(v1)\n        l2 = l1 * torch.clamp(l1 + 3, min=0, max=6)\n        l3 = l2 / 6\n        return l3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nv1 = torch.randn(8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(6, 10, bias=False)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * torch.clamp(v1 + 3, 0, 6)\n        v3 = v2 / 6\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(128, 6)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * (torch.clamp(v1 + 3, min=0, max=6) - 3)\n        v3 = v2 / 6\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(10, 1000)\n \n    def forward(self, x1):\n        y1 = self.linear(x1)\n        y2 = y1 * torch.clamp(min=0, max=6, y1 + 3)\n        y3 = y2 / 6\n        return y3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 10)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(64, 64)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * torch.clamp(v1 + 3, min=0, max=6)\n        v3 = v2 / 6\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 64)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(256, 96)\n \n    def forward(self, x1):\n        l1 = self.linear(x1)\n        l2 = l1 * (l1.clamp(min=0, max=6) + 3)\n        l3 = l2 / 6\n        return l3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 256)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(16, 24)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * torch.clamp(v1 + 3, min=0, max=6)\n        v3 = v2 / 6\n        return v3\n\n# Initializing the model\nm = Model()\n \n# Inputs to the model\nx1 = torch.randn(1, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(16 * 5 * 5, 120)\n \n    def forward(self, x1):\n        c1 = x1.reshape((-1, 16 * 5 * 5)).to(device=device)\n        v1 = self.linear(c1)\n        v2 = torch.clamp(torch.add(v1, 3), min=0, max=6) * 6\n        return v2\n\n# Initializing the model\nx = torch.randn(1, 16 * 5 * 5)\nm = Model()\n\n# Inputs to the model\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n \n    def forward(self, x1):\n        l1 = self.linear(x1)\n        l2 = l1 * torch.clamp(l1 + 3, min=0, max=6)\n        l3 = l2 / 6\n        return l3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(8, 4)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * torch.clamp(torch.min(v1) + 3, min=0, max=6)\n        v3 = v2 / 6\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(8, 64)\n \n    def forward(self, v1):\n        l1 = self.linear(v1)\n        l2 = l1 * torch.clamp(l1 + 3, min=0, max=6)\n        l3 = l2 / 6\n        return l3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nv1 = torch.randn(8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(6, 10, bias=False)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * torch.clamp(v1 + 3, 0, 6)\n        v3 = v2 / 6\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(128, 6)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * (torch.clamp(v1 + 3, min=0, max=6) - 3)\n        v3 = v2 / 6\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(10, 1000)\n \n    def forward(self, x1):\n        y1 = self.linear(x1)\n        y2 = y1 * torch.clamp(min=0, max=6, y1 + 3)\n        y3 = y2 / 6\n        return y3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 10)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(64, 64)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * torch.clamp(v1 + 3, min=0, max=6)\n        v3 = v2 / 6\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 64)\n"
            ],
            "g_time": 6.623544454574585
        }
    }
}
{
    "opt": "binary_unary=4",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(32, 16)\n \n    def forward(self, x1, x2):\n        v1 = self.linear(x1)\n        v2 = v1 + x2\n        v3 = torch.relu(v2)\n        return v3\n \n# Initializing the model\nm = Model()\n\n# Inputs to the model\nrng = torch.Generator()\nx1 = torch.randn(1, 32, generator=rng)\nx2 = torch.randn(1, 16, generator=rng)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(8, 32)\n \n    def forward(self, x1, other = torch.randn(8, 32)):\n        v1 = self.linear(x1)\n        v2 = v1 + other\n        v3 = torch.nn.functional.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 8, 256, 256)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(64 * 64 * 3, 64)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        return v1\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\n__input_tensor__ = torch.randn(1, 64 * 64 * 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(5, 2)\n \n    def forward(self, x, other):\n        v1 = self.linear(x)\n        v2 = v1 + other\n        v3 = torch.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx = torch.randn(5)\nother = torch.randn(2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(8, 8)\n \n    def forward(self, x1, other):\n        v1 = self.linear(x1)\n        v2 = v1 + other\n        v3 = torch.relu(v2)\n        return v3\n\n# Initializing the model\ndef make_model(n_features, n_labels):\n    return Model()\n\n# Inputs to the model\nx1 = torch.randn(20, 8)\nother = torch.randn(20, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 4)\n \n    def forward(self, x1, other):\n        v1 = self.linear(x1)\n        v2 = v1 + other\n        v3 = F.leaky_relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\nother = torch.randn(1, 4)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, other):\n        super().__init__()\n        self.linear = torch.nn.Linear(10, 100)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 + other\n        v3 = torch.relu(v2)\n        return v3\n\n# Initializing the model\nother = torch.randn(10)\nm = Model(other)\n\n# Inputs to the model\nx1 = torch.randn(10, 10)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(64, 64)\n \n    def forward(self, x1, other=None):\n        v1 = self.linear(x1)\n \n        if other is not None:\n            v1 += other\n\n        v2 = torch.nn.functional.relu(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(128, 64)\nx2 = torch.randn(128, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, other=None):\n        super().__init__()\n        self.linear1 = torch.nn.Linear(3, 8)\n \n    def forward(self, x1):\n        v1 = self.linear1(x1)\n        v2 = v1 + other\n        v3 = torch.relu(v2)\n        return v3\n\n# Initializing the model\nother = torch.randn(8, 8)\nm = Model(other=other)\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 4, 4)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(10, 100)\n \n    def forward(self, x1, *_, other):\n        v1 = self.linear(x1)\n        v2 = v1 + other\n        v3 = torch.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 10)\nx2 = torch.randn(1, 10)\nother = torch.randn(1, 100)\n\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(32, 16)\n \n    def forward(self, x1, x2):\n        v1 = self.linear(x1)\n        v2 = v1 + x2\n        v3 = torch.relu(v2)\n        return v3\n \n# Initializing the model\nm = Model()\n\n# Inputs to the model\nrng = torch.Generator()\nx1 = torch.randn(1, 32, generator=rng)\nx2 = torch.randn(1, 16, generator=rng)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(8, 32)\n \n    def forward(self, x1, other = torch.randn(8, 32)):\n        v1 = self.linear(x1)\n        v2 = v1 + other\n        v3 = torch.nn.functional.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 8, 256, 256)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(64 * 64 * 3, 64)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        return v1\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\n__input_tensor__ = torch.randn(1, 64 * 64 * 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(5, 2)\n \n    def forward(self, x, other):\n        v1 = self.linear(x)\n        v2 = v1 + other\n        v3 = torch.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx = torch.randn(5)\nother = torch.randn(2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(8, 8)\n \n    def forward(self, x1, other):\n        v1 = self.linear(x1)\n        v2 = v1 + other\n        v3 = torch.relu(v2)\n        return v3\n\n# Initializing the model\ndef make_model(n_features, n_labels):\n    return Model()\n\n# Inputs to the model\nx1 = torch.randn(20, 8)\nother = torch.randn(20, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 4)\n \n    def forward(self, x1, other):\n        v1 = self.linear(x1)\n        v2 = v1 + other\n        v3 = F.leaky_relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\nother = torch.randn(1, 4)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, other):\n        super().__init__()\n        self.linear = torch.nn.Linear(10, 100)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 + other\n        v3 = torch.relu(v2)\n        return v3\n\n# Initializing the model\nother = torch.randn(10)\nm = Model(other)\n\n# Inputs to the model\nx1 = torch.randn(10, 10)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(64, 64)\n \n    def forward(self, x1, other=None):\n        v1 = self.linear(x1)\n \n        if other is not None:\n            v1 += other\n\n        v2 = torch.nn.functional.relu(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(128, 64)\nx2 = torch.randn(128, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, other=None):\n        super().__init__()\n        self.linear1 = torch.nn.Linear(3, 8)\n \n    def forward(self, x1):\n        v1 = self.linear1(x1)\n        v2 = v1 + other\n        v3 = torch.relu(v2)\n        return v3\n\n# Initializing the model\nother = torch.randn(8, 8)\nm = Model(other=other)\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 4, 4)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(10, 100)\n \n    def forward(self, x1, *_, other):\n        v1 = self.linear(x1)\n        v2 = v1 + other\n        v3 = torch.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 10)\nx2 = torch.randn(1, 10)\nother = torch.randn(1, 100)\n\n"
            ],
            "g_time": 6.426253080368042
        }
    }
}
{
    "opt": "unary=1",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.lin = torch.nn.Linear(3, 8)\n \n    def forward(self, x2):\n        v1 = self.lin(x2)\n        v2 = v1 * 0.5\n        v3 = v1 + (v1 * v1 * v1) * 0.044715\n        v4 = v3 * 0.7978845608028654\n        v5 = torch.tanh(v4)\n        v6 = v5 + 1\n        v7 = v2 * v6\n        return v7\n\n# Initializing the model\nm = Model()\n  \n# Inputs to the model\nx2 = torch.randn(10, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(64, 512, bias=False)\n \n    def forward(self, x2):\n        v2 = self.linear(x2)\n        v3 = v2 * 0.5\n        v4 = v2 * v2 * v2 * 0.044715\n        v5 = v4 * 0.7978845608028654\n        v6 = torch.tanh(v5)\n        v7 = v6 + 1\n        v8 = v3 * v7\n        return v8\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx2 = torch.randn(2, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 10)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * 0.5\n        v3 = v1 + (v1 * v1 * v1) * 0.044715\n        v4 = v3 * 0.7978845608028654\n        v5 = torch.tanh(v4)\n        v6 = v5 + 1\n        v7 = v2 * v6\n        return v7\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(1, 16)\n \n    def forward(self, x1):\n        q = self.linear(x1)\n        h1 = q * 0.5\n        h2 = (q * q * q) * 0.044715\n        h3 = h2 + h1\n        h4 = h3 * 0.7978845608028654\n        e = torch.tanh(h4)\n        y = e + 1\n        v = q * y\n        return v\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 1)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(128, 256)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * 0.5\n        v3 = v1 + (v1 * v1 * v1) * 0.044715\n        v4 = v3 * 0.7978845608028654\n        v5 = torch.tanh(v4)\n        v6 = v5 + 1\n        v7 = v2 * v6\n        return v7\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 128)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * 0.5\n        v3 = v1 + (v1 * v1 * v1) * 0.044715\n        v4 = v3 * 0.7978845608028654\n        v5 = torch.tanh(v4)\n        v6 = v5 + 1\n        v7 = v2 * v6\n        return v7\n\n\n# Initializing the model and inputs to the model\nm = Model()\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(512, 256)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * 0.5\n        v3 = v1 + (v1 * v1 * v1) * 0.044715\n        v4 = v3 * 0.7978845608028654\n        v5 = torch.tanh(v4)\n        v6 = v5 + 1\n        v7 = v2 * v6\n        return v7\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 512)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.linear = torch.nn.Linear(9, 2)\n\n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v7 = v1 * 0.5\n        v2 = v1 + (v1 * v1 * v1) * 0.044715\n        v3 = v2 * 0.7978845608028654\n        v4 = torch.tanh(v3)\n        v5 = v4 + 1\n        v6 = v7 * v5\n        return v6\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 9)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(32, 5)\n \n    def forward(self, x0):\n        v0 = self.linear(x0)\n        v1 = v0 * 0.5\n        v2 = v0 + (v0 * (v0 * v0)) * 0.044715\n        v3 = v2 * 0.7978845608028654\n        v4 = torch.tanh(v3)\n        v5 = v4 + 1\n        v6 = v1 * v5\n        return v6\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx0 = torch.randn(1, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(10, 16)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * 0.5\n        v3 = v1 + (v1 * v1 * v1) * 0.044715\n        v4 = v3 * 0.7978845608028654\n        v5 = torch.tanh(v4)\n        v6 = v5 + 1\n        v7 = v2 * v6\n        return v7\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 10)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.lin = torch.nn.Linear(3, 8)\n \n    def forward(self, x2):\n        v1 = self.lin(x2)\n        v2 = v1 * 0.5\n        v3 = v1 + (v1 * v1 * v1) * 0.044715\n        v4 = v3 * 0.7978845608028654\n        v5 = torch.tanh(v4)\n        v6 = v5 + 1\n        v7 = v2 * v6\n        return v7\n\n# Initializing the model\nm = Model()\n  \n# Inputs to the model\nx2 = torch.randn(10, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(64, 512, bias=False)\n \n    def forward(self, x2):\n        v2 = self.linear(x2)\n        v3 = v2 * 0.5\n        v4 = v2 * v2 * v2 * 0.044715\n        v5 = v4 * 0.7978845608028654\n        v6 = torch.tanh(v5)\n        v7 = v6 + 1\n        v8 = v3 * v7\n        return v8\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx2 = torch.randn(2, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 10)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * 0.5\n        v3 = v1 + (v1 * v1 * v1) * 0.044715\n        v4 = v3 * 0.7978845608028654\n        v5 = torch.tanh(v4)\n        v6 = v5 + 1\n        v7 = v2 * v6\n        return v7\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(1, 16)\n \n    def forward(self, x1):\n        q = self.linear(x1)\n        h1 = q * 0.5\n        h2 = (q * q * q) * 0.044715\n        h3 = h2 + h1\n        h4 = h3 * 0.7978845608028654\n        e = torch.tanh(h4)\n        y = e + 1\n        v = q * y\n        return v\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 1)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(128, 256)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * 0.5\n        v3 = v1 + (v1 * v1 * v1) * 0.044715\n        v4 = v3 * 0.7978845608028654\n        v5 = torch.tanh(v4)\n        v6 = v5 + 1\n        v7 = v2 * v6\n        return v7\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 128)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * 0.5\n        v3 = v1 + (v1 * v1 * v1) * 0.044715\n        v4 = v3 * 0.7978845608028654\n        v5 = torch.tanh(v4)\n        v6 = v5 + 1\n        v7 = v2 * v6\n        return v7\n\n\n# Initializing the model and inputs to the model\nm = Model()\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(512, 256)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * 0.5\n        v3 = v1 + (v1 * v1 * v1) * 0.044715\n        v4 = v3 * 0.7978845608028654\n        v5 = torch.tanh(v4)\n        v6 = v5 + 1\n        v7 = v2 * v6\n        return v7\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 512)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.linear = torch.nn.Linear(9, 2)\n\n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v7 = v1 * 0.5\n        v2 = v1 + (v1 * v1 * v1) * 0.044715\n        v3 = v2 * 0.7978845608028654\n        v4 = torch.tanh(v3)\n        v5 = v4 + 1\n        v6 = v7 * v5\n        return v6\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 9)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(32, 5)\n \n    def forward(self, x0):\n        v0 = self.linear(x0)\n        v1 = v0 * 0.5\n        v2 = v0 + (v0 * (v0 * v0)) * 0.044715\n        v3 = v2 * 0.7978845608028654\n        v4 = torch.tanh(v3)\n        v5 = v4 + 1\n        v6 = v1 * v5\n        return v6\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx0 = torch.randn(1, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(10, 16)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * 0.5\n        v3 = v1 + (v1 * v1 * v1) * 0.044715\n        v4 = v3 * 0.7978845608028654\n        v5 = torch.tanh(v4)\n        v6 = v5 + 1\n        v7 = v2 * v6\n        return v7\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 10)\n"
            ],
            "g_time": 8.568053960800171
        }
    }
}
{
    "opt": "cat_mm",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x, y):\n        v1 = torch.mm(x, y)\n        v2 = torch.mm(x, y)\n        v3 = torch.mm(x, y)\n        return torch.cat([v1, v2, v3], 0)\n# Inputs to the model\nx = torch.randn(2, 2)\ny = torch.randn(3, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x, y):\n        v1 = torch.mm(x, y)\n        v2 = torch.mm(x, y)\n        v3 = torch.mm(x, y)\n        return torch.cat([1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 5, 6, 7, 8], 0)\n# Inputs to the model\nx = torch.randn(5, 5)\ny = torch.randn(5, 5)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x):\n        t1 = torch.mm(x, x)\n        v1 = torch.mm(x, x)\n        t2 = torch.cat([t1, t1, t1, t1], 1)\n        t3 = torch.mm(x, x)\n        v2 = torch.mm(x, x)\n        return torch.cat([v1, t2, v2, v1], 1)\n# Inputs to the model\nx = torch.randn(2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, x2):\n        v1 = torch.mm(x1, x2)\n        v2 = torch.mm(x1, x2)\n        v3 = torch.mm(x1, x2)\n        v4 = torch.mm(x2, x1)\n        return torch.cat([v1, v2, v3, v4], 1)\n# Inputs to the model\nx = torch.randn(2, 2)\ny = torch.randn(2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x, y):\n        v1 = torch.mm(x, y)\n        v2 = torch.mm(x, y)\n        v3 = torch.mm(x, y)\n        v4 = torch.mm(x, y)\n        return torch.cat([v1, v2, v3, v4], 0)\n# Inputs to the model\nx = torch.randn(1, 2)\ny = torch.randn(2, 1)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x):\n        v1 = torch.mm(x, x)\n        v2 = torch.mm(x, x)\n        return torch.cat([v1, v2, v1, v2], 0)\n# Inputs to the model\nx = torch.randn(3, 4)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x):\n        v1 = torch.cat([x, x, x], 0)\n        v2 = torch.cat([x, x, x], 1)\n        return v1 + v2\n# Inputs to the model\nx = torch.randn(10, 4)\n",
                "\nclass Model(torch.nn.Module):\n    def forward(self, x, y, z):\n        return torch.cat([torch.mm(x, y)] * 2 + [torch.mm(x, z)] * 3, 1)\n# Inputs to the model\nx = torch.randn(3, 2)\ny = torch.randn(2, 3)\nz = torch.randn(2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x):\n        res = torch.cat([torch.mm(x, x), torch.mm(x, x), torch.mm(x, x)], dim=1)\n        res = torch.cat([res, res], dim=1)\n        res = torch.cat([res, res], dim=1)\n        res = torch.cat([res, res], dim=1)\n        res = torch.cat([res, res], dim=1)\n        res = torch.cat([res, res], dim=1)\n        res = torch.cat([res, res], dim=1)\n        res = torch.cat([res, res], dim=1)\n        res = torch.cat([res, res], dim=1)\n        res = torch.cat([res, res], dim=1)\n        res = torch.cat([res, res], dim=1)\n        res = torch.cat([res, res], dim=1)\n        res = torch.cat([res, res], dim=1)\n        res = torch.cat([res, res], dim=1)\n        res = torch.cat([res, res], dim=1)\n        return res\n# Inputs to the model\nx = torch.randn(2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, x2):\n        v1 = torch.mm(x1, x2)\n        v2 = torch.mm(x1, x2)\n        v3 = torch.mm(x1, x2)\n        v4 = torch.mm(x1, x2)\n        return torch.cat([v1, v2, v3, v4, v1, v2, v3, v4, v1, v2, v3, v1, v2, v3, v4, v1, v2, v3, v1, v2, v3, v4], 1)\n# Inputs to the model\nx1 = torch.randn(2, 3)\nx2 = torch.randn(3, 2)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x, y):\n        v1 = torch.mm(x, y)\n        v2 = torch.mm(x, y)\n        v3 = torch.mm(x, y)\n        return torch.cat([v1, v2, v3], 0)\n# Inputs to the model\nx = torch.randn(2, 2)\ny = torch.randn(3, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x, y):\n        v1 = torch.mm(x, y)\n        v2 = torch.mm(x, y)\n        v3 = torch.mm(x, y)\n        return torch.cat([1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 5, 6, 7, 8], 0)\n# Inputs to the model\nx = torch.randn(5, 5)\ny = torch.randn(5, 5)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x):\n        t1 = torch.mm(x, x)\n        v1 = torch.mm(x, x)\n        t2 = torch.cat([t1, t1, t1, t1], 1)\n        t3 = torch.mm(x, x)\n        v2 = torch.mm(x, x)\n        return torch.cat([v1, t2, v2, v1], 1)\n# Inputs to the model\nx = torch.randn(2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, x2):\n        v1 = torch.mm(x1, x2)\n        v2 = torch.mm(x1, x2)\n        v3 = torch.mm(x1, x2)\n        v4 = torch.mm(x2, x1)\n        return torch.cat([v1, v2, v3, v4], 1)\n# Inputs to the model\nx = torch.randn(2, 2)\ny = torch.randn(2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x, y):\n        v1 = torch.mm(x, y)\n        v2 = torch.mm(x, y)\n        v3 = torch.mm(x, y)\n        v4 = torch.mm(x, y)\n        return torch.cat([v1, v2, v3, v4], 0)\n# Inputs to the model\nx = torch.randn(1, 2)\ny = torch.randn(2, 1)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x):\n        v1 = torch.mm(x, x)\n        v2 = torch.mm(x, x)\n        return torch.cat([v1, v2, v1, v2], 0)\n# Inputs to the model\nx = torch.randn(3, 4)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x):\n        v1 = torch.cat([x, x, x], 0)\n        v2 = torch.cat([x, x, x], 1)\n        return v1 + v2\n# Inputs to the model\nx = torch.randn(10, 4)\n",
                "\nclass Model(torch.nn.Module):\n    def forward(self, x, y, z):\n        return torch.cat([torch.mm(x, y)] * 2 + [torch.mm(x, z)] * 3, 1)\n# Inputs to the model\nx = torch.randn(3, 2)\ny = torch.randn(2, 3)\nz = torch.randn(2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x):\n        res = torch.cat([torch.mm(x, x), torch.mm(x, x), torch.mm(x, x)], dim=1)\n        res = torch.cat([res, res], dim=1)\n        res = torch.cat([res, res], dim=1)\n        res = torch.cat([res, res], dim=1)\n        res = torch.cat([res, res], dim=1)\n        res = torch.cat([res, res], dim=1)\n        res = torch.cat([res, res], dim=1)\n        res = torch.cat([res, res], dim=1)\n        res = torch.cat([res, res], dim=1)\n        res = torch.cat([res, res], dim=1)\n        res = torch.cat([res, res], dim=1)\n        res = torch.cat([res, res], dim=1)\n        res = torch.cat([res, res], dim=1)\n        res = torch.cat([res, res], dim=1)\n        res = torch.cat([res, res], dim=1)\n        return res\n# Inputs to the model\nx = torch.randn(2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, x2):\n        v1 = torch.mm(x1, x2)\n        v2 = torch.mm(x1, x2)\n        v3 = torch.mm(x1, x2)\n        v4 = torch.mm(x1, x2)\n        return torch.cat([v1, v2, v3, v4, v1, v2, v3, v4, v1, v2, v3, v1, v2, v3, v4, v1, v2, v3, v1, v2, v3, v4], 1)\n# Inputs to the model\nx1 = torch.randn(2, 3)\nx2 = torch.randn(3, 2)\n"
            ],
            "g_time": 10.938611030578613
        }
    }
}
{
    "opt": "sink_cat_after_pointwise",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x):\n        y = torch.cat((x, x, x), dim=0)\n        y = y + x\n        x = (y + y + y)[0]\n        return x[0]\n# Inputs to the model\nx = torch.randn(2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.weight = torch.randn(4, 6)\n    def forward(self, x):\n        z = self.weight.view(4, -1)\n        x = x.view(1, -1)\n        w = self.weight.view(-1, 6)\n        y = torch.dot(x, w)\n        y = y.tanh()\n        return y\n# Inputs to the model\nx = torch.randn(1, 4, 30, 30)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x):\n        y = x.view(x.shape[0], -1)\n        y = y.tanh()\n        x = y.view(x.shape[0], 4)\n        y = torch.cat((x, x), dim=0)\n        return y.view(x.shape[0] * y.shape[0], -1).tanh()\n# Inputs to the model\nx = torch.randn(2, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x):\n        y = torch.cat((x, x), dim=-1)\n        y = y.view(y.shape[0], -1)\n        y = y.relu()\n        return y\n# Inputs to the model\nx = torch.randn(2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.weight = torch.randn(4, 6)\n    def forward(self, x):\n        y = self.weight.clone().unsqueeze(0).expand(x.shape[0], -1, -1)\n        x = torch.cat((x, x), dim=1)\n        x = x + y.tanh()\n        print(x.shape)\n        return x\n# Inputs to the model\nx = torch.randn(2, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, y):\n        x1 = torch.cat([y.view(-1, *y.shape[2:])] * 3, dim=2)\n        x1 = x1.tanh()\n        x1 = x1.view(x1.shape[0] // 3, 3, *x1.shape[1:]).permute(2, 0, 1, 3)\n        x1 = torch.relu(x1)\n        return x1\n# Inputs to the model\ny = torch.randn(2, 4, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x):\n        y = x.view(x.shape[0], -1)\n        y = torch.relu(y)\n        z = torch.cat((y + 1, y + 1), dim=1)\n        y = z.view(z.shape[0], -1).tanh()\n        return y\n# Inputs to the model\nx = torch.randn(2, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x):\n        y = x.tanh()\n        z = x.cat((y, y), dim=1).permute(1, 2, 0)\n        return x\n# Inputs to the model\nx = torch.randn(2, 3, 4)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x):\n        x = x.view(x.shape[0], x.numel() // x.shape[0])\n        return x.sigmoid().view(x.shape[0], x.shape[1], 1)\n# Inputs to the model\nx = torch.randn(2, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x):\n        y = x.view(x.shape[1], -1, x.shape[0])\n        y = torch.clamp(y, 0, 5)\n        return y.tanh()\n# Inputs to the model\nx = torch.randn(2, 2, 2)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x):\n        y = torch.cat((x, x, x), dim=0)\n        y = y + x\n        x = (y + y + y)[0]\n        return x[0]\n# Inputs to the model\nx = torch.randn(2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.weight = torch.randn(4, 6)\n    def forward(self, x):\n        z = self.weight.view(4, -1)\n        x = x.view(1, -1)\n        w = self.weight.view(-1, 6)\n        y = torch.dot(x, w)\n        y = y.tanh()\n        return y\n# Inputs to the model\nx = torch.randn(1, 4, 30, 30)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x):\n        y = x.view(x.shape[0], -1)\n        y = y.tanh()\n        x = y.view(x.shape[0], 4)\n        y = torch.cat((x, x), dim=0)\n        return y.view(x.shape[0] * y.shape[0], -1).tanh()\n# Inputs to the model\nx = torch.randn(2, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x):\n        y = torch.cat((x, x), dim=-1)\n        y = y.view(y.shape[0], -1)\n        y = y.relu()\n        return y\n# Inputs to the model\nx = torch.randn(2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.weight = torch.randn(4, 6)\n    def forward(self, x):\n        y = self.weight.clone().unsqueeze(0).expand(x.shape[0], -1, -1)\n        x = torch.cat((x, x), dim=1)\n        x = x + y.tanh()\n        print(x.shape)\n        return x\n# Inputs to the model\nx = torch.randn(2, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, y):\n        x1 = torch.cat([y.view(-1, *y.shape[2:])] * 3, dim=2)\n        x1 = x1.tanh()\n        x1 = x1.view(x1.shape[0] // 3, 3, *x1.shape[1:]).permute(2, 0, 1, 3)\n        x1 = torch.relu(x1)\n        return x1\n# Inputs to the model\ny = torch.randn(2, 4, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x):\n        y = x.view(x.shape[0], -1)\n        y = torch.relu(y)\n        z = torch.cat((y + 1, y + 1), dim=1)\n        y = z.view(z.shape[0], -1).tanh()\n        return y\n# Inputs to the model\nx = torch.randn(2, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x):\n        y = x.tanh()\n        z = x.cat((y, y), dim=1).permute(1, 2, 0)\n        return x\n# Inputs to the model\nx = torch.randn(2, 3, 4)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x):\n        x = x.view(x.shape[0], x.numel() // x.shape[0])\n        return x.sigmoid().view(x.shape[0], x.shape[1], 1)\n# Inputs to the model\nx = torch.randn(2, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x):\n        y = x.view(x.shape[1], -1, x.shape[0])\n        y = torch.clamp(y, 0, 5)\n        return y.tanh()\n# Inputs to the model\nx = torch.randn(2, 2, 2)\n"
            ],
            "g_time": 6.442439556121826
        }
    }
}
{
    "opt": "binary=2",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(1, 1, 1, stride=8, padding=0)\n    def forward(self, x):\n        v1 = self.conv(x)\n        v2 = v1 - 0.04731179990091324\n        return v2\n# Inputs to the model\nx = torch.randn(1, 1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.conv0 = torch.nn.Conv2d( 512, 48, kernel_size=(1, 1), stride=(1, 1))\n    def forward(self, x):\n        v1 = self.conv0(x)\n        v2 = v1 - 0.31\n        return v2\n# Inputs to the model\nx = torch.randn(1, 512, 600, 1000)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(4, 1, 3, stride=1, padding=0)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 - 226.5270195933006\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 4, 1, 512)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 5, 1, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 - torch.tensor(1e+295)\n        return v2\n# Inputs to the model\nx1 = torch.randn(16, 5, 5, 5)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_0 = torch.nn.Conv2d(24, 16, 1, stride=1, padding=1)\n        self.conv_1 = torch.nn.Conv2d(24, 24, 1, stride=1, padding=1)\n        self.conv_2 = torch.nn.Conv2d(24, 24, 1, stride=1, padding=1)\n        self.conv_3 = torch.nn.Conv2d(24, 24, 1, stride=1, padding=1)\n        self.conv_4 = torch.nn.Conv2d(24, 24, 1, stride=1, padding=1)\n        self.conv_5 = torch.nn.Conv2d(24, 24, 1, stride=1, padding=1)\n        self.sigmoid = torch.nn.Sigmoid()\n    def forward(self, x):\n        v0 = self.conv_0(x)\n        v1 = self.conv_1(x)\n        v2 = self.conv_2(x)\n        v3 = self.conv_3(x)\n        v4 = self.conv_4(x)\n        v5 = self.conv_5(x)\n        v6 = self.sigmoid(v0)\n        v7 = v0 - v1\n        v8 = v0 - v2\n        v9 = v0 - v3\n        v10 = v0 - v4\n        v11 = v0 - v5\n        v12 = v0 - v6\n        return v6, v7, v8, v9, v10, v11, v12\n# Inputs to the model\nx = torch.randn(2, 24, 1, 56)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(7, 5, 1, stride=1, padding=0)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 - 0.57\n        return v2\n# Inputs to the model\nx1 = torch.randn(4, 7, 1024, 2048)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 3, 1, stride=1, padding=0)\n    def forward(self, x):\n        v1 = self.conv(x)\n        v2 = v1 - 5.2\n        return v2\n# Inputs to the model\nx = torch.randn(1, 3, 360, 1)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 1, 1, stride=1, padding=0)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 - 0.7962\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(1, 1, 1, stride=1, padding=0, groups=1)\n        self.tanh = torch.nn.Tanh()\n        self.sigmoid = torch.nn.Sigmoid()\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = self.tanh(v1)\n        v3 = self.sigmoid(v2)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 1, 256, 256)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(14, 3, 1, stride=1, padding=1)\n    def forward(self, x):\n        v1 = self.conv(x)\n        v2 = v1 - 0.6949\n        return v2\n# Inputs to the model\nx = torch.randn(1, 14, 14, 8)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(1, 1, 1, stride=8, padding=0)\n    def forward(self, x):\n        v1 = self.conv(x)\n        v2 = v1 - 0.04731179990091324\n        return v2\n# Inputs to the model\nx = torch.randn(1, 1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.conv0 = torch.nn.Conv2d( 512, 48, kernel_size=(1, 1), stride=(1, 1))\n    def forward(self, x):\n        v1 = self.conv0(x)\n        v2 = v1 - 0.31\n        return v2\n# Inputs to the model\nx = torch.randn(1, 512, 600, 1000)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(4, 1, 3, stride=1, padding=0)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 - 226.5270195933006\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 4, 1, 512)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 5, 1, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 - torch.tensor(1e+295)\n        return v2\n# Inputs to the model\nx1 = torch.randn(16, 5, 5, 5)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_0 = torch.nn.Conv2d(24, 16, 1, stride=1, padding=1)\n        self.conv_1 = torch.nn.Conv2d(24, 24, 1, stride=1, padding=1)\n        self.conv_2 = torch.nn.Conv2d(24, 24, 1, stride=1, padding=1)\n        self.conv_3 = torch.nn.Conv2d(24, 24, 1, stride=1, padding=1)\n        self.conv_4 = torch.nn.Conv2d(24, 24, 1, stride=1, padding=1)\n        self.conv_5 = torch.nn.Conv2d(24, 24, 1, stride=1, padding=1)\n        self.sigmoid = torch.nn.Sigmoid()\n    def forward(self, x):\n        v0 = self.conv_0(x)\n        v1 = self.conv_1(x)\n        v2 = self.conv_2(x)\n        v3 = self.conv_3(x)\n        v4 = self.conv_4(x)\n        v5 = self.conv_5(x)\n        v6 = self.sigmoid(v0)\n        v7 = v0 - v1\n        v8 = v0 - v2\n        v9 = v0 - v3\n        v10 = v0 - v4\n        v11 = v0 - v5\n        v12 = v0 - v6\n        return v6, v7, v8, v9, v10, v11, v12\n# Inputs to the model\nx = torch.randn(2, 24, 1, 56)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(7, 5, 1, stride=1, padding=0)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 - 0.57\n        return v2\n# Inputs to the model\nx1 = torch.randn(4, 7, 1024, 2048)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 3, 1, stride=1, padding=0)\n    def forward(self, x):\n        v1 = self.conv(x)\n        v2 = v1 - 5.2\n        return v2\n# Inputs to the model\nx = torch.randn(1, 3, 360, 1)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 1, 1, stride=1, padding=0)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 - 0.7962\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(1, 1, 1, stride=1, padding=0, groups=1)\n        self.tanh = torch.nn.Tanh()\n        self.sigmoid = torch.nn.Sigmoid()\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = self.tanh(v1)\n        v3 = self.sigmoid(v2)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 1, 256, 256)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(14, 3, 1, stride=1, padding=1)\n    def forward(self, x):\n        v1 = self.conv(x)\n        v2 = v1 - 0.6949\n        return v2\n# Inputs to the model\nx = torch.randn(1, 14, 14, 8)\n"
            ],
            "g_time": 14.973221778869629
        }
    }
}
{
    "opt": "unary=18",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.QConv2d(3, 8, 3, stride=1, padding=1)\n        self.conv2 = torch.nn.Conv2d(8, 8, 3, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = self.conv2(v1)\n        v4 = torch.sigmoid(v3)\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 4, 3, stride=1, padding=1)\n        self.conv2 = torch.nn.Conv2d(4, 5, 2, stride=1, padding=0)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = self.conv2(v1)\n        v3 = torch.sigmoid(v2)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(7, 9, 3, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = torch.sigmoid(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 7, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 8, 3, stride=1, padding=1)\n        self.conv2 = torch.nn.Conv2d(8, 16, 3, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = self.conv1(v1)\n        v4 = torch.sigmoid(v3)\n        v5 = self.conv2(v1)\n        v6 = torch.sigmoid(v5)\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1):\n        v1 = F.avg_pool2d(x1, 14, stride=2, padding=3)\n        return v1\n# Inputs to the model\nx1 = torch.randn(1, 11, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(8, 16, 3, 1, 0)\n        self.conv2 = torch.nn.Conv2d(8, 16, 3, 1, 0)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = self.conv2(x1)\n        v4 = torch.sigmoid(v3)\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 8, 28, 28)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 16, (3, 5), padding=(2, 2))\n        self.conv2 = torch.nn.Conv2d(8, 8, (1, 3), padding=(0, 0))\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = self.conv2(v2)\n        v4 = torch.sigmoid(v3)\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(16, 8, 3, padding=1)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = torch.sigmoid(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 16, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(11, 13, 1, padding=0)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = torch.sigmoid(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 11, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 12, 1, stride=1, padding=0, dilation=1)\n        self.conv2 = torch.nn.Conv2d(12, 6, 1, stride=1, padding=0, dilation=1)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = self.conv2(v2)\n        v4 = torch.sigmoid(v3)\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.QConv2d(3, 8, 3, stride=1, padding=1)\n        self.conv2 = torch.nn.Conv2d(8, 8, 3, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = self.conv2(v1)\n        v4 = torch.sigmoid(v3)\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 4, 3, stride=1, padding=1)\n        self.conv2 = torch.nn.Conv2d(4, 5, 2, stride=1, padding=0)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = self.conv2(v1)\n        v3 = torch.sigmoid(v2)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(7, 9, 3, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = torch.sigmoid(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 7, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 8, 3, stride=1, padding=1)\n        self.conv2 = torch.nn.Conv2d(8, 16, 3, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = self.conv1(v1)\n        v4 = torch.sigmoid(v3)\n        v5 = self.conv2(v1)\n        v6 = torch.sigmoid(v5)\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1):\n        v1 = F.avg_pool2d(x1, 14, stride=2, padding=3)\n        return v1\n# Inputs to the model\nx1 = torch.randn(1, 11, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(8, 16, 3, 1, 0)\n        self.conv2 = torch.nn.Conv2d(8, 16, 3, 1, 0)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = self.conv2(x1)\n        v4 = torch.sigmoid(v3)\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 8, 28, 28)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 16, (3, 5), padding=(2, 2))\n        self.conv2 = torch.nn.Conv2d(8, 8, (1, 3), padding=(0, 0))\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = self.conv2(v2)\n        v4 = torch.sigmoid(v3)\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(16, 8, 3, padding=1)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = torch.sigmoid(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 16, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(11, 13, 1, padding=0)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = torch.sigmoid(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 11, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 12, 1, stride=1, padding=0, dilation=1)\n        self.conv2 = torch.nn.Conv2d(12, 6, 1, stride=1, padding=0, dilation=1)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = self.conv2(v2)\n        v4 = torch.sigmoid(v3)\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n"
            ],
            "g_time": 7.50491189956665
        }
    }
}
{
    "opt": "cat_slice_cat",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def forward(self, x1, x2):\n        v1 = torch.cat([x1, x2], dim=1)\n        v2 = v1[:, 0:9223372036854775807]\n        v3 = v2[:, 0:]\n        v4 = torch.cat([v1, v3], dim=1)\n        return v4\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(3, 6, 3, 5)\nx2 = torch.randn(3, 4, 3, 5)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n \n    def forward(self, x1, x2):\n        x = torch.cat([x1, x2], dim=0)\n        x = x[:, 0:9223372036854775807]\n        x = x[:, 0:min(x1.size(1), x2.size(1))]\n        x = torch.cat([x1, x], dim=1)\n        return x\n \n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 280, 121)\nx2 = torch.randn(1, 10, 460, 199)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n \n    def forward(self, x1):\n        t1 = torch.cat([x1, x1 * 2], dim=1)\n        t2 = t1[:, 0:1024*1]\n        t3 = t2[:, 0:99999999999]\n        return torch.cat([t1, t3], dim=1)\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 1023, 10)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n \n    def forward(self, x1):\n        v1 = torch.cat([x1, x1, x1, x1, x1], 1) \n        v2 = v1[:, 0:9223372036854775807]\n        v3 = v2[:, 0:__SIZE__]\n        v5 = torch.cat([v1, v3], 1)\n        return v5\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, __NUM_CHANNELS__, __SIZE__, __SIZE__)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n \n    def forward(self, x1):\n        v1 = self.conv(x1)\n        # Perform the concatenation\n        v2 = torch.cat([v1, v1, v1, v1, v1, v1], dim=1)\n        v2 = v2[:, 0:9223372036854775807] # slice(0, 9223372036854775807)\n        v3 = v2[:, 0:12]\n        v4 = torch.cat([v1, v3], dim=1)\n        return v4\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, s):\n        super().__init__()\n        self.s = s\n \n    def forward(self, x1, x2):\n        v1 = torch.cat([x1, x2], dim=1)\n        v2 = v1[:, 0:9223372036854775807]\n        v3 = v2[:, :self.s]\n        v4 = torch.cat([x1, v3], dim=1)\n        return v4\n\n# Initializing the model\nm = Model(2)\n\n# Inputs to the model\nx1 = torch.randn(1, 1, 64, 64)\nx2 = torch.randn(1, 2, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n \n    def forward(self, x1, x2, x3):\n        v1 = torch.cat([x1, x2, x3], dim=1)\n        v2 = v1[:, 0:9223372036854775807]\n        v3 = v2[:, 0:16]\n        v4 = torch.cat([v1, v3], dim=1)\n        return v4\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 64, 20, 20)\nx2 = torch.randn(1, 96, 10, 10)\nx3 = torch.randn(1, 128, 5, 5)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n \n    def forward(self, x1):\n        v1 = torch.cat([x1, x1, x1], dim=1)\n        v2 = v1[:, 0:9223372036854775807]\n        v3 = v2[:, 0:171127603233]\n        v4 = torch.cat([v1, v3], dim=1)\n        return v4\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 23, 17179869184)\nx2 = torch.randn(1, 23, 17179869184)\nx3 = torch.randn(1, 23, 17179869184)\nx4 = torch.randn(1, 23, 17179869184)\nx5 = torch.randn(1, 23, 17179869183)\nx6 = torch.randn(1, 23, 17179869183)\nx7 = torch.randn(1, 23, 17179869183)\nx8 = torch.randn(1, 23, 17179869183)\nx9 = torch.randn(1, 23, 17179869182)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        pass\n\n    def forward(self, x1, x2, x3):\n        v1 = torch.cat([x1, x2, x3], dim=1)\n        v2 = v1[:, 0:9223372036854775807]\n        v3 = v2[:, 0:v1.size(3) // 2]\n        v4 = torch.cat([v1, v3], dim=1)\n        return v4\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 32)\nx2 = torch.randn(1, 16, 64, 32)\nx3 = torch.randn(1, 16, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n \n    def forward(self, x1):\n        v1 = torch.cat([x1, x1, x1], dim=1)\n        v2 = v1[:, 0:9223372036854775807]\n        v3 = v2[:, 0:32]\n        v4 = torch.cat([v1, v3], dim=1)\n        return v4\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def forward(self, x1, x2):\n        v1 = torch.cat([x1, x2], dim=1)\n        v2 = v1[:, 0:9223372036854775807]\n        v3 = v2[:, 0:]\n        v4 = torch.cat([v1, v3], dim=1)\n        return v4\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(3, 6, 3, 5)\nx2 = torch.randn(3, 4, 3, 5)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n \n    def forward(self, x1, x2):\n        x = torch.cat([x1, x2], dim=0)\n        x = x[:, 0:9223372036854775807]\n        x = x[:, 0:min(x1.size(1), x2.size(1))]\n        x = torch.cat([x1, x], dim=1)\n        return x\n \n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 280, 121)\nx2 = torch.randn(1, 10, 460, 199)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n \n    def forward(self, x1):\n        t1 = torch.cat([x1, x1 * 2], dim=1)\n        t2 = t1[:, 0:1024*1]\n        t3 = t2[:, 0:99999999999]\n        return torch.cat([t1, t3], dim=1)\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 1023, 10)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n \n    def forward(self, x1):\n        v1 = torch.cat([x1, x1, x1, x1, x1], 1) \n        v2 = v1[:, 0:9223372036854775807]\n        v3 = v2[:, 0:__SIZE__]\n        v5 = torch.cat([v1, v3], 1)\n        return v5\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, __NUM_CHANNELS__, __SIZE__, __SIZE__)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n \n    def forward(self, x1):\n        v1 = self.conv(x1)\n        # Perform the concatenation\n        v2 = torch.cat([v1, v1, v1, v1, v1, v1], dim=1)\n        v2 = v2[:, 0:9223372036854775807] # slice(0, 9223372036854775807)\n        v3 = v2[:, 0:12]\n        v4 = torch.cat([v1, v3], dim=1)\n        return v4\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, s):\n        super().__init__()\n        self.s = s\n \n    def forward(self, x1, x2):\n        v1 = torch.cat([x1, x2], dim=1)\n        v2 = v1[:, 0:9223372036854775807]\n        v3 = v2[:, :self.s]\n        v4 = torch.cat([x1, v3], dim=1)\n        return v4\n\n# Initializing the model\nm = Model(2)\n\n# Inputs to the model\nx1 = torch.randn(1, 1, 64, 64)\nx2 = torch.randn(1, 2, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n \n    def forward(self, x1, x2, x3):\n        v1 = torch.cat([x1, x2, x3], dim=1)\n        v2 = v1[:, 0:9223372036854775807]\n        v3 = v2[:, 0:16]\n        v4 = torch.cat([v1, v3], dim=1)\n        return v4\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 64, 20, 20)\nx2 = torch.randn(1, 96, 10, 10)\nx3 = torch.randn(1, 128, 5, 5)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n \n    def forward(self, x1):\n        v1 = torch.cat([x1, x1, x1], dim=1)\n        v2 = v1[:, 0:9223372036854775807]\n        v3 = v2[:, 0:171127603233]\n        v4 = torch.cat([v1, v3], dim=1)\n        return v4\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 23, 17179869184)\nx2 = torch.randn(1, 23, 17179869184)\nx3 = torch.randn(1, 23, 17179869184)\nx4 = torch.randn(1, 23, 17179869184)\nx5 = torch.randn(1, 23, 17179869183)\nx6 = torch.randn(1, 23, 17179869183)\nx7 = torch.randn(1, 23, 17179869183)\nx8 = torch.randn(1, 23, 17179869183)\nx9 = torch.randn(1, 23, 17179869182)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        pass\n\n    def forward(self, x1, x2, x3):\n        v1 = torch.cat([x1, x2, x3], dim=1)\n        v2 = v1[:, 0:9223372036854775807]\n        v3 = v2[:, 0:v1.size(3) // 2]\n        v4 = torch.cat([v1, v3], dim=1)\n        return v4\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 32)\nx2 = torch.randn(1, 16, 64, 32)\nx3 = torch.randn(1, 16, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n \n    def forward(self, x1):\n        v1 = torch.cat([x1, x1, x1], dim=1)\n        v2 = v1[:, 0:9223372036854775807]\n        v3 = v2[:, 0:32]\n        v4 = torch.cat([v1, v3], dim=1)\n        return v4\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n"
            ],
            "g_time": 13.79945683479309
        }
    }
}
{
    "opt": "permute_matmul_fusion",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, x2):\n        x = x2.permute(0, 2, 1)\n        v0 = x1.permute(0, 2, 1)\n        v0 = x1.permute(0, 2, 1)\n        v0 = x2.permute(0, 2, 1)\n        v0 = x2.permute(0, 2, 1)\n        v0 = x2.permute(0, 2, 1)\n        v1 = x1.permute(0, 2, 1)\n        v1 = x1.permute(0, 2, 1)\n        v1 = x1.permute(0, 2, 1)\n        v1 = x1.permute(0, 2, 1)\n        v2 = x2.permute(0, 2, 1)\n        v2 = x1.permute(0, 2, 1)\n        v2 = x1.permute(0, 2, 1)\n        v2 = x1.permute(0, 2, 1)\n        v3 = x2.permute(0, 2, 1)\n        v3 = x2.permute(0, 2, 1)\n        v3 = x1.permute(0, 2, 1)\n        v3 = x2.permute(0, 2, 1)\n        v4 = x1.permute(0, 2, 1)\n        v4 = x1.permute(0, 2, 1)\n        v4 = x1.permute(0, 2, 1)\n        return torch.bmm(v4, v0)\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\nx2 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, x2):\n        v1 = x1.permute(0, 2, 1)\n        v3 = v1.permute(0, 2, 1)\n        v4 = x1.permute(0, 2, 1)\n        v5 = torch.bmm(v4, x1)\n        v6 = torch.bmm(v3, x1)\n        return torch.matmul(v5, x1)\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\nx2 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, x2):\n        v1 = x1.permute(0, 1, 2)\n        v2 = x2.permute(0, 1, 2)\n        v3 = torch.matmul(x1, v1)\n        v4 = torch.matmul(x2, v2)\n        return torch.matmul(v3, v4)\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\nx2 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, x2):\n        v1 = x1.permute(0, 2, 1)\n        v2 = x1.permute(0, 2, 1)\n        v3 = v1.permute(0, 2, 1)\n        v4 = v1.permute(0, 2, 1)\n        v6 = x2.permute(0, 2, 1)\n        v5 = torch.bmm(x2, v1)\n        v7 = x1.permute(0, 2, 1)\n        return torch.matmul(v5, v7)\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\nx2 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n     def __init__(self):\n         super().__init__()\n     def forward(self, x1, x2):\n         v0 = x1.permute(0, 2, 1)\n         v0 = x2.permute(0, 2, 1)\n         v0 = x2.permute(0, 2, 1)\n         v1 = v0.permute(0, 2, 1)\n         v1 = v0.permute(0, 2, 1)\n         v1 = v0.permute(0, 2, 1)\n         v2 = torch.matmul(x1, v0)\n         v2 = torch.bmm(x2, v1)\n         v2 = x2.permute(0, 1, 2).permute(2, 1)\n         v2 = torch.matmul(x1, v0.permute(0, 2, 1))\n         v2 = v2.permute(0, 2, 1)\n         v2 = v0.permute(0, 2, 1)\n         v2 = v0.permute(0, 2, 1)\n         return v2\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\nx2 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, x2):\n        v1 = x1.permute(0, 2, 1)\n        v2 = x1.permute(0, 2, 1)\n        v3 = x1.permute(0, 2, 1)\n        v4 = x1.permute(0, 2, 1)\n        v5 = torch.bmm(v1, v1)\n        v6 = torch.matmul(v2, v2)[0][0][0]\n        return torch.bmm(v2, v6)\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\nx2 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, x2):\n        v1 = x1.permute(0, 2, 1)\n        v2 = x1.permute(0, 2, 1)\n        return torch.bmm(v1, v2)\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\nx2 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, x2):\n        v1 = x1.permute(0, 2, 1)\n        v2 = x2.permute(0, 2, 1)\n        v3 = torch.bmm(x1, x2)\n        v4 = v1.permute(0, 2, 1)\n        v5 = x2.permute(0, 2, 1)\n        v6 = torch.bmm(v2, v4)\n        return torch.matmul(x2, v2)\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\nx2 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, x2):\n        v1 = x1.permute(0, 2, 1)\n        v2 = x1.permute(0, 2, 1)\n        v3 = v1.permute(0, 2, 1)\n        v4 = v2.permute(0, 2, 1)\n        v5 = torch.bmm(x2, v1)\n        v6 = torch.bmm(v3, x2)\n        return torch.matmul(v5, v6)\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\nx2 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1):\n        v0 = x1.permute(0, 2, 1)\n        return v0\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, x2):\n        x = x2.permute(0, 2, 1)\n        v0 = x1.permute(0, 2, 1)\n        v0 = x1.permute(0, 2, 1)\n        v0 = x2.permute(0, 2, 1)\n        v0 = x2.permute(0, 2, 1)\n        v0 = x2.permute(0, 2, 1)\n        v1 = x1.permute(0, 2, 1)\n        v1 = x1.permute(0, 2, 1)\n        v1 = x1.permute(0, 2, 1)\n        v1 = x1.permute(0, 2, 1)\n        v2 = x2.permute(0, 2, 1)\n        v2 = x1.permute(0, 2, 1)\n        v2 = x1.permute(0, 2, 1)\n        v2 = x1.permute(0, 2, 1)\n        v3 = x2.permute(0, 2, 1)\n        v3 = x2.permute(0, 2, 1)\n        v3 = x1.permute(0, 2, 1)\n        v3 = x2.permute(0, 2, 1)\n        v4 = x1.permute(0, 2, 1)\n        v4 = x1.permute(0, 2, 1)\n        v4 = x1.permute(0, 2, 1)\n        return torch.bmm(v4, v0)\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\nx2 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, x2):\n        v1 = x1.permute(0, 2, 1)\n        v3 = v1.permute(0, 2, 1)\n        v4 = x1.permute(0, 2, 1)\n        v5 = torch.bmm(v4, x1)\n        v6 = torch.bmm(v3, x1)\n        return torch.matmul(v5, x1)\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\nx2 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, x2):\n        v1 = x1.permute(0, 1, 2)\n        v2 = x2.permute(0, 1, 2)\n        v3 = torch.matmul(x1, v1)\n        v4 = torch.matmul(x2, v2)\n        return torch.matmul(v3, v4)\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\nx2 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, x2):\n        v1 = x1.permute(0, 2, 1)\n        v2 = x1.permute(0, 2, 1)\n        v3 = v1.permute(0, 2, 1)\n        v4 = v1.permute(0, 2, 1)\n        v6 = x2.permute(0, 2, 1)\n        v5 = torch.bmm(x2, v1)\n        v7 = x1.permute(0, 2, 1)\n        return torch.matmul(v5, v7)\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\nx2 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n     def __init__(self):\n         super().__init__()\n     def forward(self, x1, x2):\n         v0 = x1.permute(0, 2, 1)\n         v0 = x2.permute(0, 2, 1)\n         v0 = x2.permute(0, 2, 1)\n         v1 = v0.permute(0, 2, 1)\n         v1 = v0.permute(0, 2, 1)\n         v1 = v0.permute(0, 2, 1)\n         v2 = torch.matmul(x1, v0)\n         v2 = torch.bmm(x2, v1)\n         v2 = x2.permute(0, 1, 2).permute(2, 1)\n         v2 = torch.matmul(x1, v0.permute(0, 2, 1))\n         v2 = v2.permute(0, 2, 1)\n         v2 = v0.permute(0, 2, 1)\n         v2 = v0.permute(0, 2, 1)\n         return v2\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\nx2 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, x2):\n        v1 = x1.permute(0, 2, 1)\n        v2 = x1.permute(0, 2, 1)\n        v3 = x1.permute(0, 2, 1)\n        v4 = x1.permute(0, 2, 1)\n        v5 = torch.bmm(v1, v1)\n        v6 = torch.matmul(v2, v2)[0][0][0]\n        return torch.bmm(v2, v6)\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\nx2 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, x2):\n        v1 = x1.permute(0, 2, 1)\n        v2 = x1.permute(0, 2, 1)\n        return torch.bmm(v1, v2)\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\nx2 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, x2):\n        v1 = x1.permute(0, 2, 1)\n        v2 = x2.permute(0, 2, 1)\n        v3 = torch.bmm(x1, x2)\n        v4 = v1.permute(0, 2, 1)\n        v5 = x2.permute(0, 2, 1)\n        v6 = torch.bmm(v2, v4)\n        return torch.matmul(x2, v2)\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\nx2 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, x2):\n        v1 = x1.permute(0, 2, 1)\n        v2 = x1.permute(0, 2, 1)\n        v3 = v1.permute(0, 2, 1)\n        v4 = v2.permute(0, 2, 1)\n        v5 = torch.bmm(x2, v1)\n        v6 = torch.bmm(v3, x2)\n        return torch.matmul(v5, v6)\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\nx2 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1):\n        v0 = x1.permute(0, 2, 1)\n        return v0\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\n"
            ],
            "g_time": 17.67601466178894
        }
    }
}
{
    "opt": "binary_unary=10",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, x1, x2):\n        v1 = torch.Tensor.matmul(x1,x2)\n        v2 = torch.add(v1,x2)\n        v3 = F.relu(v2)\n        return v3\n\n# Initializing the model \nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 20)\nx2 = torch.randn(1, 20)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 6, bias=False)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 + torch.tensor([1, 2, 3])\n        v3 = torch.nn.functional.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(64, 1)\n \n    def forward(self, x1, x2):\n        v1 = self.linear(x1)\n        v2 = v1 + x2\n        v3 = torch.nn.functional.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(128, 64)\nx2 = torch.randn(128, 1)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(128, 256)\n \n    def forward(self, x1, x2):\n        v1 = self.linear(x1)\n        v2 = v1 + x2\n        v3 = F.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(20, 128)\nx2 = torch.rand(20, 128)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n \n    def forward(self, x1, x2):\n        v1 = self.linear(x1)\n        v2 = v1 + x2\n        v3 = torch.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\nx2 = torch.randn(1, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n \n    def forward(self, x1, x2):\n        v1 = self.linear(x1)\n        v2 = v1 + x2 # Add the second input tensor to the output of the linear transformation\n        v3 = torch.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\nx2 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(64, 64)\n        self.other = torch.randn(64).unsqueeze(0)\n\n    def forward(self, x):\n        v1 = self.linear(x)\n        v2 = v1 + self.other\n        v3 = torch.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx = torch.randn(1, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(960, 512)\n \n    def forward(self, x1, x2):\n        v1 = self.linear(x1)\n        v2 = v1.add(x2)\n        v3 = F.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(4, 960)\nx2 = torch.randn(4, 512)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.linear = torch.nn.Linear(64, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.randn(1, 8)\n        v3 = v1 + v2\n        v4 = torch.nn.functional.relu(v3)\n        return v4\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(128, 8)\n \n    def forward(self, x1, **kwargs):\n        v1 = self.linear(x1)\n        v2 = v1 + kwargs['other']\n        v3 = F.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 128)\nother = torch.randn(1, 8)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, x1, x2):\n        v1 = torch.Tensor.matmul(x1,x2)\n        v2 = torch.add(v1,x2)\n        v3 = F.relu(v2)\n        return v3\n\n# Initializing the model \nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 20)\nx2 = torch.randn(1, 20)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 6, bias=False)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 + torch.tensor([1, 2, 3])\n        v3 = torch.nn.functional.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(64, 1)\n \n    def forward(self, x1, x2):\n        v1 = self.linear(x1)\n        v2 = v1 + x2\n        v3 = torch.nn.functional.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(128, 64)\nx2 = torch.randn(128, 1)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(128, 256)\n \n    def forward(self, x1, x2):\n        v1 = self.linear(x1)\n        v2 = v1 + x2\n        v3 = F.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(20, 128)\nx2 = torch.rand(20, 128)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n \n    def forward(self, x1, x2):\n        v1 = self.linear(x1)\n        v2 = v1 + x2\n        v3 = torch.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\nx2 = torch.randn(1, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n \n    def forward(self, x1, x2):\n        v1 = self.linear(x1)\n        v2 = v1 + x2 # Add the second input tensor to the output of the linear transformation\n        v3 = torch.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\nx2 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(64, 64)\n        self.other = torch.randn(64).unsqueeze(0)\n\n    def forward(self, x):\n        v1 = self.linear(x)\n        v2 = v1 + self.other\n        v3 = torch.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx = torch.randn(1, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(960, 512)\n \n    def forward(self, x1, x2):\n        v1 = self.linear(x1)\n        v2 = v1.add(x2)\n        v3 = F.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(4, 960)\nx2 = torch.randn(4, 512)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.linear = torch.nn.Linear(64, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.randn(1, 8)\n        v3 = v1 + v2\n        v4 = torch.nn.functional.relu(v3)\n        return v4\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(128, 8)\n \n    def forward(self, x1, **kwargs):\n        v1 = self.linear(x1)\n        v2 = v1 + kwargs['other']\n        v3 = F.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 128)\nother = torch.randn(1, 8)\n"
            ],
            "g_time": 5.943596124649048
        }
    }
}
{
    "opt": "unary=23",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(3, 1, 3, stride=2, padding=1)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = torch.tanh(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 3, 3, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(3, 14, 9, stride=4, padding=4, groups=3)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = torch.tanh(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 3, 15, 15)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(4, 7, 7, padding=2, output_padding=3)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = torch.tanh(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 4, 8, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        # 1 input image channel, 6 output channels, 7x7 square convolution\n        # kernel\n        self.conv_transpose = torch.nn.ConvTranspose2d(64, 32, 7)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = torch.tanh(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 64, 128, 128)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(128, 63, 8, stride=3, padding=2)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = torch.tanh(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 128, 28, 28)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(92, 3, kernel_size=(5, 45), bias=True, stride=(36047394, 832226597), padding=(0, 5))\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = torch.tanh(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(36047394, 832226597, 92, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(3, 2, 4, padding=1)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = torch.tanh(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 3, 8, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose3d(4, 10, 3, bias=False)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = torch.tanh(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 4, 4, 5, 5)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(10, 32, 3, stride=3, padding=1, dilation=2, output_padding=1)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = torch.tanh(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 10, 8, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(16, 98, 3, padding=1)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = torch.tanh(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 16, 8, 8)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(3, 1, 3, stride=2, padding=1)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = torch.tanh(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 3, 3, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(3, 14, 9, stride=4, padding=4, groups=3)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = torch.tanh(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 3, 15, 15)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(4, 7, 7, padding=2, output_padding=3)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = torch.tanh(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 4, 8, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        # 1 input image channel, 6 output channels, 7x7 square convolution\n        # kernel\n        self.conv_transpose = torch.nn.ConvTranspose2d(64, 32, 7)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = torch.tanh(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 64, 128, 128)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(128, 63, 8, stride=3, padding=2)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = torch.tanh(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 128, 28, 28)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(92, 3, kernel_size=(5, 45), bias=True, stride=(36047394, 832226597), padding=(0, 5))\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = torch.tanh(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(36047394, 832226597, 92, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(3, 2, 4, padding=1)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = torch.tanh(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 3, 8, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose3d(4, 10, 3, bias=False)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = torch.tanh(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 4, 4, 5, 5)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(10, 32, 3, stride=3, padding=1, dilation=2, output_padding=1)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = torch.tanh(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 10, 8, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(16, 98, 3, padding=1)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = torch.tanh(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 16, 8, 8)\n"
            ],
            "g_time": 6.478650093078613
        }
    }
}
{
    "opt": "fuse_conv_bn",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(1, 1, 2)\n        self.bn = torch.nn.ModuleList([torch.nn.BatchNorm2d(1)])\n        self.conv2 = torch.nn.Conv2d(1, 1, 2)\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn[0](x)\n        x = self.conv2(x)\n        return x\n# Inputs to the model\nx = torch.randn(1, 1, 4, 4)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(64, 16, 3, padding=1)\n        self.conv2 = torch.nn.Conv2d(16, 32, 3, padding=1)\n        self.bn1 = torch.nn.BatchNorm2d(32, affine=False)\n        self.bn2 = torch.nn.BatchNorm2d(32)\n    def forward(self, x):\n        x = torch.nn.functional.relu(self.conv1(x))\n        x = self.bn1(self.conv2(x))\n        return self.bn2(x)\n# Inputs to the model\nx = torch.randn(20, 64, 10, 10)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(1, 1, 2)\n        self.bn = torch.nn.BatchNorm2d(2)\n        self.conv2 = torch.nn.Conv2d(1, 1, 2)\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn(x)\n        x = self.conv2(x)\n        return x\n# Inputs to the model\nx = torch.randn(1, 1, 4, 4)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.ConvTranspose2d(16, 32, 3, bias=False)\n        self.bn1 = torch.nn.BatchNorm2d(32)\n        self.conv2 = torch.nn.ConvTranspose3d(32, 16, 3)\n        self.bn = torch.nn.BatchNorm3d(16)\n        self.relu2 = torch.nn.ReLU(inplace=False)\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.conv2(x)\n        return self.bn(x)\n# Inputs to the model\nx = torch.randn(1, 16, 16, 16, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(1, 8, kernel_size=(64, 7, 7), stride=(1, 2, 2))\n        self.bn1 = torch.nn.BatchNorm2d(8)\n        self.relu = torch.nn.ReLU(inplace=False)\n        self.flatten = torch.nn.Flatten()\n        self.linear1 = torch.nn.Linear(80, 64)\n        self.bn2 = torch.nn.BatchNorm1d(64)\n        self.tanh = torch.nn.Tanh()\n        self.linear2 = torch.nn.Linear(64, 1)\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.flatten(x)\n        x = self.linear1(x)\n        x = self.bn2(x)\n        return x\n# Inputs to the model\nx = torch.randn(1, 1, 112, 112)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        torch.manual_seed(1)\n        self.conv1 = torch.nn.Conv2d(16, 16, 3, padding=0)\n        torch.manual_seed(0)\n        self.conv2 = torch.nn.Conv2d(16, 16, 1, groups=4)\n        torch.manual_seed(1)\n        self.conv3 = torch.nn.Conv2d(16, 16, 3, padding=0, groups=4)\n    def forward(self, x):\n        t = self.conv1(x)\n        t1 = self.conv2(t)\n        t2 = self.conv3(t)\n        return t1 + t2\n# Inputs to the model\nx1 = torch.randn(1, 16, 5, 5)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 16, 1, bias=False)\n        self.bn = torch.nn.BatchNorm2d(10)\n        self.conv2 = torch.nn.Conv2d(10, 3, 1, bias=False)\n        self.relu = torch.nn.ReLU()\n        self.conv3 = torch.nn.Conv2d(6, 3, 1)\n    def forward(self, x):\n        x1 = self.conv1(x)\n        x = self.relu(x1)\n        x = self.bn(x1)\n        x2 = self.conv2(x)\n        v = torch.cat((x1, x2), dim=1)\n        y = self.conv3(v)\n        y1 = torch.add(y, 1)\n        y2 = torch.add(y, 2)\n        y3 = torch.add(y, 3)\n        return y3\n# Inputs to the model\nx1 = torch.randint(0, 256, (1, 3, 64, 64), dtype=torch.int)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(1, 1, 3, 1)\n        self.bn = torch.nn.BatchNorm2d(1, affine=False)\n        self.conv2 = torch.nn.Conv2d(1, 1, 3, 1)\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn(x)\n        x = self.conv2(x)\n        return x\n# Inputs to the model\nx = torch.randn(1, 1, 4, 4)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(8, 8, 2)\n        self.bn = torch.nn.BatchNorm2d(8, eps=1e-5)\n        self.conv2 = torch.nn.Conv2d(8, 8, 2)\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn(x)\n        x = self.conv2(x)\n        x = self.bn(x)\n        return x\n# Inputs to the model\nx = torch.randn(1, 8, 4, 4)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 3, 1)\n        self.conv1 = torch.nn.Conv2d(3, 3, 1)\n        self.bn = torch.nn.BatchNorm2d(3)\n    def forward(self, x):\n        v = self.conv(x)\n        v1 = self.conv1(v)\n        v2 = self.bn(v)\n        v3 = self.conv1(v2)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 3, 3, 3)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(1, 1, 2)\n        self.bn = torch.nn.ModuleList([torch.nn.BatchNorm2d(1)])\n        self.conv2 = torch.nn.Conv2d(1, 1, 2)\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn[0](x)\n        x = self.conv2(x)\n        return x\n# Inputs to the model\nx = torch.randn(1, 1, 4, 4)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(64, 16, 3, padding=1)\n        self.conv2 = torch.nn.Conv2d(16, 32, 3, padding=1)\n        self.bn1 = torch.nn.BatchNorm2d(32, affine=False)\n        self.bn2 = torch.nn.BatchNorm2d(32)\n    def forward(self, x):\n        x = torch.nn.functional.relu(self.conv1(x))\n        x = self.bn1(self.conv2(x))\n        return self.bn2(x)\n# Inputs to the model\nx = torch.randn(20, 64, 10, 10)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(1, 1, 2)\n        self.bn = torch.nn.BatchNorm2d(2)\n        self.conv2 = torch.nn.Conv2d(1, 1, 2)\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn(x)\n        x = self.conv2(x)\n        return x\n# Inputs to the model\nx = torch.randn(1, 1, 4, 4)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.ConvTranspose2d(16, 32, 3, bias=False)\n        self.bn1 = torch.nn.BatchNorm2d(32)\n        self.conv2 = torch.nn.ConvTranspose3d(32, 16, 3)\n        self.bn = torch.nn.BatchNorm3d(16)\n        self.relu2 = torch.nn.ReLU(inplace=False)\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.conv2(x)\n        return self.bn(x)\n# Inputs to the model\nx = torch.randn(1, 16, 16, 16, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(1, 8, kernel_size=(64, 7, 7), stride=(1, 2, 2))\n        self.bn1 = torch.nn.BatchNorm2d(8)\n        self.relu = torch.nn.ReLU(inplace=False)\n        self.flatten = torch.nn.Flatten()\n        self.linear1 = torch.nn.Linear(80, 64)\n        self.bn2 = torch.nn.BatchNorm1d(64)\n        self.tanh = torch.nn.Tanh()\n        self.linear2 = torch.nn.Linear(64, 1)\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.flatten(x)\n        x = self.linear1(x)\n        x = self.bn2(x)\n        return x\n# Inputs to the model\nx = torch.randn(1, 1, 112, 112)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        torch.manual_seed(1)\n        self.conv1 = torch.nn.Conv2d(16, 16, 3, padding=0)\n        torch.manual_seed(0)\n        self.conv2 = torch.nn.Conv2d(16, 16, 1, groups=4)\n        torch.manual_seed(1)\n        self.conv3 = torch.nn.Conv2d(16, 16, 3, padding=0, groups=4)\n    def forward(self, x):\n        t = self.conv1(x)\n        t1 = self.conv2(t)\n        t2 = self.conv3(t)\n        return t1 + t2\n# Inputs to the model\nx1 = torch.randn(1, 16, 5, 5)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 16, 1, bias=False)\n        self.bn = torch.nn.BatchNorm2d(10)\n        self.conv2 = torch.nn.Conv2d(10, 3, 1, bias=False)\n        self.relu = torch.nn.ReLU()\n        self.conv3 = torch.nn.Conv2d(6, 3, 1)\n    def forward(self, x):\n        x1 = self.conv1(x)\n        x = self.relu(x1)\n        x = self.bn(x1)\n        x2 = self.conv2(x)\n        v = torch.cat((x1, x2), dim=1)\n        y = self.conv3(v)\n        y1 = torch.add(y, 1)\n        y2 = torch.add(y, 2)\n        y3 = torch.add(y, 3)\n        return y3\n# Inputs to the model\nx1 = torch.randint(0, 256, (1, 3, 64, 64), dtype=torch.int)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(1, 1, 3, 1)\n        self.bn = torch.nn.BatchNorm2d(1, affine=False)\n        self.conv2 = torch.nn.Conv2d(1, 1, 3, 1)\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn(x)\n        x = self.conv2(x)\n        return x\n# Inputs to the model\nx = torch.randn(1, 1, 4, 4)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(8, 8, 2)\n        self.bn = torch.nn.BatchNorm2d(8, eps=1e-5)\n        self.conv2 = torch.nn.Conv2d(8, 8, 2)\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn(x)\n        x = self.conv2(x)\n        x = self.bn(x)\n        return x\n# Inputs to the model\nx = torch.randn(1, 8, 4, 4)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 3, 1)\n        self.conv1 = torch.nn.Conv2d(3, 3, 1)\n        self.bn = torch.nn.BatchNorm2d(3)\n    def forward(self, x):\n        v = self.conv(x)\n        v1 = self.conv1(v)\n        v2 = self.bn(v)\n        v3 = self.conv1(v2)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 3, 3, 3)\n"
            ],
            "g_time": 12.953015804290771
        }
    }
}
{
    "opt": "unary=13",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(4, 8, bias=False)\n        self.sigmoid = torch.nn.Sigmoid()\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = self.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 4)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(16, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(120, 10)\n        self.sigmoid = torch.nn.Sigmoid()\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = self.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 120)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc1 = torch.nn.Linear(10, 32)\n        self.fc2 = torch.nn.Linear(32, 10)\n \n    def forward(self, x1):\n        v1 = self.fc1(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        v4 = self.fc2(v3)\n        return v4\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 10)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 4)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(8, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(8, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n \n    def forward(self, x1):\n        m1 = self.linear(x1)\n        m2 = torch.nn.Sigmoid()(m1)\n        m3 = m1 * m2\n        return m3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(8, 8)\n        self.sigmoid = torch.nn.Sigmoid()\n \n    def forward(self, x):\n        v1 = self.linear(x)\n        v2 = self.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx = torch.randn(3, 8)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(4, 8, bias=False)\n        self.sigmoid = torch.nn.Sigmoid()\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = self.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 4)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(16, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(120, 10)\n        self.sigmoid = torch.nn.Sigmoid()\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = self.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 120)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc1 = torch.nn.Linear(10, 32)\n        self.fc2 = torch.nn.Linear(32, 10)\n \n    def forward(self, x1):\n        v1 = self.fc1(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        v4 = self.fc2(v3)\n        return v4\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 10)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 4)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(8, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(8, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n \n    def forward(self, x1):\n        m1 = self.linear(x1)\n        m2 = torch.nn.Sigmoid()(m1)\n        m3 = m1 * m2\n        return m3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(8, 8)\n        self.sigmoid = torch.nn.Sigmoid()\n \n    def forward(self, x):\n        v1 = self.linear(x)\n        v2 = self.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx = torch.randn(3, 8)\n"
            ],
            "g_time": 6.4009692668914795
        }
    }
}
{
    "opt": "binary_unary=0",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.conv1 = torch.nn.Conv2d(64, 64, 7, stride=1, padding=3)\n        self.conv2 = torch.nn.Conv2d(64, 64, 13, stride=1, padding=6)\n        self.conv3 = torch.nn.Conv2d(64, 64, 23, stride=1, padding=11)\n    def forward(self, x1, x2, x3):\n        v1 = self.conv1(x1)\n        v2 = v1 + x1\n        v3 = torch.relu(v2)\n        v4 = self.conv2(v3)\n        v5 = v4 + x2\n        v6 = torch.relu(v5)\n        v7 = self.conv3(v6)\n        v8 = v7 + x2\n        v9 = torch.relu(v8)\n        return v9\n# Inputs to the model\nx1 = torch.randn(1, 64, 128, 128)\nx2 = torch.randn(1, 64, 128, 128)\nx3 = torch.randn(1, 64, 128, 128)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 3, 7, stride=1, padding=3)\n        self.conv2 = torch.nn.Conv2d(3, 3, 7, stride=1, padding=3)\n        self.conv3 = torch.nn.Conv2d(3, 3, 7, stride=1, padding=3)\n        def forward(self, x1, x2, x3):\n            v1 = self.conv1(x1)\n            v2 = self.conv3(x2)\n            v3 = v1 + v2\n            v4 = torch.relu(v3)\n            v5 = self.conv2(v4)\n            v6 = v5 + x3\n            v7 = torch.relu(v6)\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\nx2 = torch.randn(1, 3, 64, 64)\nx3 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(16, 16, 7, stride=1, padding=3)\n        self.conv2 = torch.nn.Conv2d(16, 16, 7, stride=1, padding=3)\n        self.conv3 = torch.nn.Conv2d(16, 16, 7, stride=1, padding=3)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = torch.relu(v1)\n        v3 = self.conv2(v2)\n        v4 = torch.relu(v3)\n        v5 = self.conv3(v4)\n        v6 = torch.relu(v5)\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 16, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(16, 16, 7, stride=1, padding=3)\n        self.conv2 = torch.nn.Conv2d(16, 16, 7, stride=1, padding=3)\n        self.conv3 = torch.nn.Conv2d(16, 16, 7, stride=1, padding=3)\n    def forward(self, x, x2):\n        v1 = self.conv1(x)\n        v2 = torch.relu(v1)\n        v3 = v2 + x2\n        v4 = torch.relu(v3)\n        v5 = self.conv2(v4)\n        v6 = torch.relu(v5)\n        v7 = v6 + x\n        v8 = torch.relu(v7)\n        return v8\n# Inputs to the model\nx = torch.randn(1, 16, 64, 64)\nx2 = torch.randn(1, 16, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(16, 16, 7, stride=1, padding=3)\n        self.conv2 = torch.nn.Conv2d(16, 16, 7, stride=1, padding=3)\n        self.conv3 = torch.nn.Conv2d(16, 16, 7, stride=1, padding=3)\n    def forward(self, x1, x2):\n        v1 = self.conv1(x1)\n        v2 = v1 + x1\n        v3 = torch.relu(v2)\n        v4 = self.conv2(x2)\n        v5 = v4 + x2\n        v6 = torch.relu(v5)\n        v7 = self.conv3(v6)\n        return v7\n# Inputs to the model\nx1 = torch.randn(1, 16, 64, 64)\nx2 = torch.randn(1, 16, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(16, 16, 7, stride=1, padding=3)\n        self.conv2 = torch.nn.Conv2d(16, 16, 7, stride=1, padding=3)\n        self.conv3 = torch.nn.Conv2d(16, 16, 7, stride=1, padding=3)\n    def forward(self, x1, x2):\n        v1 = self.conv1(x1)\n        v2 = v1 + x1\n        v3 = torch.relu(v2)\n        v4 = self.conv2(v3)\n        v5 = v4 + x2\n        v6 = torch.relu(v5)\n        v7 = v4 + x1\n        v8 = torch.relu(v7)\n        v9 = self.conv3(v8)\n        return v9\n# Inputs to the model\nx1 = torch.randn(1, 16, 64, 64)\nx2 = torch.randn(1, 16, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(16, 16, 7, stride=1, padding=3)\n        self.conv2 = torch.nn.Conv2d(16, 16, 7, stride=1, padding=3)\n        self.conv3 = torch.nn.Conv2d(16, 16, 5, stride=1, padding=2)\n    def forward(self, x1, x2):\n        v1 = self.conv1(x1)\n        v2 = v1 + x1\n        v3 = torch.relu(v2)\n        v4 = self.conv2(v3)\n        v5 = v4 + x1\n        v6 = torch.relu(v5)\n        v7 = self.conv3(v6)\n        return v7\n# Inputs to the model\nx1 = torch.randn(1, 16, 64, 64)\nx2 = torch.randn(1, 16, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(16, 16, 7, stride=1, padding=3)\n        self.conv2 = torch.nn.Conv2d(16, 16, 7, stride=1, padding=3)\n        self.conv3 = torch.nn.Conv2d(16, 16, 7, stride=1, padding=3)\n    def forward(self, x1, x2, x3):\n        v1 = x1.mean(dim=0, keepdim=True)\n        v2 = x1.mean(dim=2, keepdim=False)\n        v3 = x1.mean(dim=3, keepdim=True)\n        v4 = x2.flatten(start_dim=2, end_dim=-1)\n        v5 = x3.flatten(end_dim=2)\n        v6 = self.conv1(x1)\n        v7 = self.conv2(v1)\n        v8 = self.conv3(v2)\n        v9 = v7 + v8\n        v10 = torch.relu(v9)\n        v11 = self.conv1(x2)\n        v12 = self.conv2(v4)\n        v13 = self.conv3(v3)\n        v14 = v12 + v13\n        v15 = torch.relu(v14)\n        v16 = self.conv1(x3)\n        v17 = self.conv2(v5)\n        v18 = self.conv3(v16)\n        v19 = v17 + v18\n        v20 = torch.relu(v19)\n        return v20\n# Inputs to the model\nx1 = torch.randn(16, 1, 128, 128)\nx2 = torch.randn(16, 3, 16, 16)\nx3 = torch.randn(16, 1, 32, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(16, 16, 7, stride=1, padding=3)\n        self.conv2 = torch.nn.Conv2d(16, 16, 7, stride=1, padding=3)\n        self.conv3 = torch.nn.Conv2d(16, 16, 7, stride=1, padding=3)\n    def forward(self, x1, x2, x3):\n        v1 = self.conv1(x1)\n        v2 = v1 + x2\n        v3 = torch.relu(v2)\n        v4 = self.conv2(v3)\n        v5 = v4 + x3\n        v6 = torch.relu(v5)\n        v7 = self.conv3(x1)\n        return v7\n# Inputs to the model\nx1 = torch.randn(1, 16, 64, 64)\nx2 = torch.randn(1, 16, 64, 64)\nx3 = torch.randn(1, 16, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = torch.randn(1, 16, 64, 64)\n        v7 = v1 + v2\n        return v7\n# Inputs to the model\nx1 = torch.randn(1, 16, 64, 64)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.conv1 = torch.nn.Conv2d(64, 64, 7, stride=1, padding=3)\n        self.conv2 = torch.nn.Conv2d(64, 64, 13, stride=1, padding=6)\n        self.conv3 = torch.nn.Conv2d(64, 64, 23, stride=1, padding=11)\n    def forward(self, x1, x2, x3):\n        v1 = self.conv1(x1)\n        v2 = v1 + x1\n        v3 = torch.relu(v2)\n        v4 = self.conv2(v3)\n        v5 = v4 + x2\n        v6 = torch.relu(v5)\n        v7 = self.conv3(v6)\n        v8 = v7 + x2\n        v9 = torch.relu(v8)\n        return v9\n# Inputs to the model\nx1 = torch.randn(1, 64, 128, 128)\nx2 = torch.randn(1, 64, 128, 128)\nx3 = torch.randn(1, 64, 128, 128)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 3, 7, stride=1, padding=3)\n        self.conv2 = torch.nn.Conv2d(3, 3, 7, stride=1, padding=3)\n        self.conv3 = torch.nn.Conv2d(3, 3, 7, stride=1, padding=3)\n        def forward(self, x1, x2, x3):\n            v1 = self.conv1(x1)\n            v2 = self.conv3(x2)\n            v3 = v1 + v2\n            v4 = torch.relu(v3)\n            v5 = self.conv2(v4)\n            v6 = v5 + x3\n            v7 = torch.relu(v6)\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\nx2 = torch.randn(1, 3, 64, 64)\nx3 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(16, 16, 7, stride=1, padding=3)\n        self.conv2 = torch.nn.Conv2d(16, 16, 7, stride=1, padding=3)\n        self.conv3 = torch.nn.Conv2d(16, 16, 7, stride=1, padding=3)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = torch.relu(v1)\n        v3 = self.conv2(v2)\n        v4 = torch.relu(v3)\n        v5 = self.conv3(v4)\n        v6 = torch.relu(v5)\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 16, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(16, 16, 7, stride=1, padding=3)\n        self.conv2 = torch.nn.Conv2d(16, 16, 7, stride=1, padding=3)\n        self.conv3 = torch.nn.Conv2d(16, 16, 7, stride=1, padding=3)\n    def forward(self, x, x2):\n        v1 = self.conv1(x)\n        v2 = torch.relu(v1)\n        v3 = v2 + x2\n        v4 = torch.relu(v3)\n        v5 = self.conv2(v4)\n        v6 = torch.relu(v5)\n        v7 = v6 + x\n        v8 = torch.relu(v7)\n        return v8\n# Inputs to the model\nx = torch.randn(1, 16, 64, 64)\nx2 = torch.randn(1, 16, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(16, 16, 7, stride=1, padding=3)\n        self.conv2 = torch.nn.Conv2d(16, 16, 7, stride=1, padding=3)\n        self.conv3 = torch.nn.Conv2d(16, 16, 7, stride=1, padding=3)\n    def forward(self, x1, x2):\n        v1 = self.conv1(x1)\n        v2 = v1 + x1\n        v3 = torch.relu(v2)\n        v4 = self.conv2(x2)\n        v5 = v4 + x2\n        v6 = torch.relu(v5)\n        v7 = self.conv3(v6)\n        return v7\n# Inputs to the model\nx1 = torch.randn(1, 16, 64, 64)\nx2 = torch.randn(1, 16, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(16, 16, 7, stride=1, padding=3)\n        self.conv2 = torch.nn.Conv2d(16, 16, 7, stride=1, padding=3)\n        self.conv3 = torch.nn.Conv2d(16, 16, 7, stride=1, padding=3)\n    def forward(self, x1, x2):\n        v1 = self.conv1(x1)\n        v2 = v1 + x1\n        v3 = torch.relu(v2)\n        v4 = self.conv2(v3)\n        v5 = v4 + x2\n        v6 = torch.relu(v5)\n        v7 = v4 + x1\n        v8 = torch.relu(v7)\n        v9 = self.conv3(v8)\n        return v9\n# Inputs to the model\nx1 = torch.randn(1, 16, 64, 64)\nx2 = torch.randn(1, 16, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(16, 16, 7, stride=1, padding=3)\n        self.conv2 = torch.nn.Conv2d(16, 16, 7, stride=1, padding=3)\n        self.conv3 = torch.nn.Conv2d(16, 16, 5, stride=1, padding=2)\n    def forward(self, x1, x2):\n        v1 = self.conv1(x1)\n        v2 = v1 + x1\n        v3 = torch.relu(v2)\n        v4 = self.conv2(v3)\n        v5 = v4 + x1\n        v6 = torch.relu(v5)\n        v7 = self.conv3(v6)\n        return v7\n# Inputs to the model\nx1 = torch.randn(1, 16, 64, 64)\nx2 = torch.randn(1, 16, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(16, 16, 7, stride=1, padding=3)\n        self.conv2 = torch.nn.Conv2d(16, 16, 7, stride=1, padding=3)\n        self.conv3 = torch.nn.Conv2d(16, 16, 7, stride=1, padding=3)\n    def forward(self, x1, x2, x3):\n        v1 = x1.mean(dim=0, keepdim=True)\n        v2 = x1.mean(dim=2, keepdim=False)\n        v3 = x1.mean(dim=3, keepdim=True)\n        v4 = x2.flatten(start_dim=2, end_dim=-1)\n        v5 = x3.flatten(end_dim=2)\n        v6 = self.conv1(x1)\n        v7 = self.conv2(v1)\n        v8 = self.conv3(v2)\n        v9 = v7 + v8\n        v10 = torch.relu(v9)\n        v11 = self.conv1(x2)\n        v12 = self.conv2(v4)\n        v13 = self.conv3(v3)\n        v14 = v12 + v13\n        v15 = torch.relu(v14)\n        v16 = self.conv1(x3)\n        v17 = self.conv2(v5)\n        v18 = self.conv3(v16)\n        v19 = v17 + v18\n        v20 = torch.relu(v19)\n        return v20\n# Inputs to the model\nx1 = torch.randn(16, 1, 128, 128)\nx2 = torch.randn(16, 3, 16, 16)\nx3 = torch.randn(16, 1, 32, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(16, 16, 7, stride=1, padding=3)\n        self.conv2 = torch.nn.Conv2d(16, 16, 7, stride=1, padding=3)\n        self.conv3 = torch.nn.Conv2d(16, 16, 7, stride=1, padding=3)\n    def forward(self, x1, x2, x3):\n        v1 = self.conv1(x1)\n        v2 = v1 + x2\n        v3 = torch.relu(v2)\n        v4 = self.conv2(v3)\n        v5 = v4 + x3\n        v6 = torch.relu(v5)\n        v7 = self.conv3(x1)\n        return v7\n# Inputs to the model\nx1 = torch.randn(1, 16, 64, 64)\nx2 = torch.randn(1, 16, 64, 64)\nx3 = torch.randn(1, 16, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = torch.randn(1, 16, 64, 64)\n        v7 = v1 + v2\n        return v7\n# Inputs to the model\nx1 = torch.randn(1, 16, 64, 64)\n"
            ],
            "g_time": 19.634718656539917
        }
    }
}
{
    "opt": "unary=5",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(12, 24, 2, stride=2, padding=0, bias=True)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v3 = self.conv_transpose.weight * 0.7071067811865476\n        v5 = torch.erf(v3)\n        v6 = v5 + 0.5\n        v7 = v1 * v6\n        return v7\n# Inputs to the model\nx1 = torch.randn(1, 12, 32, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(4, 4, 1, stride=1, padding=0)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 4, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(8, 16, 4, stride=4, padding=0)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n\n# Inputs to the model\nx1 = torch.randn(1, 8, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(6, 6, 5, stride=5, padding=0)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 6, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(6, 12, 1, stride=4, dilation=3)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 6, 16, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(18, 18, 5, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 18, 20, 20)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(30, 15, 31, stride=3, padding=0)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 30, 201, 201)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(1, 1, 1, stride=1)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 1, 16, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(10, 24, 8, stride=4, padding=2, group=6)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 10, 2048, 2048)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(15, 15, 21, stride=4, padding=14, dilation=4)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 15, 101, 101)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(12, 24, 2, stride=2, padding=0, bias=True)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v3 = self.conv_transpose.weight * 0.7071067811865476\n        v5 = torch.erf(v3)\n        v6 = v5 + 0.5\n        v7 = v1 * v6\n        return v7\n# Inputs to the model\nx1 = torch.randn(1, 12, 32, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(4, 4, 1, stride=1, padding=0)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 4, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(8, 16, 4, stride=4, padding=0)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n\n# Inputs to the model\nx1 = torch.randn(1, 8, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(6, 6, 5, stride=5, padding=0)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 6, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(6, 12, 1, stride=4, dilation=3)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 6, 16, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(18, 18, 5, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 18, 20, 20)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(30, 15, 31, stride=3, padding=0)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 30, 201, 201)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(1, 1, 1, stride=1)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 1, 16, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(10, 24, 8, stride=4, padding=2, group=6)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 10, 2048, 2048)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(15, 15, 21, stride=4, padding=14, dilation=4)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 15, 101, 101)\n"
            ],
            "g_time": 8.285894632339478
        }
    }
}
{
    "opt": "cat_addmm",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layers = nn.Linear(1, 5)\n    def forward(self, x):\n        x = self.layers(x)\n        x = torch.stack((x, x, x, x, x), dim=1)\n        x = torch.cat((x, x, x, x, x), dim=1)\n        return x\n# Inputs to the model\nx = torch.randn(1, 1)\n",
                "\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layers = nn.Linear(2, 4)\n    def forward(self, x):\n        x = self.layers(x)\n        return torch.cat((x, torch.zeros(4))[:, :1], dim=1)\n# Inputs to the model\nx = torch.randn(2, 2)\n",
                "\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layers = nn.Linear(6, 8)\n    def forward(self, x):\n        x = self.layers.linear(x)\n        x = torch.stack((x, x, x, x), dim=1).flatten(1)\n        return x\n# Inputs to the model\nx = torch.randn(2, 6)\n",
                "\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layers = nn.Linear(3, 1)\n    def forward(self, x):\n        x = self.layers(x)\n        x = torch.stack((x, x), dim=0)\n        x = x.flatten(1)\n        x = x.transpose(1, 0)\n        x = x[0]\n        return x\n# Inputs to the model\nx = torch.randn(2, 3)\n",
                "\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layers = nn.Linear(3, 6)\n    def forward(self, x):\n        x = self.layers(x)\n        return torch.cat((x[..., :1] + x[..., 1:2] + x[..., 2:], x), -1)\n# Inputs to the model\nx = torch.randn(2, 3)\n",
                "\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layers = nn.Linear(4, 3)\n    def forward(self, x):\n        x = self.layers(x)\n        x = torch.stack((x, x), dim=2).flatten(2)\n        return x\n# Inputs to the model\nx = torch.randn(2, 4)\n",
                "\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layers = nn.Linear(5, 4)\n        self.layers2 = nn.Linear(3, 4)\n    def forward(self, x):\n        x = self.layers(x)\n        x = x.expand(2, 5, 4)\n        x = self.layers2(x)\n        x = torch.cat((x, x), dim=0)\n        return x\n# Inputs to the model\nx = torch.randn(3, 5)\n",
                "\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layers = nn.Linear(4, 4, bias=False)\n    def forward(self, x):\n        x = self.layers(x)\n        x = torch.stack((x, x), dim=-1)\n        return x\n# Inputs to the model\nx = torch.randn(2, 4)\n",
                "\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layers = nn.Linear(5, 3)\n    def forward(self, x):\n        x = self.layers(x)\n        x = torch.stack((x, x, x), dim=1)\n        return x\n# Inputs to the model\nx = torch.randn(3, 5)\n",
                "\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layers = nn.Linear(2, 8)\n    def forward(self, x):\n        x = self.layers(x)\n        x = torch.cat((x, x, x, x), dim=1)\n        return x\n# Inputs to the model\nx = torch.randn(1, 2)\n"
            ],
            "code": [
                "\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layers = nn.Linear(1, 5)\n    def forward(self, x):\n        x = self.layers(x)\n        x = torch.stack((x, x, x, x, x), dim=1)\n        x = torch.cat((x, x, x, x, x), dim=1)\n        return x\n# Inputs to the model\nx = torch.randn(1, 1)\n",
                "\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layers = nn.Linear(2, 4)\n    def forward(self, x):\n        x = self.layers(x)\n        return torch.cat((x, torch.zeros(4))[:, :1], dim=1)\n# Inputs to the model\nx = torch.randn(2, 2)\n",
                "\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layers = nn.Linear(6, 8)\n    def forward(self, x):\n        x = self.layers.linear(x)\n        x = torch.stack((x, x, x, x), dim=1).flatten(1)\n        return x\n# Inputs to the model\nx = torch.randn(2, 6)\n",
                "\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layers = nn.Linear(3, 1)\n    def forward(self, x):\n        x = self.layers(x)\n        x = torch.stack((x, x), dim=0)\n        x = x.flatten(1)\n        x = x.transpose(1, 0)\n        x = x[0]\n        return x\n# Inputs to the model\nx = torch.randn(2, 3)\n",
                "\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layers = nn.Linear(3, 6)\n    def forward(self, x):\n        x = self.layers(x)\n        return torch.cat((x[..., :1] + x[..., 1:2] + x[..., 2:], x), -1)\n# Inputs to the model\nx = torch.randn(2, 3)\n",
                "\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layers = nn.Linear(4, 3)\n    def forward(self, x):\n        x = self.layers(x)\n        x = torch.stack((x, x), dim=2).flatten(2)\n        return x\n# Inputs to the model\nx = torch.randn(2, 4)\n",
                "\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layers = nn.Linear(5, 4)\n        self.layers2 = nn.Linear(3, 4)\n    def forward(self, x):\n        x = self.layers(x)\n        x = x.expand(2, 5, 4)\n        x = self.layers2(x)\n        x = torch.cat((x, x), dim=0)\n        return x\n# Inputs to the model\nx = torch.randn(3, 5)\n",
                "\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layers = nn.Linear(4, 4, bias=False)\n    def forward(self, x):\n        x = self.layers(x)\n        x = torch.stack((x, x), dim=-1)\n        return x\n# Inputs to the model\nx = torch.randn(2, 4)\n",
                "\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layers = nn.Linear(5, 3)\n    def forward(self, x):\n        x = self.layers(x)\n        x = torch.stack((x, x, x), dim=1)\n        return x\n# Inputs to the model\nx = torch.randn(3, 5)\n",
                "\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layers = nn.Linear(2, 8)\n    def forward(self, x):\n        x = self.layers(x)\n        x = torch.cat((x, x, x, x), dim=1)\n        return x\n# Inputs to the model\nx = torch.randn(1, 2)\n"
            ],
            "g_time": 5.128612995147705
        }
    }
}
{
    "opt": "sfdp=4",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n    def forward(self, Q, k, v, mask):\n        qk = Q @ k.transpose(-2, -1) / math.sqrt(Q.size(-1))\n        qk = qk + mask\n        attn_weight = torch.softmax(qk, dim=-1)\n        output = attn_weight @ v\n        return output\n# Inputs to the model\nQ = torch.randn(1, 64, 56, 56)\nk = torch.randn(1, 64, 56, 56)\nv = torch.randn(1, 64, 56, 56)\nmask = (torch.rand(1, 56, 56) > 0.7).fill_(-1000000000.0)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, Q, K, v, m4):\n        qk = Q @ K.transpose(-2, -1) / math.sqrt(Q.size(-1))\n        qk = qk + m4\n        attn_weight = torch.softmax(qk, dim=-1)\n        output = attn_weight @ v\n        return output\n# Inputs to the model\nQ = torch.randn(1, 64, 56, 56)\nK = torch.randn(1, 64, 56, 56)\nV = torch.randn(1, 64, 56, 56)\nmask = (torch.rand(1, 56, 56) > 0.7).fill_(-1000000000.0)\n",
                "\nmodel = Model()\nprint(model(Q, k, v, mask).shape)\n# Inputs to the model\nQ = torch.randn(64, 1, 91, 91)\nk = torch.randn(64, 1, 60, 60)\nv = torch.randn(64, 1, 60, 60)\nmask = (torch.rand(64, 91, 91) > 0.7).fill_(-1000000000.0)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.dense2 = torch.nn.Linear(100, 100)\n    def forward(self, x, x2, z, m4):\n        qk = x @ x2.transpose(-2, -1) / math.sqrt(x.size(-1))\n        qk = qk + m4\n        attn_weight = torch.softmax(qk, dim=-1)\n        output = attn_weight @ z\n        return output\n# Inputs to the model\nx = torch.randn(1, 1024, 14, 14)\nx2 = torch.randn(1, 1024, 14, 14)\nz = torch.randn(1, 1024, 14, 14)\nmask = (torch.rand(1, 14, 14) > 0.7).fill_(-1000000000.0)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, Q, K, V, m2):\n        qk = Q @ K.transpose(-2, -1) / math.sqrt(Q.size(-1))\n        qk = qk + m2\n        attn_weight = torch.softmax(qk, dim=-1)\n        output = attn_weight @ V\n        return output\n# Inputs to the model\nQ = torch.randn(1, 3, 100, 100)\nK = torch.randn(1, 3, 100, 100)\nV = torch.randn(1, 3, 100, 100)\nmask = (torch.rand(1, 100, 100) > 0.7).fill_(-1000000000.0)\n",
                "\nclass Model(torch.nn.Module):\n    def forward(self, q, c, k, v2, mask):\n        c = torch.sigmoid(q @ k.transpose(-2, -1))\n        qk = Q @ k.transpose(-2, -1) / math.sqrt(q.size(-1))\n        qk = qk + m4\n        attn_weight = torch.softmax(qk, dim=-1)\n        output = attn_weight @ v3\n        return output\n# Inputs to the model\nq = torch.randn(1, 64, 56, 56)\nc = torch.randn(1, 512, 8, 8)\nk = torch.randn(1, 64, 56, 56)\nv = torch.randn(1, 64, 56, 56)\nmask = (torch.rand(1, 56, 56) > 0.7).fill_(-1000000000.0)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x, x2, z, mask):\n        qk = x @ x2.transpose(-2, -1) / math.sqrt(x.size(-1))\n        qk = qk + mask\n        attn_weight = torch.softmax(qk, dim=-1)\n        output = attn_weight @ z\n        return output\nQ = torch.randn(1, 64, 56, 56)\nK = torch.randn(1, 64, 56, 56)\nV = torch.randn(1, 64, 56, 56)\nmask = (torch.rand(1, 56, 56) > 0.7).fill_(-1000000000.0)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, query, key, value, mask):\n        qk = query @ key.transpose(-2, -1) / math.sqrt(query.size(-1))\n        qk = qk + mask\n        attn_weight = torch.softmax(qk, dim=-1)\n        output = attn_weight @ value\n        return output\n# Inputs to the model\nquery = torch.randn(1, 64, 56, 56)\nkey = torch.randn(1, 64, 56, 56)\nvalue = torch.randn(1, 64, 56, 56)\nmask = (torch.rand(1, 56, 56) > 0.7).fill_(-1000000000.0)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, Q3, K3, V4, mask66):\n        qk = Q3 @ K3.transpose(-2, -1) / math.sqrt(Q3.size(-1))\n        qk = qk + mask66\n        attn_weight = torch.softmax(qk, dim=-1)\n        output = attn_weight @ V4\n        return output\n# Inputs to the model\nq3 = torch.randn(1, 64, 56, 56)\nk3 = torch.randn(1, 64, 56, 56)\nv4 = torch.randn(1, 64, 56, 56)\nmask66 = (torch.rand(1, 56, 56) > 0.7).fill_(-1000000000.0)\n",
                "\nclass MyModel(torch.nn.Module):\n    def __init__(self):\n        super(MyModel, self).__init__()\n        self.m = torch.nn.Softmax(dim=-1)\n    def forward(self, query, value, key):\n        Q = query @ key.transpose(-2, -1)\n        A = torch.exp(Q) / math.sqrt(query.size(-1))\n        B = torch.exp(Q)\n        softmax = self.m(A.add(B)).mul(Q)\n        output = softmax @ value\n        return output\n# Input to the model\nQ = torch.randn(1, 64, 56, 56)\nK = torch.randn(1, 64, 56, 56)\nV = torch.randn(1, 64, 56, 56)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n    def forward(self, Q, k, v, mask):\n        qk = Q @ k.transpose(-2, -1) / math.sqrt(Q.size(-1))\n        qk = qk + mask\n        attn_weight = torch.softmax(qk, dim=-1)\n        output = attn_weight @ v\n        return output\n# Inputs to the model\nQ = torch.randn(1, 64, 56, 56)\nk = torch.randn(1, 64, 56, 56)\nv = torch.randn(1, 64, 56, 56)\nmask = (torch.rand(1, 56, 56) > 0.7).fill_(-1000000000.0)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, Q, K, v, m4):\n        qk = Q @ K.transpose(-2, -1) / math.sqrt(Q.size(-1))\n        qk = qk + m4\n        attn_weight = torch.softmax(qk, dim=-1)\n        output = attn_weight @ v\n        return output\n# Inputs to the model\nQ = torch.randn(1, 64, 56, 56)\nK = torch.randn(1, 64, 56, 56)\nV = torch.randn(1, 64, 56, 56)\nmask = (torch.rand(1, 56, 56) > 0.7).fill_(-1000000000.0)\n",
                "\nmodel = Model()\nprint(model(Q, k, v, mask).shape)\n# Inputs to the model\nQ = torch.randn(64, 1, 91, 91)\nk = torch.randn(64, 1, 60, 60)\nv = torch.randn(64, 1, 60, 60)\nmask = (torch.rand(64, 91, 91) > 0.7).fill_(-1000000000.0)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.dense2 = torch.nn.Linear(100, 100)\n    def forward(self, x, x2, z, m4):\n        qk = x @ x2.transpose(-2, -1) / math.sqrt(x.size(-1))\n        qk = qk + m4\n        attn_weight = torch.softmax(qk, dim=-1)\n        output = attn_weight @ z\n        return output\n# Inputs to the model\nx = torch.randn(1, 1024, 14, 14)\nx2 = torch.randn(1, 1024, 14, 14)\nz = torch.randn(1, 1024, 14, 14)\nmask = (torch.rand(1, 14, 14) > 0.7).fill_(-1000000000.0)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, Q, K, V, m2):\n        qk = Q @ K.transpose(-2, -1) / math.sqrt(Q.size(-1))\n        qk = qk + m2\n        attn_weight = torch.softmax(qk, dim=-1)\n        output = attn_weight @ V\n        return output\n# Inputs to the model\nQ = torch.randn(1, 3, 100, 100)\nK = torch.randn(1, 3, 100, 100)\nV = torch.randn(1, 3, 100, 100)\nmask = (torch.rand(1, 100, 100) > 0.7).fill_(-1000000000.0)\n",
                "\nclass Model(torch.nn.Module):\n    def forward(self, q, c, k, v2, mask):\n        c = torch.sigmoid(q @ k.transpose(-2, -1))\n        qk = Q @ k.transpose(-2, -1) / math.sqrt(q.size(-1))\n        qk = qk + m4\n        attn_weight = torch.softmax(qk, dim=-1)\n        output = attn_weight @ v3\n        return output\n# Inputs to the model\nq = torch.randn(1, 64, 56, 56)\nc = torch.randn(1, 512, 8, 8)\nk = torch.randn(1, 64, 56, 56)\nv = torch.randn(1, 64, 56, 56)\nmask = (torch.rand(1, 56, 56) > 0.7).fill_(-1000000000.0)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x, x2, z, mask):\n        qk = x @ x2.transpose(-2, -1) / math.sqrt(x.size(-1))\n        qk = qk + mask\n        attn_weight = torch.softmax(qk, dim=-1)\n        output = attn_weight @ z\n        return output\nQ = torch.randn(1, 64, 56, 56)\nK = torch.randn(1, 64, 56, 56)\nV = torch.randn(1, 64, 56, 56)\nmask = (torch.rand(1, 56, 56) > 0.7).fill_(-1000000000.0)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, query, key, value, mask):\n        qk = query @ key.transpose(-2, -1) / math.sqrt(query.size(-1))\n        qk = qk + mask\n        attn_weight = torch.softmax(qk, dim=-1)\n        output = attn_weight @ value\n        return output\n# Inputs to the model\nquery = torch.randn(1, 64, 56, 56)\nkey = torch.randn(1, 64, 56, 56)\nvalue = torch.randn(1, 64, 56, 56)\nmask = (torch.rand(1, 56, 56) > 0.7).fill_(-1000000000.0)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, Q3, K3, V4, mask66):\n        qk = Q3 @ K3.transpose(-2, -1) / math.sqrt(Q3.size(-1))\n        qk = qk + mask66\n        attn_weight = torch.softmax(qk, dim=-1)\n        output = attn_weight @ V4\n        return output\n# Inputs to the model\nq3 = torch.randn(1, 64, 56, 56)\nk3 = torch.randn(1, 64, 56, 56)\nv4 = torch.randn(1, 64, 56, 56)\nmask66 = (torch.rand(1, 56, 56) > 0.7).fill_(-1000000000.0)\n",
                "\nclass MyModel(torch.nn.Module):\n    def __init__(self):\n        super(MyModel, self).__init__()\n        self.m = torch.nn.Softmax(dim=-1)\n    def forward(self, query, value, key):\n        Q = query @ key.transpose(-2, -1)\n        A = torch.exp(Q) / math.sqrt(query.size(-1))\n        B = torch.exp(Q)\n        softmax = self.m(A.add(B)).mul(Q)\n        output = softmax @ value\n        return output\n# Input to the model\nQ = torch.randn(1, 64, 56, 56)\nK = torch.randn(1, 64, 56, 56)\nV = torch.randn(1, 64, 56, 56)\n"
            ],
            "g_time": 10.034775733947754
        }
    }
}
{
    "opt": "unary=29",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min_value=-0.7850, max_value=0.6033):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(9, 18, 2, stride=1, padding=0)\n        self.min_value = min_value\n        self.max_value = max_value\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = torch.clamp_min(v1, self.min_value)\n        v3 = torch.clamp_max(v2, self.max_value)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 9, 32, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min_value=1.8743, max_value=2.0694):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose3d(2, 61, 3, stride=2, padding=1, output_padding=2)\n        self.min_value = min_value\n        self.max_value = max_value\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = torch.clamp_min(v1, self.min_value)\n        v3 = torch.clamp_max(v2, self.max_value)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 2, 2, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min_value=0.0, max_value=0.1083):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(8, 10, 4, stride=1, padding=15)\n        self.min_value = min_value\n        self.max_value = max_value\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = torch.clamp_min(v1, self.min_value)\n        v3 = torch.clamp_max(v2, self.max_value)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 8, 255, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min_value=0.148, max_value=0.2036):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(1, 1, 2, stride=1, padding=1)\n        self.min_value = min_value\n        self.max_value = max_value\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = torch.clamp_min(v1, self.min_value)\n        v3 = torch.clamp_max(v2, self.max_value)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 1, 373, 86)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min_value=-1534033.4497, max_value=-2545448.8808):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(3, 10, 10, stride=2, padding=2)\n        self.min_value = min_value\n        self.max_value = max_value\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = torch.clamp_min(v1, self.min_value)\n        v3 = torch.clamp_max(v2, self.max_value)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 3, 5536, 24576)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min_value=-0.6795, max_value=0.4627):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(83, 39, 1, stride=1, padding=0)\n        self.min_value = min_value\n        self.max_value = max_value\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = torch.clamp_min(v1, self.min_value)\n        v3 = torch.clamp_max(v2, self.max_value)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 83, 3877, 511)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min_value=-7.9939, max_value=12.4853):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(7, 9, 2, stride=1, padding=0)\n        self.min_value = min_value\n        self.max_value = max_value\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = torch.clamp_min(v1, self.min_value)\n        v3 = torch.clamp_max(v2, self.max_value)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 7, 32, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min_value=0.7025, max_value=0.7357):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(6, 5, 1, stride=1, padding=0)\n        self.min_value = min_value\n        self.max_value = max_value\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = torch.clamp_min(v1, self.min_value)\n        v3 = torch.clamp_max(v2, self.max_value)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 6, 32, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min_value=75.0277, max_value=152.4603):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(2, 26, (3, 6), stride=(4, 5), padding=(1, 2))\n        self.min_value = min_value\n        self.max_value = max_value\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = torch.clamp_min(v1, self.min_value)\n        v3 = torch.clamp_max(v2, self.max_value)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 2, 63, 78)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min_value=0.0493, max_value=0.2864):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(48, 49, 5, stride=2, padding=2)\n        self.min_value = min_value\n        self.max_value = max_value\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = torch.clamp_min(v1, self.min_value)\n        v3 = torch.clamp_max(v2, self.max_value)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 48, 256, 256)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min_value=-0.7850, max_value=0.6033):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(9, 18, 2, stride=1, padding=0)\n        self.min_value = min_value\n        self.max_value = max_value\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = torch.clamp_min(v1, self.min_value)\n        v3 = torch.clamp_max(v2, self.max_value)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 9, 32, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min_value=1.8743, max_value=2.0694):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose3d(2, 61, 3, stride=2, padding=1, output_padding=2)\n        self.min_value = min_value\n        self.max_value = max_value\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = torch.clamp_min(v1, self.min_value)\n        v3 = torch.clamp_max(v2, self.max_value)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 2, 2, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min_value=0.0, max_value=0.1083):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(8, 10, 4, stride=1, padding=15)\n        self.min_value = min_value\n        self.max_value = max_value\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = torch.clamp_min(v1, self.min_value)\n        v3 = torch.clamp_max(v2, self.max_value)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 8, 255, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min_value=0.148, max_value=0.2036):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(1, 1, 2, stride=1, padding=1)\n        self.min_value = min_value\n        self.max_value = max_value\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = torch.clamp_min(v1, self.min_value)\n        v3 = torch.clamp_max(v2, self.max_value)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 1, 373, 86)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min_value=-1534033.4497, max_value=-2545448.8808):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(3, 10, 10, stride=2, padding=2)\n        self.min_value = min_value\n        self.max_value = max_value\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = torch.clamp_min(v1, self.min_value)\n        v3 = torch.clamp_max(v2, self.max_value)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 3, 5536, 24576)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min_value=-0.6795, max_value=0.4627):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(83, 39, 1, stride=1, padding=0)\n        self.min_value = min_value\n        self.max_value = max_value\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = torch.clamp_min(v1, self.min_value)\n        v3 = torch.clamp_max(v2, self.max_value)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 83, 3877, 511)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min_value=-7.9939, max_value=12.4853):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(7, 9, 2, stride=1, padding=0)\n        self.min_value = min_value\n        self.max_value = max_value\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = torch.clamp_min(v1, self.min_value)\n        v3 = torch.clamp_max(v2, self.max_value)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 7, 32, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min_value=0.7025, max_value=0.7357):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(6, 5, 1, stride=1, padding=0)\n        self.min_value = min_value\n        self.max_value = max_value\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = torch.clamp_min(v1, self.min_value)\n        v3 = torch.clamp_max(v2, self.max_value)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 6, 32, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min_value=75.0277, max_value=152.4603):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(2, 26, (3, 6), stride=(4, 5), padding=(1, 2))\n        self.min_value = min_value\n        self.max_value = max_value\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = torch.clamp_min(v1, self.min_value)\n        v3 = torch.clamp_max(v2, self.max_value)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 2, 63, 78)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min_value=0.0493, max_value=0.2864):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(48, 49, 5, stride=2, padding=2)\n        self.min_value = min_value\n        self.max_value = max_value\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = torch.clamp_min(v1, self.min_value)\n        v3 = torch.clamp_max(v2, self.max_value)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 48, 256, 256)\n"
            ],
            "g_time": 8.826824426651001
        }
    }
}
{
    "opt": "binary=8",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n        self.conv2 = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = self.conv2(x1)\n        v3 = v1 + v2\n        return v1\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n        self.conv2 = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n        self.relu1 = torch.nn.ReLU()\n        self.relu2 = torch.nn.ReLU()\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = self.conv2(x1)\n        v3 = v1 + v2\n        v4 = self.relu1(v3)\n        v5 = self.relu2(v3)\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 8, 5, stride=1, padding=2)\n        self.conv2 = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = self.conv2(x1)\n        v3 = F.relu(v1)\n        v4 = v2 + v3\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n        self.conv2 = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n        self.conv3 = torch.nn.Conv2d(8, 3, 1, stride=1, padding=1)\n    def forward(self, x1, x2):\n        v1 = self.conv1(x1)\n        v2 = self.conv2(x2)\n        v3 = v1 + v2\n        v4 = self.conv3(v3)\n        v5 = self.conv3(v3)\n        return v4 + v5\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\nx2 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n        self.bn1 = torch.nn.BatchNorm2d(8)\n        self.bn2 = torch.nn.BatchNorm2d(3)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = self.bn1(v1)\n        v3 = self.bn2(x1)\n        return v2, v3\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, width):\n        super().__init__()\n        self.linear1 = torch.nn.Linear(width, width)\n        self.linear2 = torch.nn.Linear(width, width)\n        self.bn1 = torch.nn.BatchNorm1d(width)\n        self.bn2 = torch.nn.BatchNorm1d(width)\n    def forward(self, x):\n        v1 = self.bn1(self.linear1(x))\n        v2 = self.bn2(self.linear2(x))\n        return v1\n# Inputs to the model\nx1 = torch.randn(10, 1)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n        self.conv2 = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n        self.conv3 = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n        self.conv4 = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n    def forward(self, x1, x2, x3, x4):\n        v1 = self.conv1(x1)\n        v2 = self.conv2(x2)\n        v3 = self.conv3(x3)\n        v4 = self.conv4(x4)\n        v5 = v1 + v2 + v3 + v4\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\nx2 = torch.randn(1, 3, 64, 64)\nx3 = torch.randn(1, 3, 64, 64)\nx4 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n        self.conv2 = torch.nn.Conv2d(3, 8, 5, stride=2, padding=2)\n    def forward(self, x):\n        v = torch.ops.aten.max_pool2d(x, 1, [1], [1])\n        v1 = self.conv1(v)\n        v2 = self.conv2(v)\n        v3 = v1 + v2\n        return v3\n# Inputs to the model\nx = torch.randn(1, 3, 100, 100)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n        self.conv2 = torch.nn.Conv2d(3, 1, 1, stride=1, padding=1)\n    def forward(self, x1):\n        v = self.conv1(x1)\n        return self.conv2(v)\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n        self.bn1 = torch.nn.BatchNorm2d(8)\n        self.conv2 = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = self.conv2(x1)\n        v3 = v1 + v2\n        v4 = self.bn1(v3)\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n        self.conv2 = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = self.conv2(x1)\n        v3 = v1 + v2\n        return v1\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n        self.conv2 = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n        self.relu1 = torch.nn.ReLU()\n        self.relu2 = torch.nn.ReLU()\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = self.conv2(x1)\n        v3 = v1 + v2\n        v4 = self.relu1(v3)\n        v5 = self.relu2(v3)\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 8, 5, stride=1, padding=2)\n        self.conv2 = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = self.conv2(x1)\n        v3 = F.relu(v1)\n        v4 = v2 + v3\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n        self.conv2 = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n        self.conv3 = torch.nn.Conv2d(8, 3, 1, stride=1, padding=1)\n    def forward(self, x1, x2):\n        v1 = self.conv1(x1)\n        v2 = self.conv2(x2)\n        v3 = v1 + v2\n        v4 = self.conv3(v3)\n        v5 = self.conv3(v3)\n        return v4 + v5\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\nx2 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n        self.bn1 = torch.nn.BatchNorm2d(8)\n        self.bn2 = torch.nn.BatchNorm2d(3)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = self.bn1(v1)\n        v3 = self.bn2(x1)\n        return v2, v3\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, width):\n        super().__init__()\n        self.linear1 = torch.nn.Linear(width, width)\n        self.linear2 = torch.nn.Linear(width, width)\n        self.bn1 = torch.nn.BatchNorm1d(width)\n        self.bn2 = torch.nn.BatchNorm1d(width)\n    def forward(self, x):\n        v1 = self.bn1(self.linear1(x))\n        v2 = self.bn2(self.linear2(x))\n        return v1\n# Inputs to the model\nx1 = torch.randn(10, 1)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n        self.conv2 = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n        self.conv3 = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n        self.conv4 = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n    def forward(self, x1, x2, x3, x4):\n        v1 = self.conv1(x1)\n        v2 = self.conv2(x2)\n        v3 = self.conv3(x3)\n        v4 = self.conv4(x4)\n        v5 = v1 + v2 + v3 + v4\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\nx2 = torch.randn(1, 3, 64, 64)\nx3 = torch.randn(1, 3, 64, 64)\nx4 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n        self.conv2 = torch.nn.Conv2d(3, 8, 5, stride=2, padding=2)\n    def forward(self, x):\n        v = torch.ops.aten.max_pool2d(x, 1, [1], [1])\n        v1 = self.conv1(v)\n        v2 = self.conv2(v)\n        v3 = v1 + v2\n        return v3\n# Inputs to the model\nx = torch.randn(1, 3, 100, 100)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n        self.conv2 = torch.nn.Conv2d(3, 1, 1, stride=1, padding=1)\n    def forward(self, x1):\n        v = self.conv1(x1)\n        return self.conv2(v)\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n        self.bn1 = torch.nn.BatchNorm2d(8)\n        self.conv2 = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = self.conv2(x1)\n        v3 = v1 + v2\n        v4 = self.bn1(v3)\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n"
            ],
            "g_time": 12.05418586730957
        }
    }
}
{
    "opt": "binary_unary=8",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(1, 16, 1, stride=1, padding=1)\n        self.conv2 = torch.nn.Conv2d(1, 16, (1, 1), stride=1, padding=0)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = self.conv2(x1)\n        v3 = self.conv2(x1)\n        v4 = torch.relu(v1 + v2 + v3)\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 1, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(1, 16, 3, stride=2, padding=1)\n        self.conv2 = torch.nn.Conv2d(16, 16, 1, stride=1, padding=1)\n        self.conv3 = torch.nn.Conv2d(16, 16, 1, stride=1, padding=1)\n        self.conv4 = torch.nn.Conv2d(16, 16, 1, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = self.conv2(v1) + self.conv3(v1)\n        v3 = torch.relu(v2 + self.conv4(v1))\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 1, 224, 224)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.pool1 = torch.nn.MaxPool2d((2, 2), stride=2)\n        self.pool2 = torch.nn.MaxPool2d((2, 2), stride=2)\n        self.conv1 = torch.nn.Conv2d(1, 8, 1, stride=1, padding=1)\n    def forward(self, x1):\n        t1 = self.pool1(x1)\n        t2 = self.pool2(x1)\n        t3 = self.conv1(x1)\n        t4 = t1 + t2 + t3\n        t5 = torch.relu(t4)\n        return t5\n# Inputs to the model\nx1 = torch.randn(1, 1, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(1, 8, 1, stride=1, padding=0, bias=False)\n        self.conv2 = torch.nn.Conv2d(1, 8, 1, stride=1, padding=0, groups=8, bias=False)\n        self.conv3 = torch.nn.Conv2d(1, 8, 1, stride=1, padding=0, dilation=2, padding_mode='zeros', groups=8, bias=False)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = self.conv2(x1)\n        v3 = self.conv3(x1)\n        v4 = v1 + v2 + v3\n        v5 = torch.relu(v4)\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 1, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 64, 7, stride=2, padding=3)\n        self.conv2 = torch.nn.Conv2d(3, 64, 7, stride=2, padding=3)\n        self.conv3 = torch.nn.Conv2d(3, 64, 7, stride=2, padding=3)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = self.conv2(x1)\n        v3 = self.conv3(x1)\n        v4 = v1 - v2 + v3\n        v5 = torch.relu(v4)\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 3, 224, 224)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(1, 18, 1, stride=1, padding=1)\n        self.conv2 = torch.nn.Conv2d(1, 18, 1, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = self.conv2(x1)\n        v3 = self.conv2(x1)\n        v4 = v1 + v2 + v3\n        v5 = torch.relu(v4)\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 1, 96, 96)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(1, 8, 3, stride=2, padding=1)\n        self.conv2 = torch.nn.Conv2d(1, 8, 3, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = self.conv2(x1)\n        v3 = torch.relu(v1 + v2)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 1, 28, 28)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(1, 8, 1, stride=1, padding=1)\n        self.conv2 = torch.nn.Conv2d(1, 8, 1, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = torch.sum(v1, [2, 3])\n        v3 = torch.relu(v2)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 1, 32, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(1, 16, 1, stride=1, padding=0)\n        self.conv2 = torch.nn.Conv2d(1, 16, 1, stride=1, depthwise=True, padding=4)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = self.conv2(x1)\n        v3 = torch.relu(v1 + v2)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 1, 8, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 12, 3, stride=1, padding=1)\n        self.conv2 = torch.nn.Conv2d(12, 24, (1, 1), stride=1, padding=0)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = self.conv2(v1)\n        v3 = torch.relu(v2)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 3, 128, 128)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(1, 16, 1, stride=1, padding=1)\n        self.conv2 = torch.nn.Conv2d(1, 16, (1, 1), stride=1, padding=0)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = self.conv2(x1)\n        v3 = self.conv2(x1)\n        v4 = torch.relu(v1 + v2 + v3)\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 1, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(1, 16, 3, stride=2, padding=1)\n        self.conv2 = torch.nn.Conv2d(16, 16, 1, stride=1, padding=1)\n        self.conv3 = torch.nn.Conv2d(16, 16, 1, stride=1, padding=1)\n        self.conv4 = torch.nn.Conv2d(16, 16, 1, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = self.conv2(v1) + self.conv3(v1)\n        v3 = torch.relu(v2 + self.conv4(v1))\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 1, 224, 224)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.pool1 = torch.nn.MaxPool2d((2, 2), stride=2)\n        self.pool2 = torch.nn.MaxPool2d((2, 2), stride=2)\n        self.conv1 = torch.nn.Conv2d(1, 8, 1, stride=1, padding=1)\n    def forward(self, x1):\n        t1 = self.pool1(x1)\n        t2 = self.pool2(x1)\n        t3 = self.conv1(x1)\n        t4 = t1 + t2 + t3\n        t5 = torch.relu(t4)\n        return t5\n# Inputs to the model\nx1 = torch.randn(1, 1, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(1, 8, 1, stride=1, padding=0, bias=False)\n        self.conv2 = torch.nn.Conv2d(1, 8, 1, stride=1, padding=0, groups=8, bias=False)\n        self.conv3 = torch.nn.Conv2d(1, 8, 1, stride=1, padding=0, dilation=2, padding_mode='zeros', groups=8, bias=False)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = self.conv2(x1)\n        v3 = self.conv3(x1)\n        v4 = v1 + v2 + v3\n        v5 = torch.relu(v4)\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 1, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 64, 7, stride=2, padding=3)\n        self.conv2 = torch.nn.Conv2d(3, 64, 7, stride=2, padding=3)\n        self.conv3 = torch.nn.Conv2d(3, 64, 7, stride=2, padding=3)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = self.conv2(x1)\n        v3 = self.conv3(x1)\n        v4 = v1 - v2 + v3\n        v5 = torch.relu(v4)\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 3, 224, 224)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(1, 18, 1, stride=1, padding=1)\n        self.conv2 = torch.nn.Conv2d(1, 18, 1, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = self.conv2(x1)\n        v3 = self.conv2(x1)\n        v4 = v1 + v2 + v3\n        v5 = torch.relu(v4)\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 1, 96, 96)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(1, 8, 3, stride=2, padding=1)\n        self.conv2 = torch.nn.Conv2d(1, 8, 3, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = self.conv2(x1)\n        v3 = torch.relu(v1 + v2)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 1, 28, 28)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(1, 8, 1, stride=1, padding=1)\n        self.conv2 = torch.nn.Conv2d(1, 8, 1, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = torch.sum(v1, [2, 3])\n        v3 = torch.relu(v2)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 1, 32, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(1, 16, 1, stride=1, padding=0)\n        self.conv2 = torch.nn.Conv2d(1, 16, 1, stride=1, depthwise=True, padding=4)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = self.conv2(x1)\n        v3 = torch.relu(v1 + v2)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 1, 8, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 12, 3, stride=1, padding=1)\n        self.conv2 = torch.nn.Conv2d(12, 24, (1, 1), stride=1, padding=0)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = self.conv2(v1)\n        v3 = torch.relu(v2)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 3, 128, 128)\n"
            ],
            "g_time": 9.604701280593872
        }
    }
}
{
    "opt": "splitwithsizes_cat_replace",
    "response": {
        "0": {
            "raw": [
                "\nclass Model1(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.features = torch.nn.Conv2d(3, 16, 3, 1, 1)\n    def forward(self, v1):\n        split_tensors = torch.split(v1, [1, 1, 1], dim=1)\n        concatenated_tensor = torch.cat(split_tensors, dim=1)\n        return (concatenated_tensor, torch.split(v1, [1, 1, 1], dim=1))\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.branch1 = Model1()\n        self.branch2 = Model1()\n        self.other_features = torch.nn.Sequential(torch.nn.ReLU(inplace=False))\n    def forward(self, v1):\n        split_tensors = torch.split(v1, [1, 1, 1], dim=1)\n        concatenated_tensor = torch.cat(split_tensors, dim=1)\n        return (concatenated_tensor, torch.split(v1, [1, 1, 1], dim=1))\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        block = [torch.nn.Conv2d(3, 32, 3, 1, 1, bias=False)]\n        self.features = torch.nn.Sequential(*block * 3)\n    def forward(self, v1):\n        split_tensors = torch.split(v1, [1, 1, 1], dim=1)\n        if torch.equal(split_tensors[0], split_tensors[1]):\n            concatenated_tensor = torch.cat(split_tensors, dim=1)\n        else:\n            concatenated_tensor = torch.cat([split_tensors[i] for i in range(1, len(split_tensors))], dim=1)\n        return (concatenated_tensor, torch.split(v1, [1, 1, 1], dim=1))\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.features = torch.nn.ModuleList([torch.nn.Conv2d(3, 8, 3, 1, 1), torch.nn.MaxPool2d(3, 2, 1)])\n        self.other_features = torch.nn.ModuleList([torch.nn.BatchNorm2d(32), torch.nn.ReLU(inplace=False)] * 2)\n    def forward(self, v1):\n        split_tensors = torch.split(v1, [1, 1, 1], dim=1)\n        concatenated_tensor = torch.cat(split_tensors, dim=1)\n        return (concatenated_tensor, torch.split(v1, [1, 1, 1], dim=1))\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.features = torch.nn.ModuleList([Model1(2, 3), Model1(1, 2)])\n    def forward(self, v1):\n        split_tensors = torch.split(v1, [1, 1, 1], dim=1)\n        concatenated_tensor = torch.cat(\n            split_tensors,\n            dim=1)\n        return (concatenated_tensor, torch.split(v1, [1, 1, 1], dim=1))\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.features = torch.nn.Sequential(torch.nn.ReLU(inplace=False), torch.nn.Parameter(torch.randn([1, 8, 64, 64])))\n    def forward(self, v1):\n        split_tensors = torch.split(v1, [1, 1, 1], dim=1)\n        concatenated_tensor = torch.cat(split_tensors, dim=1)\n        return (concatenated_tensor, torch.split(v1, [1, 1, 1], dim=1))\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.features = torch.nn.ModuleDict({\"3\" : torch.nn.Conv2d(3, 8, 3, 1, 1), \"6\" : torch.nn.MaxPool2d(3, 2, 1), \"8\" : torch.nn.ReLU(inplace=False)})\n        self.other_features = torch.nn.Sequential(torch.nn.ReLU(inplace=False))\n    def forward(self, v1):\n        split_tensors = torch.split(v1, [1, 1, 1], dim=1)\n        concatenated_tensor = torch.cat(split_tensors, dim=1)\n        return (concatenated_tensor, torch.split(v1, [1, 1, 1], dim=1))\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.features = torch.nn.ModuleList([torch.nn.Conv2d(3, 32, 3, 1, 1, bias=False), torch.nn.ReLU(inplace=False)])\n        self.other_features = torch.nn.Sequential(*torch.nn.ModuleList([torch.cat([torch.nn.Conv2d(32, 32, 3, 1, 1, bias=False)], dim=1)]))\n        self.cat_features = torch.nn.Sequential(*torch.cat([torch.nn.MaxPool2d(3, 2, 1) for i in range(5)], dim=1))\n    def forward(self, v1):\n        split_tensors = torch.split(v1, [1, 1, 1], dim=1)\n        concatenated_tensor = torch.cat(split_tensors, dim=1)\n        return (concatenated_tensor, torch.split(v1, [1, 1, 1], dim=1))\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.features = torch.nn.Sequential(*torch.nn.Conv2d(3, 32, 3, 1, 1, bias=False))\n    def forward(self, v1):\n        split_tensors = torch.split(v1, [1, 1, 1], dim=1)\n        concatenated_tensor = torch.cat(split_tensors, dim=1)\n        return (concatenated_tensor, torch.split(v1, [1, 1, 1], dim=1))\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.features = torch.nn.ModuleList([torch.nn.Conv2d(3, 8, 3, 1, 1), torch.nn.MaxPool2d(3, 2, 1), torch.nn.ReLU(inplace=False)])\n    def forward(self, v1):\n        split_tensors = torch.split(v1, [1, 1, 1], dim=1)\n        concatenated_tensor = torch.cat(split_tensors, dim=1)\n        return (concatenated_tensor, torch.split(v1, [1, 1, 1], dim=1))\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.features = torch.nn.Sequential(torch.nn.Linear(32, 7))\n    def forward(self, v1):\n        split = torch.split(v1, [1, 1, 1], dim=1)\n        if split[0].shape[-1] > 2:\n            temp = split[0]\n        elif split[2].shape[-1] > 2:\n            temp = split[2]\n        else:\n            temp = split[1]\n        return torch.cat([temp, v1], 1)\n# Inputs to the model\nx1 = torch.randn(1, 8, 32)\n"
            ],
            "code": [
                "\nclass Model1(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.features = torch.nn.Conv2d(3, 16, 3, 1, 1)\n    def forward(self, v1):\n        split_tensors = torch.split(v1, [1, 1, 1], dim=1)\n        concatenated_tensor = torch.cat(split_tensors, dim=1)\n        return (concatenated_tensor, torch.split(v1, [1, 1, 1], dim=1))\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.branch1 = Model1()\n        self.branch2 = Model1()\n        self.other_features = torch.nn.Sequential(torch.nn.ReLU(inplace=False))\n    def forward(self, v1):\n        split_tensors = torch.split(v1, [1, 1, 1], dim=1)\n        concatenated_tensor = torch.cat(split_tensors, dim=1)\n        return (concatenated_tensor, torch.split(v1, [1, 1, 1], dim=1))\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        block = [torch.nn.Conv2d(3, 32, 3, 1, 1, bias=False)]\n        self.features = torch.nn.Sequential(*block * 3)\n    def forward(self, v1):\n        split_tensors = torch.split(v1, [1, 1, 1], dim=1)\n        if torch.equal(split_tensors[0], split_tensors[1]):\n            concatenated_tensor = torch.cat(split_tensors, dim=1)\n        else:\n            concatenated_tensor = torch.cat([split_tensors[i] for i in range(1, len(split_tensors))], dim=1)\n        return (concatenated_tensor, torch.split(v1, [1, 1, 1], dim=1))\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.features = torch.nn.ModuleList([torch.nn.Conv2d(3, 8, 3, 1, 1), torch.nn.MaxPool2d(3, 2, 1)])\n        self.other_features = torch.nn.ModuleList([torch.nn.BatchNorm2d(32), torch.nn.ReLU(inplace=False)] * 2)\n    def forward(self, v1):\n        split_tensors = torch.split(v1, [1, 1, 1], dim=1)\n        concatenated_tensor = torch.cat(split_tensors, dim=1)\n        return (concatenated_tensor, torch.split(v1, [1, 1, 1], dim=1))\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.features = torch.nn.ModuleList([Model1(2, 3), Model1(1, 2)])\n    def forward(self, v1):\n        split_tensors = torch.split(v1, [1, 1, 1], dim=1)\n        concatenated_tensor = torch.cat(\n            split_tensors,\n            dim=1)\n        return (concatenated_tensor, torch.split(v1, [1, 1, 1], dim=1))\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.features = torch.nn.Sequential(torch.nn.ReLU(inplace=False), torch.nn.Parameter(torch.randn([1, 8, 64, 64])))\n    def forward(self, v1):\n        split_tensors = torch.split(v1, [1, 1, 1], dim=1)\n        concatenated_tensor = torch.cat(split_tensors, dim=1)\n        return (concatenated_tensor, torch.split(v1, [1, 1, 1], dim=1))\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.features = torch.nn.ModuleDict({\"3\" : torch.nn.Conv2d(3, 8, 3, 1, 1), \"6\" : torch.nn.MaxPool2d(3, 2, 1), \"8\" : torch.nn.ReLU(inplace=False)})\n        self.other_features = torch.nn.Sequential(torch.nn.ReLU(inplace=False))\n    def forward(self, v1):\n        split_tensors = torch.split(v1, [1, 1, 1], dim=1)\n        concatenated_tensor = torch.cat(split_tensors, dim=1)\n        return (concatenated_tensor, torch.split(v1, [1, 1, 1], dim=1))\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.features = torch.nn.ModuleList([torch.nn.Conv2d(3, 32, 3, 1, 1, bias=False), torch.nn.ReLU(inplace=False)])\n        self.other_features = torch.nn.Sequential(*torch.nn.ModuleList([torch.cat([torch.nn.Conv2d(32, 32, 3, 1, 1, bias=False)], dim=1)]))\n        self.cat_features = torch.nn.Sequential(*torch.cat([torch.nn.MaxPool2d(3, 2, 1) for i in range(5)], dim=1))\n    def forward(self, v1):\n        split_tensors = torch.split(v1, [1, 1, 1], dim=1)\n        concatenated_tensor = torch.cat(split_tensors, dim=1)\n        return (concatenated_tensor, torch.split(v1, [1, 1, 1], dim=1))\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.features = torch.nn.Sequential(*torch.nn.Conv2d(3, 32, 3, 1, 1, bias=False))\n    def forward(self, v1):\n        split_tensors = torch.split(v1, [1, 1, 1], dim=1)\n        concatenated_tensor = torch.cat(split_tensors, dim=1)\n        return (concatenated_tensor, torch.split(v1, [1, 1, 1], dim=1))\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.features = torch.nn.ModuleList([torch.nn.Conv2d(3, 8, 3, 1, 1), torch.nn.MaxPool2d(3, 2, 1), torch.nn.ReLU(inplace=False)])\n    def forward(self, v1):\n        split_tensors = torch.split(v1, [1, 1, 1], dim=1)\n        concatenated_tensor = torch.cat(split_tensors, dim=1)\n        return (concatenated_tensor, torch.split(v1, [1, 1, 1], dim=1))\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.features = torch.nn.Sequential(torch.nn.Linear(32, 7))\n    def forward(self, v1):\n        split = torch.split(v1, [1, 1, 1], dim=1)\n        if split[0].shape[-1] > 2:\n            temp = split[0]\n        elif split[2].shape[-1] > 2:\n            temp = split[2]\n        else:\n            temp = split[1]\n        return torch.cat([temp, v1], 1)\n# Inputs to the model\nx1 = torch.randn(1, 8, 32)\n"
            ],
            "g_time": 12.005374431610107
        }
    }
}
{
    "opt": "binary_unary=6",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(6, 64)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 - 0.01\n        v3 = F.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 6)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n \n    def forward(self, x1, x2, x3):\n        v1 = self.linear(x1)\n        v2 = v1 - x2\n        v3 = F.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\nx2 = torch.randn(1, 3, 64, 64)\nx3 = torch.randn(1, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 2)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 - 1\n        v3 = F.relu(v2)\n        return v3.sum()\n        \n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(8, 64)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 - 10.\n        v3 = F.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = M.Linear(8, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 - 3\n        v3 = F.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.rand(1, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Conv2d(10, 10, 1, stride=1, padding=1)\n \n    def forward(self, x1):\n        t1 = self.linear(x1)\n        t2 = t1 - other\n        t3 = torch.nn.functional.relu(t2)\n        return t3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 10, 128, 128)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(12, 8)\n \n    def forward(self, x1, x2):\n        v1 = self.linear(x1)\n        v2 = v1 - x2\n        v3 = torch.nn.functional.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 12)\nx2 = torch.randn(1, 12)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(8,6)\n \n    def forward(self, x1, x2):\n        v1 = self.linear(x1)\n        v2 = v1 - x2\n        v3 = F.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 8)\nx2 = torch.randn(6)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(2, 5)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 - 2\n        v3 = torch.nn.functional.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(4, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 - 1\n        v3 = torch.nn.functional.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 4)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(6, 64)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 - 0.01\n        v3 = F.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 6)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n \n    def forward(self, x1, x2, x3):\n        v1 = self.linear(x1)\n        v2 = v1 - x2\n        v3 = F.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\nx2 = torch.randn(1, 3, 64, 64)\nx3 = torch.randn(1, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 2)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 - 1\n        v3 = F.relu(v2)\n        return v3.sum()\n        \n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(8, 64)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 - 10.\n        v3 = F.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = M.Linear(8, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 - 3\n        v3 = F.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.rand(1, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Conv2d(10, 10, 1, stride=1, padding=1)\n \n    def forward(self, x1):\n        t1 = self.linear(x1)\n        t2 = t1 - other\n        t3 = torch.nn.functional.relu(t2)\n        return t3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 10, 128, 128)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(12, 8)\n \n    def forward(self, x1, x2):\n        v1 = self.linear(x1)\n        v2 = v1 - x2\n        v3 = torch.nn.functional.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 12)\nx2 = torch.randn(1, 12)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(8,6)\n \n    def forward(self, x1, x2):\n        v1 = self.linear(x1)\n        v2 = v1 - x2\n        v3 = F.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 8)\nx2 = torch.randn(6)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(2, 5)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 - 2\n        v3 = torch.nn.functional.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(4, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 - 1\n        v3 = torch.nn.functional.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 4)\n"
            ],
            "g_time": 7.706684589385986
        }
    }
}
{
    "opt": "sfdp=0",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.key = torch.nn.Parameter(torch.randn(59, 106, 12, 92))\n    def forward(self, x1):\n        q = x1\n        k = x1\n        v = x1\n        inv_scale = math.sqrt(k.size(1))\n        scaled_dot_product = torch.matmul(q, k.transpose(-2, -1)) / inv_scale\n        attention_weights = scaled_dot_product.softmax(dim=-1)\n        output = attention_weights.matmul(v)\n        return output\n# Inputs to the model\nx1 = torch.randn(2, 2, 101, 72)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.key = torch.nn.Parameter(torch.randn(95, 84, 30, 90))\n    def forward(self, x1):\n        q = x1\n        k = x1\n        v = x1\n        inv_scale = math.sqrt(k.size(1))\n        scaled_dot_product = torch.matmul(q, k.transpose(-2, -1)) / inv_scale\n        attention_weights = scaled_dot_product.softmax(dim=-1)\n        output = attention_weights.matmul(v)\n        return output\n# Inputs to the model\nx1 = torch.randn(98, 6, 68, 103)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.key = torch.nn.Parameter(torch.randn(96, 118, 6, 14))\n    def forward(self, x1):\n        q = x1\n        k = x1\n        v = x1\n        inv_scale = math.sqrt(k.size(1))\n        scaled_dot_product = torch.matmul(q, k.transpose(-2, -1)) / inv_scale\n        attention_weights = scaled_dot_product.softmax(dim=-1)\n        output = attention_weights.matmul(v)\n        return output\n# Inputs to the model\nx1 = torch.randn(70, 36, 87, 27)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.key = torch.nn.Parameter(torch.randn(20, 27, 10, 51))\n    def forward(self, x1):\n        q = x1\n        k = x1\n        v = x1\n        inv_scale = math.sqrt(k.size(1))\n        scaled_dot_product = torch.matmul(q, k.transpose(-2, -1)) / inv_scale\n        attention_weights = scaled_dot_product.softmax(dim=-1)\n        output = attention_weights.matmul(v)\n        return output\n# Inputs to the model\nx1 = torch.randn(16, 12, 10, 110)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.key = torch.nn.Parameter(torch.randn(16, 110, 118, 129))\n    def forward(self, x1):\n        q = x1\n        k = x1\n        v = x1\n        inv_scale = math.sqrt(k.size(1))\n        scaled_dot_product = torch.matmul(q, k.transpose(-2, -1)) / inv_scale\n        attention_weights = scaled_dot_product.softmax(dim=-1)\n        output = attention_weights.matmul(v)\n        return output\n# Inputs to the model\nx1 = torch.randn(26, 76, 31, 125)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.key = torch.nn.Parameter(torch.randn(86, 50, 165, 196))\n    def forward(self, x1):\n        q = x1\n        k = x1\n        v = x1\n        inv_scale = math.sqrt(k.size(1))\n        scaled_dot_product = torch.matmul(q, k.transpose(-2, -1)) / inv_scale\n        attention_weights = scaled_dot_product.softmax(dim=-1)\n        output = attention_weights.matmul(v)\n        return output\n# Inputs to the model\nx1 = torch.randn(98, 110, 95, 89)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.key = torch.nn.Parameter(torch.randn(24, 87, 74, 13))\n    def forward(self, x1):\n        q = x1\n        k = x1\n        v = x1\n        inv_scale = math.sqrt(k.size(1))\n        scaled_dot_product = torch.matmul(q, k.transpose(-2, -1)) / inv_scale\n        attention_weights = scaled_dot_product.softmax(dim=-1)\n        output = attention_weights.matmul(v)\n        return output\n# Inputs to the model\nx1 = torch.randn(60, 170, 12, 195)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.key = torch.nn.Parameter(torch.randn(27, 99, 45, 129))\n    def forward(self, x1):\n        q = x1\n        k = x1\n        v = x1\n        inv_scale = math.sqrt(k.size(1))\n        scaled_dot_product = torch.matmul(q, k.transpose(-2, -1)) / inv_scale\n        attention_weights = scaled_dot_product.softmax(dim=-1)\n        output = attention_weights.matmul(v)\n        return output\n# Inputs to the model\nx1 = torch.randn(80, 5, 46, 86)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.key = torch.nn.Parameter(torch.randn(77, 64, 88, 10))\n    def forward(self, x1):\n        q = x1\n        k = x1\n        v = x1\n        inv_scale = math.sqrt(k.size(1))\n        scaled_dot_product = torch.matmul(q, k.transpose(-2, -1)) / inv_scale\n        attention_weights = scaled_dot_product.softmax(dim=-1)\n        output = attention_weights.matmul(v)\n        return output\n# Inputs to the model\nx1 = torch.randn(37, 31, 34, 29)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.key = torch.nn.Parameter(torch.randn(7, 76, 89, 66))\n    def forward(self, x1):\n        q = x1\n        k = x1\n        v = x1\n        inv_scale = math.sqrt(k.size(1))\n        scaled_dot_product = torch.matmul(q, k.transpose(-2, -1)) / inv_scale\n        attention_weights = scaled_dot_product.softmax(dim=-1)\n        output = attention_weights.matmul(v)\n        return output\n# Inputs to the model\nx1 = torch.randn(1, 93, 72, 57)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.key = torch.nn.Parameter(torch.randn(59, 106, 12, 92))\n    def forward(self, x1):\n        q = x1\n        k = x1\n        v = x1\n        inv_scale = math.sqrt(k.size(1))\n        scaled_dot_product = torch.matmul(q, k.transpose(-2, -1)) / inv_scale\n        attention_weights = scaled_dot_product.softmax(dim=-1)\n        output = attention_weights.matmul(v)\n        return output\n# Inputs to the model\nx1 = torch.randn(2, 2, 101, 72)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.key = torch.nn.Parameter(torch.randn(95, 84, 30, 90))\n    def forward(self, x1):\n        q = x1\n        k = x1\n        v = x1\n        inv_scale = math.sqrt(k.size(1))\n        scaled_dot_product = torch.matmul(q, k.transpose(-2, -1)) / inv_scale\n        attention_weights = scaled_dot_product.softmax(dim=-1)\n        output = attention_weights.matmul(v)\n        return output\n# Inputs to the model\nx1 = torch.randn(98, 6, 68, 103)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.key = torch.nn.Parameter(torch.randn(96, 118, 6, 14))\n    def forward(self, x1):\n        q = x1\n        k = x1\n        v = x1\n        inv_scale = math.sqrt(k.size(1))\n        scaled_dot_product = torch.matmul(q, k.transpose(-2, -1)) / inv_scale\n        attention_weights = scaled_dot_product.softmax(dim=-1)\n        output = attention_weights.matmul(v)\n        return output\n# Inputs to the model\nx1 = torch.randn(70, 36, 87, 27)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.key = torch.nn.Parameter(torch.randn(20, 27, 10, 51))\n    def forward(self, x1):\n        q = x1\n        k = x1\n        v = x1\n        inv_scale = math.sqrt(k.size(1))\n        scaled_dot_product = torch.matmul(q, k.transpose(-2, -1)) / inv_scale\n        attention_weights = scaled_dot_product.softmax(dim=-1)\n        output = attention_weights.matmul(v)\n        return output\n# Inputs to the model\nx1 = torch.randn(16, 12, 10, 110)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.key = torch.nn.Parameter(torch.randn(16, 110, 118, 129))\n    def forward(self, x1):\n        q = x1\n        k = x1\n        v = x1\n        inv_scale = math.sqrt(k.size(1))\n        scaled_dot_product = torch.matmul(q, k.transpose(-2, -1)) / inv_scale\n        attention_weights = scaled_dot_product.softmax(dim=-1)\n        output = attention_weights.matmul(v)\n        return output\n# Inputs to the model\nx1 = torch.randn(26, 76, 31, 125)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.key = torch.nn.Parameter(torch.randn(86, 50, 165, 196))\n    def forward(self, x1):\n        q = x1\n        k = x1\n        v = x1\n        inv_scale = math.sqrt(k.size(1))\n        scaled_dot_product = torch.matmul(q, k.transpose(-2, -1)) / inv_scale\n        attention_weights = scaled_dot_product.softmax(dim=-1)\n        output = attention_weights.matmul(v)\n        return output\n# Inputs to the model\nx1 = torch.randn(98, 110, 95, 89)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.key = torch.nn.Parameter(torch.randn(24, 87, 74, 13))\n    def forward(self, x1):\n        q = x1\n        k = x1\n        v = x1\n        inv_scale = math.sqrt(k.size(1))\n        scaled_dot_product = torch.matmul(q, k.transpose(-2, -1)) / inv_scale\n        attention_weights = scaled_dot_product.softmax(dim=-1)\n        output = attention_weights.matmul(v)\n        return output\n# Inputs to the model\nx1 = torch.randn(60, 170, 12, 195)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.key = torch.nn.Parameter(torch.randn(27, 99, 45, 129))\n    def forward(self, x1):\n        q = x1\n        k = x1\n        v = x1\n        inv_scale = math.sqrt(k.size(1))\n        scaled_dot_product = torch.matmul(q, k.transpose(-2, -1)) / inv_scale\n        attention_weights = scaled_dot_product.softmax(dim=-1)\n        output = attention_weights.matmul(v)\n        return output\n# Inputs to the model\nx1 = torch.randn(80, 5, 46, 86)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.key = torch.nn.Parameter(torch.randn(77, 64, 88, 10))\n    def forward(self, x1):\n        q = x1\n        k = x1\n        v = x1\n        inv_scale = math.sqrt(k.size(1))\n        scaled_dot_product = torch.matmul(q, k.transpose(-2, -1)) / inv_scale\n        attention_weights = scaled_dot_product.softmax(dim=-1)\n        output = attention_weights.matmul(v)\n        return output\n# Inputs to the model\nx1 = torch.randn(37, 31, 34, 29)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.key = torch.nn.Parameter(torch.randn(7, 76, 89, 66))\n    def forward(self, x1):\n        q = x1\n        k = x1\n        v = x1\n        inv_scale = math.sqrt(k.size(1))\n        scaled_dot_product = torch.matmul(q, k.transpose(-2, -1)) / inv_scale\n        attention_weights = scaled_dot_product.softmax(dim=-1)\n        output = attention_weights.matmul(v)\n        return output\n# Inputs to the model\nx1 = torch.randn(1, 93, 72, 57)\n"
            ],
            "g_time": 7.534504652023315
        }
    }
}
{
    "opt": "pointless_cumsum_replacement",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1):\n        b = {}\n        a = {}\n        b['dtype'] = torch.float16\n        b['layout'] = torch.strided\n        b['device'] = torch.device('cuda:0')\n        a['dtype'] = torch.float16\n        a['layout'] = torch.strided\n        a['device'] = torch.device('cuda:0')\n        a['dtype_to'] = torch.float16\n        a['dtype_from'] = torch.float16\n        b['dtype_to'] = torch.float16\n        b['dtype_from'] = torch.float16\n        t1 = torch.full([8192, 1024], 1, dtype=b['dtype'], layout=b['layout'], device=b['device'], pin_memory=False)\n        t2 = t1.to(dtype=a['dtype'])\n        t3 = torch.cumsum(t2, 1)\n        return t3\n# Inputs to the model\nx1 = torch.randn(8192, 1024, device='cuda:0')\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1):\n        b = {}\n        a = {}\n        b['dtype'] = torch.float64\n        b['layout'] = torch.strided\n        b['device'] = torch.device('cuda:1')\n        a['dtype'] = torch.int8\n        a['layout'] = torch.strided\n        a['device'] = torch.device('cuda:1')\n        a['dtype_to'] = torch.float32\n        a['dtype_from'] = torch.int8\n        b['dtype_to'] = torch.float32\n        b['dtype_from'] = torch.float32\n        t1 = torch.full([256, 128], 1, dtype=b['dtype'], layout=b['layout'], device=b['device'], pin_memory=False)\n        t2 = t1.to(dtype=a['dtype'])\n        t3 = torch.cumsum(t2, 1)\n        return t3\n# Inputs to the model\nx1 = torch.randn(256, 128, device='cuda:1')\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1):\n        b = {}\n        a = {}\n        b['dtype'] = torch.int8\n        b['layout'] = torch.strided\n        b['device'] = torch.device('cuda:1')\n        a['dtype'] = torch.float16\n        a['layout'] = torch.strided\n        a['device'] = torch.device('cuda:1')\n        a['dtype_to'] = torch.int8\n        a['dtype_from'] = torch.float16\n        b['dtype_to'] = torch.float16\n        b['dtype_from'] = torch.int8\n        t1 = torch.full([384, 512], 1, dtype=b['dtype'], layout=b['layout'], device=b['device'], pin_memory=False)\n        t2 = t1.to(dtype=a['dtype'])\n        t3 = torch.cumsum(t2, 1)\n        return t3\n# Inputs to the model\nx1 = torch.randn(384, 512, device='cuda:1')\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1):\n        b = {}\n        a = {}\n        b['dtype'] = torch.bool\n        b['layout'] = torch.strided\n        b['device'] = torch.device('cuda:0')\n        a['dtype'] = torch.int32\n        a['layout'] = torch.strided\n        a['device'] = torch.device('cuda:0')\n        a['dtype_to'] = torch.float16\n        a['dtype_from'] = torch.int32\n        b['dtype_to'] = torch.float16\n        b['dtype_from'] = torch.int32\n        t1 = torch.full([4725600], 1, dtype=b['dtype'], layout=b['layout'], device=b['device'], pin_memory=False)\n        t2 = t1.to(dtype=a['dtype'])\n        t3 = torch.cumsum(t2, 0)\n        return t3\n# Inputs to the model\nx1 = 1\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1):\n        b = {}\n        a = {}\n        b['dtype'] = torch.float16\n        b['layout'] = torch.strided\n        b['device'] = torch.device('cpu')\n        a['dtype'] = torch.float16\n        a['layout'] = torch.strided\n        a['device'] = torch.device('cpu')\n        a['dtype_to'] = torch.float16\n        a['dtype_from'] = torch.float16\n        b['dtype_to'] = torch.float16\n        b['dtype_from'] = torch.float16\n        t1 = torch.full([8192, 2], 1, dtype=b['dtype'], layout=b['layout'], device=b['device'], pin_memory=False)\n        t2 = t1.to(dtype=a['dtype'])\n        t3 = torch.cumsum(t2, 1)\n        return t3\n# Inputs to the model\nx1 = 1\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1):\n        b = {}\n        a = {}\n        b['dtype'] = torch.float64\n        b['layout'] = torch.strided\n        b['device'] = torch.device('cpu')\n        a['dtype'] = torch.float16\n        a['layout'] = torch.strided\n        a['device'] = torch.device('cpu')\n        a['dtype_to'] = torch.half\n        a['dtype_from'] = torch.float32\n        b['dtype_to'] = torch.uint8\n        b['dtype_from'] = torch.float32\n        t1 = torch.full([128, 128], 1, dtype=b['dtype'], layout=b['layout'], device=b['device'], pin_memory=False)\n        t2 = t1.to(dtype=a['dtype'])\n        t3 = torch.cumsum(t2, 1)\n        return t3\n# Inputs to the model\nx1 = torch.randn(128, 128, device='cpu')\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1):\n        b = {}\n        a = {}\n        b['dtype'] = torch.float64\n        b['layout'] = torch.sparse_coo\n        b['device'] = torch.device('cpu')\n        a['dtype'] = torch.float32\n        a['layout'] = torch.strided\n        a['device'] = torch.device('cpu')\n        a['dtype_to'] = torch.float32\n        a['dtype_from'] = torch.float64\n        b['dtype_to'] = torch.float32\n        b['dtype_from'] = torch.float64\n        t1 = torch.full([arg1, arg2], 1, dtype=b['dtype'], layout=b['layout'], device=b['device'], pin_memory=False)\n        t2 = t1.to(dtype=a['dtype'])\n        t3 = torch.cumsum(t2, 1)\n        return t3\n# Inputs to the model\na = torch.arange(16480.0, device='cpu', dtype=torch.float64)\nb = torch.cat([a, a])\nx1 = b[4:8, 17:60]\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1):\n        b = {}\n        a = {}\n        b['dtype'] = torch.float32\n        b['layout'] = torch.strided\n        b['device'] = torch.device('cuda:0')\n        a['dtype'] = torch.int8\n        a['layout'] = torch.strided\n        a['device'] = torch.device('cuda:1')\n        a['dtype_to'] = torch.float16\n        a['dtype_from'] = torch.int8\n        b['dtype_to'] = torch.float32\n        b['dtype_from'] = torch.int8\n        t1 = torch.full([96, 5960], 1, dtype=b['dtype'], layout=b['layout'], device=b['device'], pin_memory=False)\n        t2 = t1.to(dtype=a['dtype'])\n        t3 = torch.cumsum(t2, 1)\n        return t3\n# Inputs to the model\nx1 = torch.rand(96, 5960, dtype=torch.float64, device='cuda:1')\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1):\n        b = {}\n        a = {}\n        b['dtype'] = torch.int32\n        b['layout'] = torch.strided\n        b['device'] = torch.device('cpu')\n        a['dtype'] = torch.float16\n        a['layout'] = torch.strided\n        a['device'] = torch.device('cpu')\n        a['dtype_to'] = torch.int32\n        a['dtype_from'] = torch.float16\n        b['dtype_to'] = torch.int32\n        b['dtype_from'] = torch.float16\n        t1 = torch.full([32, 256], 1, dtype=b['dtype'], layout=b['layout'], device=b['device'], pin_memory=False)\n        t2 = t1.to(dtype=a['dtype'])\n        t3 = torch.cumsum(t2, 1)\n        return t3\n# Inputs to the model\nx1 = torch.randn(32, 256, device='cpu')\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1):\n        b = {}\n        a = {}\n        b['dtype'] = torch.float16\n        b['layout'] = torch.strided\n        b['device'] = torch.device('cpu')\n        a['dtype'] = torch.float64\n        a['layout'] = torch.strided\n        a['device'] = torch.device('cpu')\n        a['dtype_to'] = torch.float64\n        a['dtype_from'] = torch.float16\n        b['dtype_to'] = torch.float64\n        b['dtype_from'] = torch.float16\n        t1 = torch.full([4, 8], 1, dtype=b['dtype'], layout=b['layout'], device=b['device'], pin_memory=False)\n        t2 = t1.to(dtype=a['dtype'])\n        t3 = torch.cumsum(t2, 1)\n        return t3\n# Inputs to the model\nx1 = torch.randn(4, 8, device='cpu')\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1):\n        b = {}\n        a = {}\n        b['dtype'] = torch.float16\n        b['layout'] = torch.strided\n        b['device'] = torch.device('cuda:0')\n        a['dtype'] = torch.float16\n        a['layout'] = torch.strided\n        a['device'] = torch.device('cuda:0')\n        a['dtype_to'] = torch.float16\n        a['dtype_from'] = torch.float16\n        b['dtype_to'] = torch.float16\n        b['dtype_from'] = torch.float16\n        t1 = torch.full([8192, 1024], 1, dtype=b['dtype'], layout=b['layout'], device=b['device'], pin_memory=False)\n        t2 = t1.to(dtype=a['dtype'])\n        t3 = torch.cumsum(t2, 1)\n        return t3\n# Inputs to the model\nx1 = torch.randn(8192, 1024, device='cuda:0')\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1):\n        b = {}\n        a = {}\n        b['dtype'] = torch.float64\n        b['layout'] = torch.strided\n        b['device'] = torch.device('cuda:1')\n        a['dtype'] = torch.int8\n        a['layout'] = torch.strided\n        a['device'] = torch.device('cuda:1')\n        a['dtype_to'] = torch.float32\n        a['dtype_from'] = torch.int8\n        b['dtype_to'] = torch.float32\n        b['dtype_from'] = torch.float32\n        t1 = torch.full([256, 128], 1, dtype=b['dtype'], layout=b['layout'], device=b['device'], pin_memory=False)\n        t2 = t1.to(dtype=a['dtype'])\n        t3 = torch.cumsum(t2, 1)\n        return t3\n# Inputs to the model\nx1 = torch.randn(256, 128, device='cuda:1')\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1):\n        b = {}\n        a = {}\n        b['dtype'] = torch.int8\n        b['layout'] = torch.strided\n        b['device'] = torch.device('cuda:1')\n        a['dtype'] = torch.float16\n        a['layout'] = torch.strided\n        a['device'] = torch.device('cuda:1')\n        a['dtype_to'] = torch.int8\n        a['dtype_from'] = torch.float16\n        b['dtype_to'] = torch.float16\n        b['dtype_from'] = torch.int8\n        t1 = torch.full([384, 512], 1, dtype=b['dtype'], layout=b['layout'], device=b['device'], pin_memory=False)\n        t2 = t1.to(dtype=a['dtype'])\n        t3 = torch.cumsum(t2, 1)\n        return t3\n# Inputs to the model\nx1 = torch.randn(384, 512, device='cuda:1')\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1):\n        b = {}\n        a = {}\n        b['dtype'] = torch.bool\n        b['layout'] = torch.strided\n        b['device'] = torch.device('cuda:0')\n        a['dtype'] = torch.int32\n        a['layout'] = torch.strided\n        a['device'] = torch.device('cuda:0')\n        a['dtype_to'] = torch.float16\n        a['dtype_from'] = torch.int32\n        b['dtype_to'] = torch.float16\n        b['dtype_from'] = torch.int32\n        t1 = torch.full([4725600], 1, dtype=b['dtype'], layout=b['layout'], device=b['device'], pin_memory=False)\n        t2 = t1.to(dtype=a['dtype'])\n        t3 = torch.cumsum(t2, 0)\n        return t3\n# Inputs to the model\nx1 = 1\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1):\n        b = {}\n        a = {}\n        b['dtype'] = torch.float16\n        b['layout'] = torch.strided\n        b['device'] = torch.device('cpu')\n        a['dtype'] = torch.float16\n        a['layout'] = torch.strided\n        a['device'] = torch.device('cpu')\n        a['dtype_to'] = torch.float16\n        a['dtype_from'] = torch.float16\n        b['dtype_to'] = torch.float16\n        b['dtype_from'] = torch.float16\n        t1 = torch.full([8192, 2], 1, dtype=b['dtype'], layout=b['layout'], device=b['device'], pin_memory=False)\n        t2 = t1.to(dtype=a['dtype'])\n        t3 = torch.cumsum(t2, 1)\n        return t3\n# Inputs to the model\nx1 = 1\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1):\n        b = {}\n        a = {}\n        b['dtype'] = torch.float64\n        b['layout'] = torch.strided\n        b['device'] = torch.device('cpu')\n        a['dtype'] = torch.float16\n        a['layout'] = torch.strided\n        a['device'] = torch.device('cpu')\n        a['dtype_to'] = torch.half\n        a['dtype_from'] = torch.float32\n        b['dtype_to'] = torch.uint8\n        b['dtype_from'] = torch.float32\n        t1 = torch.full([128, 128], 1, dtype=b['dtype'], layout=b['layout'], device=b['device'], pin_memory=False)\n        t2 = t1.to(dtype=a['dtype'])\n        t3 = torch.cumsum(t2, 1)\n        return t3\n# Inputs to the model\nx1 = torch.randn(128, 128, device='cpu')\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1):\n        b = {}\n        a = {}\n        b['dtype'] = torch.float64\n        b['layout'] = torch.sparse_coo\n        b['device'] = torch.device('cpu')\n        a['dtype'] = torch.float32\n        a['layout'] = torch.strided\n        a['device'] = torch.device('cpu')\n        a['dtype_to'] = torch.float32\n        a['dtype_from'] = torch.float64\n        b['dtype_to'] = torch.float32\n        b['dtype_from'] = torch.float64\n        t1 = torch.full([arg1, arg2], 1, dtype=b['dtype'], layout=b['layout'], device=b['device'], pin_memory=False)\n        t2 = t1.to(dtype=a['dtype'])\n        t3 = torch.cumsum(t2, 1)\n        return t3\n# Inputs to the model\na = torch.arange(16480.0, device='cpu', dtype=torch.float64)\nb = torch.cat([a, a])\nx1 = b[4:8, 17:60]\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1):\n        b = {}\n        a = {}\n        b['dtype'] = torch.float32\n        b['layout'] = torch.strided\n        b['device'] = torch.device('cuda:0')\n        a['dtype'] = torch.int8\n        a['layout'] = torch.strided\n        a['device'] = torch.device('cuda:1')\n        a['dtype_to'] = torch.float16\n        a['dtype_from'] = torch.int8\n        b['dtype_to'] = torch.float32\n        b['dtype_from'] = torch.int8\n        t1 = torch.full([96, 5960], 1, dtype=b['dtype'], layout=b['layout'], device=b['device'], pin_memory=False)\n        t2 = t1.to(dtype=a['dtype'])\n        t3 = torch.cumsum(t2, 1)\n        return t3\n# Inputs to the model\nx1 = torch.rand(96, 5960, dtype=torch.float64, device='cuda:1')\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1):\n        b = {}\n        a = {}\n        b['dtype'] = torch.int32\n        b['layout'] = torch.strided\n        b['device'] = torch.device('cpu')\n        a['dtype'] = torch.float16\n        a['layout'] = torch.strided\n        a['device'] = torch.device('cpu')\n        a['dtype_to'] = torch.int32\n        a['dtype_from'] = torch.float16\n        b['dtype_to'] = torch.int32\n        b['dtype_from'] = torch.float16\n        t1 = torch.full([32, 256], 1, dtype=b['dtype'], layout=b['layout'], device=b['device'], pin_memory=False)\n        t2 = t1.to(dtype=a['dtype'])\n        t3 = torch.cumsum(t2, 1)\n        return t3\n# Inputs to the model\nx1 = torch.randn(32, 256, device='cpu')\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1):\n        b = {}\n        a = {}\n        b['dtype'] = torch.float16\n        b['layout'] = torch.strided\n        b['device'] = torch.device('cpu')\n        a['dtype'] = torch.float64\n        a['layout'] = torch.strided\n        a['device'] = torch.device('cpu')\n        a['dtype_to'] = torch.float64\n        a['dtype_from'] = torch.float16\n        b['dtype_to'] = torch.float64\n        b['dtype_from'] = torch.float16\n        t1 = torch.full([4, 8], 1, dtype=b['dtype'], layout=b['layout'], device=b['device'], pin_memory=False)\n        t2 = t1.to(dtype=a['dtype'])\n        t3 = torch.cumsum(t2, 1)\n        return t3\n# Inputs to the model\nx1 = torch.randn(4, 8, device='cpu')\n"
            ],
            "g_time": 12.023220777511597
        }
    }
}
{
    "opt": "unary=22",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(128, 256)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.tanh(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 128)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(32 * 32, 10)\n        self.conv = torch.nn.Conv2d(3, 10, 3, stride=2, padding=1)\n \n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = self.linear(v1.reshape(-1, 32 * 32))\n        v3 = torch.tanh(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 32, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 4)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.tanh(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(8, 6)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.tanh(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(256, 10)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.tanh(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(256, 256)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc = torch.nn.Linear(256, 4)\n \n    def forward(self, x1):\n        v1 = self.fc(x1)\n        v2 = torch.tanh(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 256)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(1, 1)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.tanh(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 1)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(16, 32)\n \n    def forward(self, x):\n        linear = self.linear(x)\n        tanh = torch.tanh(linear)\n        return tanh\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx = torch.randn(1, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(5, 10)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.tanh(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 5)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(8, 1)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.tanh(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 8)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(128, 256)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.tanh(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 128)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(32 * 32, 10)\n        self.conv = torch.nn.Conv2d(3, 10, 3, stride=2, padding=1)\n \n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = self.linear(v1.reshape(-1, 32 * 32))\n        v3 = torch.tanh(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 32, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 4)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.tanh(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(8, 6)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.tanh(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(256, 10)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.tanh(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(256, 256)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc = torch.nn.Linear(256, 4)\n \n    def forward(self, x1):\n        v1 = self.fc(x1)\n        v2 = torch.tanh(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 256)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(1, 1)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.tanh(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 1)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(16, 32)\n \n    def forward(self, x):\n        linear = self.linear(x)\n        tanh = torch.tanh(linear)\n        return tanh\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx = torch.randn(1, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(5, 10)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.tanh(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 5)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(8, 1)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.tanh(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 8)\n"
            ],
            "g_time": 6.235235691070557
        }
    }
}
{
    "opt": "binary=0",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(7, 6, 1, stride=1, padding=1)\n    def forward(self, x1, x2=None):\n        v1 = self.conv(x1)\n        if x2 == None:\n            x2 = torch.randn(v1.shape)\n        v2 = x1 + v1\n        v2 = v2 + x2\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 7, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(25, 11, 1, stride=1, padding=1)\n    def forward(self, x1, other):\n        v1 = self.conv(x1 * other)\n        v2 = v1 * other\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 25, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(8, 10, 1, stride=1, padding=1)\n    def forward(self, x1, other=None, other2=None):\n        v1 = self.conv(x1)\n        if other == None:\n            other = torch.randn(v1.shape)\n        v2 = v1 + other\n        if other2 == None:\n            other2 = torch.randn(v1.shape)\n        v3 = v2 + other2\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 8, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(4, 3, 1, stride=1, padding=1)\n    def forward(self, x1, x2=None):\n        v1 = self.conv(x1)\n        if x2 == None:\n            x2 = torch.randn(v1.shape)\n        v2 = v1 + x2\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 4, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(12, 13, 2, stride=2, padding=2)\n    def forward(self, x1, x2=True, padding1=None, padding2=True):\n        v1 = self.conv(x1)\n        if x2 == True:\n            x2 = torch.randn(v1.shape)\n        v2 = v1 + x2\n        if padding1 == None:\n            padding1 = torch.randn(v1.shape)\n        v3 = v2 + padding1\n        if padding2 == True:\n            padding2 = torch.randn(v2.shape)\n        v4 = v3 + padding2\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 12, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(9, 11, 1)\n    def forward(self, x1, x2, other=None):\n        v1 = self.conv(x1)\n        v2 = v1 + x2\n        if other == None:\n            other = torch.randn(v1.shape)\n        v3 = v2 + other\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 9, 64, 64)\nx2 = torch.randn(4, 11, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(5, 23, 1, stride=1, padding=1)\n    def forward(self, x1, x2=None, padding1=None):\n        v1 = self.conv(x1)\n        if x2 is None:\n            x2 = torch.randn(v1.shape)\n        v2 = v1 + x2\n        if padding1 == None:\n            padding1 = torch.randn(v2.shape)\n        v3 = v2 + padding1\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 5, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(10, 5, 1, stride=1, padding=1)\n    def forward(self, input, padding=None):\n        v1 = self.conv(input)\n        v2 = v1 + 5\n        if padding == None:\n            padding = torch.randn(v1.shape)\n        v3 = v2 + padding\n        return v3\n# Inputs to the model\ninput = torch.randn(1, 10, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(8, 4, 3, stride=2, padding=2)\n        self.conv2 = torch.nn.Conv2d(7, 3, 3, stride=2, padding=2)\n    def forward(self,x1,x2=torch.randn(1, 7, 1024, 1024),padding1=-20.7):\n        v1=self.conv(x1)\n        if x2 == None:\n            x2 = torch.randn(v1.shape)\n        v2=self.conv2(x2)\n        v3=v1+v2\n        v4=v3.permute(0, 1, 3, 2).contiguous().squeeze(2)\n        return v4\n# Inputs to the model\nx1=torch.randn(1, 8, 128, 128)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(16, 32, 1, stride=1, padding=1)\n    def forward(self, x1, padding1=None, padding2=None):\n        v1 = self.conv(x1)\n        v2 = v1 + padding1\n        v3 = v2 + padding2\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 16, 64, 64)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(7, 6, 1, stride=1, padding=1)\n    def forward(self, x1, x2=None):\n        v1 = self.conv(x1)\n        if x2 == None:\n            x2 = torch.randn(v1.shape)\n        v2 = x1 + v1\n        v2 = v2 + x2\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 7, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(25, 11, 1, stride=1, padding=1)\n    def forward(self, x1, other):\n        v1 = self.conv(x1 * other)\n        v2 = v1 * other\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 25, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(8, 10, 1, stride=1, padding=1)\n    def forward(self, x1, other=None, other2=None):\n        v1 = self.conv(x1)\n        if other == None:\n            other = torch.randn(v1.shape)\n        v2 = v1 + other\n        if other2 == None:\n            other2 = torch.randn(v1.shape)\n        v3 = v2 + other2\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 8, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(4, 3, 1, stride=1, padding=1)\n    def forward(self, x1, x2=None):\n        v1 = self.conv(x1)\n        if x2 == None:\n            x2 = torch.randn(v1.shape)\n        v2 = v1 + x2\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 4, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(12, 13, 2, stride=2, padding=2)\n    def forward(self, x1, x2=True, padding1=None, padding2=True):\n        v1 = self.conv(x1)\n        if x2 == True:\n            x2 = torch.randn(v1.shape)\n        v2 = v1 + x2\n        if padding1 == None:\n            padding1 = torch.randn(v1.shape)\n        v3 = v2 + padding1\n        if padding2 == True:\n            padding2 = torch.randn(v2.shape)\n        v4 = v3 + padding2\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 12, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(9, 11, 1)\n    def forward(self, x1, x2, other=None):\n        v1 = self.conv(x1)\n        v2 = v1 + x2\n        if other == None:\n            other = torch.randn(v1.shape)\n        v3 = v2 + other\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 9, 64, 64)\nx2 = torch.randn(4, 11, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(5, 23, 1, stride=1, padding=1)\n    def forward(self, x1, x2=None, padding1=None):\n        v1 = self.conv(x1)\n        if x2 is None:\n            x2 = torch.randn(v1.shape)\n        v2 = v1 + x2\n        if padding1 == None:\n            padding1 = torch.randn(v2.shape)\n        v3 = v2 + padding1\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 5, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(10, 5, 1, stride=1, padding=1)\n    def forward(self, input, padding=None):\n        v1 = self.conv(input)\n        v2 = v1 + 5\n        if padding == None:\n            padding = torch.randn(v1.shape)\n        v3 = v2 + padding\n        return v3\n# Inputs to the model\ninput = torch.randn(1, 10, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(8, 4, 3, stride=2, padding=2)\n        self.conv2 = torch.nn.Conv2d(7, 3, 3, stride=2, padding=2)\n    def forward(self,x1,x2=torch.randn(1, 7, 1024, 1024),padding1=-20.7):\n        v1=self.conv(x1)\n        if x2 == None:\n            x2 = torch.randn(v1.shape)\n        v2=self.conv2(x2)\n        v3=v1+v2\n        v4=v3.permute(0, 1, 3, 2).contiguous().squeeze(2)\n        return v4\n# Inputs to the model\nx1=torch.randn(1, 8, 128, 128)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(16, 32, 1, stride=1, padding=1)\n    def forward(self, x1, padding1=None, padding2=None):\n        v1 = self.conv(x1)\n        v2 = v1 + padding1\n        v3 = v2 + padding2\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 16, 64, 64)\n"
            ],
            "g_time": 9.245003461837769
        }
    }
}
{
    "opt": "unary=4",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(8, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(64, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(10, 10)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 10)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = nn.Linear(32,64)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(224, 224, False)\n    \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 224, 224)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(8, 16)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(128, 1024)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 128)\nx2 = x1 * 0.0009765625\nx3 = x1 * 0.0009711390513643345\nx4 = (v3 + 1e-38)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(32, 32)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 32)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(8, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(64, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(10, 10)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 10)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = nn.Linear(32,64)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(224, 224, False)\n    \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 224, 224)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(8, 16)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(128, 1024)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 128)\nx2 = x1 * 0.0009765625\nx3 = x1 * 0.0009711390513643345\nx4 = (v3 + 1e-38)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(32, 32)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 32)\n"
            ],
            "g_time": 8.910181522369385
        }
    }
}
{
    "opt": "unary=2",
    "response": {
        "0": {
            "raw": [
                "\nclass ModelV2(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.deconv = torch.nn.ConvTranspose2d(10, 1024, (10, 10), (5, 5), (2, 2), bias=True)\n    def forward(self, x1):\n        v1 = self.deconv(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * v1 * v1\n        v4 = v3 * 0.044715\n        v5 = v1 + v4\n        v6 = v5 * 0.7978845608028654\n        v7 = torch.tanh(v6)\n        v8 = v7 + 1\n        v9 = v2 * v8\n        return v9\n# Inputs to the model\nx1 = torch.randn(1, 10, 25, 25)\n",
                "\nclass ModelV2(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.deconv = torch.nn.ConvTranspose2d(1, 1, 1, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.deconv(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * v1 * v1\n        v4 = v3 * 0.044715\n        v5 = v1 + v4\n        v6 = v5 * 0.7978845608028654\n        v7 = torch.tanh(v6)\n        v8 = v7 + 1\n        v9 = v2 * v8\n        return v9\n# Inputs to the model\nx1 = torch.randn(56, 1, 56, 181)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(10, 32, (5, 5), (2, 3), (1, 2), 10, bias=False)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * v1 * v1\n        v4 = v3 * 0.044715\n        v5 = v1 + v4\n        v6 = v5 * 0.7978845608028654\n        v7 = torch.tanh(v6)\n        v8 = v7 + 1\n        v9 = v2 * v8\n        return v9\n# Inputs to the model\nx1 = torch.randn(3, 10, 112, 112)\n# Model begins\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x):\n        v1 = x\n        v2 = torch.transpose(v1, 3, 2)\n        v3 = x * 0.5\n        v4 = v2 * v3\n        v5 = torch.transpose(v4, 3, 2)\n        v6 = torch.sum(v5)\n        return v6\n# Inputs to the model\nx1 = torch.randn(3, 4, 5, 6)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.mul_ = torch.nn.Parameter(torch.tensor(59.0403, dtype=torch.float32))\n        self.add_ = torch.nn.Parameter(torch.tensor(-0.0413, dtype=torch.float32))\n        self.conv = torch.nn.Conv2d(3, 64, 1, 1, 0, bias=True)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1.mul(self.mul_)\n        v3 = v2 + self.add_\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 3, 224, 224)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(26, 13, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * v1 * v1\n        v4 = v3 * 0.044715\n        v5 = v1 + v4\n        v6 = v5 * 0.7978845608028654\n        v7 = torch.tanh(v6)\n        v8 = v7 + 1\n        v9 = v2 * v8\n        return v9\n# Inputs to the model\nx1 = torch.randn(6, 26, 15, 15)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(1024, 1000, 2, 1, 0, bias=True)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * v1 * v1\n        v4 = v3 * 0.044715\n        v5 = v1 + v4\n        v6 = v5 * 0.7978845608028654\n        v7 = torch.tanh(v6)\n        v8 = v7 + 1\n        v9 = v2 * v8\n        return v9\n# Inputs to the model\nx1 = torch.randn(12, 1024, 200, 200)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose1d(10, 10, 2, 1, 3, bias=False)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * v1 * v1\n        v4 = v3 * 0.044715\n        v5 = v1 + v4\n        v6 = v5 * 0.7978845608028654\n        v7 = torch.tanh(v6)\n        v8 = v7 + 1\n        v9 = v2 * v8\n        return v9\n# Inputs to the model\nx1 = torch.randn(1, 10, 32, 5)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose1d(3, 9, 1, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * v1 * v1\n        v4 = v3 * 0.044715\n        v5 = v1 + v4\n        v6 = v5 * 0.7978845608028654\n        v7 = torch.tanh(v6)\n        v8 = v7 + 1\n        v9 = v2 * v8\n        return v9\n# Inputs to the model\nx1 = torch.randn(3, 3, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.ConvTranspose2d(10, 1, (2, 1), stride=(1, 2), padding=(1, 2), dilation=(2, 3), groups=2)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * v1 * v1\n        v4 = v3 * 0.044715\n        v5 = v1 + v4\n        v6 = v5 * 0.7978845608028654\n        v7 = torch.tanh(v6)\n        v8 = v7 + 1\n        v9 = v2 * v8\n        return v9\n# Inputs to the model\nx1 = torch.randn(1, 10, 2, 3)\n"
            ],
            "code": [
                "\nclass ModelV2(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.deconv = torch.nn.ConvTranspose2d(10, 1024, (10, 10), (5, 5), (2, 2), bias=True)\n    def forward(self, x1):\n        v1 = self.deconv(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * v1 * v1\n        v4 = v3 * 0.044715\n        v5 = v1 + v4\n        v6 = v5 * 0.7978845608028654\n        v7 = torch.tanh(v6)\n        v8 = v7 + 1\n        v9 = v2 * v8\n        return v9\n# Inputs to the model\nx1 = torch.randn(1, 10, 25, 25)\n",
                "\nclass ModelV2(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.deconv = torch.nn.ConvTranspose2d(1, 1, 1, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.deconv(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * v1 * v1\n        v4 = v3 * 0.044715\n        v5 = v1 + v4\n        v6 = v5 * 0.7978845608028654\n        v7 = torch.tanh(v6)\n        v8 = v7 + 1\n        v9 = v2 * v8\n        return v9\n# Inputs to the model\nx1 = torch.randn(56, 1, 56, 181)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(10, 32, (5, 5), (2, 3), (1, 2), 10, bias=False)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * v1 * v1\n        v4 = v3 * 0.044715\n        v5 = v1 + v4\n        v6 = v5 * 0.7978845608028654\n        v7 = torch.tanh(v6)\n        v8 = v7 + 1\n        v9 = v2 * v8\n        return v9\n# Inputs to the model\nx1 = torch.randn(3, 10, 112, 112)\n# Model begins\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x):\n        v1 = x\n        v2 = torch.transpose(v1, 3, 2)\n        v3 = x * 0.5\n        v4 = v2 * v3\n        v5 = torch.transpose(v4, 3, 2)\n        v6 = torch.sum(v5)\n        return v6\n# Inputs to the model\nx1 = torch.randn(3, 4, 5, 6)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.mul_ = torch.nn.Parameter(torch.tensor(59.0403, dtype=torch.float32))\n        self.add_ = torch.nn.Parameter(torch.tensor(-0.0413, dtype=torch.float32))\n        self.conv = torch.nn.Conv2d(3, 64, 1, 1, 0, bias=True)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1.mul(self.mul_)\n        v3 = v2 + self.add_\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 3, 224, 224)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(26, 13, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * v1 * v1\n        v4 = v3 * 0.044715\n        v5 = v1 + v4\n        v6 = v5 * 0.7978845608028654\n        v7 = torch.tanh(v6)\n        v8 = v7 + 1\n        v9 = v2 * v8\n        return v9\n# Inputs to the model\nx1 = torch.randn(6, 26, 15, 15)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(1024, 1000, 2, 1, 0, bias=True)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * v1 * v1\n        v4 = v3 * 0.044715\n        v5 = v1 + v4\n        v6 = v5 * 0.7978845608028654\n        v7 = torch.tanh(v6)\n        v8 = v7 + 1\n        v9 = v2 * v8\n        return v9\n# Inputs to the model\nx1 = torch.randn(12, 1024, 200, 200)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose1d(10, 10, 2, 1, 3, bias=False)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * v1 * v1\n        v4 = v3 * 0.044715\n        v5 = v1 + v4\n        v6 = v5 * 0.7978845608028654\n        v7 = torch.tanh(v6)\n        v8 = v7 + 1\n        v9 = v2 * v8\n        return v9\n# Inputs to the model\nx1 = torch.randn(1, 10, 32, 5)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose1d(3, 9, 1, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * v1 * v1\n        v4 = v3 * 0.044715\n        v5 = v1 + v4\n        v6 = v5 * 0.7978845608028654\n        v7 = torch.tanh(v6)\n        v8 = v7 + 1\n        v9 = v2 * v8\n        return v9\n# Inputs to the model\nx1 = torch.randn(3, 3, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.ConvTranspose2d(10, 1, (2, 1), stride=(1, 2), padding=(1, 2), dilation=(2, 3), groups=2)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * v1 * v1\n        v4 = v3 * 0.044715\n        v5 = v1 + v4\n        v6 = v5 * 0.7978845608028654\n        v7 = torch.tanh(v6)\n        v8 = v7 + 1\n        v9 = v2 * v8\n        return v9\n# Inputs to the model\nx1 = torch.randn(1, 10, 2, 3)\n"
            ],
            "g_time": 9.92389965057373
        }
    }
}
{
    "opt": "sfdp=2",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self, num_heads, max_length=4):\n        super().__init__()\n        self.qkv = torch.nn.Linear(max_length, max_length)\n        for p in self.qkv.parameters():\n            p.requires_grad = False\n        self.num_heads = num_heads\n        self.max_length = max_length\n\n    def forward(self, query, key, value, dropout_p=0):\n        k = self.qkv(key)\n        q = self.qkv(query)\n        v = self.qkv(value)\n        q = q.split(self.max_length // self.num_heads, dim=-1)\n        k = k.split(self.max_length // self.num_heads, dim=-1)\n        v = v.split(self.max_length // self.num_heads, dim=-1)\n        q = torch.cat(q, dim = 0)\n        k = torch.cat(k, dim = 0)\n        v = torch.cat(v, dim = 0)        \n        q *= self.max_length ** -0.5\n        qk = torch.matmul(q, k.transpose(-2, -1))\n        dropout_qk = torch.nn.functional.dropout(qk.softmax(dim=-1), p=dropout_p)\n        output = dropout_qk.matmul(v)\n        output = output.split(self.num_heads, dim=0)\n        output = torch.cat(output, dim=-1)\n        return output\n\n# Initializing the model\nm = Model(num_heads=3)\n\n# Inputs to the model\nquery = torch.randn(6, 8//3, 2, 3)\nkey = torch.randn(4, 8//3, 3, 3)\nvalue = torch.randn(4, 8//3, 3, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n \n    def forward(self, query, key, value):\n        qk = torch.matmul(query, key.transpose(-2, -1))\n        inv_scale_factor = 1 / np.sqrt(query.shape[-1])\n        scaled_qk = qk.div(inv_scale_factor)\n        softmax_qk = scaled_qk.softmax(dim=-1)\n        dropout_qk = torch.nn.functional.dropout(softmax_qk, p=0.1)\n        output = dropout_qk.matmul(value)\n        return output\n\n# Initializing the model\nm = Model()\n\n# Initialize inputs of the model\nquery = torch.randn(1, 1, 64)\nkey = torch.randn(1, 1, 128)\nvalue = torch.randn(1, 1, 128)\n\n# Inference through the model\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.inv_scale_factor = torch.nn.Parameter(torch.tensor(1.0))\n        self.dropout_p = 0.1\n \n    def forward(self, xq, xk, xv):\n        qk = torch.matmul(xq, xk.transpose(-2, -1))\n        scaled_qk = qk.div(self.inv_scale_factor)\n        softmax_qk = scaled_qk.softmax(dim=-1)\n        dropout_qk = torch.nn.functional.dropout(softmax_qk, p=self.dropout_p)\n        output = dropout_qk.matmul(xv)\n        return output\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nxq = torch.randn(3, 8, 32)\nxk = torch.randn(3, 8, 32)\nxv = torch.randn(3, 8, 32)\n",
                "\nclass SelfAttention(torch.nn.Module):\n    def __init__(self, num_heads, input_size, inner_size, dropout):\n        self.dropout = dropout\n        super().__init__()\n        self.query_mapping = Linear(input_size, inner_size, bias=False)\n        self.key_mapping = Linear(input_size, inner_size, bias=False)\n        self.value_mapping = Linear(input_size, inner_size, bias=False)\n        self.output_mapping = Linear(inner_size, input_size)\n \n    def forward(self, x1):\n        qk = torch.matmul(self.query_mapping(x1), self.key_mapping(x1).transpose(-2, -1))\n        scaled_qk = qk.div(torch.sqrt(torch.tensor(float(inner_size)).float()))\n        softmax_qk = scaled_qk.softmax(dim=-1)\n        dropout_qk = torch.nn.functional.dropout(softmax_qk, p=self.dropout)\n        output = self.output_mapping.forward(torch.matmul(dropout_qk, self.value_mapping(x1)))\n        return output\n\n# Initializing the model\nm = SelfAttention(16, 32, 64, 0.1)\n\n# Inputs to the model\nx1 = torch.randn(1, 16, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        pass\n \n    def forward(self, x1, x2, x3, x4, x5):\n        v1 = torch.matmul(x1, x2.transpose(-2, -1))\n        v2 = v1.div(x3)\n        v3 = v2.softmax(dim=-1)\n        v4 = torch.nn.functional.dropout(v3, x5)\n        v5 = torch.matmul(v4, x4)\n        return v5\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 8, 64, 64)\nx2 = torch.randn(1, 8, 3072)\nx3 = torch.randn(1, 16, 1, 1).log().softmax(dim=1)\nx4 = torch.randn(1, 16, 64, 64)\nx5 = torch.randn(1)\n",
                "\nclass Linear(torch.nn.Module):\n    def __init__(self, in_features, out_features, bias=True):\n        super(Linear, self).__init__()\n        self.linear = torch.nn.Linear(in_features, out_features, bias)\n \n    def forward(self, query, key, value):\n        qk = torch.matmul(query, key.transpose(-2, -1))\n        inv_scale_factor = 64**0.5\n        scaled_qk = qk.div(inv_scale_factor)\n        softmax_qk = scaled_qk.softmax(dim=-1)\n        dropout_qk = torch.nn.functional.dropout(softmax_qk, p=0.1)\n        output = torch.matmul(dropout_qk, value)\n        return output\n\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.project = Linear(512, 1, bias=False)\n \n    def forward(self, query, key, value):\n        r1 = self.project(query, key, value)\n        return r1\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nquery = torch.randn(1, 512, 4)\nkey = torch.randn(1,4,5,5)\nvalue = torch.randn(1, 512, 4, 17, 17)\n\nr1 = m(query, key, value)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, dropout_p=0.2, dim=512, num_heads=4, window_size=512):\n        super().__init__()\n        self.scale_factor = (dim//num_heads)**(-0.5)\n        self.inv_scale_factor = (dim//num_heads)**(0.5)\n        self.linear_q = nn.Linear(dim, dim, bias=False)\n        self.linear_k = nn.Linear(dim, dim, bias=False)\n        self.wq = torch.nn.Linear(dim, dim, bias=False)\n        self.wk = torch.nn.Linear(dim, dim, bias=False)\n        self.wk = torch.nn.Linear(dim, dim, bias=False)\n        self.window_size = window_size\n \n    def forward(self, query, key, value):\n        q = torch.nn.functional.dropout(query, p=dropout_p)\n        q = self.linear_q(q)\n        k = torch.nn.functional.dropout(key, p=dropout_p)\n        k = self.linear_k(k)\n        v = torch.nn.functional.dropout(value, p=dropout_p)\n \n        q = q.unfold(2, self.window_size, 1).unfold(3, self.window_size, 1).transpose_(1, 2)\n        k = k.unfold(2, self.window_size, 1).unfold(3, self.window_size, 1).transpose_(1, 2)\n \n        q = self.wq(q).transpose(1, 2).transpose(2, 3)\n        k = self.wk(k).transpose(1, 2).transpose(2, 3)\n \n        q = q.transpose(1, 2)\n        k = k.transpose(1, 2)\n \n        unsqueezed = False\n        if q.dim() == 3:\n            q = q.unsqueeze(1)\n            unsqueezed = True\n        elif k.dim() == 3:\n            k = k.unsqueeze(1)\n            unsqueezed = True\n \n        qk = torch.matmul(q, k)\n \n        unfolded = False\n        if qk.dim() == 3:\n            qk = qk.unsqueeze(0)\n            unfolded = True\n        else:\n            qk = qk.unsqueeze(1).unsqueeze(-1)\n        \n        if unsqueezed:\n            qk = qk.squeeze(1)\n \n        qk = qk.mul(1/self.inv_scale_factor)\n \n        output = qk.softmax(dim=-1)\n        output = torch.nn.functional.dropout(output, p=dropout_p)\n        output = torch.matmul(output, v).squeeze(-1).unsqueeze(-1)\n \n        if unsqueezed:\n            output = output.squeeze(1)\n        if unfolded:\n            output = output.squeeze(0)\n        return output\n \n# Initializing the model\nm = Model()\n\n# Inputs to the model\nquery = torch.randn(1, 4, 32, 32)\nkey = torch.randn(1, 4, 32, 32)\nvalue = torch.randn(1, 4, 32, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n \n    def forward(self, q, k, v, p):\n        i_ = q.matmul(k.transpose(1, 2)) / p\n        o = torch.nn.functional.dropout(torch.nn.functional.softmax(i_, dim=-1), p=p).matmul(v)\n        return o\n\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nn = 3\np = 2.4\nq = torch.randn(1, 8, 128, 64)\nk = torch.randn(1, 8, 128, 64 * 2)\nv = torch.randn(1, 8, 128, 64 * 2)\n",
                "\nclass TransformerEncoderLayer(nn.TransformerEncoderLayer):\n    def __init__(self, num_heads, d_model, dim_feedforward=2048, dropout=0.1, activation=\"relu\"):\n        super(TransformerEncoderLayer, self).__init__()\n        self.self_attn = nn.MultiheadAttention(d_model, num_heads, dropout=dropout)\n        self.linear1 = nn.Linear(d_model, dim_feedforward)\n        self.dropout = nn.Dropout(dropout)\n        self.linear2 = nn.Linear(dim_feedforward, d_model)\n \n    def forward(self, src, mask=None, src_key_padding_mask=None):\n        r",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(32, 32, bias=True)\n \n    def forward(self, x1, x2, x3):\n        v1 = self.linear(x1).relu()\n        v2 = torch.matmul(x2, x3.T)\n        v3 = torch.div(v2, 0.1)\n        v4 = F.softmax(v3, dim=-1)\n        v5 = F.dropout(v4, p=0.4, training=torch.is_grad_enabled())\n        v6 = torch.matmul(v5, x2)\n        return v1 + v6\n\n# Initializing the model\nm = Model()\n \n# Inputs to the model\nx1 = torch.randn(4, 16)\nx2 = torch.randn(4, 8, 8)\nx3 = torch.randn(8, 8)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self, num_heads, max_length=4):\n        super().__init__()\n        self.qkv = torch.nn.Linear(max_length, max_length)\n        for p in self.qkv.parameters():\n            p.requires_grad = False\n        self.num_heads = num_heads\n        self.max_length = max_length\n\n    def forward(self, query, key, value, dropout_p=0):\n        k = self.qkv(key)\n        q = self.qkv(query)\n        v = self.qkv(value)\n        q = q.split(self.max_length // self.num_heads, dim=-1)\n        k = k.split(self.max_length // self.num_heads, dim=-1)\n        v = v.split(self.max_length // self.num_heads, dim=-1)\n        q = torch.cat(q, dim = 0)\n        k = torch.cat(k, dim = 0)\n        v = torch.cat(v, dim = 0)        \n        q *= self.max_length ** -0.5\n        qk = torch.matmul(q, k.transpose(-2, -1))\n        dropout_qk = torch.nn.functional.dropout(qk.softmax(dim=-1), p=dropout_p)\n        output = dropout_qk.matmul(v)\n        output = output.split(self.num_heads, dim=0)\n        output = torch.cat(output, dim=-1)\n        return output\n\n# Initializing the model\nm = Model(num_heads=3)\n\n# Inputs to the model\nquery = torch.randn(6, 8//3, 2, 3)\nkey = torch.randn(4, 8//3, 3, 3)\nvalue = torch.randn(4, 8//3, 3, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n \n    def forward(self, query, key, value):\n        qk = torch.matmul(query, key.transpose(-2, -1))\n        inv_scale_factor = 1 / np.sqrt(query.shape[-1])\n        scaled_qk = qk.div(inv_scale_factor)\n        softmax_qk = scaled_qk.softmax(dim=-1)\n        dropout_qk = torch.nn.functional.dropout(softmax_qk, p=0.1)\n        output = dropout_qk.matmul(value)\n        return output\n\n# Initializing the model\nm = Model()\n\n# Initialize inputs of the model\nquery = torch.randn(1, 1, 64)\nkey = torch.randn(1, 1, 128)\nvalue = torch.randn(1, 1, 128)\n\n# Inference through the model\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.inv_scale_factor = torch.nn.Parameter(torch.tensor(1.0))\n        self.dropout_p = 0.1\n \n    def forward(self, xq, xk, xv):\n        qk = torch.matmul(xq, xk.transpose(-2, -1))\n        scaled_qk = qk.div(self.inv_scale_factor)\n        softmax_qk = scaled_qk.softmax(dim=-1)\n        dropout_qk = torch.nn.functional.dropout(softmax_qk, p=self.dropout_p)\n        output = dropout_qk.matmul(xv)\n        return output\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nxq = torch.randn(3, 8, 32)\nxk = torch.randn(3, 8, 32)\nxv = torch.randn(3, 8, 32)\n",
                "\nclass SelfAttention(torch.nn.Module):\n    def __init__(self, num_heads, input_size, inner_size, dropout):\n        self.dropout = dropout\n        super().__init__()\n        self.query_mapping = Linear(input_size, inner_size, bias=False)\n        self.key_mapping = Linear(input_size, inner_size, bias=False)\n        self.value_mapping = Linear(input_size, inner_size, bias=False)\n        self.output_mapping = Linear(inner_size, input_size)\n \n    def forward(self, x1):\n        qk = torch.matmul(self.query_mapping(x1), self.key_mapping(x1).transpose(-2, -1))\n        scaled_qk = qk.div(torch.sqrt(torch.tensor(float(inner_size)).float()))\n        softmax_qk = scaled_qk.softmax(dim=-1)\n        dropout_qk = torch.nn.functional.dropout(softmax_qk, p=self.dropout)\n        output = self.output_mapping.forward(torch.matmul(dropout_qk, self.value_mapping(x1)))\n        return output\n\n# Initializing the model\nm = SelfAttention(16, 32, 64, 0.1)\n\n# Inputs to the model\nx1 = torch.randn(1, 16, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        pass\n \n    def forward(self, x1, x2, x3, x4, x5):\n        v1 = torch.matmul(x1, x2.transpose(-2, -1))\n        v2 = v1.div(x3)\n        v3 = v2.softmax(dim=-1)\n        v4 = torch.nn.functional.dropout(v3, x5)\n        v5 = torch.matmul(v4, x4)\n        return v5\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 8, 64, 64)\nx2 = torch.randn(1, 8, 3072)\nx3 = torch.randn(1, 16, 1, 1).log().softmax(dim=1)\nx4 = torch.randn(1, 16, 64, 64)\nx5 = torch.randn(1)\n",
                "\nclass Linear(torch.nn.Module):\n    def __init__(self, in_features, out_features, bias=True):\n        super(Linear, self).__init__()\n        self.linear = torch.nn.Linear(in_features, out_features, bias)\n \n    def forward(self, query, key, value):\n        qk = torch.matmul(query, key.transpose(-2, -1))\n        inv_scale_factor = 64**0.5\n        scaled_qk = qk.div(inv_scale_factor)\n        softmax_qk = scaled_qk.softmax(dim=-1)\n        dropout_qk = torch.nn.functional.dropout(softmax_qk, p=0.1)\n        output = torch.matmul(dropout_qk, value)\n        return output\n\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.project = Linear(512, 1, bias=False)\n \n    def forward(self, query, key, value):\n        r1 = self.project(query, key, value)\n        return r1\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nquery = torch.randn(1, 512, 4)\nkey = torch.randn(1,4,5,5)\nvalue = torch.randn(1, 512, 4, 17, 17)\n\nr1 = m(query, key, value)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, dropout_p=0.2, dim=512, num_heads=4, window_size=512):\n        super().__init__()\n        self.scale_factor = (dim//num_heads)**(-0.5)\n        self.inv_scale_factor = (dim//num_heads)**(0.5)\n        self.linear_q = nn.Linear(dim, dim, bias=False)\n        self.linear_k = nn.Linear(dim, dim, bias=False)\n        self.wq = torch.nn.Linear(dim, dim, bias=False)\n        self.wk = torch.nn.Linear(dim, dim, bias=False)\n        self.wk = torch.nn.Linear(dim, dim, bias=False)\n        self.window_size = window_size\n \n    def forward(self, query, key, value):\n        q = torch.nn.functional.dropout(query, p=dropout_p)\n        q = self.linear_q(q)\n        k = torch.nn.functional.dropout(key, p=dropout_p)\n        k = self.linear_k(k)\n        v = torch.nn.functional.dropout(value, p=dropout_p)\n \n        q = q.unfold(2, self.window_size, 1).unfold(3, self.window_size, 1).transpose_(1, 2)\n        k = k.unfold(2, self.window_size, 1).unfold(3, self.window_size, 1).transpose_(1, 2)\n \n        q = self.wq(q).transpose(1, 2).transpose(2, 3)\n        k = self.wk(k).transpose(1, 2).transpose(2, 3)\n \n        q = q.transpose(1, 2)\n        k = k.transpose(1, 2)\n \n        unsqueezed = False\n        if q.dim() == 3:\n            q = q.unsqueeze(1)\n            unsqueezed = True\n        elif k.dim() == 3:\n            k = k.unsqueeze(1)\n            unsqueezed = True\n \n        qk = torch.matmul(q, k)\n \n        unfolded = False\n        if qk.dim() == 3:\n            qk = qk.unsqueeze(0)\n            unfolded = True\n        else:\n            qk = qk.unsqueeze(1).unsqueeze(-1)\n        \n        if unsqueezed:\n            qk = qk.squeeze(1)\n \n        qk = qk.mul(1/self.inv_scale_factor)\n \n        output = qk.softmax(dim=-1)\n        output = torch.nn.functional.dropout(output, p=dropout_p)\n        output = torch.matmul(output, v).squeeze(-1).unsqueeze(-1)\n \n        if unsqueezed:\n            output = output.squeeze(1)\n        if unfolded:\n            output = output.squeeze(0)\n        return output\n \n# Initializing the model\nm = Model()\n\n# Inputs to the model\nquery = torch.randn(1, 4, 32, 32)\nkey = torch.randn(1, 4, 32, 32)\nvalue = torch.randn(1, 4, 32, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n \n    def forward(self, q, k, v, p):\n        i_ = q.matmul(k.transpose(1, 2)) / p\n        o = torch.nn.functional.dropout(torch.nn.functional.softmax(i_, dim=-1), p=p).matmul(v)\n        return o\n\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nn = 3\np = 2.4\nq = torch.randn(1, 8, 128, 64)\nk = torch.randn(1, 8, 128, 64 * 2)\nv = torch.randn(1, 8, 128, 64 * 2)\n",
                "\nclass TransformerEncoderLayer(nn.TransformerEncoderLayer):\n    def __init__(self, num_heads, d_model, dim_feedforward=2048, dropout=0.1, activation=\"relu\"):\n        super(TransformerEncoderLayer, self).__init__()\n        self.self_attn = nn.MultiheadAttention(d_model, num_heads, dropout=dropout)\n        self.linear1 = nn.Linear(d_model, dim_feedforward)\n        self.dropout = nn.Dropout(dropout)\n        self.linear2 = nn.Linear(dim_feedforward, d_model)\n \n    def forward(self, src, mask=None, src_key_padding_mask=None):\n        r",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(32, 32, bias=True)\n \n    def forward(self, x1, x2, x3):\n        v1 = self.linear(x1).relu()\n        v2 = torch.matmul(x2, x3.T)\n        v3 = torch.div(v2, 0.1)\n        v4 = F.softmax(v3, dim=-1)\n        v5 = F.dropout(v4, p=0.4, training=torch.is_grad_enabled())\n        v6 = torch.matmul(v5, x2)\n        return v1 + v6\n\n# Initializing the model\nm = Model()\n \n# Inputs to the model\nx1 = torch.randn(4, 16)\nx2 = torch.randn(4, 8, 8)\nx3 = torch.randn(8, 8)\n"
            ],
            "g_time": 26.44774842262268
        }
    }
}
{
    "opt": "binary_unary=2",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 8, 1, stride=4, padding=(3, 2))\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        return v1\n# Inputs to the model\nx1 = torch.randn(1, 3, 112, 112)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 64, 3, stride=1, padding=1)\n        self.conv2 = torch.nn.Conv2d(64, 64, 1, stride=1, padding=0)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = self.conv2(v1)\n        v3 = v2 - 9\n        v4 = F.leaky_relu(v3, negative_slope=0.05)\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 3, 52, 400)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 16, 1, stride=1, padding=0)\n        self.conv2 = torch.nn.Conv2d(16, 16, 5, stride=1, padding=2)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = self.conv2(v1)\n        v3 = v2 - 11\n        v4 = F.relu(v3)\n        x2 = v4.flatten(1)\n        return x2\n# Inputs to the model\nx1 = torch.randn(1, 3, 362, 362)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(16, 16, 3, stride=1, padding=0)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 - 0.0023\n        v3 = F.relu(v2)\n        v4 = torch.nn.functional.relu(v3)\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 16, 119, 119)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv2d = torch.nn.Conv2d(in_channels=3, out_channels=96, kernel_size=11, stride=4)\n    def forward(self, x):\n        v1 = self.conv2d(x) - 12\n        v2 = F.relu(v1)\n        return v2\n# Inputs to the model\nx = torch.randn(1, 3, 224, 224)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(256, 512, 1, stride=1, padding=0, bias=False)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 - 1.0\n        v3 = v2 * 2.0\n        v4 = F.relu(v3)\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 256, 28, 28)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(64, 16, kernel_size=(5, 5), stride=(1, 1), padding=(5, 5))\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 - 128.0\n        v3 = F.relu(v2)\n        v4 = torch.zeros_like(v3)\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 64, 20, 20)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(64, 128, 1, padding=0)\n        self.conv2 = torch.nn.Conv2d(128, 128, 1, padding=0)\n        self.fc = torch.nn.Linear(128, 8)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = self.conv2(v1)\n        v3 = v2 - 0.3\n        v4 = F.relu(v3)\n        v5 = self.fc(v4)\n        v6 = v5 - torch.floor(v5)\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 64, 128, 128)\n",
                "\nimport collections\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.modules = collections.OrderedDict()\n        self.modules['conv1'] = torch.nn.Conv2d(4, 4, 3, stride=2, padding=1)\n        self.modules['conv2'] = torch.nn.Conv2d(4, 4, 1, stride=1)\n        self.sequential_1 = torch.nn.Sequential(self.modules)\n        self.max_pool2d_1 = torch.nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n        self.modules_1 = collections.OrderedDict()\n        self.modules_1['conv'] = torch.nn.Conv2d(4, 4, 3, stride=2, padding=1)\n        self.modules_1['leakyrelu'] = torch.nn.LeakyReLU(negative_slope=0.01)\n        self.sequential_2 = torch.nn.Sequential(self.modules_1)\n    def forward(self, x1):\n        v1 = self.sequential_1(x1)\n        v2 = self.max_pool2d_1(v1)\n        v3 = self.sequential_2(v2)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 4, 183, 183)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 16, 1, stride=1, padding=0)\n        self.conv2 = torch.nn.Conv2d(16, 64, 3, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = self.conv2(v1)\n        v3 = v2 - 31\n        v4 = F.relu(v3)\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 3, 416, 416)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 8, 1, stride=4, padding=(3, 2))\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        return v1\n# Inputs to the model\nx1 = torch.randn(1, 3, 112, 112)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 64, 3, stride=1, padding=1)\n        self.conv2 = torch.nn.Conv2d(64, 64, 1, stride=1, padding=0)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = self.conv2(v1)\n        v3 = v2 - 9\n        v4 = F.leaky_relu(v3, negative_slope=0.05)\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 3, 52, 400)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 16, 1, stride=1, padding=0)\n        self.conv2 = torch.nn.Conv2d(16, 16, 5, stride=1, padding=2)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = self.conv2(v1)\n        v3 = v2 - 11\n        v4 = F.relu(v3)\n        x2 = v4.flatten(1)\n        return x2\n# Inputs to the model\nx1 = torch.randn(1, 3, 362, 362)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(16, 16, 3, stride=1, padding=0)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 - 0.0023\n        v3 = F.relu(v2)\n        v4 = torch.nn.functional.relu(v3)\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 16, 119, 119)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv2d = torch.nn.Conv2d(in_channels=3, out_channels=96, kernel_size=11, stride=4)\n    def forward(self, x):\n        v1 = self.conv2d(x) - 12\n        v2 = F.relu(v1)\n        return v2\n# Inputs to the model\nx = torch.randn(1, 3, 224, 224)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(256, 512, 1, stride=1, padding=0, bias=False)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 - 1.0\n        v3 = v2 * 2.0\n        v4 = F.relu(v3)\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 256, 28, 28)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(64, 16, kernel_size=(5, 5), stride=(1, 1), padding=(5, 5))\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 - 128.0\n        v3 = F.relu(v2)\n        v4 = torch.zeros_like(v3)\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 64, 20, 20)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(64, 128, 1, padding=0)\n        self.conv2 = torch.nn.Conv2d(128, 128, 1, padding=0)\n        self.fc = torch.nn.Linear(128, 8)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = self.conv2(v1)\n        v3 = v2 - 0.3\n        v4 = F.relu(v3)\n        v5 = self.fc(v4)\n        v6 = v5 - torch.floor(v5)\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 64, 128, 128)\n",
                "\nimport collections\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.modules = collections.OrderedDict()\n        self.modules['conv1'] = torch.nn.Conv2d(4, 4, 3, stride=2, padding=1)\n        self.modules['conv2'] = torch.nn.Conv2d(4, 4, 1, stride=1)\n        self.sequential_1 = torch.nn.Sequential(self.modules)\n        self.max_pool2d_1 = torch.nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n        self.modules_1 = collections.OrderedDict()\n        self.modules_1['conv'] = torch.nn.Conv2d(4, 4, 3, stride=2, padding=1)\n        self.modules_1['leakyrelu'] = torch.nn.LeakyReLU(negative_slope=0.01)\n        self.sequential_2 = torch.nn.Sequential(self.modules_1)\n    def forward(self, x1):\n        v1 = self.sequential_1(x1)\n        v2 = self.max_pool2d_1(v1)\n        v3 = self.sequential_2(v2)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 4, 183, 183)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 16, 1, stride=1, padding=0)\n        self.conv2 = torch.nn.Conv2d(16, 64, 3, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = self.conv2(v1)\n        v3 = v2 - 31\n        v4 = F.relu(v3)\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 3, 416, 416)\n"
            ],
            "g_time": 11.935310363769531
        }
    }
}
{
    "opt": "unary=17",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.model = torch.nn.Sequential(torch.nn.ConvTranspose2d(1, 8, 3, stride=2), torch.nn.PReLU())\n    def forward(self, x):\n        v1 = self.model(x)\n        v2 = torch.relu(v1)\n        return v2\n# Inputs to the model\nx = torch.randn(1, 1, 10, 10)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv2d = torch.nn.ConvTranspose1d(64, 128, 5, stride=3)\n    def forward(self, x1):\n        v1 = self.conv2d(x1)\n        v2 = torch.relu(v1)\n        v3 = torch.sigmoid(v2)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 64, 49)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(19, 19, 1)\n        self.conv0 = torch.nn.ConvTranspose2d(19, 19, 1)\n        self.relu = torch.nn.ReLU()\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = self.conv0(v1)\n        v3 = self.relu(v2)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 19, 224, 224)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1):\n        v1 = torch.nn.ConvTranspose2d(4, 32, (1, 1), stride=(1, 1), groups=4)(x1)\n        v2 = torch.relu(v1)\n        v3 = torch.nn.ConvTranspose2d(32, 16, 1, stride=1, padding=0)(v2)\n        v4 = torch.sigmoid(v3)\n        v5 = torch.nn.ConvTranspose2d(16, 32, (3, 3), stride=(1, 1), output_padding=(1, 1), groups=4)(v4)\n        v6 = torch.sigmoid(v5)\n        v7 = torch.nn.ConvTranspose2d(32, 64, kernel_size=(1, 1), stride=(2, 2), padding=0, output_padding=(0, 0))(v6)\n        v8 = torch.sigmoid(v7)\n        return v8\n# Inputs to the model\nx1 = torch.randn(1, 4, 1024, 512)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(64, 3, kernel_size=4, stride=2, padding=1, dilation=2)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = torch.relu(v1)\n        v3 = torch.sigmoid(v2)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 64, 1024, 1024)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.ConvTranspose2d(128, 32, 3, stride=1, padding=1)\n        self.conv2 = torch.nn.ConvTranspose2d(32, 1, 3, stride=2, padding=0)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = torch.relu(v1)\n        v3 = self.conv2(v2)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 128, 32, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(32, 3, (3,3), padding=(2, 1), stride=(2, 2))\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = torch.relu(v1)\n        return v1\n# Inputs to the model\nx1 = torch.randn(1, 32, 176, 264)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv0 = torch.nn.ConvTranspose2d(3, 64, (3, 3))\n        self.conv1 = torch.nn.ConvTranspose2d(64, 64, (3, 3))\n    def forward(self, x1):\n        v1 = self.conv0(x1)\n        v2 = self.conv1((v1))\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 3, 224, 224)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 1, 1)\n        self.conv1 = torch.nn.Conv2d(1, 2, 1)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = torch.relu(v1)\n        v3 = self.conv1(v2)\n        v4 = torch.sigmoid(v3)\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(64, 1, 3, padding=1)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        return v1\n# Inputs to the model\nx1 = torch.randn(1, 64, 25, 25)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.model = torch.nn.Sequential(torch.nn.ConvTranspose2d(1, 8, 3, stride=2), torch.nn.PReLU())\n    def forward(self, x):\n        v1 = self.model(x)\n        v2 = torch.relu(v1)\n        return v2\n# Inputs to the model\nx = torch.randn(1, 1, 10, 10)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv2d = torch.nn.ConvTranspose1d(64, 128, 5, stride=3)\n    def forward(self, x1):\n        v1 = self.conv2d(x1)\n        v2 = torch.relu(v1)\n        v3 = torch.sigmoid(v2)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 64, 49)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(19, 19, 1)\n        self.conv0 = torch.nn.ConvTranspose2d(19, 19, 1)\n        self.relu = torch.nn.ReLU()\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = self.conv0(v1)\n        v3 = self.relu(v2)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 19, 224, 224)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1):\n        v1 = torch.nn.ConvTranspose2d(4, 32, (1, 1), stride=(1, 1), groups=4)(x1)\n        v2 = torch.relu(v1)\n        v3 = torch.nn.ConvTranspose2d(32, 16, 1, stride=1, padding=0)(v2)\n        v4 = torch.sigmoid(v3)\n        v5 = torch.nn.ConvTranspose2d(16, 32, (3, 3), stride=(1, 1), output_padding=(1, 1), groups=4)(v4)\n        v6 = torch.sigmoid(v5)\n        v7 = torch.nn.ConvTranspose2d(32, 64, kernel_size=(1, 1), stride=(2, 2), padding=0, output_padding=(0, 0))(v6)\n        v8 = torch.sigmoid(v7)\n        return v8\n# Inputs to the model\nx1 = torch.randn(1, 4, 1024, 512)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(64, 3, kernel_size=4, stride=2, padding=1, dilation=2)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = torch.relu(v1)\n        v3 = torch.sigmoid(v2)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 64, 1024, 1024)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.ConvTranspose2d(128, 32, 3, stride=1, padding=1)\n        self.conv2 = torch.nn.ConvTranspose2d(32, 1, 3, stride=2, padding=0)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = torch.relu(v1)\n        v3 = self.conv2(v2)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 128, 32, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(32, 3, (3,3), padding=(2, 1), stride=(2, 2))\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = torch.relu(v1)\n        return v1\n# Inputs to the model\nx1 = torch.randn(1, 32, 176, 264)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv0 = torch.nn.ConvTranspose2d(3, 64, (3, 3))\n        self.conv1 = torch.nn.ConvTranspose2d(64, 64, (3, 3))\n    def forward(self, x1):\n        v1 = self.conv0(x1)\n        v2 = self.conv1((v1))\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 3, 224, 224)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 1, 1)\n        self.conv1 = torch.nn.Conv2d(1, 2, 1)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = torch.relu(v1)\n        v3 = self.conv1(v2)\n        v4 = torch.sigmoid(v3)\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(64, 1, 3, padding=1)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        return v1\n# Inputs to the model\nx1 = torch.randn(1, 64, 25, 25)\n"
            ],
            "g_time": 10.006079912185669
        }
    }
}
{
    "opt": "unary=15",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 64, 7, stride=1, padding=3)\n        self.conv2 = torch.nn.Conv2d(64, 128, 3, stride=1, padding=1)\n        self.conv3 = torch.nn.Conv2d(128, 64, 1, stride=1, padding=0)\n        self.conv4 = torch.nn.Conv2d(64, 32, 3, stride=1, padding=1)\n        self.conv5 = torch.nn.Conv2d(32, 3, 3, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = torch.relu(v1)\n        v3 = self.conv2(v2)\n        v4 = torch.relu(v3)\n        v5 = self.conv3(v4)\n        v6 = torch.relu(v5)\n        v7 = self.conv4(v6)\n        v8 = torch.relu(v7)\n        v9 = self.conv5(v8)\n        v10 = torch.relu(v9)\n        return v10\n# Inputs to the model\nx1 = torch.randn(3, 3, 224, 224)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv1d(1, 1, 3, stride=1, padding=1)\n        self.conv2 = torch.nn.Conv1d(1, 1, 5, stride=1, padding=2)\n        self.conv3 = torch.nn.Conv1d(1, 1, 7, stride=1, padding=3)\n        self.conv4 = torch.nn.Conv1d(1, 1, 9, stride=1, padding=4)\n    def forward(self, x1):\n        v1 = x1\n        v2 = self.conv1(v1)\n        v3 = torch.relu(v2)\n        v4 = self.conv2(v3)\n        v5 = torch.relu(v4)\n        v6 = v5.permute(0, 2, 1)\n        v7 = self.conv3(v6)\n        v8 = v7.contiguous().permute(0, 2, 1)\n        v9 = torch.relu(v8)\n        v10 = v9.permute(0, 2, 1)\n        v11 = self.conv4(v10)\n        v12 = v11.permute(0, 2, 1)\n        v13 = torch.relu(v12)\n        return v13\n# Inputs to the model\nx1 = torch.randn(2, 1, 1001)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(1, 64, 1, stride=2, padding=0)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v3 = torch.nn.functional.pad(v1, (0, 0, 0, 0), 'constant', 8414204576.0)\n        v4 = 71617988457.0\n        return v3\n# Inputs to the model\nx1 = torch.randn(4, 1, 28, 28)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 32, 3, stride=1, padding=1)\n        self.conv2 = torch.nn.Conv2d(32, 128, 3, stride=2, padding=1)\n        self.conv3 = torch.nn.Conv2d(128, 128, 3, stride=1, padding=1)\n        self.conv4 = torch.nn.Conv2d(128, 128, 3, stride=1, padding=2)\n        self.conv5 = torch.nn.Conv2d(128, 128, 1, stride=2, padding=2)\n        self.conv6 = torch.nn.Conv2d(128, 64, 3, stride=1, padding=1)\n        self.conv7 = torch.nn.Conv2d(64, 32, 3, stride=1, padding=1)\n        self.conv8 = torch.nn.Conv2d(32, 3, 1, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = torch.relu(v1)\n        v3 = self.conv2(v2)\n        v4 = torch.relu(v3)\n        v5 = self.conv3(v4)\n        v6 = torch.relu(v5)\n        v7 = self.conv4(v6)\n        v8 = torch.relu(v7)\n        v9 = self.conv5(v8)\n        v10 = torch.relu(v9)\n        v11 = self.conv6(v10)\n        v12 = torch.relu(v11)\n        v13 = self.conv7(v12)\n        v14 = torch.relu(v13)\n        v15 = self.conv8(v14)\n        v16 = torch.relu(v15)\n        return v16\n# Inputs to the model\nx1 = torch.randn(4, 3, 512, 512)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 64, 5, stride=1, padding=0)\n        self.conv2 = torch.nn.Conv2d(64, 32, 5, stride=2, padding=2)\n        self.conv3 = torch.nn.Conv2d(32, 16, 3, stride=1, padding=1)\n        self.conv4 = torch.nn.Conv2d(16, 1, 3, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = torch.relu(v1)\n        v3 = self.conv2(v2)\n        v4 = torch.relu(v3)\n        v5 = self.conv3(v4)\n        v6 = torch.relu(v5)\n        v7 = self.conv4(v6)\n        v8 = torch.relu(v7)\n        return v8\n# Inputs to the model\nx1 = torch.randn(1, 3, 256, 256)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 6, 5, stride=1, padding=2)\n        self.conv2 = torch.nn.Conv2d(6, 16, 5, stride=2, padding=2)\n        self.conv3 = torch.nn.Conv2d(16, 12, 3, stride=1, padding=1)\n        self.conv4 = torch.nn.Conv2d(12, 12, 3, stride=2, padding=1)\n        self.conv5 = torch.nn.Conv2d(12, 64, 1, stride=1, padding=0)\n        self.conv6 = torch.nn.Conv2d(64, 32, 3, stride=1, padding=1)\n        self.conv7 = torch.nn.Conv2d(32, 3, 3, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = torch.relu(v1)\n        v3 = self.conv2(v2)\n        v4 = torch.relu(v3)\n        v5 = self.conv3(v4)\n        v6 = torch.relu(v5)\n        v7 = self.conv4(v6)\n        v8 = torch.relu(v7)\n        v9 = self.conv5(v8)\n        v10 = torch.relu(v9)\n        v11 = self.conv6(v10)\n        v12 = torch.relu(v11)\n        v13 = self.conv7(v12)\n        v14 = torch.relu(v13)\n        return v14\n# Inputs to the model\nx1 = torch.randn(4, 3, 512, 512)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear1 = torch.nn.Linear(288, 1024, bias=True)\n        self.linear2 = torch.nn.Linear(1024, 1024, bias=True)\n        self.linear3 = torch.nn.Linear(1024, 16, bias=True)\n    def forward(self, x1):\n        v1 = self.linear1(x1)\n        v2 = torch.relu(v1)\n        v3 = self.linear2(v2)\n        v4 = torch.relu(v3)\n        v5 = self.linear3(v4)\n        v6 = torch.view(v5, (-1, 16, 1, 4))\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 288)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 5, 5, stride=2, padding=2)\n        self.conv2 = torch.nn.Conv2d(5, 10, 5, stride=2, padding=2)\n        self.conv3 = torch.nn.Conv2d(10, 20, 5, stride=2, padding=2)\n        self.conv4 = torch.nn.Conv2d(20, 48, 3, stride=2, padding=2)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = torch.relu(v1)\n        v3 = self.conv2(v2)\n        v4 = torch.tanh(v3)\n        v5 = self.conv3(v4)\n        v6 = torch.relu(v5)\n        v7 = self.conv4(v6)\n        v8 = torch.sigmoid(v7)\n        return v8\n# Inputs to the model\nx1 = torch.randn(4, 3, 512, 512)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(1, 32, 1, stride=1, padding=0)\n        self.conv2 = torch.nn.Conv2d(32, 32, 3, stride=1, padding=1)\n        self.conv3 = torch.nn.Conv2d(32, 32, 1, stride=1, padding=0)\n        self.conv4 = torch.nn.Conv2d(32, 32, 3, stride=1, padding=1)\n        self.conv5 = torch.nn.Conv2d(32, 32, 1, stride=1, padding=0)\n        self.conv6 = torch.nn.Conv2d(32, 32, 3, stride=1, padding=1)\n        self.conv7 = torch.nn.Conv2d(32, 32, 1, stride=1, padding=0)\n        self.conv8 = torch.nn.Conv2d(32, 16, 1, stride=1, padding=0)\n        self.conv9 = torch.nn.Conv2d(16, 16, 3, stride=1, padding=1)\n        self.conv10 = torch.nn.Conv2d(16, 16, 1, stride=1, padding=0)\n        self.conv11 = torch.nn.Conv2d(16, 16, 3, stride=1, padding=1)\n        self.conv12 = torch.nn.Conv2d(16, 16, 1, stride=1, padding=0)\n        self.conv13 = torch.nn.Conv2d(16, 2, 1, stride=1, padding=0)\n    def forward(self, x1):\n        v1 = torch.nn.functional.pad(x1, (2, 2, 3, 3), 'constant', 0)\n        v2 = self.conv1(v1)\n        v3 = torch.relu(v2)\n        v4 = self.conv2(v3)\n        v5 = torch.relu(v4)\n        v6 = self.conv3(v5)\n        v7 = torch.relu(v6)\n        v8 = self.conv4(v7)\n        v9 = torch.relu(v8)\n        v10 = self.conv5(v9)\n        v11 = torch.relu(v10)\n        v12 = self.conv6(v11)\n        v13 = torch.relu(v12)\n        v14 = self.conv7(v13)\n        v15 = torch.relu(v14)\n        v16 = self.conv8(v15)\n        v17 = torch.relu(v16)\n        v18 = self.conv9(v17)\n        v19 = torch.relu(v18)\n        v20 = self.conv10(v19)\n        v21 = torch.relu(v20)\n        v22 = self.conv11(v21)\n        v23 = torch.relu(v22)\n        v24 = self.conv12(v23)\n        v25 = torch.relu(v24)\n        v26 = self.conv13(v25)\n        v27 = torch.nn.functional.pad(v26, (1, 1, 2, 2), 'constant', 0)\n        return v27\n# Inputs to the model\nx1 = torch.randn(1, 1, 15, 15)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(6, 20, 3, stride=1, padding=1)\n        self.conv2 = torch.nn.Conv2d(20, 40, 3, stride=1, padding=1)\n        self.conv3 = torch.nn.Conv2d(40, 60, 3, stride=1, padding=1)\n        self.conv4 = torch.nn.Conv2d(60, 80, 3, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = torch.nn.functional.pad(x1, (2, 2, 0, 0), 'constant', 0)\n        v2 = self.conv1(v1)\n        v3 = self.conv2(v2)\n        v4 = self.conv3(v3)\n        v5 = self.conv4(v4)\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 6, 512, 512)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 64, 7, stride=1, padding=3)\n        self.conv2 = torch.nn.Conv2d(64, 128, 3, stride=1, padding=1)\n        self.conv3 = torch.nn.Conv2d(128, 64, 1, stride=1, padding=0)\n        self.conv4 = torch.nn.Conv2d(64, 32, 3, stride=1, padding=1)\n        self.conv5 = torch.nn.Conv2d(32, 3, 3, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = torch.relu(v1)\n        v3 = self.conv2(v2)\n        v4 = torch.relu(v3)\n        v5 = self.conv3(v4)\n        v6 = torch.relu(v5)\n        v7 = self.conv4(v6)\n        v8 = torch.relu(v7)\n        v9 = self.conv5(v8)\n        v10 = torch.relu(v9)\n        return v10\n# Inputs to the model\nx1 = torch.randn(3, 3, 224, 224)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv1d(1, 1, 3, stride=1, padding=1)\n        self.conv2 = torch.nn.Conv1d(1, 1, 5, stride=1, padding=2)\n        self.conv3 = torch.nn.Conv1d(1, 1, 7, stride=1, padding=3)\n        self.conv4 = torch.nn.Conv1d(1, 1, 9, stride=1, padding=4)\n    def forward(self, x1):\n        v1 = x1\n        v2 = self.conv1(v1)\n        v3 = torch.relu(v2)\n        v4 = self.conv2(v3)\n        v5 = torch.relu(v4)\n        v6 = v5.permute(0, 2, 1)\n        v7 = self.conv3(v6)\n        v8 = v7.contiguous().permute(0, 2, 1)\n        v9 = torch.relu(v8)\n        v10 = v9.permute(0, 2, 1)\n        v11 = self.conv4(v10)\n        v12 = v11.permute(0, 2, 1)\n        v13 = torch.relu(v12)\n        return v13\n# Inputs to the model\nx1 = torch.randn(2, 1, 1001)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(1, 64, 1, stride=2, padding=0)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v3 = torch.nn.functional.pad(v1, (0, 0, 0, 0), 'constant', 8414204576.0)\n        v4 = 71617988457.0\n        return v3\n# Inputs to the model\nx1 = torch.randn(4, 1, 28, 28)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 32, 3, stride=1, padding=1)\n        self.conv2 = torch.nn.Conv2d(32, 128, 3, stride=2, padding=1)\n        self.conv3 = torch.nn.Conv2d(128, 128, 3, stride=1, padding=1)\n        self.conv4 = torch.nn.Conv2d(128, 128, 3, stride=1, padding=2)\n        self.conv5 = torch.nn.Conv2d(128, 128, 1, stride=2, padding=2)\n        self.conv6 = torch.nn.Conv2d(128, 64, 3, stride=1, padding=1)\n        self.conv7 = torch.nn.Conv2d(64, 32, 3, stride=1, padding=1)\n        self.conv8 = torch.nn.Conv2d(32, 3, 1, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = torch.relu(v1)\n        v3 = self.conv2(v2)\n        v4 = torch.relu(v3)\n        v5 = self.conv3(v4)\n        v6 = torch.relu(v5)\n        v7 = self.conv4(v6)\n        v8 = torch.relu(v7)\n        v9 = self.conv5(v8)\n        v10 = torch.relu(v9)\n        v11 = self.conv6(v10)\n        v12 = torch.relu(v11)\n        v13 = self.conv7(v12)\n        v14 = torch.relu(v13)\n        v15 = self.conv8(v14)\n        v16 = torch.relu(v15)\n        return v16\n# Inputs to the model\nx1 = torch.randn(4, 3, 512, 512)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 64, 5, stride=1, padding=0)\n        self.conv2 = torch.nn.Conv2d(64, 32, 5, stride=2, padding=2)\n        self.conv3 = torch.nn.Conv2d(32, 16, 3, stride=1, padding=1)\n        self.conv4 = torch.nn.Conv2d(16, 1, 3, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = torch.relu(v1)\n        v3 = self.conv2(v2)\n        v4 = torch.relu(v3)\n        v5 = self.conv3(v4)\n        v6 = torch.relu(v5)\n        v7 = self.conv4(v6)\n        v8 = torch.relu(v7)\n        return v8\n# Inputs to the model\nx1 = torch.randn(1, 3, 256, 256)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 6, 5, stride=1, padding=2)\n        self.conv2 = torch.nn.Conv2d(6, 16, 5, stride=2, padding=2)\n        self.conv3 = torch.nn.Conv2d(16, 12, 3, stride=1, padding=1)\n        self.conv4 = torch.nn.Conv2d(12, 12, 3, stride=2, padding=1)\n        self.conv5 = torch.nn.Conv2d(12, 64, 1, stride=1, padding=0)\n        self.conv6 = torch.nn.Conv2d(64, 32, 3, stride=1, padding=1)\n        self.conv7 = torch.nn.Conv2d(32, 3, 3, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = torch.relu(v1)\n        v3 = self.conv2(v2)\n        v4 = torch.relu(v3)\n        v5 = self.conv3(v4)\n        v6 = torch.relu(v5)\n        v7 = self.conv4(v6)\n        v8 = torch.relu(v7)\n        v9 = self.conv5(v8)\n        v10 = torch.relu(v9)\n        v11 = self.conv6(v10)\n        v12 = torch.relu(v11)\n        v13 = self.conv7(v12)\n        v14 = torch.relu(v13)\n        return v14\n# Inputs to the model\nx1 = torch.randn(4, 3, 512, 512)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear1 = torch.nn.Linear(288, 1024, bias=True)\n        self.linear2 = torch.nn.Linear(1024, 1024, bias=True)\n        self.linear3 = torch.nn.Linear(1024, 16, bias=True)\n    def forward(self, x1):\n        v1 = self.linear1(x1)\n        v2 = torch.relu(v1)\n        v3 = self.linear2(v2)\n        v4 = torch.relu(v3)\n        v5 = self.linear3(v4)\n        v6 = torch.view(v5, (-1, 16, 1, 4))\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 288)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 5, 5, stride=2, padding=2)\n        self.conv2 = torch.nn.Conv2d(5, 10, 5, stride=2, padding=2)\n        self.conv3 = torch.nn.Conv2d(10, 20, 5, stride=2, padding=2)\n        self.conv4 = torch.nn.Conv2d(20, 48, 3, stride=2, padding=2)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = torch.relu(v1)\n        v3 = self.conv2(v2)\n        v4 = torch.tanh(v3)\n        v5 = self.conv3(v4)\n        v6 = torch.relu(v5)\n        v7 = self.conv4(v6)\n        v8 = torch.sigmoid(v7)\n        return v8\n# Inputs to the model\nx1 = torch.randn(4, 3, 512, 512)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(1, 32, 1, stride=1, padding=0)\n        self.conv2 = torch.nn.Conv2d(32, 32, 3, stride=1, padding=1)\n        self.conv3 = torch.nn.Conv2d(32, 32, 1, stride=1, padding=0)\n        self.conv4 = torch.nn.Conv2d(32, 32, 3, stride=1, padding=1)\n        self.conv5 = torch.nn.Conv2d(32, 32, 1, stride=1, padding=0)\n        self.conv6 = torch.nn.Conv2d(32, 32, 3, stride=1, padding=1)\n        self.conv7 = torch.nn.Conv2d(32, 32, 1, stride=1, padding=0)\n        self.conv8 = torch.nn.Conv2d(32, 16, 1, stride=1, padding=0)\n        self.conv9 = torch.nn.Conv2d(16, 16, 3, stride=1, padding=1)\n        self.conv10 = torch.nn.Conv2d(16, 16, 1, stride=1, padding=0)\n        self.conv11 = torch.nn.Conv2d(16, 16, 3, stride=1, padding=1)\n        self.conv12 = torch.nn.Conv2d(16, 16, 1, stride=1, padding=0)\n        self.conv13 = torch.nn.Conv2d(16, 2, 1, stride=1, padding=0)\n    def forward(self, x1):\n        v1 = torch.nn.functional.pad(x1, (2, 2, 3, 3), 'constant', 0)\n        v2 = self.conv1(v1)\n        v3 = torch.relu(v2)\n        v4 = self.conv2(v3)\n        v5 = torch.relu(v4)\n        v6 = self.conv3(v5)\n        v7 = torch.relu(v6)\n        v8 = self.conv4(v7)\n        v9 = torch.relu(v8)\n        v10 = self.conv5(v9)\n        v11 = torch.relu(v10)\n        v12 = self.conv6(v11)\n        v13 = torch.relu(v12)\n        v14 = self.conv7(v13)\n        v15 = torch.relu(v14)\n        v16 = self.conv8(v15)\n        v17 = torch.relu(v16)\n        v18 = self.conv9(v17)\n        v19 = torch.relu(v18)\n        v20 = self.conv10(v19)\n        v21 = torch.relu(v20)\n        v22 = self.conv11(v21)\n        v23 = torch.relu(v22)\n        v24 = self.conv12(v23)\n        v25 = torch.relu(v24)\n        v26 = self.conv13(v25)\n        v27 = torch.nn.functional.pad(v26, (1, 1, 2, 2), 'constant', 0)\n        return v27\n# Inputs to the model\nx1 = torch.randn(1, 1, 15, 15)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(6, 20, 3, stride=1, padding=1)\n        self.conv2 = torch.nn.Conv2d(20, 40, 3, stride=1, padding=1)\n        self.conv3 = torch.nn.Conv2d(40, 60, 3, stride=1, padding=1)\n        self.conv4 = torch.nn.Conv2d(60, 80, 3, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = torch.nn.functional.pad(x1, (2, 2, 0, 0), 'constant', 0)\n        v2 = self.conv1(v1)\n        v3 = self.conv2(v2)\n        v4 = self.conv3(v3)\n        v5 = self.conv4(v4)\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 6, 512, 512)\n"
            ],
            "g_time": 30.608712196350098
        }
    }
}
{
    "opt": "unary=21",
    "response": {
        "0": {
            "raw": [
                "\nclass MyModel(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.avgpool = torch.nn.AvgPool2d((3, 2), stride=1)\n        self.conv1 = torch.nn.Conv2d(3, 64, (1, 1), stride=(1, 1))\n        self.conv2 = torch.nn.Conv2d(64, 128, (3, 3), stride=1, padding=1)\n        self.conv3 = torch.nn.Conv2d(128, 128, (1, 1), stride=(1, 1))\n        self.relu = torch.nn.ReLU()\n        self.maxpool = torch.nn.MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\n    def forward(self, x):\n        x = self.relu(self.conv1(self.avgpool(x)))\n        x = self.relu(self.conv2(x))\n        x = self.relu(self.conv3(x))\n        x = self.maxpool(x)\n        return x\n# Inputs to the model\nx = torch.randn(3, 1, 56, 56)\n",
                "\nclass MyModel(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.features = torch.nn.Sequential(torch.nn.Conv2d(1, 22, 1, stride=1, padding=0))\n    def forward(self, x):\n        return torch.tanh(self.features(x))\n# Inputs to the model\nx = torch.randn(120, 1, 80, 20)\n",
                "\nclass MyModel(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.features = torch.nn.LSTM(16, 1024)\n    def forward(self, x):\n        return torch.tanh(self.features(x))\n# Inputs to the model\nx = torch.randn(16, 1, 1024)\n",
                "\nclass MyModel(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(1, 6, 3)\n    def forward(self, x):\n        v1 = self.conv1(x)\n        v2 = torch.tanh(x + v1)\n        return v2\n# Inputs to the model\nx = torch.randn(1, 1, 224, 256)\n",
                "\nclass ModelTanh(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 17, (1, 7), groups=17, bias=True)\n    def forward(self, x):\n        v1 = self.conv(x)\n        v2 = torch.tanh(v1)\n        return v2\n# Inputs to the model\nx = torch.randn(7, 3, 46, 77)\n",
                "\nclass MyModel(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 100, 3, stride=2)\n        self.convtranspose = torch.nn.ConvTranspose2d(100, 50, 2, stride=2)\n    def forward(self, x):\n        v1 = self.conv(x)\n        v2 = torch.tanh(v1)\n        v3 = self.convtranspose(v2)\n        return v3\n# Inputs to the model\nx = torch.randn(1, 3, 128, 128)\n",
                "\nclass MyModel(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(1, 64, 1)\n    def forward(self, x):\n        tanh_out = torch.tanh(self.conv(x))\n        tanh_out = torch.abs(tanh_out)\n        tanh_out = torch.tanh(tanh_out)\n        tanh_out_1 = torch.abs(tanh_out)\n        tanh_out_2 = torch.tanh(tanh_out_1)\n        tanh_out_3 = torch.sigmoid(tanh_out_2)\n        return tanh_out_3\n# Inputs to the model\nx = torch.randn(1, 1, 100, 100)\n",
                "\nclass MyModel(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(1, 8, 3)\n        self.conv2 = torch.nn.Conv2d(8, 16, 3, 1, 1)\n        self.conv3 = torch.nn.Conv2d(16, 32, 3, 1, 1)\n        self.conv4 = torch.nn.Conv2d(32, 32, 3, 1, 1)\n        self.conv5 = torch.nn.Conv2d(32, 32, 3, 1, 1)\n    def forward(self, x):\n        v1 = torch.tanh(self.conv1(x))\n        v2 = self.conv2(x)\n        v3 = self.conv3(v2)\n        v4 = self.conv4(v3)\n        v5 = self.conv5(v4)\n        return v5\n# Inputs to the model\nx = torch.randn(1, 1, 28, 28)\n",
                "\nclass myModel(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.my_conv = torch.nn.Conv2d(3, 48, 3, stride=1, padding=1)\n    def forward(self, x):\n        v1 = self.my_conv(x)\n        return torch.tanh(v1)\n# Inputs to the model\nx = torch.randn(1, 3, 64, 64)\n",
                "\nclass MyModel(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.features = torch.nn.Sequential(torch.nn.Conv2d(1, 46, 1, bias=False))\n    def forward(self, x):\n        v1 = torch.tanh(self.features(x))\n        return v1\n# Inputs to the model\nx = torch.randn(55, 1, 90, 20)\n"
            ],
            "code": [
                "\nclass MyModel(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.avgpool = torch.nn.AvgPool2d((3, 2), stride=1)\n        self.conv1 = torch.nn.Conv2d(3, 64, (1, 1), stride=(1, 1))\n        self.conv2 = torch.nn.Conv2d(64, 128, (3, 3), stride=1, padding=1)\n        self.conv3 = torch.nn.Conv2d(128, 128, (1, 1), stride=(1, 1))\n        self.relu = torch.nn.ReLU()\n        self.maxpool = torch.nn.MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\n    def forward(self, x):\n        x = self.relu(self.conv1(self.avgpool(x)))\n        x = self.relu(self.conv2(x))\n        x = self.relu(self.conv3(x))\n        x = self.maxpool(x)\n        return x\n# Inputs to the model\nx = torch.randn(3, 1, 56, 56)\n",
                "\nclass MyModel(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.features = torch.nn.Sequential(torch.nn.Conv2d(1, 22, 1, stride=1, padding=0))\n    def forward(self, x):\n        return torch.tanh(self.features(x))\n# Inputs to the model\nx = torch.randn(120, 1, 80, 20)\n",
                "\nclass MyModel(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.features = torch.nn.LSTM(16, 1024)\n    def forward(self, x):\n        return torch.tanh(self.features(x))\n# Inputs to the model\nx = torch.randn(16, 1, 1024)\n",
                "\nclass MyModel(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(1, 6, 3)\n    def forward(self, x):\n        v1 = self.conv1(x)\n        v2 = torch.tanh(x + v1)\n        return v2\n# Inputs to the model\nx = torch.randn(1, 1, 224, 256)\n",
                "\nclass ModelTanh(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 17, (1, 7), groups=17, bias=True)\n    def forward(self, x):\n        v1 = self.conv(x)\n        v2 = torch.tanh(v1)\n        return v2\n# Inputs to the model\nx = torch.randn(7, 3, 46, 77)\n",
                "\nclass MyModel(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 100, 3, stride=2)\n        self.convtranspose = torch.nn.ConvTranspose2d(100, 50, 2, stride=2)\n    def forward(self, x):\n        v1 = self.conv(x)\n        v2 = torch.tanh(v1)\n        v3 = self.convtranspose(v2)\n        return v3\n# Inputs to the model\nx = torch.randn(1, 3, 128, 128)\n",
                "\nclass MyModel(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(1, 64, 1)\n    def forward(self, x):\n        tanh_out = torch.tanh(self.conv(x))\n        tanh_out = torch.abs(tanh_out)\n        tanh_out = torch.tanh(tanh_out)\n        tanh_out_1 = torch.abs(tanh_out)\n        tanh_out_2 = torch.tanh(tanh_out_1)\n        tanh_out_3 = torch.sigmoid(tanh_out_2)\n        return tanh_out_3\n# Inputs to the model\nx = torch.randn(1, 1, 100, 100)\n",
                "\nclass MyModel(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(1, 8, 3)\n        self.conv2 = torch.nn.Conv2d(8, 16, 3, 1, 1)\n        self.conv3 = torch.nn.Conv2d(16, 32, 3, 1, 1)\n        self.conv4 = torch.nn.Conv2d(32, 32, 3, 1, 1)\n        self.conv5 = torch.nn.Conv2d(32, 32, 3, 1, 1)\n    def forward(self, x):\n        v1 = torch.tanh(self.conv1(x))\n        v2 = self.conv2(x)\n        v3 = self.conv3(v2)\n        v4 = self.conv4(v3)\n        v5 = self.conv5(v4)\n        return v5\n# Inputs to the model\nx = torch.randn(1, 1, 28, 28)\n",
                "\nclass myModel(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.my_conv = torch.nn.Conv2d(3, 48, 3, stride=1, padding=1)\n    def forward(self, x):\n        v1 = self.my_conv(x)\n        return torch.tanh(v1)\n# Inputs to the model\nx = torch.randn(1, 3, 64, 64)\n",
                "\nclass MyModel(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.features = torch.nn.Sequential(torch.nn.Conv2d(1, 46, 1, bias=False))\n    def forward(self, x):\n        v1 = torch.tanh(self.features(x))\n        return v1\n# Inputs to the model\nx = torch.randn(55, 1, 90, 20)\n"
            ],
            "g_time": 9.302090883255005
        }
    }
}
{
    "opt": "sfdp=5",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.dropout = 0.5597\n        self.heads = 7975\n        self.seq_len = 989815\n        self.dim = 32193 // self.heads\n    def forward(self, query, key, value, attn_mask):\n        qk = query @ key.transpose(-2, -1) / math.sqrt(query.size(-1))\n        qk = qk + attn_mask\n        attn_weight = torch.softmax(qk, dim=-1)\n        attn_weight = torch.dropout(attn_weight, self.dropout, True)\n        output = attn_weight @ value\n        return output\n# Inputs to the model\nquery = torch.randn(1, 7975, 989815, 32193)\nkey = torch.randn(1, 7975, 989815, 32193)\nvalue = torch.randn(1, 7975, 989815, 32193)\nattn_mask = torch.randn(1, 1, 989815, 989815)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.dropout = 0.0\n        self.heads = 32\n        self.seq_len = 16\n        self.dim = 128 // self.heads\n    def forward(self, query, key, value, attn_mask):\n        qk = query @ key.transpose(-2, -1) / math.sqrt(query.size(-1))\n        qk = qk + attn_mask\n        attn_weight = torch.softmax(qk, dim=-1)\n        attn_weight = torch.dropout(attn_weight, self.dropout, True)\n        output = attn_weight @ value\n        return output\n# Inputs to the model\nquery = torch.randn(1, 8, 16, 1024)\nkey = torch.randn(1, 8, 16, 1024)\nvalue = torch.randn(1, 8, 16, 1024)\nattn_mask = torch.randn(1, 1, 16, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.heads = 8\n        self.seq_len = 77\n        self.dim = 10016 // self.heads\n    def forward(self, query, key, value, attn_mask):\n        qk = query @ key.transpose(-2, -1) / math.sqrt(query.size(-1))\n        qk = qk + attn_mask\n        attn_weight = torch.softmax(qk, dim=-1)\n        attn_weight = torch.dropout(attn_weight, 0.3, True)\n        output = attn_weight @ value\n        return output\n# Inputs to the model\nquery = torch.randn(1, 128, 77, 10016)\nkey = torch.randn(1, 128, 77, 10016)\nvalue = torch.randn(1, 128, 77, 10016)\nattn_mask = torch.randn(1, 1, 77, 77)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.heads = 793\n        self.seq_len = 741\n        self.dim = 1060 // self.heads\n    def forward(self, query, key, value, attn_mask):\n        qk = query @ key.transpose(-2, -1) / math.sqrt(query.size(-1))\n        qk = qk + attn_mask\n        attn_weight = torch.softmax(qk, dim=-1)\n        attn_weight = torch.dropout(attn_weight, 0.6, True)\n        output = attn_weight @ value\n        return output\n# Inputs to the model\nquery = torch.randn(1, 256, 741, 1060)\nkey = torch.randn(1, 256, 741, 1060)\nvalue = torch.randn(1, 256, 741, 1060)\nattn_mask = torch.randn(1, 1, 741, 741)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.in_channels = 192\n        self.embed_size = 512\n    def forward(self, input):\n        return torch.softmax(input, dim=-1)\n# Inputs to the model\ninput = torch.randn(1, 192, 512)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.heads = 40\n        self.seq_len = 40\n        self.dim = 160 // self.heads\n    def forward(self, query, key, value, attn_mask):\n        qk = query @ key.transpose(-2, -1) / math.sqrt(query.size(-1))\n        qk = qk + attn_mask\n        attn_weight = torch.softmax(qk, dim=-1)\n        attn_weight = torch.dropout(attn_weight, 0.1, True)\n        output = attn_weight @ value\n        return output\n# Inputs to the model\nquery = torch.randn(1, 400, 240, 64)\nkey = torch.randn(1, 400, 240, 64)\nvalue = torch.randn(1, 400, 240, 64)\nattn_mask = torch.randn(1, 1, 240, 240)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.heads = 16\n        self.seq_len = 128\n        self.dim = 128 // self.heads\n    def forward(self, query, key, value, attn_mask):\n        qk = query @ key.transpose(-2, -1) / math.sqrt(query.size(-1))\n        qk = qk + attn_mask\n        attn_weight = torch.softmax(qk, dim=-1)\n        # attn_weight = torch.dropout(attn_weight, 0.1, True)\n        output = attn_weight @ value\n        return output\n# Inputs to the model\nquery = torch.randn(1, 256, 128, 8)\nkey = torch.randn(1, 256, 128, 8)\nvalue = torch.randn(1, 256, 128, 8)\nattn_mask = torch.randn(1, 1, 128, 128)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.heads = 16\n        self.seq_len = 128\n        self.dim = 10752 // self.heads\n    def forward(self, query, key, value, attn_mask):\n        qk = query @ key.transpose(-2, -1) / math.sqrt(query.size(-1))\n        qk = qk + attn_mask\n        attn_weight = torch.softmax(qk, dim=-1)\n        attn_weight = torch.dropout(attn_weight, 0.1, True)\n        output = attn_weight @ value\n        return output\n# Inputs to the model\nquery = torch.randn(1, 16, 128, 10752)\nkey = torch.randn(1, 16, 128, 10752)\nvalue = torch.randn(1, 16, 128, 10752)\nattn_mask = torch.randn(1, 1, 128, 128)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.dropout = 0.5\n        self.heads = 32\n        self.seq_len = 512\n        self.dim = 2048 // self.heads\n    def forward(self, query, key, value, attn_mask):\n        qk = query @ key.transpose(-2, -1) / math.sqrt(query.size(-1))\n        qk = qk + attn_mask\n        attn_weight = torch.softmax(qk, dim=-1)\n        attn_weight = torch.dropout(attn_weight, dropout, True)\n        output = attn_weight @ value\n        return output\n# Inputs to the model\nquery = torch.randn(1, 32, 512, 2048)\nkey = torch.randn(1, 32, 512, 2048)\nvalue = torch.randn(1, 32, 512, 2048)\nattn_mask = torch.randn(1, 1, 512, 512)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.seq_len = 1024\n        self.heads = 16\n        self.dim = 7168 // self.heads\n    def forward(self, query, key, value, attn_mask):\n        qk = query @ key.transpose(-2, -1) / math.sqrt(query.size(-1))\n        qk = qk + attn_mask\n        attn_weight = torch.softmax(qk, dim=-1)\n        attn_weight = torch.dropout(attn_weight, 0.1, True)\n        output = attn_weight @ value\n        return output\n# Inputs to the model\nquery = torch.randn(1, 8, 1024, 7168)\nkey = torch.randn(1, 8, 1024, 7168)\nvalue = torch.randn(1, 8, 1024, 7168)\nattn_mask = torch.randn(1, 1, 1024, 1024)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.dropout = 0.5597\n        self.heads = 7975\n        self.seq_len = 989815\n        self.dim = 32193 // self.heads\n    def forward(self, query, key, value, attn_mask):\n        qk = query @ key.transpose(-2, -1) / math.sqrt(query.size(-1))\n        qk = qk + attn_mask\n        attn_weight = torch.softmax(qk, dim=-1)\n        attn_weight = torch.dropout(attn_weight, self.dropout, True)\n        output = attn_weight @ value\n        return output\n# Inputs to the model\nquery = torch.randn(1, 7975, 989815, 32193)\nkey = torch.randn(1, 7975, 989815, 32193)\nvalue = torch.randn(1, 7975, 989815, 32193)\nattn_mask = torch.randn(1, 1, 989815, 989815)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.dropout = 0.0\n        self.heads = 32\n        self.seq_len = 16\n        self.dim = 128 // self.heads\n    def forward(self, query, key, value, attn_mask):\n        qk = query @ key.transpose(-2, -1) / math.sqrt(query.size(-1))\n        qk = qk + attn_mask\n        attn_weight = torch.softmax(qk, dim=-1)\n        attn_weight = torch.dropout(attn_weight, self.dropout, True)\n        output = attn_weight @ value\n        return output\n# Inputs to the model\nquery = torch.randn(1, 8, 16, 1024)\nkey = torch.randn(1, 8, 16, 1024)\nvalue = torch.randn(1, 8, 16, 1024)\nattn_mask = torch.randn(1, 1, 16, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.heads = 8\n        self.seq_len = 77\n        self.dim = 10016 // self.heads\n    def forward(self, query, key, value, attn_mask):\n        qk = query @ key.transpose(-2, -1) / math.sqrt(query.size(-1))\n        qk = qk + attn_mask\n        attn_weight = torch.softmax(qk, dim=-1)\n        attn_weight = torch.dropout(attn_weight, 0.3, True)\n        output = attn_weight @ value\n        return output\n# Inputs to the model\nquery = torch.randn(1, 128, 77, 10016)\nkey = torch.randn(1, 128, 77, 10016)\nvalue = torch.randn(1, 128, 77, 10016)\nattn_mask = torch.randn(1, 1, 77, 77)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.heads = 793\n        self.seq_len = 741\n        self.dim = 1060 // self.heads\n    def forward(self, query, key, value, attn_mask):\n        qk = query @ key.transpose(-2, -1) / math.sqrt(query.size(-1))\n        qk = qk + attn_mask\n        attn_weight = torch.softmax(qk, dim=-1)\n        attn_weight = torch.dropout(attn_weight, 0.6, True)\n        output = attn_weight @ value\n        return output\n# Inputs to the model\nquery = torch.randn(1, 256, 741, 1060)\nkey = torch.randn(1, 256, 741, 1060)\nvalue = torch.randn(1, 256, 741, 1060)\nattn_mask = torch.randn(1, 1, 741, 741)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.in_channels = 192\n        self.embed_size = 512\n    def forward(self, input):\n        return torch.softmax(input, dim=-1)\n# Inputs to the model\ninput = torch.randn(1, 192, 512)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.heads = 40\n        self.seq_len = 40\n        self.dim = 160 // self.heads\n    def forward(self, query, key, value, attn_mask):\n        qk = query @ key.transpose(-2, -1) / math.sqrt(query.size(-1))\n        qk = qk + attn_mask\n        attn_weight = torch.softmax(qk, dim=-1)\n        attn_weight = torch.dropout(attn_weight, 0.1, True)\n        output = attn_weight @ value\n        return output\n# Inputs to the model\nquery = torch.randn(1, 400, 240, 64)\nkey = torch.randn(1, 400, 240, 64)\nvalue = torch.randn(1, 400, 240, 64)\nattn_mask = torch.randn(1, 1, 240, 240)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.heads = 16\n        self.seq_len = 128\n        self.dim = 128 // self.heads\n    def forward(self, query, key, value, attn_mask):\n        qk = query @ key.transpose(-2, -1) / math.sqrt(query.size(-1))\n        qk = qk + attn_mask\n        attn_weight = torch.softmax(qk, dim=-1)\n        # attn_weight = torch.dropout(attn_weight, 0.1, True)\n        output = attn_weight @ value\n        return output\n# Inputs to the model\nquery = torch.randn(1, 256, 128, 8)\nkey = torch.randn(1, 256, 128, 8)\nvalue = torch.randn(1, 256, 128, 8)\nattn_mask = torch.randn(1, 1, 128, 128)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.heads = 16\n        self.seq_len = 128\n        self.dim = 10752 // self.heads\n    def forward(self, query, key, value, attn_mask):\n        qk = query @ key.transpose(-2, -1) / math.sqrt(query.size(-1))\n        qk = qk + attn_mask\n        attn_weight = torch.softmax(qk, dim=-1)\n        attn_weight = torch.dropout(attn_weight, 0.1, True)\n        output = attn_weight @ value\n        return output\n# Inputs to the model\nquery = torch.randn(1, 16, 128, 10752)\nkey = torch.randn(1, 16, 128, 10752)\nvalue = torch.randn(1, 16, 128, 10752)\nattn_mask = torch.randn(1, 1, 128, 128)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.dropout = 0.5\n        self.heads = 32\n        self.seq_len = 512\n        self.dim = 2048 // self.heads\n    def forward(self, query, key, value, attn_mask):\n        qk = query @ key.transpose(-2, -1) / math.sqrt(query.size(-1))\n        qk = qk + attn_mask\n        attn_weight = torch.softmax(qk, dim=-1)\n        attn_weight = torch.dropout(attn_weight, dropout, True)\n        output = attn_weight @ value\n        return output\n# Inputs to the model\nquery = torch.randn(1, 32, 512, 2048)\nkey = torch.randn(1, 32, 512, 2048)\nvalue = torch.randn(1, 32, 512, 2048)\nattn_mask = torch.randn(1, 1, 512, 512)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.seq_len = 1024\n        self.heads = 16\n        self.dim = 7168 // self.heads\n    def forward(self, query, key, value, attn_mask):\n        qk = query @ key.transpose(-2, -1) / math.sqrt(query.size(-1))\n        qk = qk + attn_mask\n        attn_weight = torch.softmax(qk, dim=-1)\n        attn_weight = torch.dropout(attn_weight, 0.1, True)\n        output = attn_weight @ value\n        return output\n# Inputs to the model\nquery = torch.randn(1, 8, 1024, 7168)\nkey = torch.randn(1, 8, 1024, 7168)\nvalue = torch.randn(1, 8, 1024, 7168)\nattn_mask = torch.randn(1, 1, 1024, 1024)\n"
            ],
            "g_time": 11.848081827163696
        }
    }
}
{
    "opt": "unary=16",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Sequential(_Linear(100, 1))\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = F.relu(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 100)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torcho.nn.Linear(3, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.relu(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(16, 1)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        return F.relu(v1)\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 5)\n \n    def forward(self, x):\n        v1 = self.linear(x)\n        v2 = F.relu(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx = torch.randn(4, 3) \n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(2, 4)\n       \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.relu(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(300, 200, False)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.nn.functional.relu(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 300)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(64, 10)\n \n    def forward(self, x):\n        v1 = self.linear(x)\n        v2 = F.relu(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(4, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = F.relu(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 4)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(6, 2)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = F.relu(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(128, 6)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(8, 3)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.relu(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 8)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Sequential(_Linear(100, 1))\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = F.relu(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 100)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torcho.nn.Linear(3, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.relu(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(16, 1)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        return F.relu(v1)\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 5)\n \n    def forward(self, x):\n        v1 = self.linear(x)\n        v2 = F.relu(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx = torch.randn(4, 3) \n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(2, 4)\n       \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.relu(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(300, 200, False)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.nn.functional.relu(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 300)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(64, 10)\n \n    def forward(self, x):\n        v1 = self.linear(x)\n        v2 = F.relu(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(4, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = F.relu(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 4)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(6, 2)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = F.relu(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(128, 6)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(8, 3)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.relu(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 8)\n"
            ],
            "g_time": 4.38054895401001
        }
    }
}
{
    "opt": "unary=24",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(22, 75, 3, stride=1, padding=1)\n    def forward(self, x):\n        negative_slope = 0\n        v1 = self.conv(x)\n        v2 = v1 > 0\n        v3 = v1 * negative_slope\n        v4 = torch.where(v2, v1, v3)\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 22, 33, 97)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(41, 33, 17, stride=4, padding=3, dilation=2)\n    def forward(self, x):\n        negative_slope = 32.393062\n        v1 = self.conv(x)\n        v2 = v1 > 0\n        v3 = v1 * negative_slope\n        v4 = torch.where(v2, v1, v3)\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 41, 10, 10)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(7, 49, 6, stride=2, padding=2)\n        self.conv_copy = torch.nn.Conv2d(7, 49, 6, stride=1, padding=2)\n    def forward(self, x):\n        negative_slope = 1.0\n        v1 = self.conv(x)\n        v2 = v1 > 0\n        v3 = v1 * negative_slope\n        v4 = torch.where(v2, v1, v3)\n        v5 = self.conv_copy(x)\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 7, 73, 153)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(10, 11, (10, 10), stride=7, padding=8)\n    def forward(self, x):\n        negative_slope = 0\n        v1 = self.conv(x)\n        v2 = v1 > 0\n        v3 = v1 * negative_slope\n        v4 = torch.where(v2, v1, v3)\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 10, 109, 65)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(5, 25, 6, stride=2, padding=0)\n    def forward(self, x):\n        negative_slope = 0\n        v1 = self.conv(x)\n        v2 = v1 > 0\n        v3 = v1 * negative_slope\n        v4 = torch.where(v2, v1, v3)\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 5, 200, 170)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(10, 53, (5, 7), stride=2, padding=0, dilation=2)\n    def forward(self, x):\n        negative_slope = -1.241812\n        v1 = self.conv(x)\n        v2 = v1 > 0\n        v3 = v1 * negative_slope\n        v4 = torch.where(v2, v1, v3)\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 10, 27, 144)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(5, 49, 1, stride=1, padding=0)\n        self.conv2 = torch.nn.Conv2d(49, 34, 5, stride=5, padding=5)\n    def forward(self, x):\n        negative_slope = 0\n        v1 = self.conv1(x)\n        v2 = v1 > 0\n        v3 = v1 * negative_slope\n        v4 = torch.where(v2, v1, v3)\n        v5 = self.conv2(v4)\n        v6 = v5 > 0\n        v7 = v5 * negative_slope\n        v8 = torch.where(v6, v5, v7)\n        return v8\n# Inputs to the model\nx1 = torch.randn(1, 5, 80, 52)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(4, 13, 2, stride=1, padding=1)\n    def forward(self, x):\n        negative_slope = 0.69844\n        v1 = self.conv(x)\n        v2 = v1 > 0\n        v3 = v1 * negative_slope\n        v4 = torch.where(v2, v1, v3)\n        return v4\n# Inputs to the model\ntorch.randn(1, 4, 48, 48)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(9, 3, 7, stride=1, padding=3)\n    def forward(self, x):\n        negative_slope = 0.129691\n        v1 = self.conv(x)\n        v2 = v1 > 0\n        v3 = v1 * negative_slope\n        v4 = torch.where(v2, v1, v3)\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 9, 84, 55)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(10, 32, 4, stride=1, padding=1)\n    def forward(self, x):\n        negative_slope = -0.072032\n        v1 = self.conv(x)\n        v2 = v1 > 0\n        v3 = v1 * negative_slope\n        v4 = torch.where(v2, v1, v3)\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 10, 126, 112)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(22, 75, 3, stride=1, padding=1)\n    def forward(self, x):\n        negative_slope = 0\n        v1 = self.conv(x)\n        v2 = v1 > 0\n        v3 = v1 * negative_slope\n        v4 = torch.where(v2, v1, v3)\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 22, 33, 97)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(41, 33, 17, stride=4, padding=3, dilation=2)\n    def forward(self, x):\n        negative_slope = 32.393062\n        v1 = self.conv(x)\n        v2 = v1 > 0\n        v3 = v1 * negative_slope\n        v4 = torch.where(v2, v1, v3)\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 41, 10, 10)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(7, 49, 6, stride=2, padding=2)\n        self.conv_copy = torch.nn.Conv2d(7, 49, 6, stride=1, padding=2)\n    def forward(self, x):\n        negative_slope = 1.0\n        v1 = self.conv(x)\n        v2 = v1 > 0\n        v3 = v1 * negative_slope\n        v4 = torch.where(v2, v1, v3)\n        v5 = self.conv_copy(x)\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 7, 73, 153)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(10, 11, (10, 10), stride=7, padding=8)\n    def forward(self, x):\n        negative_slope = 0\n        v1 = self.conv(x)\n        v2 = v1 > 0\n        v3 = v1 * negative_slope\n        v4 = torch.where(v2, v1, v3)\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 10, 109, 65)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(5, 25, 6, stride=2, padding=0)\n    def forward(self, x):\n        negative_slope = 0\n        v1 = self.conv(x)\n        v2 = v1 > 0\n        v3 = v1 * negative_slope\n        v4 = torch.where(v2, v1, v3)\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 5, 200, 170)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(10, 53, (5, 7), stride=2, padding=0, dilation=2)\n    def forward(self, x):\n        negative_slope = -1.241812\n        v1 = self.conv(x)\n        v2 = v1 > 0\n        v3 = v1 * negative_slope\n        v4 = torch.where(v2, v1, v3)\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 10, 27, 144)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(5, 49, 1, stride=1, padding=0)\n        self.conv2 = torch.nn.Conv2d(49, 34, 5, stride=5, padding=5)\n    def forward(self, x):\n        negative_slope = 0\n        v1 = self.conv1(x)\n        v2 = v1 > 0\n        v3 = v1 * negative_slope\n        v4 = torch.where(v2, v1, v3)\n        v5 = self.conv2(v4)\n        v6 = v5 > 0\n        v7 = v5 * negative_slope\n        v8 = torch.where(v6, v5, v7)\n        return v8\n# Inputs to the model\nx1 = torch.randn(1, 5, 80, 52)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(4, 13, 2, stride=1, padding=1)\n    def forward(self, x):\n        negative_slope = 0.69844\n        v1 = self.conv(x)\n        v2 = v1 > 0\n        v3 = v1 * negative_slope\n        v4 = torch.where(v2, v1, v3)\n        return v4\n# Inputs to the model\ntorch.randn(1, 4, 48, 48)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(9, 3, 7, stride=1, padding=3)\n    def forward(self, x):\n        negative_slope = 0.129691\n        v1 = self.conv(x)\n        v2 = v1 > 0\n        v3 = v1 * negative_slope\n        v4 = torch.where(v2, v1, v3)\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 9, 84, 55)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(10, 32, 4, stride=1, padding=1)\n    def forward(self, x):\n        negative_slope = -0.072032\n        v1 = self.conv(x)\n        v2 = v1 > 0\n        v3 = v1 * negative_slope\n        v4 = torch.where(v2, v1, v3)\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 10, 126, 112)\n"
            ],
            "g_time": 7.939877033233643
        }
    }
}
{
    "opt": "unary=14",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose_3_1 = torch.nn.ConvTranspose2d(14, 14, kernel_size=(3, 3), stride=(3, 4), padding=(1, 1))\n    def forward(self, x1):\n        v1 = self.conv_transpose_3_1(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 14, 64, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose_9 = torch.nn.ConvTranspose2d(640, 512, 3, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv_transpose_9(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 640, 4, 1)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose1 = torch.nn.ConvTranspose1d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n    def forward(self, x1):\n        v1 = self.conv_transpose1(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 16, 82, 82)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose_7 = torch.nn.ConvTranspose2d(3, 6, kernel_size=3, stride=2, padding=1)\n    def forward(self, x1):\n        v1 = self.conv_transpose_7(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 3, 49, 49)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose17 = torch.nn.ConvTranspose2d(118, 4, (1, 1))\n    def forward(self, x1):\n        v1 = self.conv_transpose17(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 118, 50, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv8 = torch.nn.Conv2d(in_channels=299, out_channels=5, kernel_size=(5, 1), stride=(1, 1),\n                                    padding=(2, 0))\n    def forward(self, x1):\n        v1 = self.conv8(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 299, 102, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose3_7 = torch.nn.ConvTranspose2d(100, 100, kernel_size=(4, 4), stride=(2, 1), padding=(1, 1))\n    def forward(self, x1):\n        v1 = self.conv_transpose3_7(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 100, 16, 59)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.convtranspose2 = torch.nn.ConvTranspose2d(1024, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    def forward(self, x1):\n        v1 = self.convtranspose2(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 1024, 4, 4)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\t\n        self.conv_transpose_2 = torch.nn.ConvTranspose2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1),\n                                                       bias=False)\n    def forward(self, x):\n        v = self.conv_transpose_2(x)\n        return v\n# Inputs to the model\nx = torch.randn(1, 64, 42, 23)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1d_1 = torch.nn.Conv1d(1, 1, 3, stride=1, padding=1)\n    def forward(self, x):\n        v1 = self.conv1d_1(x)\n        return v1\n# Inputs to the model\nx = torch.randn(1, 1, 32)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose_3_1 = torch.nn.ConvTranspose2d(14, 14, kernel_size=(3, 3), stride=(3, 4), padding=(1, 1))\n    def forward(self, x1):\n        v1 = self.conv_transpose_3_1(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 14, 64, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose_9 = torch.nn.ConvTranspose2d(640, 512, 3, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv_transpose_9(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 640, 4, 1)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose1 = torch.nn.ConvTranspose1d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n    def forward(self, x1):\n        v1 = self.conv_transpose1(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 16, 82, 82)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose_7 = torch.nn.ConvTranspose2d(3, 6, kernel_size=3, stride=2, padding=1)\n    def forward(self, x1):\n        v1 = self.conv_transpose_7(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 3, 49, 49)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose17 = torch.nn.ConvTranspose2d(118, 4, (1, 1))\n    def forward(self, x1):\n        v1 = self.conv_transpose17(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 118, 50, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv8 = torch.nn.Conv2d(in_channels=299, out_channels=5, kernel_size=(5, 1), stride=(1, 1),\n                                    padding=(2, 0))\n    def forward(self, x1):\n        v1 = self.conv8(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 299, 102, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose3_7 = torch.nn.ConvTranspose2d(100, 100, kernel_size=(4, 4), stride=(2, 1), padding=(1, 1))\n    def forward(self, x1):\n        v1 = self.conv_transpose3_7(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 100, 16, 59)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.convtranspose2 = torch.nn.ConvTranspose2d(1024, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    def forward(self, x1):\n        v1 = self.convtranspose2(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 1024, 4, 4)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\t\n        self.conv_transpose_2 = torch.nn.ConvTranspose2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1),\n                                                       bias=False)\n    def forward(self, x):\n        v = self.conv_transpose_2(x)\n        return v\n# Inputs to the model\nx = torch.randn(1, 64, 42, 23)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1d_1 = torch.nn.Conv1d(1, 1, 3, stride=1, padding=1)\n    def forward(self, x):\n        v1 = self.conv1d_1(x)\n        return v1\n# Inputs to the model\nx = torch.randn(1, 1, 32)\n"
            ],
            "g_time": 5.459524631500244
        }
    }
}
{
    "opt": "sfdp=3",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self, q, k, v):\n        super().__init__()\n        self.q = torch.nn.Linear(3, 5)\n        self.k = torch.nn.Linear(5, 8)\n        self.v = torch.nn.Linear(5, 8)\n        self.scale_factor = 1/( math.sqrt(3)*10 )\n        self.dropout_p = 0.1\n \n    def forward(self, x1, x2):\n        v1 = self.q(x1)\n        v2 = self.k(x2)\n        v3 = torch.matmul(v1, v2.transpose(-2, -1))\n        v4 = v3.mul(self.scale_factor)\n        v5 = torch.nn.functional.softmax(v4, dim=-1)\n        v6 = torch.nn.functional.dropout(v5, p=self.dropout_p)\n        v7 = torch.matmul(v6, self.v.weight.transpose(-2, -1))\n        return v7\n\n# Initializing the model\nq = torch.nn.Linear(3, 5)\nk = torch.nn.Linear(5, 8)\nv = torch.nn.Linear(5, 8)\nm = Model(q, k, v)\n\n# Inputs to the model\nx1 = torch.randn(4, 3)\nx2 = torch.randn(4, 5)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.scale_factor = torch.empty(10)\n        torch.nn.init.uniform_(self.scale_factor, -10.0, 10.0)\n        self.dropout_p = 0.2\n\n    def forward(self, query, key, value):\n        qk = torch.matmul(query, key.transpose(-2, -1))\n        scaled_qk = qk.mul(self.scale_factor)\n        softmax_qk = scaled_qk.softmax(dim=-1)\n        dropout_qk = torch.nn.functional.dropout(softmax_qk, p=self.dropout_p)\n        output = dropout_qk.matmul(value)\n        return output\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nquery = torch.randn(1, 16, 5)\nkey = torch.randn(1, 16, 10)\nvalue = torch.randn(1, 16, 10)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, dim_query, dim_kv):\n        super().__init__()\n        self.scale_factor = 1 / (dim_kv**0.5)\n \n    def forward(self, queries, keys, values, dropout_p):\n        qk = torch.matmul(queries, keys.transpose(-2, -1))\n        scaled_qk = qk.mul(self.scale_factor)\n        softmax_qk = scaled_qk.softmax(dim=-1)\n        dropout_qk = torch.nn.functional.dropout(softmax_qk, p=dropout_p)\n        output = dropout_qk.matmul(values)\n        return output\n\n# Initializing the model\nm = Model(dim_query=256, dim_kv=512)\n\n# Inputs to the model\nqueries = torch.randn(1, 32, 256)\nkeys = torch.randn(1, 32, 512)\nvalues = torch.randn(1, 32, 512)\ndropout_p = 0.1\n",
                "\nclass Model(torch.nn.Module):\n    def forward(self, x1, x2, x3):\n        q = x1\n        k = x2\n        v = x3\n        scale_factor = self.scale_factor or (int(q.size(-1)) ** -0.5)\n        qk = torch.matmul(q, k.transpose(-2, -1))\n        scaled_qk = qk * scale_factor\n        softmax_qk = scaled_qk.softmax(dim=-1)\n        dropout_qk = torch.nn.functional.dropout(softmax_qk, p=0.2)\n        output = dropout_qk.matmul(v)\n        return output\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nquery = torch.randn(2, 768, 56, 56)\nkey = torch.randn(2, 768, 28, 28)\nvalue = torch.randn(2, 768, 28, 28)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, dropout_p=0.8, scale_factor=128**-0.5):\n        super().__init__()\n        self.scale_factor = scale_factor\n        self.dropout_p = dropout_p\n\n    def forward(self, query, key, value, attention_mask):\n        qk = torch.matmul(query, key.transpose(-2, -1))\n        scaled_qk = qk * self.scale_factor\n        softmax_qk = scaled_qk.softmax(dim=-1)\n        dropout_qk = torch.nn.functional.dropout(softmax_qk, p=self.dropout_p)\n        output = dropout_qk.matmul(value)\n        output_after_mask = output + attention_mask * -1e10   # We don't actually encourage to apply softmax to masked positions\n        masked_softmax_qk = softmax_qk * attention_mask + (1 - attention_mask) * -1e10 # We don't actually encourage to apply softmax to masked positions\n        return output_after_mask, masked_softmax_qk\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nquery = torch.randn(1, 3, 64, 64)\nkey = torch.randn(1, 3, 64, 64)\nvalue = torch.randn(1, 3, 64, 64)\nattention_mask = torch.ones(1, 3, 64, 64).bool()\n__output_after_mask__, __masked_softmax_qk__ = m(query, key, value, attention_mask)\n\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, query_tensor, key_tensor, value_tensor, scale_factor, dropout_p):\n        qk = torch.matmul(query_tensor, key_tensor.transpose(-2, -1))\n        scaled_qk = qk.mul(scale_factor)\n        softmax_qk = scaled_qk.softmax(dim=-1)\n        dropout_qk = torch.nn.functional.dropout(softmax_qk, p=dropout_p)\n        output = dropout_qk.matmul(value_tensor)\n        return output\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nquery = torch.randn(1, 512, 64)\nkey = torch.randn(1, 512, 64)\nvalue = torch.randn(1, 512, 64)\nscale_factor = torch.scalar_tensor(28.22)\ndropout_p = torch.scalar_tensor(0.552)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n \n    def forward(self, queries, keys, value, scale_factor, dropout_p):\n        v1 = torch.matmul(queries, keys.transpose(-2, -1))\n        v2 = v1 * scale_factor\n        v3 = v2.softmax(dim=-1)\n        v4 = torch.nn.functional.dropout(v3, p=dropout_p)\n        v5 = torch.matmul(v4, value)\n        return v5\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nqueries = torch.randn(16, 8, 512)\nkeys = torch.randn(16, 16, 512)\nvalue = torch.randn(16, 16, 1024)\nscale_factor = torch.FloatTensor([512].float())\ndropout_p = 0.1\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, d_k=16, d_v=16):\n        super().__init__()\n        self.scale_factor = (d_k ** -0.5)\n \n    def forward(self, q1, k1, v1, dropout_p=0.1):\n        qk = torch.matmul(q1, k1.transpose(-2, -1))\n        scaled_qk = qk.mul(self.scale_factor)\n        softmax_qk = scaled_qk.softmax(dim=-1)\n        dropout_qk = torch.nn.functional.dropout(softmax_qk, p=dropout_p)\n        output = dropout_qk.matmul(v1)\n        return output\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nq1 = torch.randn(1, 3, 1024)\nk1 = torch.randn(1, 3, 1024)\nv1 = torch.randn(1, 3, 1024)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n \n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\ndef mlp(input_tensor, hidden_size):\n    v1 = torch.transpose(input_tensor, 0, -1)\n    v2 = torch.nn.Linear(v1.size()[-1], hidden_size)\n    v3 = v2(v1)\n    v4 = torch.tanh(v3)\n    v5 = torch.nn.Linear(hidden_size, v1.size()[-1])\n    v6 = v5(v4)\n    v7 = torch.transpose(v6, -1, 0)\n    v8 = torch.nn.Linear(v7.size()[-1], hidden_size)\n    v9 = v8(v6)\n    v10 = torch.tanh(v9)\n    v11 = torch.transpose(v10, 0, -1)\n    v12 = torch.nn.Linear(v7.size()[-1], v1.size()[-1])\n    v13 = v12(v11)\n    v14 = torch.transpose(v13, -1, 0)\n    v15 = v14 + v1\n    return v15\n\nclass Model(torch.nn.Module):\n    def __init__(self, hidden_size, dropout_p):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(6, 8, 1, stride=1, padding=1)\n        self.linear = torch.nn.Linear(8 * 64 * 64, 1)\n        self.dropout = torch.nn.Dropout(dropout_p)\n        self.activation = torch.tanh\n\n    def forward(self, x1, x2):\n        x1_1 = self.conv(x1)\n        x2_1 = self.conv(x2)\n        x_mlp = mlp(torch.transpose(torch.stack((x1_1, x2_1)), 0, 1), 128)\n        v1 = torch.stack((x1_1, x2_1, x_mlp))\n        v1 = torch.flatten(torch.transpose(torch.cat(v1), 0, 1), start_dim=1)\n        v10 = self.linear(v1)\n        v2 = torch.transpose(v10, 0, -1)\n        v3 = self.dropout\n        v4 = self.activation\n        v5 = v4(v3(v2))\n        v6 = self.linear(v5)\n        v7 = torch.transpose(v6, -1, 0)\n        v8 = v4(v7)\n        v9 = torch.transpose(v8, 0, -1)\n        v10 = v6 + v9\n        return v10\n\n# Initializing the model\nhidden_size = 8\ndropout_p = 0.2\nm = Model(hidden_size, dropout_p)\n\n# Input tensors to the model\nx1 = torch.randn(1, 6, 32, 32)\nx2 = torch.randn(1, 6, 32, 32)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self, q, k, v):\n        super().__init__()\n        self.q = torch.nn.Linear(3, 5)\n        self.k = torch.nn.Linear(5, 8)\n        self.v = torch.nn.Linear(5, 8)\n        self.scale_factor = 1/( math.sqrt(3)*10 )\n        self.dropout_p = 0.1\n \n    def forward(self, x1, x2):\n        v1 = self.q(x1)\n        v2 = self.k(x2)\n        v3 = torch.matmul(v1, v2.transpose(-2, -1))\n        v4 = v3.mul(self.scale_factor)\n        v5 = torch.nn.functional.softmax(v4, dim=-1)\n        v6 = torch.nn.functional.dropout(v5, p=self.dropout_p)\n        v7 = torch.matmul(v6, self.v.weight.transpose(-2, -1))\n        return v7\n\n# Initializing the model\nq = torch.nn.Linear(3, 5)\nk = torch.nn.Linear(5, 8)\nv = torch.nn.Linear(5, 8)\nm = Model(q, k, v)\n\n# Inputs to the model\nx1 = torch.randn(4, 3)\nx2 = torch.randn(4, 5)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.scale_factor = torch.empty(10)\n        torch.nn.init.uniform_(self.scale_factor, -10.0, 10.0)\n        self.dropout_p = 0.2\n\n    def forward(self, query, key, value):\n        qk = torch.matmul(query, key.transpose(-2, -1))\n        scaled_qk = qk.mul(self.scale_factor)\n        softmax_qk = scaled_qk.softmax(dim=-1)\n        dropout_qk = torch.nn.functional.dropout(softmax_qk, p=self.dropout_p)\n        output = dropout_qk.matmul(value)\n        return output\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nquery = torch.randn(1, 16, 5)\nkey = torch.randn(1, 16, 10)\nvalue = torch.randn(1, 16, 10)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, dim_query, dim_kv):\n        super().__init__()\n        self.scale_factor = 1 / (dim_kv**0.5)\n \n    def forward(self, queries, keys, values, dropout_p):\n        qk = torch.matmul(queries, keys.transpose(-2, -1))\n        scaled_qk = qk.mul(self.scale_factor)\n        softmax_qk = scaled_qk.softmax(dim=-1)\n        dropout_qk = torch.nn.functional.dropout(softmax_qk, p=dropout_p)\n        output = dropout_qk.matmul(values)\n        return output\n\n# Initializing the model\nm = Model(dim_query=256, dim_kv=512)\n\n# Inputs to the model\nqueries = torch.randn(1, 32, 256)\nkeys = torch.randn(1, 32, 512)\nvalues = torch.randn(1, 32, 512)\ndropout_p = 0.1\n",
                "\nclass Model(torch.nn.Module):\n    def forward(self, x1, x2, x3):\n        q = x1\n        k = x2\n        v = x3\n        scale_factor = self.scale_factor or (int(q.size(-1)) ** -0.5)\n        qk = torch.matmul(q, k.transpose(-2, -1))\n        scaled_qk = qk * scale_factor\n        softmax_qk = scaled_qk.softmax(dim=-1)\n        dropout_qk = torch.nn.functional.dropout(softmax_qk, p=0.2)\n        output = dropout_qk.matmul(v)\n        return output\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nquery = torch.randn(2, 768, 56, 56)\nkey = torch.randn(2, 768, 28, 28)\nvalue = torch.randn(2, 768, 28, 28)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, dropout_p=0.8, scale_factor=128**-0.5):\n        super().__init__()\n        self.scale_factor = scale_factor\n        self.dropout_p = dropout_p\n\n    def forward(self, query, key, value, attention_mask):\n        qk = torch.matmul(query, key.transpose(-2, -1))\n        scaled_qk = qk * self.scale_factor\n        softmax_qk = scaled_qk.softmax(dim=-1)\n        dropout_qk = torch.nn.functional.dropout(softmax_qk, p=self.dropout_p)\n        output = dropout_qk.matmul(value)\n        output_after_mask = output + attention_mask * -1e10   # We don't actually encourage to apply softmax to masked positions\n        masked_softmax_qk = softmax_qk * attention_mask + (1 - attention_mask) * -1e10 # We don't actually encourage to apply softmax to masked positions\n        return output_after_mask, masked_softmax_qk\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nquery = torch.randn(1, 3, 64, 64)\nkey = torch.randn(1, 3, 64, 64)\nvalue = torch.randn(1, 3, 64, 64)\nattention_mask = torch.ones(1, 3, 64, 64).bool()\n__output_after_mask__, __masked_softmax_qk__ = m(query, key, value, attention_mask)\n\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, query_tensor, key_tensor, value_tensor, scale_factor, dropout_p):\n        qk = torch.matmul(query_tensor, key_tensor.transpose(-2, -1))\n        scaled_qk = qk.mul(scale_factor)\n        softmax_qk = scaled_qk.softmax(dim=-1)\n        dropout_qk = torch.nn.functional.dropout(softmax_qk, p=dropout_p)\n        output = dropout_qk.matmul(value_tensor)\n        return output\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nquery = torch.randn(1, 512, 64)\nkey = torch.randn(1, 512, 64)\nvalue = torch.randn(1, 512, 64)\nscale_factor = torch.scalar_tensor(28.22)\ndropout_p = torch.scalar_tensor(0.552)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n \n    def forward(self, queries, keys, value, scale_factor, dropout_p):\n        v1 = torch.matmul(queries, keys.transpose(-2, -1))\n        v2 = v1 * scale_factor\n        v3 = v2.softmax(dim=-1)\n        v4 = torch.nn.functional.dropout(v3, p=dropout_p)\n        v5 = torch.matmul(v4, value)\n        return v5\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nqueries = torch.randn(16, 8, 512)\nkeys = torch.randn(16, 16, 512)\nvalue = torch.randn(16, 16, 1024)\nscale_factor = torch.FloatTensor([512].float())\ndropout_p = 0.1\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, d_k=16, d_v=16):\n        super().__init__()\n        self.scale_factor = (d_k ** -0.5)\n \n    def forward(self, q1, k1, v1, dropout_p=0.1):\n        qk = torch.matmul(q1, k1.transpose(-2, -1))\n        scaled_qk = qk.mul(self.scale_factor)\n        softmax_qk = scaled_qk.softmax(dim=-1)\n        dropout_qk = torch.nn.functional.dropout(softmax_qk, p=dropout_p)\n        output = dropout_qk.matmul(v1)\n        return output\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nq1 = torch.randn(1, 3, 1024)\nk1 = torch.randn(1, 3, 1024)\nv1 = torch.randn(1, 3, 1024)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n \n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\ndef mlp(input_tensor, hidden_size):\n    v1 = torch.transpose(input_tensor, 0, -1)\n    v2 = torch.nn.Linear(v1.size()[-1], hidden_size)\n    v3 = v2(v1)\n    v4 = torch.tanh(v3)\n    v5 = torch.nn.Linear(hidden_size, v1.size()[-1])\n    v6 = v5(v4)\n    v7 = torch.transpose(v6, -1, 0)\n    v8 = torch.nn.Linear(v7.size()[-1], hidden_size)\n    v9 = v8(v6)\n    v10 = torch.tanh(v9)\n    v11 = torch.transpose(v10, 0, -1)\n    v12 = torch.nn.Linear(v7.size()[-1], v1.size()[-1])\n    v13 = v12(v11)\n    v14 = torch.transpose(v13, -1, 0)\n    v15 = v14 + v1\n    return v15\n\nclass Model(torch.nn.Module):\n    def __init__(self, hidden_size, dropout_p):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(6, 8, 1, stride=1, padding=1)\n        self.linear = torch.nn.Linear(8 * 64 * 64, 1)\n        self.dropout = torch.nn.Dropout(dropout_p)\n        self.activation = torch.tanh\n\n    def forward(self, x1, x2):\n        x1_1 = self.conv(x1)\n        x2_1 = self.conv(x2)\n        x_mlp = mlp(torch.transpose(torch.stack((x1_1, x2_1)), 0, 1), 128)\n        v1 = torch.stack((x1_1, x2_1, x_mlp))\n        v1 = torch.flatten(torch.transpose(torch.cat(v1), 0, 1), start_dim=1)\n        v10 = self.linear(v1)\n        v2 = torch.transpose(v10, 0, -1)\n        v3 = self.dropout\n        v4 = self.activation\n        v5 = v4(v3(v2))\n        v6 = self.linear(v5)\n        v7 = torch.transpose(v6, -1, 0)\n        v8 = v4(v7)\n        v9 = torch.transpose(v8, 0, -1)\n        v10 = v6 + v9\n        return v10\n\n# Initializing the model\nhidden_size = 8\ndropout_p = 0.2\nm = Model(hidden_size, dropout_p)\n\n# Input tensors to the model\nx1 = torch.randn(1, 6, 32, 32)\nx2 = torch.randn(1, 6, 32, 32)\n"
            ],
            "g_time": 22.088386297225952
        }
    }
}
{
    "opt": "unary=27",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min, max):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(15, 8, 3, stride=3, padding=0)\n        self.min = min\n        self.max = max\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = torch.clamp_min(v1, self.min)\n        v3 = torch.clamp_max(v2, self.max)\n        return v3\nmin = 0.075\nmax = 0.61\n# Inputs to the model\nx1 = torch.randn(1, 15, 5, 10)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min, max):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 3, 3, stride=1, padding=1)\n        self.min = min\n        self.max = max\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = torch.clamp_min(v1, self.min, out=torch.cuda.FloatTensor())\n        v3 = torch.clamp_max(v2, self.max, out=torch.cuda.FloatTensor())\n        return v3\nmin = 0.54\nmax = 0.2\n# Inputs to the model\nx1 = torch.randn(1, 3, 30, 30)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min, max):\n        super().__init__()\n        self.min_tensor = torch.tensor([min])\n        self.max_tensor = torch.tensor([max])\n        self.conv = torch.nn.Conv2d(5, 4, 1, stride=1, padding=0)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = torch.clamp_min(v1, self.min_tensor)\n        v3 = torch.clamp_max(v2, self.max_tensor)\n        return v3\nmin = 0.41\nmax = 0.05\n# Inputs to the model\nx1 = torch.randn(1, 5, 20, 20)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min, max):\n        super().__init__()\n        self.linear = torch.nn.Linear(1, 1)\n        self.min = min\n        self.max = max\n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.clamp_min(v1, self.min)\n        v3 = torch.clamp_max(v2, self.max)\n        return v3\nmin = torch.tensor(0.75)\nmax = 0.02\n# Inputs to the model\nx1 = torch.randn(1, 1)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min, max):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(10, 1, 1, stride=1, padding=0)\n        self.min = min\n        self.max = max\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = torch.clamp_min(v1, self.min, out=torch.cuda.FloatTensor())\n        v3 = torch.clamp_max(v2, self.max, out=torch.cuda.FloatTensor())\n        return v3\nmin = 0.018\nmax = 3.141\n# Inputs to the model\nx1 = torch.randn(1, 10, 112, 112)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min, max):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(192, 12, 1, stride=1, padding=0)\n        self.min = min\n        self.max = max\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = torch.clamp_min(v1, self.min)\n        v3 = torch.clamp_max(v2, self.max)\n        return v3\nmin = 0.64\nmax = 0.80\n# Inputs to the model\nx1 = torch.randn(1, 192, 27, 27)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min, max):\n        super().__init__()\n        self.linear1 = torch.nn.Linear(12, 60)\n        self.linear2 = torch.nn.Linear(60, 10)\n\n        self.min = min\n        self.max = max\n\n    def forward(self, x):\n        v1 = self.linear1(x)\n\n        v2 = self.linear2(v1)\n        v3 = torch.clamp_min(v2, self.min)\n        v4 = torch.clamp_max(v3, self.max)\n        return v4\nmin = 0.12\nmax = 0.22\n# Inputs to the model\nx = torch.randn(1, 12)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min, max):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(23, 21, 3, stride=1, padding=1)\n        self.min = min\n        self.max = max\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = torch.clamp_min(v1, self.min)\n        v3 = torch.clamp_max(v2, self.max)\n        return v3\nmin = 0.22\nmax = 0.41\n# Inputs to the model\nx1 = torch.randn(1, 23, 56, 56)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min, max):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(8, 17, 3, stride=1, padding=1)\n        self.min = min\n        self.max = max\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        t1 = torch.clamp_min(v1, self.min)\n        t2 = torch.clamp_max(t1, self.max)\n        v3 = torch.clamp_max(t2, self.max)\n        return v3\nmin = 0.2\nmax = 0.95\n# Inputs to the model\nx1 = torch.randn(1, 8, 56, 56)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min, max):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(1, 1, 1, stride=1, padding=0)\n        self.min = min\n        self.max = max\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = torch.clamp_min(v1, self.min)\n        v3 = torch.clamp_max(v2, self.max)\n        return v3\nmin = -0.03\nmax = 0.16\n# Inputs to the model\nx1 = torch.randn(1, 1, 38, 38)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min, max):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(15, 8, 3, stride=3, padding=0)\n        self.min = min\n        self.max = max\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = torch.clamp_min(v1, self.min)\n        v3 = torch.clamp_max(v2, self.max)\n        return v3\nmin = 0.075\nmax = 0.61\n# Inputs to the model\nx1 = torch.randn(1, 15, 5, 10)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min, max):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 3, 3, stride=1, padding=1)\n        self.min = min\n        self.max = max\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = torch.clamp_min(v1, self.min, out=torch.cuda.FloatTensor())\n        v3 = torch.clamp_max(v2, self.max, out=torch.cuda.FloatTensor())\n        return v3\nmin = 0.54\nmax = 0.2\n# Inputs to the model\nx1 = torch.randn(1, 3, 30, 30)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min, max):\n        super().__init__()\n        self.min_tensor = torch.tensor([min])\n        self.max_tensor = torch.tensor([max])\n        self.conv = torch.nn.Conv2d(5, 4, 1, stride=1, padding=0)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = torch.clamp_min(v1, self.min_tensor)\n        v3 = torch.clamp_max(v2, self.max_tensor)\n        return v3\nmin = 0.41\nmax = 0.05\n# Inputs to the model\nx1 = torch.randn(1, 5, 20, 20)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min, max):\n        super().__init__()\n        self.linear = torch.nn.Linear(1, 1)\n        self.min = min\n        self.max = max\n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.clamp_min(v1, self.min)\n        v3 = torch.clamp_max(v2, self.max)\n        return v3\nmin = torch.tensor(0.75)\nmax = 0.02\n# Inputs to the model\nx1 = torch.randn(1, 1)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min, max):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(10, 1, 1, stride=1, padding=0)\n        self.min = min\n        self.max = max\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = torch.clamp_min(v1, self.min, out=torch.cuda.FloatTensor())\n        v3 = torch.clamp_max(v2, self.max, out=torch.cuda.FloatTensor())\n        return v3\nmin = 0.018\nmax = 3.141\n# Inputs to the model\nx1 = torch.randn(1, 10, 112, 112)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min, max):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(192, 12, 1, stride=1, padding=0)\n        self.min = min\n        self.max = max\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = torch.clamp_min(v1, self.min)\n        v3 = torch.clamp_max(v2, self.max)\n        return v3\nmin = 0.64\nmax = 0.80\n# Inputs to the model\nx1 = torch.randn(1, 192, 27, 27)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min, max):\n        super().__init__()\n        self.linear1 = torch.nn.Linear(12, 60)\n        self.linear2 = torch.nn.Linear(60, 10)\n\n        self.min = min\n        self.max = max\n\n    def forward(self, x):\n        v1 = self.linear1(x)\n\n        v2 = self.linear2(v1)\n        v3 = torch.clamp_min(v2, self.min)\n        v4 = torch.clamp_max(v3, self.max)\n        return v4\nmin = 0.12\nmax = 0.22\n# Inputs to the model\nx = torch.randn(1, 12)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min, max):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(23, 21, 3, stride=1, padding=1)\n        self.min = min\n        self.max = max\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = torch.clamp_min(v1, self.min)\n        v3 = torch.clamp_max(v2, self.max)\n        return v3\nmin = 0.22\nmax = 0.41\n# Inputs to the model\nx1 = torch.randn(1, 23, 56, 56)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min, max):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(8, 17, 3, stride=1, padding=1)\n        self.min = min\n        self.max = max\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        t1 = torch.clamp_min(v1, self.min)\n        t2 = torch.clamp_max(t1, self.max)\n        v3 = torch.clamp_max(t2, self.max)\n        return v3\nmin = 0.2\nmax = 0.95\n# Inputs to the model\nx1 = torch.randn(1, 8, 56, 56)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min, max):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(1, 1, 1, stride=1, padding=0)\n        self.min = min\n        self.max = max\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = torch.clamp_min(v1, self.min)\n        v3 = torch.clamp_max(v2, self.max)\n        return v3\nmin = -0.03\nmax = 0.16\n# Inputs to the model\nx1 = torch.randn(1, 1, 38, 38)\n"
            ],
            "g_time": 7.0372350215911865
        }
    }
}
{
    "opt": "unary=11",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose3d(2, 4, 3, stride=2, padding=1)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v4 / 6\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 2, 20, 10, 10)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(8, 6, (5, 3), [13, 6], 3, [2, 2], bias=False, dilation=[1, 2], groups=4)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v4 / 6\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 8, 68, 191)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(256, 128, 4, stride=3, padding=2)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v4 / 6\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 256, 16, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(3, 4, kernel_size=(5, 5), stride=(3, 3), padding=(1, 1), output_padding=(1, 1), dilation=(3, 3), groups=1, bias=False)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v4 / 6\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose3d(3, 3, 4, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v4 / 6\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 3, 32, 32, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(6, 40, kernel_size=(7, 2), stride=(1, 1), padding=(6, 4), dilation=(1, 1), groups=2, bias=None, padding_mode='zeros')\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v4 / 6\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 6, 82, 47)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose3d(3, 6, kernel_size=5, stride=1, padding=2, output_padding=1, groups=1, bias=False, dilation=1)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v4 / 6\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(6, 1, kernel_size=(5, 5), stride=(5, 5), padding=(2, 2), dilation=(2, 2), groups=1, bias=False, padding_mode='zeros')\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v4 / 6\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 6, 32, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(8, 2, kernel_size=(6, 6), stride=(1, 1), padding=(1, 1))\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v4 / 6\n        return v5\n# Inputs to the model\nx1 = torch.randn(64, 8, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(64, 32, 1, stride=1, padding=2, dilation=4)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v4 / 6\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 64, 20, 30)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose3d(2, 4, 3, stride=2, padding=1)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v4 / 6\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 2, 20, 10, 10)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(8, 6, (5, 3), [13, 6], 3, [2, 2], bias=False, dilation=[1, 2], groups=4)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v4 / 6\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 8, 68, 191)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(256, 128, 4, stride=3, padding=2)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v4 / 6\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 256, 16, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(3, 4, kernel_size=(5, 5), stride=(3, 3), padding=(1, 1), output_padding=(1, 1), dilation=(3, 3), groups=1, bias=False)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v4 / 6\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose3d(3, 3, 4, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v4 / 6\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 3, 32, 32, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(6, 40, kernel_size=(7, 2), stride=(1, 1), padding=(6, 4), dilation=(1, 1), groups=2, bias=None, padding_mode='zeros')\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v4 / 6\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 6, 82, 47)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose3d(3, 6, kernel_size=5, stride=1, padding=2, output_padding=1, groups=1, bias=False, dilation=1)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v4 / 6\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(6, 1, kernel_size=(5, 5), stride=(5, 5), padding=(2, 2), dilation=(2, 2), groups=1, bias=False, padding_mode='zeros')\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v4 / 6\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 6, 32, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(8, 2, kernel_size=(6, 6), stride=(1, 1), padding=(1, 1))\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v4 / 6\n        return v5\n# Inputs to the model\nx1 = torch.randn(64, 8, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(64, 32, 1, stride=1, padding=2, dilation=4)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v4 / 6\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 64, 20, 30)\n"
            ],
            "g_time": 7.182425260543823
        }
    }
}
{
    "opt": "unary=6",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 5, 5, stride=1, padding=2)\n        self.bn = torch.nn.BatchNorm2d(5)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v1 * v4\n        v6 = v5 / 6\n        v7 = self.bn(v6)\n        return v7.permute(2, 3, 1, 0).unsqueeze(-1)\n# Inputs to the model\nx1 = torch.randn(1, 3, 256, 256)\n",
                "\nclass Model(torch.nn.AvgPool2d):\n    def __init__(self):\n        super().__init__(1, stride=1, padding=0)\n    def forward(self, x1):\n        v1 = self(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp(v2, 0, 6)\n        v4 = v1 * v3\n        v5 = v4 / 6\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 3, 256, 256)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 11, 3, stride=3, padding=4)\n        self.bn = torch.nn.BatchNorm2d(3, eps=1e-05, momentum=0.1)\n        self.hardtanh = torch.nn.Hardtanh(inplace=False)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = F.relu(v1)\n        v3 = v2.permute(0, 2, 3, 1).unsqueeze(-1)\n        return self.bn(v3)\n# Inputs to the model\nx1 = torch.randn(1, 3, 192, 192)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(6, 6, 1, stride=1, padding=1)\n    def forward(self, x1):\n        x2 = 3 + x1\n        v1 = self.conv(x2)\n        v2 = v1 + x1\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v1 * v4\n        v6 = v5 / 6\n        return v6.permute(2, 3, 1, 0).unsqueeze(-1)\n# Inputs to the model\nx1 = torch.randn(1, 6, 128, 128)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1_1 = torch.nn.Conv2d(3, 64, 3, stride=2, padding=1)\n        self.conv1_2 = torch.nn.Conv2d(3, 64, 1, stride=1, padding=0)\n        self.maxpool1 = torch.nn.MaxPool2d(3, stride=2, padding=1)\n        self.conv2_1 = torch.nn.Conv2d(64, 64, 3, stride=1, padding=1)\n        self.conv2_2 = torch.nn.Conv2d(64, 64, 1, stride=1, padding=0)\n        self.maxpool2 = torch.nn.MaxPool2d(3, stride=2, padding=1)\n    def forward(self, x1):\n        v1 = self.maxpool1(self.conv1_1(x1))\n        v2 = self.conv1_2(x1)\n        v3 = v2 + v1\n        v4 = torch.clamp(v3, 0, 6)\n        v5 = self.maxpool2(self.conv2_1(v4))\n        v6 = self.conv2_2(v4)\n        v7 = v6 + v5\n        v8 = torch.clamp(v7, 0, 6)\n        return v8.permute(0, 3, 1, 2)\n# Inputs to the model\nx1 = torch.randn(1, 3, 256, 256)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1_1 = torch.nn.Conv2d(3, 26, 2, stride=1, padding=0)\n        self.conv1_2 = torch.nn.Conv2d(26, 12, 2, stride=1, padding=1)\n        # self.conv1_3 = torch.nn.Conv2d(14, 26, 2, stride=1, padding=0)\n        self.relu1_1 = torch.nn.ReLU()\n        self.relu1_2 = torch.nn.ReLU()\n        self.relu1_3 = torch.nn.ReLU()\n        self.relu = torch.nn.ReLU()\n        self.avg_pool = torch.nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = torch.nn.Linear(12, 10)\n    def forward(self, x):\n        x_in = x.clone()\n        x_1 = self.conv1_1(x_in)\n        # x_1_2 = self.conv1_2(x_1)\n        x_1_3 = self.conv1_3(x_1)\n        x_1_4 = self.relu1_1(x_1)\n        x_1_5 = self.relu1_2(x_1_4)\n        x_1_6 = self.avg_pool(x_1_5)\n        # x_1_7 = x_1_6.squeeze()\n        x_1_7 = x_1_5.flatten()\n        x_1_8 = self.fc(x_1_7)\n        return x_1_8\n        # return x_1_2\n# Inputs to the model\nx1 = torch.randn(2, 3, 32, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 3, 1, stride=1, padding=1, dilation=2)\n        self.bn = torch.nn.BatchNorm2d(3)\n        self.act = torch.nn.Hardtanh()\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp(v2, 0, 6)\n        v4 = v1 * v3\n        v5 = v4 / 6\n        v6 = self.bn(v5)\n        v7 = self.act(v6)\n        return v7\n# Inputs to the model\nx1 = torch.randn(1, 3, 128, 128)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 3, 1, stride=2, padding=1)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp(v2, 0, 6)\n        v4 = v1 * v3\n        v5 = v4 / 6\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 3, 256, 256)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 64, 1, stride=1, padding=1)\n        self.maxpool = torch.nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = torch.clamp_min(v1, 0)\n        v3 = torch.clamp_max(v2, 6)\n        v4 = v1 * v3\n        v5 = v4 / 6\n        v6 = self.maxpool(v5)\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 32, 1, stride=1, padding=1)\n        self.prelu = torch.nn.PReLU()\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp(v2, 0, 6)\n        v4 = v1 * v3\n        v5 = v4 / 6\n        v6 = self.prelu(v5)\n        return v6 + v5\n# Inputs to the model\nx1 = torch.randn(1, 3, 128, 128)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 5, 5, stride=1, padding=2)\n        self.bn = torch.nn.BatchNorm2d(5)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v1 * v4\n        v6 = v5 / 6\n        v7 = self.bn(v6)\n        return v7.permute(2, 3, 1, 0).unsqueeze(-1)\n# Inputs to the model\nx1 = torch.randn(1, 3, 256, 256)\n",
                "\nclass Model(torch.nn.AvgPool2d):\n    def __init__(self):\n        super().__init__(1, stride=1, padding=0)\n    def forward(self, x1):\n        v1 = self(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp(v2, 0, 6)\n        v4 = v1 * v3\n        v5 = v4 / 6\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 3, 256, 256)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 11, 3, stride=3, padding=4)\n        self.bn = torch.nn.BatchNorm2d(3, eps=1e-05, momentum=0.1)\n        self.hardtanh = torch.nn.Hardtanh(inplace=False)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = F.relu(v1)\n        v3 = v2.permute(0, 2, 3, 1).unsqueeze(-1)\n        return self.bn(v3)\n# Inputs to the model\nx1 = torch.randn(1, 3, 192, 192)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(6, 6, 1, stride=1, padding=1)\n    def forward(self, x1):\n        x2 = 3 + x1\n        v1 = self.conv(x2)\n        v2 = v1 + x1\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v1 * v4\n        v6 = v5 / 6\n        return v6.permute(2, 3, 1, 0).unsqueeze(-1)\n# Inputs to the model\nx1 = torch.randn(1, 6, 128, 128)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1_1 = torch.nn.Conv2d(3, 64, 3, stride=2, padding=1)\n        self.conv1_2 = torch.nn.Conv2d(3, 64, 1, stride=1, padding=0)\n        self.maxpool1 = torch.nn.MaxPool2d(3, stride=2, padding=1)\n        self.conv2_1 = torch.nn.Conv2d(64, 64, 3, stride=1, padding=1)\n        self.conv2_2 = torch.nn.Conv2d(64, 64, 1, stride=1, padding=0)\n        self.maxpool2 = torch.nn.MaxPool2d(3, stride=2, padding=1)\n    def forward(self, x1):\n        v1 = self.maxpool1(self.conv1_1(x1))\n        v2 = self.conv1_2(x1)\n        v3 = v2 + v1\n        v4 = torch.clamp(v3, 0, 6)\n        v5 = self.maxpool2(self.conv2_1(v4))\n        v6 = self.conv2_2(v4)\n        v7 = v6 + v5\n        v8 = torch.clamp(v7, 0, 6)\n        return v8.permute(0, 3, 1, 2)\n# Inputs to the model\nx1 = torch.randn(1, 3, 256, 256)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1_1 = torch.nn.Conv2d(3, 26, 2, stride=1, padding=0)\n        self.conv1_2 = torch.nn.Conv2d(26, 12, 2, stride=1, padding=1)\n        # self.conv1_3 = torch.nn.Conv2d(14, 26, 2, stride=1, padding=0)\n        self.relu1_1 = torch.nn.ReLU()\n        self.relu1_2 = torch.nn.ReLU()\n        self.relu1_3 = torch.nn.ReLU()\n        self.relu = torch.nn.ReLU()\n        self.avg_pool = torch.nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = torch.nn.Linear(12, 10)\n    def forward(self, x):\n        x_in = x.clone()\n        x_1 = self.conv1_1(x_in)\n        # x_1_2 = self.conv1_2(x_1)\n        x_1_3 = self.conv1_3(x_1)\n        x_1_4 = self.relu1_1(x_1)\n        x_1_5 = self.relu1_2(x_1_4)\n        x_1_6 = self.avg_pool(x_1_5)\n        # x_1_7 = x_1_6.squeeze()\n        x_1_7 = x_1_5.flatten()\n        x_1_8 = self.fc(x_1_7)\n        return x_1_8\n        # return x_1_2\n# Inputs to the model\nx1 = torch.randn(2, 3, 32, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 3, 1, stride=1, padding=1, dilation=2)\n        self.bn = torch.nn.BatchNorm2d(3)\n        self.act = torch.nn.Hardtanh()\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp(v2, 0, 6)\n        v4 = v1 * v3\n        v5 = v4 / 6\n        v6 = self.bn(v5)\n        v7 = self.act(v6)\n        return v7\n# Inputs to the model\nx1 = torch.randn(1, 3, 128, 128)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 3, 1, stride=2, padding=1)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp(v2, 0, 6)\n        v4 = v1 * v3\n        v5 = v4 / 6\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 3, 256, 256)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 64, 1, stride=1, padding=1)\n        self.maxpool = torch.nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = torch.clamp_min(v1, 0)\n        v3 = torch.clamp_max(v2, 6)\n        v4 = v1 * v3\n        v5 = v4 / 6\n        v6 = self.maxpool(v5)\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 32, 1, stride=1, padding=1)\n        self.prelu = torch.nn.PReLU()\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp(v2, 0, 6)\n        v4 = v1 * v3\n        v5 = v4 / 6\n        v6 = self.prelu(v5)\n        return v6 + v5\n# Inputs to the model\nx1 = torch.randn(1, 3, 128, 128)\n"
            ],
            "g_time": 14.653714179992676
        }
    }
}
{
    "opt": "replace_fx",
    "response": {
        "0": {
            "raw": [
                "\nclass model(torch.nn.Module):\n    def forward(self, x):\n        a1 = torch.nn.functional.dropout(x, p=0.6)\n        if a1 and torch.rand(1) > 0.5:\n            return True\n        else:\n           return False\n# Inputs to the model\nx = torch.rand(2, 2)\n",
                "\nclass Dropout(torch.nn.Module):\n    def forward(self, x):\n        a = torch.nn.functional.dropout(x, p=0.4, training=self.training, inplace=True)\n        return 1\n# Inputs to the model\nx = torch.randn(1)\n",
                "\nclass m(torch.nn.Module):\n    def forward(self, x):\n        t2 = torch.rand_like(x, dtype = torch.float16, device = torch.device('cuda'), layout = torch.strided, memory_format = torch.preserve_format)\n        return t2\n# Inputs to the model\nx = torch.randn(1)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = nn.Conv2d(\n            in_channels=1, out_channels=1, kernel_size=3, stride=2, padding=1,\n            dilation=3, groups=1, bias=True\n        )\n    def forward(self, x):\n        x_in = torch.nn.functional.dropout(x, p=0.2, training=True, inplace=False)\n        x_out = self.conv(x_in)\n        x_out = torch.nn.functional.dropout(x_in, p=0.2, training=True, inplace=False)\n        return x_out\n# Inputs to the model\nx = torch.randn(1, 1, 10, 10)\n",
                "\nclass model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x):\n        a1 = torch.nn.functional.dropout(x, p=0.3)\n        a2 = torch.nn.functional.dropout(x, p=0.05)\n        return a1\n# Inputs to the model\nx = torch.randn(1)\n",
                "\nclass test_module1(torch.nn.Module):\n    def __init__(self):\n        super(test_module1, self).__init__()\n        self.block1 = torch.nn.Sequential(torch.nn.Conv2d(3, 16, 1, stride=1, padding=0, dilation=1, out_channels=16, groups=1, bias=True),\n                                          torch.nn.BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n                                          torch.nn.ReLU(inplace=True))\n\n    def forward(self, x):\n        x = self.block1(x)\n        x1 = torch.nn.functional.dropout(x, p=0.01, training=True)\n        x2 = torch.nn.functional.softmax(x1, dim=1)\n        return x2\n# Inputs to the model\nx = torch.randn(1, 3, 4, 4)\n",
                "\nclass foo(torch.nn.Module):\n    def forward(self, x):\n        # Dropout module will be removed due to the pattern\n        x = torch.nn.Dropout(p=0.5)(x)\n        a = torch.rand(1)\n        return x\n# Inputs to the model\nx = torch.rand(1)\n",
                "\nclass M(nn.Module):\n    def __init__(self):\n        super(M, self).__init__()\n        self.model = nn.Sequential()\n        self.model.add_module('d1', nn.Dropout())\n        self.model.add_module('d2', nn.Dropout())\n        for param in self.model.parameters():\n            nn.init.constant_(param, 1)\n\n    def forward(self, x):\n        return self.model(x)\n# Inputs to the model\nx = torch.randn(2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x, x2, x3, x4):\n        b = F.dropout(x)\n        a = F.dropout(x)\n        c = F.dropout(x)\n        d = F.dropout(x)\n        return d\n# Inputs to the model\nx = torch.randn(1)\nx2 = torch.randn(1)\nx3 = torch.randn(1)\nx4 = torch.randn(1)\n",
                "\nclass model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x):\n        a1 = torch.nn.functional.dropout(x, p=0)\n        return 1\n# Inputs to the model\nx = torch.randn(2)\n"
            ],
            "code": [
                "\nclass model(torch.nn.Module):\n    def forward(self, x):\n        a1 = torch.nn.functional.dropout(x, p=0.6)\n        if a1 and torch.rand(1) > 0.5:\n            return True\n        else:\n           return False\n# Inputs to the model\nx = torch.rand(2, 2)\n",
                "\nclass Dropout(torch.nn.Module):\n    def forward(self, x):\n        a = torch.nn.functional.dropout(x, p=0.4, training=self.training, inplace=True)\n        return 1\n# Inputs to the model\nx = torch.randn(1)\n",
                "\nclass m(torch.nn.Module):\n    def forward(self, x):\n        t2 = torch.rand_like(x, dtype = torch.float16, device = torch.device('cuda'), layout = torch.strided, memory_format = torch.preserve_format)\n        return t2\n# Inputs to the model\nx = torch.randn(1)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = nn.Conv2d(\n            in_channels=1, out_channels=1, kernel_size=3, stride=2, padding=1,\n            dilation=3, groups=1, bias=True\n        )\n    def forward(self, x):\n        x_in = torch.nn.functional.dropout(x, p=0.2, training=True, inplace=False)\n        x_out = self.conv(x_in)\n        x_out = torch.nn.functional.dropout(x_in, p=0.2, training=True, inplace=False)\n        return x_out\n# Inputs to the model\nx = torch.randn(1, 1, 10, 10)\n",
                "\nclass model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x):\n        a1 = torch.nn.functional.dropout(x, p=0.3)\n        a2 = torch.nn.functional.dropout(x, p=0.05)\n        return a1\n# Inputs to the model\nx = torch.randn(1)\n",
                "\nclass test_module1(torch.nn.Module):\n    def __init__(self):\n        super(test_module1, self).__init__()\n        self.block1 = torch.nn.Sequential(torch.nn.Conv2d(3, 16, 1, stride=1, padding=0, dilation=1, out_channels=16, groups=1, bias=True),\n                                          torch.nn.BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n                                          torch.nn.ReLU(inplace=True))\n\n    def forward(self, x):\n        x = self.block1(x)\n        x1 = torch.nn.functional.dropout(x, p=0.01, training=True)\n        x2 = torch.nn.functional.softmax(x1, dim=1)\n        return x2\n# Inputs to the model\nx = torch.randn(1, 3, 4, 4)\n",
                "\nclass foo(torch.nn.Module):\n    def forward(self, x):\n        # Dropout module will be removed due to the pattern\n        x = torch.nn.Dropout(p=0.5)(x)\n        a = torch.rand(1)\n        return x\n# Inputs to the model\nx = torch.rand(1)\n",
                "\nclass M(nn.Module):\n    def __init__(self):\n        super(M, self).__init__()\n        self.model = nn.Sequential()\n        self.model.add_module('d1', nn.Dropout())\n        self.model.add_module('d2', nn.Dropout())\n        for param in self.model.parameters():\n            nn.init.constant_(param, 1)\n\n    def forward(self, x):\n        return self.model(x)\n# Inputs to the model\nx = torch.randn(2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x, x2, x3, x4):\n        b = F.dropout(x)\n        a = F.dropout(x)\n        c = F.dropout(x)\n        d = F.dropout(x)\n        return d\n# Inputs to the model\nx = torch.randn(1)\nx2 = torch.randn(1)\nx3 = torch.randn(1)\nx4 = torch.randn(1)\n",
                "\nclass model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x):\n        a1 = torch.nn.functional.dropout(x, p=0)\n        return 1\n# Inputs to the model\nx = torch.randn(2)\n"
            ],
            "g_time": 7.0686140060424805
        }
    }
}
{
    "opt": "unary=19",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 3)\n \n    def forward(self, x0):\n        v0 = self.linear(x0)\n        v1 = torch.sigmoid(v0)\n        return v1\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx0 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(256, 64, False)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.sigmoid(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 256)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(12, 512)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.sigmoid(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 12)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.linear = torch.nn.Linear(10, 1)\n \n    def forward(self, x):\n        y = self.linear(x)\n        y_hat = torch.sigmoid(y)\n        return y_hat\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 10)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(10000, 10000)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.sigmoid(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 10000)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3*4*4, 10)\n        self.linear.bias.data.fill_(0.0)\n \n    def forward(self, x1):\n        v1 = self.linear(x1.reshape(-1, 3*4*4))\n        v2 = torch.sigmoid(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(32, 3)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.sigmoid(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 10, False)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.sigmoid(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 1)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.sigmoid(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.sigmoid(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 3)\n \n    def forward(self, x0):\n        v0 = self.linear(x0)\n        v1 = torch.sigmoid(v0)\n        return v1\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx0 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(256, 64, False)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.sigmoid(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 256)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(12, 512)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.sigmoid(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 12)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.linear = torch.nn.Linear(10, 1)\n \n    def forward(self, x):\n        y = self.linear(x)\n        y_hat = torch.sigmoid(y)\n        return y_hat\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 10)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(10000, 10000)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.sigmoid(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 10000)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3*4*4, 10)\n        self.linear.bias.data.fill_(0.0)\n \n    def forward(self, x1):\n        v1 = self.linear(x1.reshape(-1, 3*4*4))\n        v2 = torch.sigmoid(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(32, 3)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.sigmoid(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 10, False)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.sigmoid(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 1)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.sigmoid(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.sigmoid(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n"
            ],
            "g_time": 5.3484814167022705
        }
    }
}
{
    "opt": "unary=20",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_t = torch.nn.ConvTranspose2d(198, 57, kernel_size=(1, 1), stride=(1, 1), padding=(0, 0))\n    def forward(self, x1):\n        v1 = self.conv_t(x1)\n        v2 = torch.sigmoid(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 198, 21, 97)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_t = torch.nn.ConvTranspose2d(128, 64, kernel_size=4, stride=4, padding=0)\n        self.bn1 = torch.nn.BatchNorm2d(64)\n    def forward(self, x1):\n        v1 = self.conv_t(x1)\n        v2 = self.bn1(v1)\n        v3 = torch.sigmoid(v2)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 128, 256, 256)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_t = torch.nn.ConvTranspose2d(146, 3, kernel_size=(12, 55), stride=(27, 73), padding=(53, 23))\n    def forward(self, x1):\n        v1 = self.conv_t(x1)\n        v2 = torch.sigmoid(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 146, 54, 25)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_t = torch.nn.ConvTranspose2d(1, 25, kernel_size=(11, 1), stride=(11, 21), padding=(0, 20))\n    def forward(self, x1):\n        v1 = self.conv_t(x1)\n        v2 = torch.sigmoid(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 1, 17, 22)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_t = torch.nn.ConvTranspose2d(256, 63, kernel_size=2, stride=2, padding=0)\n    def forward(self, x1):\n        v1 = self.conv_t(x1)\n        v2 = torch.sigmoid(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 256, 11, 11)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_t = torch.nn.ConvTranspose2d(37, 37, kernel_size=41, stride=41, padding=39)\n    def forward(self, x1):\n        v1 = self.conv_t(x1)\n        v2 = torch.sigmoid(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 37, 13, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.conv_t = torch.nn.ConvTranspose2d(64, 15, kernel_size=2, stride=6, padding=1)\n    def forward(self, x):\n        v1 = self.conv_t(x)\n        v2 = torch.sigmoid(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 64, 119, 21)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_t = torch.nn.ConvTranspose2d(8, 53, kernel_size=(10, 8), stride=(10, 8), padding=(4, 6))\n    def forward(self, x1):\n        v1 = self.conv_t(x1)\n        v2 = torch.sigmoid(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 8, 63, 24)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_t = torch.nn.ConvTranspose2d(123, 64, kernel_size=(3, 35), stride=(3, 35), padding=(1, 34))\n    def forward(self, x1):\n        v1 = self.conv_t(x1)\n        v2 = torch.sigmoid(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 123, 98, 44)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_t = torch.nn.ConvTranspose2d(1, 1, kernel_size=(41, 1), stride=(41, 1), padding=(21, 1))\n    def forward(self, x1):\n        v1 = self.conv_t(x1)\n        v2 = torch.sigmoid(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 1, 124, 4)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_t = torch.nn.ConvTranspose2d(198, 57, kernel_size=(1, 1), stride=(1, 1), padding=(0, 0))\n    def forward(self, x1):\n        v1 = self.conv_t(x1)\n        v2 = torch.sigmoid(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 198, 21, 97)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_t = torch.nn.ConvTranspose2d(128, 64, kernel_size=4, stride=4, padding=0)\n        self.bn1 = torch.nn.BatchNorm2d(64)\n    def forward(self, x1):\n        v1 = self.conv_t(x1)\n        v2 = self.bn1(v1)\n        v3 = torch.sigmoid(v2)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 128, 256, 256)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_t = torch.nn.ConvTranspose2d(146, 3, kernel_size=(12, 55), stride=(27, 73), padding=(53, 23))\n    def forward(self, x1):\n        v1 = self.conv_t(x1)\n        v2 = torch.sigmoid(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 146, 54, 25)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_t = torch.nn.ConvTranspose2d(1, 25, kernel_size=(11, 1), stride=(11, 21), padding=(0, 20))\n    def forward(self, x1):\n        v1 = self.conv_t(x1)\n        v2 = torch.sigmoid(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 1, 17, 22)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_t = torch.nn.ConvTranspose2d(256, 63, kernel_size=2, stride=2, padding=0)\n    def forward(self, x1):\n        v1 = self.conv_t(x1)\n        v2 = torch.sigmoid(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 256, 11, 11)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_t = torch.nn.ConvTranspose2d(37, 37, kernel_size=41, stride=41, padding=39)\n    def forward(self, x1):\n        v1 = self.conv_t(x1)\n        v2 = torch.sigmoid(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 37, 13, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.conv_t = torch.nn.ConvTranspose2d(64, 15, kernel_size=2, stride=6, padding=1)\n    def forward(self, x):\n        v1 = self.conv_t(x)\n        v2 = torch.sigmoid(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 64, 119, 21)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_t = torch.nn.ConvTranspose2d(8, 53, kernel_size=(10, 8), stride=(10, 8), padding=(4, 6))\n    def forward(self, x1):\n        v1 = self.conv_t(x1)\n        v2 = torch.sigmoid(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 8, 63, 24)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_t = torch.nn.ConvTranspose2d(123, 64, kernel_size=(3, 35), stride=(3, 35), padding=(1, 34))\n    def forward(self, x1):\n        v1 = self.conv_t(x1)\n        v2 = torch.sigmoid(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 123, 98, 44)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_t = torch.nn.ConvTranspose2d(1, 1, kernel_size=(41, 1), stride=(41, 1), padding=(21, 1))\n    def forward(self, x1):\n        v1 = self.conv_t(x1)\n        v2 = torch.sigmoid(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 1, 124, 4)\n"
            ],
            "g_time": 5.306955099105835
        }
    }
}
{
    "opt": "linear_permute_fusion",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.pooling = torch.nn.AdaptiveAvgPool2d(output_size=(1, 2))\n        self.flatten = torch.nn.Flatten()\n        self.conv = torch.nn.Conv1d(1, 3, 2, padding=0, stride=1, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, requires_grad=True)\n        self.relu = torch.nn.ReLU()\n        self.linear = torch.nn.Linear(3, 3, bias=True)\n    def forward(self, x3):\n        v3 = self.pooling(x3)\n        v4 = self.flatten(v3).squeeze(1)\n        v5 = self.conv(v4)\n        v6 = self.relu(v5)\n        v7 = self.linear(v6)\n        return v7.unsqueeze(1)\n# Inputs to the model\nx3 = torch.randn(1, 1, 3, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        lstm1 = torch.nn.LSTM(1, 3, bidirectional=True, batch_first=True)\n        self.linear = torch.nn.Linear(6, 1)\n    def forward(self, x3):\n        v3_1 = lstm1(x3)\n        v3_2 = v3_1[0]\n        v3_2_re = v3_2.repeat(1, 1, 1)\n        v2 = torch.nn.functional.linear(v3_2, self.linear.weight, self.linear.bias)\n        lstm3 = torch.nn.LSTM(1, 3, bidirectional=True, batch_first=True)\n        v4 = lstm3(v2.permute(1, 0, 2))\n        v5 = torch.nn.functional.linear(v4[0], self.linear.weight, self.linear.bias)\n        return v5\n# Inputs to the model\nx3 = torch.randn(1, 3, 1)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(1, 1)\n    def forward(self, x3):\n        v3 = torch.nn.functional.linear(x3, self.linear.weight, self.linear.bias)\n        flatten2 = torch.flatten(v3, 2)\n        reshape2 = flatten2.view([1, 1, 1])\n        reshape2_2 = torch.flatten(reshape2, 2)\n        return reshape2_2\n\n# Inputs to the model\nx3 = torch.randn(1, 1, 3)\n\n\n\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(2, 2)\n    def forward(self, x2):\n        v2 = torch.nn.functional.linear(x2, self.linear.weight, self.linear.bias)\n        v3 = v2.permute(0, 2, 1)\n        return v3\n# Inputs to the model\nx2 = torch.randn(1, 3, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(2, 2)\n    def forward(self, x3):\n        v2 = torch.nn.functional.linear(x3, self.linear.weight, self.linear.bias)\n        v3 = v2.permute(0, 2, 1)\n        self.lstm = torch.nn.LSTM(2, 2)\n        lstm = self.lstm(v3)[0]\n        return self.linear(lstm)\n# Inputs to the model\nx3 = torch.randn(1, 3, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(2, 2)\n    def forward(self, x3):\n      v3 = torch.nn.functional.linear(x3, self.linear.weight, self.linear.bias)\n      v4 = v3.permute(0, 2, 1)\n      v5 = v4[:, 2:, :]\n      lstm3 = torch.nn.LSTMCell(2, 2)\n      for _ in range(10):\n        r = torch.nn.functional.relu(v5)\n        i, o = lstm3(r)\n        v5 = i + o\n      return v5\n# Input(s) to the model\nx3 = torch.randn(1, 3, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear1 = torch.nn.Linear(3, 3)\n        self.linear2 = torch.nn.Linear(3, 2)\n    def forward(self, x1):\n        v2 = torch.nn.functional.linear(x1, self.linear1.weight, self.linear1.bias)\n        v3 = torch.nn.functional.linear(v2, self.linear2.weight, self.linear2.bias)\n        return v3.permute(0, 2, 1)\n# Inputs to the model\nx1 = torch.randn(1, 3, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(2, 2)\n    def forward(self, x1):\n        v1 = torch.nn.functional.linear(x1, self.linear.weight, self.linear.bias)\n        v2 = v1.permute(0, 2, 1).unsqueeze(-1)\n        v3 = v2.squeeze(0)\n        lstm2 = torch.nn.LSTM(1, 1)\n        return lstm2(v3)[0]\n# Inputs to the model\nx1 = torch.randn(1, 3, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(2, 2)\n        lstm = torch.nn.LSTM(2, 2)\n    def forward(self, input_):\n        v1 = torch.nn.functional.linear(x1, self.linear.weight, self.linear.bias)\n        v2 = lstm(v1.permute(1, 0, 2))\n        v3 = torch.nn.functional.linear(v2[0], self.linear.weight, self.linear.bias)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(2, 2)\n    def forward(self, x1, x2):\n        v1 = torch.nn.functional.linear(x1, self.linear.weight, self.linear.bias)\n        v2 = torch.nn.functional.linear(x2, self.linear.weight, self.linear.bias)\n        lstm = torch.nn.LSTM(2, 2)\n        v3 = lstm(v1.permute(1, 0, 2))\n        v4 = lstm(v2.permute(1, 0, 2))\n        return v3[0] + v4[0]\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\nx2 = torch.randn(1, 2, 2)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.pooling = torch.nn.AdaptiveAvgPool2d(output_size=(1, 2))\n        self.flatten = torch.nn.Flatten()\n        self.conv = torch.nn.Conv1d(1, 3, 2, padding=0, stride=1, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, requires_grad=True)\n        self.relu = torch.nn.ReLU()\n        self.linear = torch.nn.Linear(3, 3, bias=True)\n    def forward(self, x3):\n        v3 = self.pooling(x3)\n        v4 = self.flatten(v3).squeeze(1)\n        v5 = self.conv(v4)\n        v6 = self.relu(v5)\n        v7 = self.linear(v6)\n        return v7.unsqueeze(1)\n# Inputs to the model\nx3 = torch.randn(1, 1, 3, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        lstm1 = torch.nn.LSTM(1, 3, bidirectional=True, batch_first=True)\n        self.linear = torch.nn.Linear(6, 1)\n    def forward(self, x3):\n        v3_1 = lstm1(x3)\n        v3_2 = v3_1[0]\n        v3_2_re = v3_2.repeat(1, 1, 1)\n        v2 = torch.nn.functional.linear(v3_2, self.linear.weight, self.linear.bias)\n        lstm3 = torch.nn.LSTM(1, 3, bidirectional=True, batch_first=True)\n        v4 = lstm3(v2.permute(1, 0, 2))\n        v5 = torch.nn.functional.linear(v4[0], self.linear.weight, self.linear.bias)\n        return v5\n# Inputs to the model\nx3 = torch.randn(1, 3, 1)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(1, 1)\n    def forward(self, x3):\n        v3 = torch.nn.functional.linear(x3, self.linear.weight, self.linear.bias)\n        flatten2 = torch.flatten(v3, 2)\n        reshape2 = flatten2.view([1, 1, 1])\n        reshape2_2 = torch.flatten(reshape2, 2)\n        return reshape2_2\n\n# Inputs to the model\nx3 = torch.randn(1, 1, 3)\n\n\n\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(2, 2)\n    def forward(self, x2):\n        v2 = torch.nn.functional.linear(x2, self.linear.weight, self.linear.bias)\n        v3 = v2.permute(0, 2, 1)\n        return v3\n# Inputs to the model\nx2 = torch.randn(1, 3, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(2, 2)\n    def forward(self, x3):\n        v2 = torch.nn.functional.linear(x3, self.linear.weight, self.linear.bias)\n        v3 = v2.permute(0, 2, 1)\n        self.lstm = torch.nn.LSTM(2, 2)\n        lstm = self.lstm(v3)[0]\n        return self.linear(lstm)\n# Inputs to the model\nx3 = torch.randn(1, 3, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(2, 2)\n    def forward(self, x3):\n      v3 = torch.nn.functional.linear(x3, self.linear.weight, self.linear.bias)\n      v4 = v3.permute(0, 2, 1)\n      v5 = v4[:, 2:, :]\n      lstm3 = torch.nn.LSTMCell(2, 2)\n      for _ in range(10):\n        r = torch.nn.functional.relu(v5)\n        i, o = lstm3(r)\n        v5 = i + o\n      return v5\n# Input(s) to the model\nx3 = torch.randn(1, 3, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear1 = torch.nn.Linear(3, 3)\n        self.linear2 = torch.nn.Linear(3, 2)\n    def forward(self, x1):\n        v2 = torch.nn.functional.linear(x1, self.linear1.weight, self.linear1.bias)\n        v3 = torch.nn.functional.linear(v2, self.linear2.weight, self.linear2.bias)\n        return v3.permute(0, 2, 1)\n# Inputs to the model\nx1 = torch.randn(1, 3, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(2, 2)\n    def forward(self, x1):\n        v1 = torch.nn.functional.linear(x1, self.linear.weight, self.linear.bias)\n        v2 = v1.permute(0, 2, 1).unsqueeze(-1)\n        v3 = v2.squeeze(0)\n        lstm2 = torch.nn.LSTM(1, 1)\n        return lstm2(v3)[0]\n# Inputs to the model\nx1 = torch.randn(1, 3, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(2, 2)\n        lstm = torch.nn.LSTM(2, 2)\n    def forward(self, input_):\n        v1 = torch.nn.functional.linear(x1, self.linear.weight, self.linear.bias)\n        v2 = lstm(v1.permute(1, 0, 2))\n        v3 = torch.nn.functional.linear(v2[0], self.linear.weight, self.linear.bias)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(2, 2)\n    def forward(self, x1, x2):\n        v1 = torch.nn.functional.linear(x1, self.linear.weight, self.linear.bias)\n        v2 = torch.nn.functional.linear(x2, self.linear.weight, self.linear.bias)\n        lstm = torch.nn.LSTM(2, 2)\n        v3 = lstm(v1.permute(1, 0, 2))\n        v4 = lstm(v2.permute(1, 0, 2))\n        return v3[0] + v4[0]\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\nx2 = torch.randn(1, 2, 2)\n"
            ],
            "g_time": 8.40781283378601
        }
    }
}
{
    "opt": "unary=26",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_t1 = torch.nn.ConvTranspose1d(1, 1, 1)\n    def forward(self, x18):\n        f1 = self.conv_t1(x18)\n        f2 = f1 > 0\n        f3 = f1 * 0.273\n        f4 = torch.where(f2, f1, f3)\n        return f4\n# Inputs to the model\nx18 = torch.randn(7, 1, 46)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_t1 = torch.nn.ConvTranspose2d(3, 16, 5, stride=1, padding=1)\n        self.conv_t2 = torch.nn.ConvTranspose2d(16, 32, 3, stride=2, padding=1)\n        self.conv_t3 = torch.nn.ConvTranspose2d(32, 64, 3, stride=2)\n    def forward(self, x50):\n        s1 = self.conv_t1(x50)\n        x1 = s1 > 0\n        x2 = s1 * -0.138\n        x3 = torch.where(x1, s1, x2)\n        x4 = self.conv_t2(x3)\n        x5 = x4 > 0\n        x6 = x4 * -0.149\n        x7 = torch.where(x5, x4, x6)\n        x8 = self.conv_t3(x7)\n        x9 = x8 > 0\n        x10 = x8 * -1.715\n        s2 = torch.where(x9, x8, x10)\n        return torch.nn.functional.adaptive_avg_pool2d(s2, (1, 1))\n# Inputs to the model\nx50 = torch.randn(1, 3, 30, 122)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, negative_slope):\n        super().__init__()\n        self.conv_t1 = torch.nn.ConvTranspose2d(1, 1048, 3, stride=1, padding=1, bias=False)\n        self.conv_t2 = torch.nn.ConvTranspose2d(1048, 512, 1, stride=1, bias=False)\n        self.conv_t3 = torch.nn.ConvTranspose2d(512, 128, 1, stride=1, bias=False)\n        self.conv_t4 = torch.nn.ConvTranspose2d(128, 3, 3, stride=1, padding=1, bias=False)\n        self.negative_slope = negative_slope\n    def forward(self, x42):\n        t1 = self.conv_t1(x42)\n        t2 = t1 > 0\n        t3 = t1 * self.negative_slope\n        t4 = torch.where(t2, t1, t3)\n        t5 = self.conv_t2(t4)\n        t6 = t5 > 0\n        t7 = t5 * self.negative_slope\n        t8 = torch.where(t6, t5, t7)\n        t9 = self.conv_t3(t8)\n        t10 = t9 > 0\n        t11 = t9 * self.negative_slope\n        t12 = torch.where(t10, t9, t11)\n        t13 = self.conv_t4(t12)\n        t14 = t13 > 0\n        t15 = t13 * self.negative_slope\n        t16 = torch.where(t14, t13, t15)\n        return torch.nn.functional.softmax(t16)\nnegative_slope = 0.109\n# Inputs to the model\nx42 = torch.randn(1, 1, 178, 675)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_t1 = torch.nn.ConvTranspose2d(32, 84, 4, stride=2, padding=1)\n        self.conv_t2 = torch.nn.ConvTranspose2d(84, 168, 1, stride=1, padding=0)\n    def forward(self, x1):\n        t1 = self.conv_t1(x1)\n        t2 = t1 > 0\n        t3 = t1 * -0.236\n        t4 = torch.where(t2, t1, t3)\n        t5 = self.conv_t2(t4)\n        t6 = t5 > 0\n        t7 = t5 * -0.293\n        t8 = torch.where(t6, t5, t7)\n        return t8\n# Inputs to the model\nx1 = torch.randn(16, 32, 26, 48)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, negative_slope):\n        super().__init__()\n        self.conv_t1 = torch.nn.ConvTranspose2d(16, 480, 3, stride=1, padding=1, output_padding=1, bias=False)\n        self.negative_slope = negative_slope\n    def forward(self, x26):\n        t1 = self.conv_t1(x26)\n        f1 = t1 * self.negative_slope\n        return f1\nnegative_slope = 0.0\n# Inputs to the model\nx26 = torch.randn(16, 16, 16, 24)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, negative_slope):\n        super().__init__()\n        self.conv_t1 = torch.nn.ConvTranspose2d(11, 3, 2, stride=2)\n        self.conv_t2 = torch.nn.ConvTranspose2d(3, 3, 2, stride=2)\n        self.negative_slope = negative_slope\n    def forward(self, input):\n        t1 = self.conv_t1(input)\n        t2 = t1 > 0\n        t3 = t1 * self.negative_slope\n        t4 = torch.where(t2, t1, t3)\n        t5 = self.conv_t2(t4)\n        t6 = t5 > 0\n        t7 = t5 * self.negative_slope\n        t8 = torch.where(t6, t5, t7)\n        return t8\nnegative_slope = (-8.4088, 5.1927)\n# Inputs to the model\nx1 = torch.randn(1, 11, 33, 19)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_t = torch.nn.ConvTranspose2d(1, 1, 1, stride=1)\n    def forward(self, x):\n        t1 = self.conv_t(x)\n        t2 = t1 > 0\n        t3 = t1 * -0.345\n        t4 = torch.where(t2, t1, t3)\n        return t4\n# Inputs to the model\nx = torch.randn(1, 1, 10, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_t1 = torch.nn.ConvTranspose2d(94, 84, 3, stride=2, bias=False)\n        self.conv_t2 = torch.nn.ConvTranspose2d(84, 144, 3, stride=2, padding=1, bias=False)\n        self.conv_t3 = torch.nn.ConvTranspose2d(144, 160, 3, stride=2, bias=False)\n        self.conv_t4 = torch.nn.ConvTranspose2d(160, 160, 1, stride=2, bias=False)\n    def forward(self, x1):\n        t1 = self.conv_t1(x1)\n        t2 = t1 > 0\n        t3 = t1 * -0.042\n        t4 = torch.where(t2, t1, t3)\n        t5 = self.conv_t2(t4)\n        t6 = t5 > 0\n        t7 = t5 * 0.302\n        t8 = torch.where(t6, t5, t7)\n        t9 = self.conv_t3(t8)\n        t10 = t9 > 0\n        t11 = t9 * -0.261\n        t12 = torch.where(t10, t9, t11)\n        t13 = self.conv_t4(t12)\n        t14 = t13 > 0\n        t15 = t13 * -0.083\n        t16 = torch.where(t14, t13, t15)\n        return t16\n# Inputs to the model\nx1 = torch.randn(16, 94, 12, 18)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_t1 = torch.nn.ConvTranspose2d(128, 25, 2, stride=2)\n        self.conv_t2 = torch.nn.ConvTranspose2d(25, 25, 2, stride=2)\n    def forward(self, x1):\n        t1 = self.conv_t1(x1)\n        t2 = t1 > 0\n        t3 = t1 * 0.044\n        t4 = torch.where(t2, t1, t3)\n        t5 = self.conv_t2(t4)\n        t6 = t5 > 0\n        t7 = t5 * 0.044\n        t8 = torch.where(t6, t5, t7)\n        return torch.nn.functional.adaptive_avg_pool2d(torch.nn.Softplus()(t8), (1, 1))\n# Inputs to the model\nx1 = torch.randn(16, 128, 8, 8)\n",
                "\nclass LeakySoftmaxModel(torch.nn.Module):\n    def __init__(self, conv_transpose_in_channels):\n        super().__init__()\n        self.conv1 = torch.nn.ConvTranspose2d(30, 50, 5)\n        self.conv2 = torch.nn.ConvTranspose2d(10, 20, 8)\n        self.conv3 = torch.nn.ConvTranspose2d(20, 24, 3, 1, 1)\n        self.conv4 = torch.nn.ConvTranspose2d(10, 46, 5)\n        self.convt = torch.nn.ConvTranspose2d(50, conv_transpose_in_channels, 1)\n    def forward(self, input):\n        x1 = self.conv2(self.conv1(input))\n        x2 = torch.nn.LeakyReLU(0.05)(x1)\n        x3 = torch.nn.functional.softmax(x2, dim=1)\n        x4 = x1 + x3\n        x5 = self.conv4(self.conv3(x4))\n        x6 = torch.nn.LeakyReLU(0.02)(x5)\n        x7 = torch.nn.functional.softmax(x6, dim=1)\n        x8 = self.convt(x5 * x7)\n        return x8\n# Inputs to the model\ninput = torch.randn(1, 30, 57, 85)\n\n# Model begins\nclass LeakySoftmaxModel(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.ConvTranspose2d(33, 55, 4, 2, padding=0, bias=False)\n        self.conv2 = torch.nn.ConvTranspose2d(30, 36, 8, 1, bias=False)\n        self.conv3 = torch.nn.ConvTranspose2d(36, 45, 3, 1, bias=False)\n        self.conv4 = torch.nn.ConvTranspose2d(12, 48, 6, 1, bias=False)\n    def forward(self, input):\n        x1 = self.conv1(input)\n        x2 = torch.nn.LeakyReLU(0.09)(x1)\n        x3 = torch.nn.functional.softmax(x2, dim=1)\n        x4 = x1 + x3\n        x5 = self.conv4(self.conv3(self.conv2(x4)))\n        x6 = torch.nn.functional.leaky_relu(x5)\n        x7 = torch.nn.functional.softmax(x6, dim=1)\n        return x5\n# Inputs to the model\ninput = torch.randn(1, 33, 57, 85)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_t1 = torch.nn.ConvTranspose1d(1, 1, 1)\n    def forward(self, x18):\n        f1 = self.conv_t1(x18)\n        f2 = f1 > 0\n        f3 = f1 * 0.273\n        f4 = torch.where(f2, f1, f3)\n        return f4\n# Inputs to the model\nx18 = torch.randn(7, 1, 46)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_t1 = torch.nn.ConvTranspose2d(3, 16, 5, stride=1, padding=1)\n        self.conv_t2 = torch.nn.ConvTranspose2d(16, 32, 3, stride=2, padding=1)\n        self.conv_t3 = torch.nn.ConvTranspose2d(32, 64, 3, stride=2)\n    def forward(self, x50):\n        s1 = self.conv_t1(x50)\n        x1 = s1 > 0\n        x2 = s1 * -0.138\n        x3 = torch.where(x1, s1, x2)\n        x4 = self.conv_t2(x3)\n        x5 = x4 > 0\n        x6 = x4 * -0.149\n        x7 = torch.where(x5, x4, x6)\n        x8 = self.conv_t3(x7)\n        x9 = x8 > 0\n        x10 = x8 * -1.715\n        s2 = torch.where(x9, x8, x10)\n        return torch.nn.functional.adaptive_avg_pool2d(s2, (1, 1))\n# Inputs to the model\nx50 = torch.randn(1, 3, 30, 122)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, negative_slope):\n        super().__init__()\n        self.conv_t1 = torch.nn.ConvTranspose2d(1, 1048, 3, stride=1, padding=1, bias=False)\n        self.conv_t2 = torch.nn.ConvTranspose2d(1048, 512, 1, stride=1, bias=False)\n        self.conv_t3 = torch.nn.ConvTranspose2d(512, 128, 1, stride=1, bias=False)\n        self.conv_t4 = torch.nn.ConvTranspose2d(128, 3, 3, stride=1, padding=1, bias=False)\n        self.negative_slope = negative_slope\n    def forward(self, x42):\n        t1 = self.conv_t1(x42)\n        t2 = t1 > 0\n        t3 = t1 * self.negative_slope\n        t4 = torch.where(t2, t1, t3)\n        t5 = self.conv_t2(t4)\n        t6 = t5 > 0\n        t7 = t5 * self.negative_slope\n        t8 = torch.where(t6, t5, t7)\n        t9 = self.conv_t3(t8)\n        t10 = t9 > 0\n        t11 = t9 * self.negative_slope\n        t12 = torch.where(t10, t9, t11)\n        t13 = self.conv_t4(t12)\n        t14 = t13 > 0\n        t15 = t13 * self.negative_slope\n        t16 = torch.where(t14, t13, t15)\n        return torch.nn.functional.softmax(t16)\nnegative_slope = 0.109\n# Inputs to the model\nx42 = torch.randn(1, 1, 178, 675)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_t1 = torch.nn.ConvTranspose2d(32, 84, 4, stride=2, padding=1)\n        self.conv_t2 = torch.nn.ConvTranspose2d(84, 168, 1, stride=1, padding=0)\n    def forward(self, x1):\n        t1 = self.conv_t1(x1)\n        t2 = t1 > 0\n        t3 = t1 * -0.236\n        t4 = torch.where(t2, t1, t3)\n        t5 = self.conv_t2(t4)\n        t6 = t5 > 0\n        t7 = t5 * -0.293\n        t8 = torch.where(t6, t5, t7)\n        return t8\n# Inputs to the model\nx1 = torch.randn(16, 32, 26, 48)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, negative_slope):\n        super().__init__()\n        self.conv_t1 = torch.nn.ConvTranspose2d(16, 480, 3, stride=1, padding=1, output_padding=1, bias=False)\n        self.negative_slope = negative_slope\n    def forward(self, x26):\n        t1 = self.conv_t1(x26)\n        f1 = t1 * self.negative_slope\n        return f1\nnegative_slope = 0.0\n# Inputs to the model\nx26 = torch.randn(16, 16, 16, 24)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, negative_slope):\n        super().__init__()\n        self.conv_t1 = torch.nn.ConvTranspose2d(11, 3, 2, stride=2)\n        self.conv_t2 = torch.nn.ConvTranspose2d(3, 3, 2, stride=2)\n        self.negative_slope = negative_slope\n    def forward(self, input):\n        t1 = self.conv_t1(input)\n        t2 = t1 > 0\n        t3 = t1 * self.negative_slope\n        t4 = torch.where(t2, t1, t3)\n        t5 = self.conv_t2(t4)\n        t6 = t5 > 0\n        t7 = t5 * self.negative_slope\n        t8 = torch.where(t6, t5, t7)\n        return t8\nnegative_slope = (-8.4088, 5.1927)\n# Inputs to the model\nx1 = torch.randn(1, 11, 33, 19)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_t = torch.nn.ConvTranspose2d(1, 1, 1, stride=1)\n    def forward(self, x):\n        t1 = self.conv_t(x)\n        t2 = t1 > 0\n        t3 = t1 * -0.345\n        t4 = torch.where(t2, t1, t3)\n        return t4\n# Inputs to the model\nx = torch.randn(1, 1, 10, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_t1 = torch.nn.ConvTranspose2d(94, 84, 3, stride=2, bias=False)\n        self.conv_t2 = torch.nn.ConvTranspose2d(84, 144, 3, stride=2, padding=1, bias=False)\n        self.conv_t3 = torch.nn.ConvTranspose2d(144, 160, 3, stride=2, bias=False)\n        self.conv_t4 = torch.nn.ConvTranspose2d(160, 160, 1, stride=2, bias=False)\n    def forward(self, x1):\n        t1 = self.conv_t1(x1)\n        t2 = t1 > 0\n        t3 = t1 * -0.042\n        t4 = torch.where(t2, t1, t3)\n        t5 = self.conv_t2(t4)\n        t6 = t5 > 0\n        t7 = t5 * 0.302\n        t8 = torch.where(t6, t5, t7)\n        t9 = self.conv_t3(t8)\n        t10 = t9 > 0\n        t11 = t9 * -0.261\n        t12 = torch.where(t10, t9, t11)\n        t13 = self.conv_t4(t12)\n        t14 = t13 > 0\n        t15 = t13 * -0.083\n        t16 = torch.where(t14, t13, t15)\n        return t16\n# Inputs to the model\nx1 = torch.randn(16, 94, 12, 18)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_t1 = torch.nn.ConvTranspose2d(128, 25, 2, stride=2)\n        self.conv_t2 = torch.nn.ConvTranspose2d(25, 25, 2, stride=2)\n    def forward(self, x1):\n        t1 = self.conv_t1(x1)\n        t2 = t1 > 0\n        t3 = t1 * 0.044\n        t4 = torch.where(t2, t1, t3)\n        t5 = self.conv_t2(t4)\n        t6 = t5 > 0\n        t7 = t5 * 0.044\n        t8 = torch.where(t6, t5, t7)\n        return torch.nn.functional.adaptive_avg_pool2d(torch.nn.Softplus()(t8), (1, 1))\n# Inputs to the model\nx1 = torch.randn(16, 128, 8, 8)\n",
                "\nclass LeakySoftmaxModel(torch.nn.Module):\n    def __init__(self, conv_transpose_in_channels):\n        super().__init__()\n        self.conv1 = torch.nn.ConvTranspose2d(30, 50, 5)\n        self.conv2 = torch.nn.ConvTranspose2d(10, 20, 8)\n        self.conv3 = torch.nn.ConvTranspose2d(20, 24, 3, 1, 1)\n        self.conv4 = torch.nn.ConvTranspose2d(10, 46, 5)\n        self.convt = torch.nn.ConvTranspose2d(50, conv_transpose_in_channels, 1)\n    def forward(self, input):\n        x1 = self.conv2(self.conv1(input))\n        x2 = torch.nn.LeakyReLU(0.05)(x1)\n        x3 = torch.nn.functional.softmax(x2, dim=1)\n        x4 = x1 + x3\n        x5 = self.conv4(self.conv3(x4))\n        x6 = torch.nn.LeakyReLU(0.02)(x5)\n        x7 = torch.nn.functional.softmax(x6, dim=1)\n        x8 = self.convt(x5 * x7)\n        return x8\n# Inputs to the model\ninput = torch.randn(1, 30, 57, 85)\n\n# Model begins\nclass LeakySoftmaxModel(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.ConvTranspose2d(33, 55, 4, 2, padding=0, bias=False)\n        self.conv2 = torch.nn.ConvTranspose2d(30, 36, 8, 1, bias=False)\n        self.conv3 = torch.nn.ConvTranspose2d(36, 45, 3, 1, bias=False)\n        self.conv4 = torch.nn.ConvTranspose2d(12, 48, 6, 1, bias=False)\n    def forward(self, input):\n        x1 = self.conv1(input)\n        x2 = torch.nn.LeakyReLU(0.09)(x1)\n        x3 = torch.nn.functional.softmax(x2, dim=1)\n        x4 = x1 + x3\n        x5 = self.conv4(self.conv3(self.conv2(x4)))\n        x6 = torch.nn.functional.leaky_relu(x5)\n        x7 = torch.nn.functional.softmax(x6, dim=1)\n        return x5\n# Inputs to the model\ninput = torch.randn(1, 33, 57, 85)\n"
            ],
            "g_time": 21.11319637298584
        }
    }
}
{
    "opt": "permute_linear_fusion",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.convbn1 = torch.nn.BatchNorm2d(2, affine=True)\n        self.convbn2 = torch.nn.BatchNorm2d(2, affine=True)\n        self.convbn3 = torch.nn.BatchNorm2d(2, affine=True)\n    def forward(self, x1):\n        x2 = x1.permute(0, 2, 1)\n        x3 = x2.permute(2, 1, 0)\n        x4 = x3.permute(1, 2, 0)\n        x5 = self.convbn1(x1)\n        x6 = self.convbn2(x5)\n        x7 = self.convbn3(x6)\n        return x7\n# Inputs to the model\nx1 = torch.randn(1, 2, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear1 = torch.nn.Linear(2, 2)\n        self.linear2 = torch.nn.Linear(2, 2)\n    def forward(self, x1):\n        x2 = x1.permute(0, 2, 1)\n        x3 = x2 * self.linear1.weight\n        x4 = x3.permute(2, 1, 0)\n        x5 = x4 * self.linear2.weight\n        x6 = x5.permute(1, 2, 0)\n        return x6\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.x1 = nn.Linear(5, 4)\n        self.x2 = nn.Linear(4, 3)\n        self.x3 = nn.Linear(3, 2)\n        self.x4 = nn.Linear(2, 1)\n    def forward(self, x):\n        x1 = nn.functional.relu(x)\n        x2 = nn.functional.sigmoid(x1)\n        x3 = nn.functional.tanh(x2)\n        res1 = self.x1(x3)\n        res2 = self.x2(x3)\n        res3 = self.x3(x3)\n        res4 = self.x4(x3)\n        a = torch.cat([res1, res2], 3)\n        b = torch.cat([res2, res3], 3)\n        c = torch.cat([res3, torch.max(res4, res2)], 2)\n        d = torch.cat([res4, res3], 3)\n        v1 = [a, b, c, d]\n        result = torch.cat(v1, 1)\n        return result\n# Inputs to the model\nx = torch.randn(1, 5)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear1 = torch.nn.Linear(2, 2)\n        self.linear2 = torch.nn.Linear(2, 2)\n        self.linear3 = torch.nn.Linear(2, 2)\n        self.linear4 = torch.nn.Linear(2, 2)\n        self.sigmoid = torch.nn.Sigmoid()\n    def forward(self, x1):\n        v1 = x1.permute(0, 2, 1)\n        v2 = torch.nn.functional.linear(v1, self.linear1.weight, self.linear1.bias)\n        v3 = torch.nn.functional.linear(v2, self.linear2.weight, self.linear2.bias)\n        v4 = torch.nn.functional.linear(v3, self.linear3.weight, self.linear3.bias)\n        v5 = torch.nn.functional.linear(v4, self.linear4.weight, self.linear4.bias)\n        v6 = self.sigmoid(v5)\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear1 = torch.nn.Linear(2, 2)\n        self.linear2 = torch.nn.Linear(2, 2)\n    def forward(self, x1):\n        x2 = x1.permute(2, 1, 0)\n        x3 = x2.permute(1, 2, 0)\n        x4 = torch.nn.functional.linear(x2, self.linear1.weight, self.linear1.bias)\n        x5 = torch.nn.functional.linear(x4, self.linear2.weight, self.linear2.bias)\n        return x5\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear1 = torch.nn.Linear(2, 2)\n        self.linear2 = torch.nn.Linear(2, 2)\n        self.conv = torch.nn.Conv2d(1, 1, (1, 1), stride=(2, 2))\n    def forward(self, x1):\n        x2 = torch.nn.functional.linear(x1, self.linear1.weight, self.linear1.bias)\n        x3 = torch.nn.functional.linear(x2, self.linear2.weight, self.linear2.bias)\n        x4 = x3.permute(2, 1, 0).unsqueeze(0)\n        x5 = self.conv(x4)\n        return x5\n# Inputs to the model\nx1 = torch.randn(2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear1 = torch.nn.Linear(2, 6)\n        self.linear2 = torch.nn.Linear(2, 6)\n        self.linear3 = torch.nn.Linear(2, 2)\n        self.linear4 = torch.nn.Linear(2, 2)\n    def forward(self, x1):\n        x2 = x1.permute(1, 2, 0)\n        x3 = x1.permute(2, 1, 0)\n        x4 = x3.permute(1, 2, 0)\n        x5 = torch.nn.functional.linear(x4, self.linear1.weight, self.linear1.bias)\n        x6 = torch.nn.functional.linear(x5, self.linear2.weight, self.linear2.bias)\n        x7 = torch.nn.functional.linear(x6, self.linear3.weight, self.linear3.bias)\n        return x7\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.t0 = torch.tensor([[1, 2, 3]])\n        self.linear = torch.nn.Linear(2, 2)\n        self.convbn = torch.nn.BatchNorm2d(2, affine=True)\n    def forward(self, x1):\n        t1 = x1+self.t0\n        t2 = t1.permute(2, 1, 0)\n        v1 = self.convbn(t2)\n        v2 = v1.permute(2, 1, 0)\n        v3 = torch.nn.functional.linear(v2, self.linear.weight, self.linear.bias)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear1 = torch.nn.Linear(2, 2)\n        self.linear2 = torch.nn.Linear(2, 2)\n        self.sigmoid1 = torch.nn.Sigmoid()\n    def forward(self, x1):\n        x2 = x1.permute(0, 2, 1)\n        x3 = x2.permute(2, 1, 0)\n        x4 = x3.permute(1, 2, 0)\n        x5 = torch.nn.functional.linear(x2, self.linear1.weight, self.linear1.bias)\n        x6 = torch.nn.functional.linear(x5, self.linear2.weight, self.linear2.bias)\n        x7 = self.sigmoid1(x6)\n        return x7\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear1 = torch.nn.Linear(5, 5)\n        self.linear2 = torch.nn.Linear(5, 5)\n    def forward(self, x1):\n        x1 = x1.permute(0, 3, 2, 1)\n        x2 = torch.nn.functional.relu(self.linear1(x1))\n        x3 = torch.nn.functional.linear(x2, self.linear2.weight, self.linear2.bias)\n        x4 = x3.permute(0, 2, 3, 1)\n        x5 = torch.nn.functional.relu(x4)\n        x6 = x5.permute(0, 3, 1, 2)\n        x7 = torch.nn.functional.linear(x6, torch.tensor([[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]).permute(3, 1, 0), torch.tensor([[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]]).permute(3, 1, 0))\n        x8 = torch.nn.functional.sigmoid(x7)\n        return x8\n# Inputs to the model\nx1 = torch.randn(1, 2, 2, 2)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.convbn1 = torch.nn.BatchNorm2d(2, affine=True)\n        self.convbn2 = torch.nn.BatchNorm2d(2, affine=True)\n        self.convbn3 = torch.nn.BatchNorm2d(2, affine=True)\n    def forward(self, x1):\n        x2 = x1.permute(0, 2, 1)\n        x3 = x2.permute(2, 1, 0)\n        x4 = x3.permute(1, 2, 0)\n        x5 = self.convbn1(x1)\n        x6 = self.convbn2(x5)\n        x7 = self.convbn3(x6)\n        return x7\n# Inputs to the model\nx1 = torch.randn(1, 2, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear1 = torch.nn.Linear(2, 2)\n        self.linear2 = torch.nn.Linear(2, 2)\n    def forward(self, x1):\n        x2 = x1.permute(0, 2, 1)\n        x3 = x2 * self.linear1.weight\n        x4 = x3.permute(2, 1, 0)\n        x5 = x4 * self.linear2.weight\n        x6 = x5.permute(1, 2, 0)\n        return x6\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.x1 = nn.Linear(5, 4)\n        self.x2 = nn.Linear(4, 3)\n        self.x3 = nn.Linear(3, 2)\n        self.x4 = nn.Linear(2, 1)\n    def forward(self, x):\n        x1 = nn.functional.relu(x)\n        x2 = nn.functional.sigmoid(x1)\n        x3 = nn.functional.tanh(x2)\n        res1 = self.x1(x3)\n        res2 = self.x2(x3)\n        res3 = self.x3(x3)\n        res4 = self.x4(x3)\n        a = torch.cat([res1, res2], 3)\n        b = torch.cat([res2, res3], 3)\n        c = torch.cat([res3, torch.max(res4, res2)], 2)\n        d = torch.cat([res4, res3], 3)\n        v1 = [a, b, c, d]\n        result = torch.cat(v1, 1)\n        return result\n# Inputs to the model\nx = torch.randn(1, 5)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear1 = torch.nn.Linear(2, 2)\n        self.linear2 = torch.nn.Linear(2, 2)\n        self.linear3 = torch.nn.Linear(2, 2)\n        self.linear4 = torch.nn.Linear(2, 2)\n        self.sigmoid = torch.nn.Sigmoid()\n    def forward(self, x1):\n        v1 = x1.permute(0, 2, 1)\n        v2 = torch.nn.functional.linear(v1, self.linear1.weight, self.linear1.bias)\n        v3 = torch.nn.functional.linear(v2, self.linear2.weight, self.linear2.bias)\n        v4 = torch.nn.functional.linear(v3, self.linear3.weight, self.linear3.bias)\n        v5 = torch.nn.functional.linear(v4, self.linear4.weight, self.linear4.bias)\n        v6 = self.sigmoid(v5)\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear1 = torch.nn.Linear(2, 2)\n        self.linear2 = torch.nn.Linear(2, 2)\n    def forward(self, x1):\n        x2 = x1.permute(2, 1, 0)\n        x3 = x2.permute(1, 2, 0)\n        x4 = torch.nn.functional.linear(x2, self.linear1.weight, self.linear1.bias)\n        x5 = torch.nn.functional.linear(x4, self.linear2.weight, self.linear2.bias)\n        return x5\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear1 = torch.nn.Linear(2, 2)\n        self.linear2 = torch.nn.Linear(2, 2)\n        self.conv = torch.nn.Conv2d(1, 1, (1, 1), stride=(2, 2))\n    def forward(self, x1):\n        x2 = torch.nn.functional.linear(x1, self.linear1.weight, self.linear1.bias)\n        x3 = torch.nn.functional.linear(x2, self.linear2.weight, self.linear2.bias)\n        x4 = x3.permute(2, 1, 0).unsqueeze(0)\n        x5 = self.conv(x4)\n        return x5\n# Inputs to the model\nx1 = torch.randn(2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear1 = torch.nn.Linear(2, 6)\n        self.linear2 = torch.nn.Linear(2, 6)\n        self.linear3 = torch.nn.Linear(2, 2)\n        self.linear4 = torch.nn.Linear(2, 2)\n    def forward(self, x1):\n        x2 = x1.permute(1, 2, 0)\n        x3 = x1.permute(2, 1, 0)\n        x4 = x3.permute(1, 2, 0)\n        x5 = torch.nn.functional.linear(x4, self.linear1.weight, self.linear1.bias)\n        x6 = torch.nn.functional.linear(x5, self.linear2.weight, self.linear2.bias)\n        x7 = torch.nn.functional.linear(x6, self.linear3.weight, self.linear3.bias)\n        return x7\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.t0 = torch.tensor([[1, 2, 3]])\n        self.linear = torch.nn.Linear(2, 2)\n        self.convbn = torch.nn.BatchNorm2d(2, affine=True)\n    def forward(self, x1):\n        t1 = x1+self.t0\n        t2 = t1.permute(2, 1, 0)\n        v1 = self.convbn(t2)\n        v2 = v1.permute(2, 1, 0)\n        v3 = torch.nn.functional.linear(v2, self.linear.weight, self.linear.bias)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear1 = torch.nn.Linear(2, 2)\n        self.linear2 = torch.nn.Linear(2, 2)\n        self.sigmoid1 = torch.nn.Sigmoid()\n    def forward(self, x1):\n        x2 = x1.permute(0, 2, 1)\n        x3 = x2.permute(2, 1, 0)\n        x4 = x3.permute(1, 2, 0)\n        x5 = torch.nn.functional.linear(x2, self.linear1.weight, self.linear1.bias)\n        x6 = torch.nn.functional.linear(x5, self.linear2.weight, self.linear2.bias)\n        x7 = self.sigmoid1(x6)\n        return x7\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear1 = torch.nn.Linear(5, 5)\n        self.linear2 = torch.nn.Linear(5, 5)\n    def forward(self, x1):\n        x1 = x1.permute(0, 3, 2, 1)\n        x2 = torch.nn.functional.relu(self.linear1(x1))\n        x3 = torch.nn.functional.linear(x2, self.linear2.weight, self.linear2.bias)\n        x4 = x3.permute(0, 2, 3, 1)\n        x5 = torch.nn.functional.relu(x4)\n        x6 = x5.permute(0, 3, 1, 2)\n        x7 = torch.nn.functional.linear(x6, torch.tensor([[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]).permute(3, 1, 0), torch.tensor([[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]]).permute(3, 1, 0))\n        x8 = torch.nn.functional.sigmoid(x7)\n        return x8\n# Inputs to the model\nx1 = torch.randn(1, 2, 2, 2)\n"
            ],
            "g_time": 13.924575567245483
        }
    }
}

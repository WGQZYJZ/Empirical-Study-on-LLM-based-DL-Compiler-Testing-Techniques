{
    "opt": "unary=15",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 32, 1, stride=1, padding=0)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = torch.elu(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 3, 256, 256)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(160, 160, 3, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = torch.relu(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 160, 14, 14)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc1 = torch.nn.Linear(3 * 28 * 28, 320)\n        self.fc2 = torch.nn.Linear(320, 10)\n    def forward(self, x1):\n        v1 = x1.view(-1, 3, 28, 28)\n        v2 = self.fc1(v1)\n        v3 = self.fc2(v2)\n        return v3\n# Inputs to the model\nx1 = torch.randn(8, 3, 28, 28)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 16, 1, stride=1, padding=0)\n        self.bn = torch.nn.BatchNorm2d(16)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = self.bn(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 3, 28, 28)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(10, 5)\n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.sigmoind(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 10)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 64, 3, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = torch.relu(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 3, 224, 224)\n",
                "",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 16, 3, stride=1, padding=1)\n        self.bn = torch.nn.BatchNorm2d(16)\n        self.conv2 = torch.nn.Conv2d(16, 16, 3, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = self.bn(v1)\n        v3 = torch.relu(v2)\n        v4 = self.conv2(v3)\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 3, 256, 256)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 32, 3, stride=1, padding=0)\n        self.bn1 = torch.nn.BatchNorm2d(32)\n        self.conv2 = torch.nn.Conv2d(32, 32, 3, stride=2, padding=1)\n        self.conv3 = torch.nn.Conv2d(32, 32, 1, stride=1, padding=0)\n        self.bn2 = torch.nn.BatchNorm2d(32)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = self.bn1(v1)\n        out = torch.relu(v2)\n        out = self.conv2(out)\n        out = torch.relu(out)\n        out = self.conv3(out)\n        out = self.bn2(out)\n        out = torch.relu(out)\n        return out\n# Inputs to the model\nx1 = torch.randn(1, 3, 256, 256)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 32, 3, stride=1, padding=1)\n        self.conv1 = torch.nn.Conv2d(32, 16, 1, stride=1, padding=0)\n        self.conv2 = torch.nn.Conv2d(16, 8, 1, stride=1, padding=0)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = self.conv1(v1)\n        v3 = torch.tanh(v2)\n        v4 = self.conv2(v3)\n        v5 = torch.tanh(v4)\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 3, 256, 256)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 32, 1, stride=1, padding=0)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = torch.elu(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 3, 256, 256)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(160, 160, 3, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = torch.relu(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 160, 14, 14)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc1 = torch.nn.Linear(3 * 28 * 28, 320)\n        self.fc2 = torch.nn.Linear(320, 10)\n    def forward(self, x1):\n        v1 = x1.view(-1, 3, 28, 28)\n        v2 = self.fc1(v1)\n        v3 = self.fc2(v2)\n        return v3\n# Inputs to the model\nx1 = torch.randn(8, 3, 28, 28)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 16, 1, stride=1, padding=0)\n        self.bn = torch.nn.BatchNorm2d(16)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = self.bn(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 3, 28, 28)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(10, 5)\n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.sigmoind(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 10)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 64, 3, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = torch.relu(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 3, 224, 224)\n",
                "",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 16, 3, stride=1, padding=1)\n        self.bn = torch.nn.BatchNorm2d(16)\n        self.conv2 = torch.nn.Conv2d(16, 16, 3, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = self.bn(v1)\n        v3 = torch.relu(v2)\n        v4 = self.conv2(v3)\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 3, 256, 256)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 32, 3, stride=1, padding=0)\n        self.bn1 = torch.nn.BatchNorm2d(32)\n        self.conv2 = torch.nn.Conv2d(32, 32, 3, stride=2, padding=1)\n        self.conv3 = torch.nn.Conv2d(32, 32, 1, stride=1, padding=0)\n        self.bn2 = torch.nn.BatchNorm2d(32)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = self.bn1(v1)\n        out = torch.relu(v2)\n        out = self.conv2(out)\n        out = torch.relu(out)\n        out = self.conv3(out)\n        out = self.bn2(out)\n        out = torch.relu(out)\n        return out\n# Inputs to the model\nx1 = torch.randn(1, 3, 256, 256)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 32, 3, stride=1, padding=1)\n        self.conv1 = torch.nn.Conv2d(32, 16, 1, stride=1, padding=0)\n        self.conv2 = torch.nn.Conv2d(16, 8, 1, stride=1, padding=0)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = self.conv1(v1)\n        v3 = torch.tanh(v2)\n        v4 = self.conv2(v3)\n        v5 = torch.tanh(v4)\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 3, 256, 256)\n"
            ],
            "g_time": 9.564631700515747
        }
    }
}
{
    "opt": "unary=21",
    "response": {
        "0": {
            "raw": [
                "\nclass ModelTanh(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(4, 8, 2, stride=1)\n        self.conv2 = torch.nn.Conv2d(8, 64, 1, stride=1)\n    def forward(self, x):\n        v1 = self.conv(x)\n        v2 = torch.tanh(v1)\n        return self.conv2(v2)\n# Inputs to the model\nx = torch.randn(32, 4, 3, 49)\n",
                "\nclass ModelTanh(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(64, 64, 1, stride=1)\n        self.sigmoid = torch.nn.Sigmoid()\n    def forward(self, v1):\n        v2 = self.conv(v1)\n        v3 = self.sigmoid(v2)\n        v4 = torch.tanh(v3)\n        return v4\n# Inputs to the model\nv1 = torch.randn(100, 64, 64, 64)\n",
                "\nclass ModelTanh(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 8, 2, stride=1)\n        self.conv2 = torch.nn.Conv2d(8, 5, 1, stride=1)\n    def forward(self, x):\n        v1 = self.conv1(x)\n        v3 = self.conv2(v1)\n        v4 = torch.tanh(v3)\n        return v4\n# Inputs to the model\nx = torch.randn(4, 3, 49, 45)\n",
                "\nclass ModelTanh(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(2, 2, 1, stride=1)\n    def forward(self, x):\n        v1 = self.conv(x)\n        v2 = torch.tanh(v1)\n        return v2\n# Inputs to the model\nx = torch.randn(1, 2, 8, 8)\n",
                "\nclass ModelTanh(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.ConvTranspose2d(10, 3, 2, stride=1)\n    def forward(self, x):\n        v1 = self.conv(x)\n        v2 = torch.tanh(v1)\n        return v2\n# Inputs to the model\nx = torch.randn(1, 10, 10, 10)\n",
                "\nclass ModelTanh(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(6, 8, 2, stride=2)\n        self.conv2 = torch.nn.Conv2d(8, 16, 2, stride=2)\n    def forward(self, x):\n        v1 = self.conv(x)\n        v2 = torch.tanh(v1)\n        return self.conv2(v2)\n# Inputs to the model\nx = torch.randn(128, 6, 77, 87)\n",
                "\nclass ModelTanh(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(1, 9, 2, stride=2)\n    def forward(self, x):\n        v1 = self.conv(x)\n        v2 = torch.tanh(v1)\n        return v2\n# Inputs to the model\nx = torch.randn(1, 1, 132, 85)\n",
                "\nclass ModelTanh(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 256, 1, padding=(0,1), stride=1)\n        self.conv2 = torch.nn.Conv2d(256, 256, (1, 5), padding=(0,2), stride=1)\n        self.conv3 = torch.nn.Conv2d(256, 3, (1, 7), padding=(0,3), stride=1)\n    def forward(self, x1):\n        v2 = self.conv1(x1)\n        v2 = torch.tanh(v2)\n        v2 = self.conv2(v2)\n        v2 = torch.tanh(v2)\n        v2 = self.conv3(v2)\n        v3 = torch.tanh(v2)\n        return v3\n# Inputs to the model\nx1 = torch.randn(4, 3, 256, 241)\n",
                "\nclass ModelTanh(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(22, 96, 2, stride=1)\n        self.conv1 = torch.nn.Conv2d(96, 96, 2, stride=1)\n    def forward(self, x4):\n        v3 = self.conv(x4)\n        v3 = torch.tanh(v3)\n        return v3\n# Inputs to the model\nx4 = torch.randn(1, 22, 33, 33)\n",
                "\nclass ModelTanh(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(64, 128, 3, stride=1)\n    def forward(self, x):\n        t1 = self.conv(x)\n        t2 = torch.tanh(t1)\n        return t2\n# Inputs to the model\nx = torch.randn(10, 64, 21, 48)\n"
            ],
            "code": [
                "\nclass ModelTanh(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(4, 8, 2, stride=1)\n        self.conv2 = torch.nn.Conv2d(8, 64, 1, stride=1)\n    def forward(self, x):\n        v1 = self.conv(x)\n        v2 = torch.tanh(v1)\n        return self.conv2(v2)\n# Inputs to the model\nx = torch.randn(32, 4, 3, 49)\n",
                "\nclass ModelTanh(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(64, 64, 1, stride=1)\n        self.sigmoid = torch.nn.Sigmoid()\n    def forward(self, v1):\n        v2 = self.conv(v1)\n        v3 = self.sigmoid(v2)\n        v4 = torch.tanh(v3)\n        return v4\n# Inputs to the model\nv1 = torch.randn(100, 64, 64, 64)\n",
                "\nclass ModelTanh(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 8, 2, stride=1)\n        self.conv2 = torch.nn.Conv2d(8, 5, 1, stride=1)\n    def forward(self, x):\n        v1 = self.conv1(x)\n        v3 = self.conv2(v1)\n        v4 = torch.tanh(v3)\n        return v4\n# Inputs to the model\nx = torch.randn(4, 3, 49, 45)\n",
                "\nclass ModelTanh(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(2, 2, 1, stride=1)\n    def forward(self, x):\n        v1 = self.conv(x)\n        v2 = torch.tanh(v1)\n        return v2\n# Inputs to the model\nx = torch.randn(1, 2, 8, 8)\n",
                "\nclass ModelTanh(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.ConvTranspose2d(10, 3, 2, stride=1)\n    def forward(self, x):\n        v1 = self.conv(x)\n        v2 = torch.tanh(v1)\n        return v2\n# Inputs to the model\nx = torch.randn(1, 10, 10, 10)\n",
                "\nclass ModelTanh(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(6, 8, 2, stride=2)\n        self.conv2 = torch.nn.Conv2d(8, 16, 2, stride=2)\n    def forward(self, x):\n        v1 = self.conv(x)\n        v2 = torch.tanh(v1)\n        return self.conv2(v2)\n# Inputs to the model\nx = torch.randn(128, 6, 77, 87)\n",
                "\nclass ModelTanh(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(1, 9, 2, stride=2)\n    def forward(self, x):\n        v1 = self.conv(x)\n        v2 = torch.tanh(v1)\n        return v2\n# Inputs to the model\nx = torch.randn(1, 1, 132, 85)\n",
                "\nclass ModelTanh(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 256, 1, padding=(0,1), stride=1)\n        self.conv2 = torch.nn.Conv2d(256, 256, (1, 5), padding=(0,2), stride=1)\n        self.conv3 = torch.nn.Conv2d(256, 3, (1, 7), padding=(0,3), stride=1)\n    def forward(self, x1):\n        v2 = self.conv1(x1)\n        v2 = torch.tanh(v2)\n        v2 = self.conv2(v2)\n        v2 = torch.tanh(v2)\n        v2 = self.conv3(v2)\n        v3 = torch.tanh(v2)\n        return v3\n# Inputs to the model\nx1 = torch.randn(4, 3, 256, 241)\n",
                "\nclass ModelTanh(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(22, 96, 2, stride=1)\n        self.conv1 = torch.nn.Conv2d(96, 96, 2, stride=1)\n    def forward(self, x4):\n        v3 = self.conv(x4)\n        v3 = torch.tanh(v3)\n        return v3\n# Inputs to the model\nx4 = torch.randn(1, 22, 33, 33)\n",
                "\nclass ModelTanh(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(64, 128, 3, stride=1)\n    def forward(self, x):\n        t1 = self.conv(x)\n        t2 = torch.tanh(t1)\n        return t2\n# Inputs to the model\nx = torch.randn(10, 64, 21, 48)\n"
            ],
            "g_time": 7.677495956420898
        }
    }
}
{
    "opt": "unary=4",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(64, 128)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(8, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(5, 5)\n \n    def forward(self, x1):\n        v3 = self.linear(x1)\n        v2 = v3 * 0.5\n        v3 = v3 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 5, 3, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layers0 = torch.nn.Linear(256, 128)\n        self.layers1 = torch.nn.Linear(128, 256)\n \n    def forward(self, x1):\n        v1 = self.layers0(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        v7 = self.layers1(v6)\n        return v7\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 256)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(8, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(8, 16)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 8)\n",
                "\nclass Model2(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc = torch.nn.Linear(149, 8)\n \n    def forward(self, x1):\n        v1 = self.fc(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n\n# Initializing the model\nm = Model2()\n\n# Inputs to the model\nx1 = torch.randn(1, 149)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(10, 10)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(10)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(64, 128)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(8, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(5, 5)\n \n    def forward(self, x1):\n        v3 = self.linear(x1)\n        v2 = v3 * 0.5\n        v3 = v3 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 5, 3, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layers0 = torch.nn.Linear(256, 128)\n        self.layers1 = torch.nn.Linear(128, 256)\n \n    def forward(self, x1):\n        v1 = self.layers0(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        v7 = self.layers1(v6)\n        return v7\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 256)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(8, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(8, 16)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 8)\n",
                "\nclass Model2(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc = torch.nn.Linear(149, 8)\n \n    def forward(self, x1):\n        v1 = self.fc(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n\n# Initializing the model\nm = Model2()\n\n# Inputs to the model\nx1 = torch.randn(1, 149)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(10, 10)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(10)\n"
            ],
            "g_time": 7.495925426483154
        }
    }
}
{
    "opt": "unary=16",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(8, 128)\n        self.relu = torch.nn.ReLU()\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = self.relu(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(4, 1)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.nn.functional.relu(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 4)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(10, 2)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.nn.functional.relu(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 10)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n\n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.nn.functional.relu(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(3, 5)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear_relu_stack = torch.nn.Sequential(\n            torch.nn.Linear(8, 32),\n            torch.nn.ReLU(),\n            torch.nn.Linear(32, 64),\n            torch.nn.ReLU(),\n            torch.nn.Linear(64, 32),\n        )\n \n    def forward(self, x1):\n        v1 = self.linear_relu_stack(x1)\n        return v1\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(3, 8)\nx2 = torch.randn(7, 6)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(8, 4)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1.relu()\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(10, 20)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1.relu()\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 10)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(21, 1)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = F.relu(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 21)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(64, 64)\n \n    def forward(self, x1):\n        t1 = self.linear(x1)\n        t2 = torch.nn.functional.relu(t1)\n        return t2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.dense = torch.nn.Linear(16, 16)\n \n    def forward(self, x1):\n        v1 = self.dense(x1)\n        v2 = torch.relu(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 16)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(8, 128)\n        self.relu = torch.nn.ReLU()\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = self.relu(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(4, 1)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.nn.functional.relu(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 4)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(10, 2)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.nn.functional.relu(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 10)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n\n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.nn.functional.relu(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(3, 5)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear_relu_stack = torch.nn.Sequential(\n            torch.nn.Linear(8, 32),\n            torch.nn.ReLU(),\n            torch.nn.Linear(32, 64),\n            torch.nn.ReLU(),\n            torch.nn.Linear(64, 32),\n        )\n \n    def forward(self, x1):\n        v1 = self.linear_relu_stack(x1)\n        return v1\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(3, 8)\nx2 = torch.randn(7, 6)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(8, 4)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1.relu()\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(10, 20)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1.relu()\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 10)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(21, 1)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = F.relu(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 21)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(64, 64)\n \n    def forward(self, x1):\n        t1 = self.linear(x1)\n        t2 = torch.nn.functional.relu(t1)\n        return t2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.dense = torch.nn.Linear(16, 16)\n \n    def forward(self, x1):\n        v1 = self.dense(x1)\n        v2 = torch.relu(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 16)\n"
            ],
            "g_time": 5.6766955852508545
        }
    }
}
{
    "opt": "sfdp=2",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self, attention_hidden_size, num_attention_heads, dropout_p):\n        super().__init__()\n        self.dropout = torch.nn.Dropout(dropout_p)\n        self.softmax = torch.nn.Softmax()\n \n    def forward(self, query, key, value, scale_factor=None, dropout_p=None):\n        qk = torch.matmul(query, key.transpose(-2, -1))\n        scaled_qk = qk/scale_factor\n        softmax_qk = self.softmax(scaled_qk, dim=-1)\n        dropout_qk = self.dropout(softmax_qk)\n        output = dropout_qk.matmul(value)\n        return output\n\n# Initializing the model\nm = Model(attention_hidden_size=512, num_attention_heads=4, dropout_p=0.5)\n\n# Inputs to the model\nquery = torch.randn(15, 4, 512)\nkey = torch.randn(16, 4, 512)\nvalue = torch.randn(16, 4, 512)\nscale_factor = torch.scalar_tensor(1/math.sqrt(512))\n",
                "\nm = Model()\n\n# Inputs to the model\nquery = torch.randn(b, t, n)\nkey = torch.randn(b, t, n)\nvalue = torch.randn(b, t, n)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, dim_query, dim_key, dim_value, dropout_p, inv_scale_factor):\n        super().__init__()\n        \n    def forward(self, query, key, value):\n        qk = torch.matmul(query, key.transpose(-2, -1))\n        scaled_qk = qk.div(inv_scale_factor)\n        softmax_qk = scaled_qk.softmax(dim=-1)\n        dropout_qk = torch.nn.functional.dropout(softmax_qk, p=dropout_p)\n        output = dropout_qk.matmul(value)\n        return output\n\n# Initializing the model\nm = Model(dim_query=8192, dim_key=8192, dim_value=8192, dropout_p=0.1, inv_scale_factor=48.2)\n\n# Inputs to the model\nquery = torch.randn(1, 8192, 32)\nkey = torch.randn(1, 8192, 32)\nvalue = torch.randn(1, 8192, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, dropout_p, inv_scale_factor):\n        super().__init__()\n        self.dropout_p = dropout_p\n        self.inv_scale_factor = inv_scale_factor\n \n    def forward(self, query, key, value):\n        qk = query.matmul(key.transpose(-2, -1))\n        scaled_qk = qk.div(self.inv_scale_factor)\n        softmax_qk = scaled_qk.softmax(dim=-1)\n        dropout_qk = torch.nn.functional.dropout(softmax_qk, p=self.dropout_p)\n        output = dropout_qk.matmul(value)\n        return output\n\n# Initializing the model\nm = Model(0.1, 0.5)\n\n# Input to the model\nquery = torch.randn(16, 32, 512)\nkey = torch.randn(16, 32, 256)\nvalue = torch.randn(16, 32, 256)\n\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.dropout_11 = torch.nn.Dropout(0.1)\n        self.softmax_6 = torch.nn.Softmax(dim=-1)\n        self.dropout_26 = torch.nn.Dropout(0.1)\n        self.softmax_7 = torch.nn.Softmax(dim=-1)\n        self.matmul_8 = torch.matmul()\n        self.dropout_9 = torch.nn.Dropout(0.1)\n        self.matmul_10 = torch.matmul()\n \n    def forward(self, v4, v5, v6):\n        v1 = self.dropout_11(v4)\n        v2 = self.softmax_6(v1)\n        v3 = v2 / 0.05241306459982384\n        v11 = self.dropout_26(v5)\n        v12 = self.softmax_7(v11)\n        v13 = v12 / 0.00521668520555902\n        v7 = self.matmul_8(v3, v6)\n        v8 = self.dropout_9(v7)\n        v9 = self.matmul_10(v13, v8)\n        return v9\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nv4 = torch.randn(1, 1024, 64)\nv5 = torch.randn(1, 5, 512)\nv6 = torch.randn(5, 512, 512)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n \n    def forward(self, x1, x2, x3):\n        v1 = torch.matmul(x1, x2.transpose(-2, -1))\n        v2 = v1.div(0.5)\n        v3 = torch.nn.functional.softmax(v2, dim=-1)\n        v4 = torch.nn.functional.dropout(v3, p=0.25)\n        v5 = torch.matmul(v4, x3)\n        return v5\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 10, 32)\nx2 = torch.randn(1, 32, 16)\nx3 = torch.randn(1, 16, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.d_model = 2\n        self.nhead = 2\n        self.dropout_p = 0.0\n        self.w_qkv = self.w_proj = torch.nn.Linear(self.d_model, self.d_model * 3)\n        self.attention = torch.nn.MultiheadAttention(self.d_model, self.nhead, torch.nn.Dropout(self.dropout_p))\n \n    def forward(self, query, key, value, inv_scale_factor):\n        qkv = self.w_qkv(query).chunk(3, dim=-1)\n        query = qkv[0].contiguous().view(query.size(0), -1, self.nhead, self.d_model // self.nhead)\n        key = qkv[1].contiguous().view(key.size(0), -1, self.nhead, self.d_model // self.nhead)\n        value = qkv[2].contiguous().view(value.size(0), -1, self.nhead, self.d_model // self.nhead)\n        scaled_qk = self.attention(query, key, value, need_weights=False, attn_mask=None)[0]\n        softmax_qk = scaled_qk * (self.d_model // self.nhead) ** -0.5\n        dropout_qk = torch.nn.functional.dropout(softmax_qk, p=self.dropout_p)\n        output = dropout_qk.transpose(1, 2).contiguous().view(query.size(0), -1, self.nhead * self.d_model // self.nhead)\n        output = self.w_proj(output)\n        return output, dropout_qk\n \n# Initializing the model\nm = Model()\n\n# Inputs to the model\nquery = torch.randn(1, 1, 2)\nkey = torch.randn(1, 5, 2)\nvalue = torch.randn(1, 5, 2)\ninv_scale_factor = 1. / math.sqrt(2)\n__output__, dout = m(query, key, value, inv_scale_factor)\n```\n\n# References\n1. https://discuss.pytorch.org/t/how-to-get-the-model-summary/455\n2. https://stackoverflow.com/questions/50126264/how-does-one-generate-model-summary-in-pytorch\n3. https://towardsdatascience.com/model-summary-in-pytorch-47d7a8459168\n4. https://github.com/sksq96/pytorch-summary/blob/669a381341bf6f1e13ad16b1c3e5c66429d26222/torchsummary/torchsummary.py\n5. https://github.com/sksq96/pytorch-summary/blob/669a381341bf6f1e13ad16b1c3e5c66429d26222/torchsummary/models/resnet.py\n6. https://github.com/sksq96/pytorch-summary/blob/669a381341bf6f1e13ad16b1c3e5c66429d26222/torchsummary/torchsummary.py#L300\n7. https://github.com/sksq96/pytorch-summary/blob/669a381341bf6f1e13ad16b1c3e5c66429d26222/torchsummary/torchsummary.py#L397\n8. https://github.com/sksq96/pytorch-summary/blob/669a381341bf6f1e13ad16b1c3e5c66429d26222/torchsummary/torchsummary.py#L608\n9. https://github.com/sksq96/pytorch-summary/blob/669a381341bf6f1e13ad16b1c3e5c66429d26222/torchsummary/torchsummary.py#L661\n10. https://github.com/sksq96/pytorch-summary/blob/669a381341bf6f1e13ad16b1c3e5c66429d26222/torchsummary/torchsummary.py#L642\n11. https://github.com/sksq96/pytorch-summary/blob/669a381341bf6f1e13ad16b1c3e5c66429d26222/torchsummary/torchsummary.py#L610-L611\n12. https://github.com/sksq96/pytorch-summary/blob/669a381341bf6f1e13ad16b1c3e5c66429d26222/torchsummary/torchsummary.py#L716\n13. https://github.com/sksq96/pytorch-summary/blob/669a381341bf6f1e13ad16b1c3e5c66429d26222/torchsummary/torchsummary.py#L754\n14. https://github.com/sksq96/pytorch-summary/blob/669a381341bf6f1e13ad16b1c3e5c66429d26222/torchsummary/torchsummary.py#L715\n15. https://github.com/sksq96/pytorch-summary/blob/669a381341bf6f1e13ad16b1c3e5c66429d26222/torchsummary/torchsummary.py#L613-L614\n16. https://github.com/sksq96/pytorch-summary/blob/669a381341bf6f1e13ad16b1c3e5c66429d26222/torchsummary/torchsummary.py#L767-L770\n17. https://github.com/sksq96/pytorch-summary/blob/669a381341bf6f1e13ad16b1c3e5c66429d26222/torchsummary/torchsummary.py#L44\n18. https://github.com/sksq96/pytorch-summary/blob/669a381341bf6f1e13ad16b1c3e5c66429d26222/torchsummary/models/resnet.py#L25\n19. https://github.com/sksq96/pytorch-summary/blob/669a381341bf6f1e13ad16b1c3e5c66429d26222/torchsummary/models/resnet.py#L354",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.qk = torch.nn.Linear(dim, dim)\n        self.value = torch.nn.Linear(dim, dim)\n \n    def forward(self, x1, x2, x3, input_mask):\n        qk = self.qk(x1)\n        value = self.value(x2)\n        inv_scale_factor = torch.rsqrt(torch.sum(qk.mul(qk), dim=-1, keepdim=True))\n        qk = qk.mul(inv_scale_factor).softmax(dim=-1)\n        input_mask = input_mask.to(torch.float)\n        dropout_qk = torch.nn.functional.dropout(qk, p=0.5)\n        qk = qk.mul(input_mask)\n        output = qk.matmul(value)\n        output = output.unsqueeze(0)\n\n        return output\n\n# Initializing the model\nm = Model(8)\n\n# Inputs to the model\nx1 = torch.randn(1, 8, 10, 16)\nx2 = torch.randn(1, 8, 16, 10)\nx3 = torch.randn(1, 10, 16)\ninput_mask = torch.randn(1, 8, 10, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.dim = dim\n        self.proj = torch.nn.Linear(dim, dim)\n \n    def forward(self, query, key, value):\n        q = self.proj(query).view(query.size(0), query.size(1), 1, self.dim)\n        k = self.proj(key).view(key.size(0), 1, -1, self.dim)\n        res = torch.matmul(q, k.transpose(-2, -1))\n        res = res / math.sqrt(self.dim)\n        res = torch.nn.functional.softmax(res, dim=-1)\n        res = torch.nn.functional.dropout(res, p=0.5)\n        res = torch.matmul(res, value)\n        return res\n\n# Initializing the model\nm = Model(512)\n\n# Inputs to the model\nquery = torch.randn(1, 2, 512)\nkey = torch.randn(1, 2, 512)\nvalue = torch.randn(1, 2, 512)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.proj = torch.nn.Linear(inp_dim, inp_dim)\n        self.scale = np.power(inp_dim, -0.5)\n \n    def forward(self, x1, x2):\n        q = 2 * x1 - 1\n        k = self.proj(2 * x2 - 1)\n        v = 2 * x2 - 1\n        scaled_qkv = torch.matmul(q, k.transpose(-2, -1)).mul_(self.scale).softmax(dim=-1)\n        output = scaled_qkv.matmul(v)\n        return output\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, inp_dim)\nx2 = torch.randn(1, inp_dim)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self, attention_hidden_size, num_attention_heads, dropout_p):\n        super().__init__()\n        self.dropout = torch.nn.Dropout(dropout_p)\n        self.softmax = torch.nn.Softmax()\n \n    def forward(self, query, key, value, scale_factor=None, dropout_p=None):\n        qk = torch.matmul(query, key.transpose(-2, -1))\n        scaled_qk = qk/scale_factor\n        softmax_qk = self.softmax(scaled_qk, dim=-1)\n        dropout_qk = self.dropout(softmax_qk)\n        output = dropout_qk.matmul(value)\n        return output\n\n# Initializing the model\nm = Model(attention_hidden_size=512, num_attention_heads=4, dropout_p=0.5)\n\n# Inputs to the model\nquery = torch.randn(15, 4, 512)\nkey = torch.randn(16, 4, 512)\nvalue = torch.randn(16, 4, 512)\nscale_factor = torch.scalar_tensor(1/math.sqrt(512))\n",
                "\nm = Model()\n\n# Inputs to the model\nquery = torch.randn(b, t, n)\nkey = torch.randn(b, t, n)\nvalue = torch.randn(b, t, n)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, dim_query, dim_key, dim_value, dropout_p, inv_scale_factor):\n        super().__init__()\n        \n    def forward(self, query, key, value):\n        qk = torch.matmul(query, key.transpose(-2, -1))\n        scaled_qk = qk.div(inv_scale_factor)\n        softmax_qk = scaled_qk.softmax(dim=-1)\n        dropout_qk = torch.nn.functional.dropout(softmax_qk, p=dropout_p)\n        output = dropout_qk.matmul(value)\n        return output\n\n# Initializing the model\nm = Model(dim_query=8192, dim_key=8192, dim_value=8192, dropout_p=0.1, inv_scale_factor=48.2)\n\n# Inputs to the model\nquery = torch.randn(1, 8192, 32)\nkey = torch.randn(1, 8192, 32)\nvalue = torch.randn(1, 8192, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, dropout_p, inv_scale_factor):\n        super().__init__()\n        self.dropout_p = dropout_p\n        self.inv_scale_factor = inv_scale_factor\n \n    def forward(self, query, key, value):\n        qk = query.matmul(key.transpose(-2, -1))\n        scaled_qk = qk.div(self.inv_scale_factor)\n        softmax_qk = scaled_qk.softmax(dim=-1)\n        dropout_qk = torch.nn.functional.dropout(softmax_qk, p=self.dropout_p)\n        output = dropout_qk.matmul(value)\n        return output\n\n# Initializing the model\nm = Model(0.1, 0.5)\n\n# Input to the model\nquery = torch.randn(16, 32, 512)\nkey = torch.randn(16, 32, 256)\nvalue = torch.randn(16, 32, 256)\n\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.dropout_11 = torch.nn.Dropout(0.1)\n        self.softmax_6 = torch.nn.Softmax(dim=-1)\n        self.dropout_26 = torch.nn.Dropout(0.1)\n        self.softmax_7 = torch.nn.Softmax(dim=-1)\n        self.matmul_8 = torch.matmul()\n        self.dropout_9 = torch.nn.Dropout(0.1)\n        self.matmul_10 = torch.matmul()\n \n    def forward(self, v4, v5, v6):\n        v1 = self.dropout_11(v4)\n        v2 = self.softmax_6(v1)\n        v3 = v2 / 0.05241306459982384\n        v11 = self.dropout_26(v5)\n        v12 = self.softmax_7(v11)\n        v13 = v12 / 0.00521668520555902\n        v7 = self.matmul_8(v3, v6)\n        v8 = self.dropout_9(v7)\n        v9 = self.matmul_10(v13, v8)\n        return v9\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nv4 = torch.randn(1, 1024, 64)\nv5 = torch.randn(1, 5, 512)\nv6 = torch.randn(5, 512, 512)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n \n    def forward(self, x1, x2, x3):\n        v1 = torch.matmul(x1, x2.transpose(-2, -1))\n        v2 = v1.div(0.5)\n        v3 = torch.nn.functional.softmax(v2, dim=-1)\n        v4 = torch.nn.functional.dropout(v3, p=0.25)\n        v5 = torch.matmul(v4, x3)\n        return v5\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 10, 32)\nx2 = torch.randn(1, 32, 16)\nx3 = torch.randn(1, 16, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.d_model = 2\n        self.nhead = 2\n        self.dropout_p = 0.0\n        self.w_qkv = self.w_proj = torch.nn.Linear(self.d_model, self.d_model * 3)\n        self.attention = torch.nn.MultiheadAttention(self.d_model, self.nhead, torch.nn.Dropout(self.dropout_p))\n \n    def forward(self, query, key, value, inv_scale_factor):\n        qkv = self.w_qkv(query).chunk(3, dim=-1)\n        query = qkv[0].contiguous().view(query.size(0), -1, self.nhead, self.d_model // self.nhead)\n        key = qkv[1].contiguous().view(key.size(0), -1, self.nhead, self.d_model // self.nhead)\n        value = qkv[2].contiguous().view(value.size(0), -1, self.nhead, self.d_model // self.nhead)\n        scaled_qk = self.attention(query, key, value, need_weights=False, attn_mask=None)[0]\n        softmax_qk = scaled_qk * (self.d_model // self.nhead) ** -0.5\n        dropout_qk = torch.nn.functional.dropout(softmax_qk, p=self.dropout_p)\n        output = dropout_qk.transpose(1, 2).contiguous().view(query.size(0), -1, self.nhead * self.d_model // self.nhead)\n        output = self.w_proj(output)\n        return output, dropout_qk\n \n# Initializing the model\nm = Model()\n\n# Inputs to the model\nquery = torch.randn(1, 1, 2)\nkey = torch.randn(1, 5, 2)\nvalue = torch.randn(1, 5, 2)\ninv_scale_factor = 1. / math.sqrt(2)\n__output__, dout = m(query, key, value, inv_scale_factor)\n```\n\n# References\n1. https://discuss.pytorch.org/t/how-to-get-the-model-summary/455\n2. https://stackoverflow.com/questions/50126264/how-does-one-generate-model-summary-in-pytorch\n3. https://towardsdatascience.com/model-summary-in-pytorch-47d7a8459168\n4. https://github.com/sksq96/pytorch-summary/blob/669a381341bf6f1e13ad16b1c3e5c66429d26222/torchsummary/torchsummary.py\n5. https://github.com/sksq96/pytorch-summary/blob/669a381341bf6f1e13ad16b1c3e5c66429d26222/torchsummary/models/resnet.py\n6. https://github.com/sksq96/pytorch-summary/blob/669a381341bf6f1e13ad16b1c3e5c66429d26222/torchsummary/torchsummary.py#L300\n7. https://github.com/sksq96/pytorch-summary/blob/669a381341bf6f1e13ad16b1c3e5c66429d26222/torchsummary/torchsummary.py#L397\n8. https://github.com/sksq96/pytorch-summary/blob/669a381341bf6f1e13ad16b1c3e5c66429d26222/torchsummary/torchsummary.py#L608\n9. https://github.com/sksq96/pytorch-summary/blob/669a381341bf6f1e13ad16b1c3e5c66429d26222/torchsummary/torchsummary.py#L661\n10. https://github.com/sksq96/pytorch-summary/blob/669a381341bf6f1e13ad16b1c3e5c66429d26222/torchsummary/torchsummary.py#L642\n11. https://github.com/sksq96/pytorch-summary/blob/669a381341bf6f1e13ad16b1c3e5c66429d26222/torchsummary/torchsummary.py#L610-L611\n12. https://github.com/sksq96/pytorch-summary/blob/669a381341bf6f1e13ad16b1c3e5c66429d26222/torchsummary/torchsummary.py#L716\n13. https://github.com/sksq96/pytorch-summary/blob/669a381341bf6f1e13ad16b1c3e5c66429d26222/torchsummary/torchsummary.py#L754\n14. https://github.com/sksq96/pytorch-summary/blob/669a381341bf6f1e13ad16b1c3e5c66429d26222/torchsummary/torchsummary.py#L715\n15. https://github.com/sksq96/pytorch-summary/blob/669a381341bf6f1e13ad16b1c3e5c66429d26222/torchsummary/torchsummary.py#L613-L614\n16. https://github.com/sksq96/pytorch-summary/blob/669a381341bf6f1e13ad16b1c3e5c66429d26222/torchsummary/torchsummary.py#L767-L770\n17. https://github.com/sksq96/pytorch-summary/blob/669a381341bf6f1e13ad16b1c3e5c66429d26222/torchsummary/torchsummary.py#L44\n18. https://github.com/sksq96/pytorch-summary/blob/669a381341bf6f1e13ad16b1c3e5c66429d26222/torchsummary/models/resnet.py#L25\n19. https://github.com/sksq96/pytorch-summary/blob/669a381341bf6f1e13ad16b1c3e5c66429d26222/torchsummary/models/resnet.py#L354",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.qk = torch.nn.Linear(dim, dim)\n        self.value = torch.nn.Linear(dim, dim)\n \n    def forward(self, x1, x2, x3, input_mask):\n        qk = self.qk(x1)\n        value = self.value(x2)\n        inv_scale_factor = torch.rsqrt(torch.sum(qk.mul(qk), dim=-1, keepdim=True))\n        qk = qk.mul(inv_scale_factor).softmax(dim=-1)\n        input_mask = input_mask.to(torch.float)\n        dropout_qk = torch.nn.functional.dropout(qk, p=0.5)\n        qk = qk.mul(input_mask)\n        output = qk.matmul(value)\n        output = output.unsqueeze(0)\n\n        return output\n\n# Initializing the model\nm = Model(8)\n\n# Inputs to the model\nx1 = torch.randn(1, 8, 10, 16)\nx2 = torch.randn(1, 8, 16, 10)\nx3 = torch.randn(1, 10, 16)\ninput_mask = torch.randn(1, 8, 10, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.dim = dim\n        self.proj = torch.nn.Linear(dim, dim)\n \n    def forward(self, query, key, value):\n        q = self.proj(query).view(query.size(0), query.size(1), 1, self.dim)\n        k = self.proj(key).view(key.size(0), 1, -1, self.dim)\n        res = torch.matmul(q, k.transpose(-2, -1))\n        res = res / math.sqrt(self.dim)\n        res = torch.nn.functional.softmax(res, dim=-1)\n        res = torch.nn.functional.dropout(res, p=0.5)\n        res = torch.matmul(res, value)\n        return res\n\n# Initializing the model\nm = Model(512)\n\n# Inputs to the model\nquery = torch.randn(1, 2, 512)\nkey = torch.randn(1, 2, 512)\nvalue = torch.randn(1, 2, 512)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.proj = torch.nn.Linear(inp_dim, inp_dim)\n        self.scale = np.power(inp_dim, -0.5)\n \n    def forward(self, x1, x2):\n        q = 2 * x1 - 1\n        k = self.proj(2 * x2 - 1)\n        v = 2 * x2 - 1\n        scaled_qkv = torch.matmul(q, k.transpose(-2, -1)).mul_(self.scale).softmax(dim=-1)\n        output = scaled_qkv.matmul(v)\n        return output\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, inp_dim)\nx2 = torch.randn(1, inp_dim)\n"
            ],
            "g_time": 57.82816219329834
        }
    }
}
{
    "opt": "unary=17",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 20, 3, padding=0, stride=1)\n        self.conv2 = torch.nn.ConvTranspose2d(20, 10, 3, padding=1, stride=1)\n        self.conv3 = torch.nn.ConvTranspose2d(10, 5, 3, padding=0, stride=1)\n        self.conv4 = torch.nn.ConvTranspose2d(5, 1, 3, padding=1, stride=1)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = torch.relu(v1)\n        v3 = self.conv2(v2)\n        v4 = torch.relu(v3)\n        v5 = self.conv3(v4)\n        v6 = torch.sigmoid(v5)\n        v7 = self.conv4(v6)\n        v8 = torch.tanh(v7)\n        return v8\n# Inputs to the model\nx1 = torch.randn(1, 3, 128, 128)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(1, 4, 3, stride=2)\n        self.conv2 = torch.nn.ConvTranspose2d(4, 2, 3, stride=2)\n        self.flatten = torch.flatten\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = torch.relu(v1)\n        v3 = self.conv2(v2)\n        v4 = torch.relu(v3)\n        v5 = self.flatten(v4)\n        v6 = torch.tanh(v5)\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 1, 16, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.ConvTranspose2d(3, 20, 3, padding=1, stride=2, output_padding=0)\n        self.conv2 = torch.nn.ConvTranspose2d(20, 10, 3, padding=1, stride=2, output_padding=1)\n        self.conv3 = torch.nn.ConvTranspose2d(10, 5, 3, padding=1, stride=2, output_padding=0)\n        self.conv4 = torch.nn.ConvTranspose2d(5, 1, 3, padding=1, stride=2, output_padding=1)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = torch.relu(v1)\n        v3 = self.conv2(v2)\n        v4 = torch.relu(v3)\n        v5 = self.conv3(v4)\n        v6 = torch.relu(v5)\n        v7 = self.conv4(v6)\n        return v7\n# Inputs to the model\nx1 = torch.randn(1, 3, 128, 128)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.ConvTranspose2d(3, 16, 2, stride=2, padding=3)\n        self.conv2 = torch.nn.ConvTranspose2d(16, 8, 3, stride=3, padding=4)\n        self.conv3 = torch.nn.ConvTranspose2d(8, 4, 4, stride=4, padding=0)\n        self.conv4 = torch.nn.ConvTranspose2d(4, 2, 4, stride=4, padding=1)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = torch.relu(v1)\n        v3 = torch.sigmoid(v2)\n        v4 = self.conv2(v3)\n        v5 = torch.relu(v4)\n        v6 = self.conv3(v5)\n        v7 = torch.sigmoid(v6)\n        v8 = self.conv4(v7)\n        v9 = torch.tanh(v8)\n        return v9\n# Inputs to the model\nx1 = torch.randn(1, 3, 128, 128)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(1, 8, 7, 1)\n        self.conv2 = torch.nn.ConvTranspose2d(8, 32, 6, 1)\n        self.conv3 = torch.nn.ConvTranspose2d(32, 64, 3, 1)\n        self.conv4 = torch.nn.ConvTranspose2d(64, 128, 6, 1)\n        self.conv5 = torch.nn.ConvTranspose2d(128, 256, 7, 1)\n        self.conv6 = torch.nn.ConvTranspose2d(256, 2, 3, 1)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = torch.relu(v1)\n        v3 = torch.relu(v2)\n        v4 = torch.relu(v3)\n        v5 = self.conv2(v4)\n        v6 = torch.relu(v5)\n        v7 = torch.relu(v6)\n        v8 = self.conv3(v7)\n        v9 = torch.relu(v8)\n        v10 = torch.relu(v9)\n        v11 = self.conv4(v10)\n        v12 = torch.relu(v11)\n        v13 = torch.relu(v12)\n        v14 = self.conv5(v13)\n        v15 = torch.relu(v14)\n        v16 = torch.relu(v15)\n        v17 = self.conv6(v16)\n        return v17\n# Inputs to the model\nx1 = torch.randn(1, 1, 32, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_1 = torch.nn.Conv2d(2, 1, 2)\n        self.conv_transpose_1 = torch.nn.ConvTranspose2d(1, 1, 2)\n        self.conv_2 = torch.nn.Conv2d(1, 1, 2)\n    def forward(self, x1):\n        v1 = self.conv_1(x1)\n        v2 = self.conv_transpose_1(v1)\n        v3 = torch.relu(v2)\n        v4 = self.conv_2(v3)\n        return v4\n# Inputs to the model\nx1 = torch.randn(2, 2, 3, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(1, 3, kernel_size=3, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        return v1\n# Inputs to the model\nx1 = torch.randn(1, 1, 12, 12)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(1, 6, 3, padding=1, stride=1)\n        self.conv2 = torch.nn.ConvTranspose2d(6, 3, 2, padding=1, stride=1)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = torch.relu(v1)\n        v3 = self.conv2(v2)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 1, 6, 6)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 3, 5, stride=2, padding=2)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = torch.tanh(v2)\n        return v3   \n# Inputs to the model\nx1 = torch.randn(1, 3, 128, 128)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_1 = torch.nn.Conv2d(10, 200, 3, padding=0, stride=1)\n        self.conv_2 = torch.nn.ConvTranspose2d(200, 100, 3, padding=0, stride=1)\n        self.conv_3 = torch.nn.Conv2d(100, 256, 3, padding=2, stride=2)\n        self.conv_4 = torch.nn.ConvTranspose2d(256, 100, 3, padding=1, stride=1)\n    def forward(self, x1):\n        v1 = self.conv_1(x1)\n        v2 = torch.relu(v1)\n        v3 = self.conv_2(v2)\n        v4 = self.conv_3(v3)\n        v5 = torch.relu(v4)\n        v6 = self.conv_4(v5)\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 10, 64, 64)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 20, 3, padding=0, stride=1)\n        self.conv2 = torch.nn.ConvTranspose2d(20, 10, 3, padding=1, stride=1)\n        self.conv3 = torch.nn.ConvTranspose2d(10, 5, 3, padding=0, stride=1)\n        self.conv4 = torch.nn.ConvTranspose2d(5, 1, 3, padding=1, stride=1)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = torch.relu(v1)\n        v3 = self.conv2(v2)\n        v4 = torch.relu(v3)\n        v5 = self.conv3(v4)\n        v6 = torch.sigmoid(v5)\n        v7 = self.conv4(v6)\n        v8 = torch.tanh(v7)\n        return v8\n# Inputs to the model\nx1 = torch.randn(1, 3, 128, 128)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(1, 4, 3, stride=2)\n        self.conv2 = torch.nn.ConvTranspose2d(4, 2, 3, stride=2)\n        self.flatten = torch.flatten\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = torch.relu(v1)\n        v3 = self.conv2(v2)\n        v4 = torch.relu(v3)\n        v5 = self.flatten(v4)\n        v6 = torch.tanh(v5)\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 1, 16, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.ConvTranspose2d(3, 20, 3, padding=1, stride=2, output_padding=0)\n        self.conv2 = torch.nn.ConvTranspose2d(20, 10, 3, padding=1, stride=2, output_padding=1)\n        self.conv3 = torch.nn.ConvTranspose2d(10, 5, 3, padding=1, stride=2, output_padding=0)\n        self.conv4 = torch.nn.ConvTranspose2d(5, 1, 3, padding=1, stride=2, output_padding=1)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = torch.relu(v1)\n        v3 = self.conv2(v2)\n        v4 = torch.relu(v3)\n        v5 = self.conv3(v4)\n        v6 = torch.relu(v5)\n        v7 = self.conv4(v6)\n        return v7\n# Inputs to the model\nx1 = torch.randn(1, 3, 128, 128)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.ConvTranspose2d(3, 16, 2, stride=2, padding=3)\n        self.conv2 = torch.nn.ConvTranspose2d(16, 8, 3, stride=3, padding=4)\n        self.conv3 = torch.nn.ConvTranspose2d(8, 4, 4, stride=4, padding=0)\n        self.conv4 = torch.nn.ConvTranspose2d(4, 2, 4, stride=4, padding=1)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = torch.relu(v1)\n        v3 = torch.sigmoid(v2)\n        v4 = self.conv2(v3)\n        v5 = torch.relu(v4)\n        v6 = self.conv3(v5)\n        v7 = torch.sigmoid(v6)\n        v8 = self.conv4(v7)\n        v9 = torch.tanh(v8)\n        return v9\n# Inputs to the model\nx1 = torch.randn(1, 3, 128, 128)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(1, 8, 7, 1)\n        self.conv2 = torch.nn.ConvTranspose2d(8, 32, 6, 1)\n        self.conv3 = torch.nn.ConvTranspose2d(32, 64, 3, 1)\n        self.conv4 = torch.nn.ConvTranspose2d(64, 128, 6, 1)\n        self.conv5 = torch.nn.ConvTranspose2d(128, 256, 7, 1)\n        self.conv6 = torch.nn.ConvTranspose2d(256, 2, 3, 1)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = torch.relu(v1)\n        v3 = torch.relu(v2)\n        v4 = torch.relu(v3)\n        v5 = self.conv2(v4)\n        v6 = torch.relu(v5)\n        v7 = torch.relu(v6)\n        v8 = self.conv3(v7)\n        v9 = torch.relu(v8)\n        v10 = torch.relu(v9)\n        v11 = self.conv4(v10)\n        v12 = torch.relu(v11)\n        v13 = torch.relu(v12)\n        v14 = self.conv5(v13)\n        v15 = torch.relu(v14)\n        v16 = torch.relu(v15)\n        v17 = self.conv6(v16)\n        return v17\n# Inputs to the model\nx1 = torch.randn(1, 1, 32, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_1 = torch.nn.Conv2d(2, 1, 2)\n        self.conv_transpose_1 = torch.nn.ConvTranspose2d(1, 1, 2)\n        self.conv_2 = torch.nn.Conv2d(1, 1, 2)\n    def forward(self, x1):\n        v1 = self.conv_1(x1)\n        v2 = self.conv_transpose_1(v1)\n        v3 = torch.relu(v2)\n        v4 = self.conv_2(v3)\n        return v4\n# Inputs to the model\nx1 = torch.randn(2, 2, 3, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(1, 3, kernel_size=3, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        return v1\n# Inputs to the model\nx1 = torch.randn(1, 1, 12, 12)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(1, 6, 3, padding=1, stride=1)\n        self.conv2 = torch.nn.ConvTranspose2d(6, 3, 2, padding=1, stride=1)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = torch.relu(v1)\n        v3 = self.conv2(v2)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 1, 6, 6)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 3, 5, stride=2, padding=2)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = torch.tanh(v2)\n        return v3   \n# Inputs to the model\nx1 = torch.randn(1, 3, 128, 128)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_1 = torch.nn.Conv2d(10, 200, 3, padding=0, stride=1)\n        self.conv_2 = torch.nn.ConvTranspose2d(200, 100, 3, padding=0, stride=1)\n        self.conv_3 = torch.nn.Conv2d(100, 256, 3, padding=2, stride=2)\n        self.conv_4 = torch.nn.ConvTranspose2d(256, 100, 3, padding=1, stride=1)\n    def forward(self, x1):\n        v1 = self.conv_1(x1)\n        v2 = torch.relu(v1)\n        v3 = self.conv_2(v2)\n        v4 = self.conv_3(v3)\n        v5 = torch.relu(v4)\n        v6 = self.conv_4(v5)\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 10, 64, 64)\n"
            ],
            "g_time": 14.146347045898438
        }
    }
}
{
    "opt": "unary=27",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1):\n        v1 = x1.sum(dim=1, keepdim=True)\n        v2 = torch.clamp_min(v1, 0.1)\n        v3 = torch.clamp_max(v2, 0.2)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 12, 17, 4)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min, max):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(1, 1, 3, stride=1)\n        self.min = min\n        self.max = max\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = torch.clamp_min(v1, self.min)\n        v3 = torch.clamp_max(v2, self.max)\n        return v3\nmin = -0.1\nmax = 0.5\n# Inputs to the model\nx1 = torch.randn(1, 1, 7, 7)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min, max):\n        super().__init__()\n        self.relu = torch.nn.ReLU()\n        self.conv = torch.nn.Conv2d(6, 12, 9, stride=1)\n        self.min = min\n        self.max = max\n    def forward(self, x1):\n        v1 = self.relu(x1)\n        v2 = self.conv(v1)\n        v3 = torch.clamp_min(v2, self.min)\n        v4 = torch.clamp_max(v3, self.max)\n        return v4\nmin = 0.5\nmax = 1.5\n# Inputs to the model\nx1 = torch.randn(1, 6, 100, 120)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min, max):\n        super().__init__()\n        self.relu = torch.nn.ReLU()\n        self.conv = torch.nn.Conv2d(32, 96, 3, stride=2, padding=1)\n        self.min = min\n        self.max = max\n    def forward(self, x1):\n        v1 = self.relu(x1)\n        v2 = self.conv(v1)\n        v3 = torch.clamp_min(v2, self.min)\n        v4 = torch.clamp_max(v3, self.max)\n        return v4\nmin = 0.1\nmax = 1.8\n# Inputs to the model\nx1 = torch.randn(1, 32, 224, 224)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min_value):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(2, 10, 4)\n        self.relu = torch.nn.ReLU()\n        self.maxpool = torch.nn.MaxPool2d(3, stride=1, padding=1)\n        self.flatten = torch.nn.Flatten()\n        self.linear = torch.nn.Linear(1440, 500)\n        self.tanh = torch.nn.Tanh()\n        self.min_value = min_value\n    def forward(self, x1, x2):\n        v1 = self.conv(x1)\n        v2 = torch.clamp_min(v1, self.min_value)\n        v3 = self.relu(v2)\n        v4 = self.maxpool(v3)\n        v5 = self.conv(x2)\n        v6 = torch.clamp_min(v5, self.min_value)\n        v7 = self.relu(v6)\n        v8 = self.maxpool(v7)\n        v9 = self.flatten(v4)\n        v10 = self.flatten(v8)\n        v11 = torch.add(v9, v10)\n        v12 = self.linear(v11)\n        v13 = self.tanh(v12)\n        return v13\nmin_value = -0.1\n# Inputs to the model\nx1 = torch.randn(1, 2, 398, 398)\nx2 = torch.randn(1, 2, 398, 398)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min, max):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(2, 16, 5, padding=1)\n        self.conv2 = torch.nn.Conv2d(16, 2, 3, padding=2)\n        self.conv3 = torch.nn.Conv2d(2, 8, 5, padding=1)\n        self.conv4 = torch.nn.Conv2d(8, 16, 3, padding=2)\n        self.conv5 = torch.nn.Conv2d(16, 2, 3, padding=1)\n        self.conv6 = torch.nn.Conv2d(16, 2, 1, padding=1)\n        self.min = min\n        self.max = max\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = self.conv2(v1)\n        v3 = self.conv3(v2)\n        v4 = self.conv4(v3)\n        v5 = self.conv5(v4)\n        v6 = self.conv6(v5)\n        v7 = torch.clamp_min(v6, self.min)\n        v8 = torch.clamp_max(v7, self.max)\n        return v8\nmin = 0.5\nmax = 1.0\n# Inputs to the model\nx1 = torch.randn(1, 2, 320, 103)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min, max):\n        super().__init__()\n        \n        self.conv1 = torch.nn.Conv2d(3, 20, 4, stride=2, padding=1)\n        self.conv2 = torch.nn.Conv2d(20, 20, 4, stride=2, padding=1)\n        \n        self.min = min\n        self.max = max\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = self.conv2(v1)\n        v3 = torch.clamp_min(v2, self.min)\n        v4 = torch.clamp_max(v3, self.max)\n        return v4\nmin = -0.5\nmax = 1.3373875217437744\n# Inputs to the model\nx1 = torch.randn(1, 3, 500, 800)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min, max):\n        super().__init__()\n        self.conv = torch.nn.Conv1d(13, 10, 11, stride=3, dilation=2)\n        self.min = min\n        self.max = max\n    def forward(self, x1):\n        v1 = torch.nn.functional.avg_pool2d(x1, 3, stride=2, padding=2)\n        v2 = self.conv(v1)\n        v3 = torch.clamp_min(v2, self.min)\n        v4 = torch.clamp_max(v3, self.max)\n        return v4\nmin = 0.6\nmax = 1.6\n# Inputs to the model\nx1 = torch.randn(1, 13, 100)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min, max):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(20, 8, 5, groups=3, padding=1)\n        self.conv1 = torch.nn.Conv2d(1, 8, 1, groups=2)\n        self.avg = torch.nn.AvgPool2d(3, stride=2, padding=0)\n        self.max = torch.nn.MaxPool2d((2, 2), stride=(1, 2))\n        self.min = min\n        self.max = max\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = torch.clamp_max(v1, self.max)\n        v3 = torch.clamp_max(v2, min=self.max)\n        v4 = self.conv1(v3)\n        v5 = torch.clamp_min(v4, self.max)\n        v6 = torch.clamp_min(v5, min=self.min)\n        v7 = self.avg(v6)\n        v8 = torch.clamp_min(v7, max=self.min)\n        v9 = torch.clamp_min(v8, min=self.min)\n        v10 = torch.clamp_max(v9, max=self.max)\n        v11 = self.max(v10)\n        v12 = torch.clamp_min(v11, self.max)\n        v13 = torch.clamp_max(v12, self.min)\n        return v13\nmin = -0.1\nmax = 0.05\n# Inputs to the model\nx1 = torch.randn(1, 20, 98, 97)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min, max, num):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(num, num, 3, stride=1, padding=1)\n        self.pad = torch.nn.ReflectionPad2d((3, 3, 3, 3))\n        self.deconv1 = torch.nn.ConvTranspose2d(num, num, 3, stride=1, padding=1)\n        self.conv2 = torch.nn.Conv2d(num, num, 5, stride=1, padding=2)\n        self.min = min\n        self.max = max\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = self.pad(v1)\n        v3 = self.deconv1(v2)\n        v4 = self.conv2(v3)\n        v5 = torch.clamp_min(v4, self.min)\n        v6 = torch.clamp_max(v5, self.max)\n        return v6\nnum = -9\nmin = 0.7\nmax = -0.6\n# Inputs to the model\nx1 = torch.randn(1, num, 50, 50)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1):\n        v1 = x1.sum(dim=1, keepdim=True)\n        v2 = torch.clamp_min(v1, 0.1)\n        v3 = torch.clamp_max(v2, 0.2)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 12, 17, 4)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min, max):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(1, 1, 3, stride=1)\n        self.min = min\n        self.max = max\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = torch.clamp_min(v1, self.min)\n        v3 = torch.clamp_max(v2, self.max)\n        return v3\nmin = -0.1\nmax = 0.5\n# Inputs to the model\nx1 = torch.randn(1, 1, 7, 7)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min, max):\n        super().__init__()\n        self.relu = torch.nn.ReLU()\n        self.conv = torch.nn.Conv2d(6, 12, 9, stride=1)\n        self.min = min\n        self.max = max\n    def forward(self, x1):\n        v1 = self.relu(x1)\n        v2 = self.conv(v1)\n        v3 = torch.clamp_min(v2, self.min)\n        v4 = torch.clamp_max(v3, self.max)\n        return v4\nmin = 0.5\nmax = 1.5\n# Inputs to the model\nx1 = torch.randn(1, 6, 100, 120)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min, max):\n        super().__init__()\n        self.relu = torch.nn.ReLU()\n        self.conv = torch.nn.Conv2d(32, 96, 3, stride=2, padding=1)\n        self.min = min\n        self.max = max\n    def forward(self, x1):\n        v1 = self.relu(x1)\n        v2 = self.conv(v1)\n        v3 = torch.clamp_min(v2, self.min)\n        v4 = torch.clamp_max(v3, self.max)\n        return v4\nmin = 0.1\nmax = 1.8\n# Inputs to the model\nx1 = torch.randn(1, 32, 224, 224)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min_value):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(2, 10, 4)\n        self.relu = torch.nn.ReLU()\n        self.maxpool = torch.nn.MaxPool2d(3, stride=1, padding=1)\n        self.flatten = torch.nn.Flatten()\n        self.linear = torch.nn.Linear(1440, 500)\n        self.tanh = torch.nn.Tanh()\n        self.min_value = min_value\n    def forward(self, x1, x2):\n        v1 = self.conv(x1)\n        v2 = torch.clamp_min(v1, self.min_value)\n        v3 = self.relu(v2)\n        v4 = self.maxpool(v3)\n        v5 = self.conv(x2)\n        v6 = torch.clamp_min(v5, self.min_value)\n        v7 = self.relu(v6)\n        v8 = self.maxpool(v7)\n        v9 = self.flatten(v4)\n        v10 = self.flatten(v8)\n        v11 = torch.add(v9, v10)\n        v12 = self.linear(v11)\n        v13 = self.tanh(v12)\n        return v13\nmin_value = -0.1\n# Inputs to the model\nx1 = torch.randn(1, 2, 398, 398)\nx2 = torch.randn(1, 2, 398, 398)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min, max):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(2, 16, 5, padding=1)\n        self.conv2 = torch.nn.Conv2d(16, 2, 3, padding=2)\n        self.conv3 = torch.nn.Conv2d(2, 8, 5, padding=1)\n        self.conv4 = torch.nn.Conv2d(8, 16, 3, padding=2)\n        self.conv5 = torch.nn.Conv2d(16, 2, 3, padding=1)\n        self.conv6 = torch.nn.Conv2d(16, 2, 1, padding=1)\n        self.min = min\n        self.max = max\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = self.conv2(v1)\n        v3 = self.conv3(v2)\n        v4 = self.conv4(v3)\n        v5 = self.conv5(v4)\n        v6 = self.conv6(v5)\n        v7 = torch.clamp_min(v6, self.min)\n        v8 = torch.clamp_max(v7, self.max)\n        return v8\nmin = 0.5\nmax = 1.0\n# Inputs to the model\nx1 = torch.randn(1, 2, 320, 103)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min, max):\n        super().__init__()\n        \n        self.conv1 = torch.nn.Conv2d(3, 20, 4, stride=2, padding=1)\n        self.conv2 = torch.nn.Conv2d(20, 20, 4, stride=2, padding=1)\n        \n        self.min = min\n        self.max = max\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = self.conv2(v1)\n        v3 = torch.clamp_min(v2, self.min)\n        v4 = torch.clamp_max(v3, self.max)\n        return v4\nmin = -0.5\nmax = 1.3373875217437744\n# Inputs to the model\nx1 = torch.randn(1, 3, 500, 800)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min, max):\n        super().__init__()\n        self.conv = torch.nn.Conv1d(13, 10, 11, stride=3, dilation=2)\n        self.min = min\n        self.max = max\n    def forward(self, x1):\n        v1 = torch.nn.functional.avg_pool2d(x1, 3, stride=2, padding=2)\n        v2 = self.conv(v1)\n        v3 = torch.clamp_min(v2, self.min)\n        v4 = torch.clamp_max(v3, self.max)\n        return v4\nmin = 0.6\nmax = 1.6\n# Inputs to the model\nx1 = torch.randn(1, 13, 100)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min, max):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(20, 8, 5, groups=3, padding=1)\n        self.conv1 = torch.nn.Conv2d(1, 8, 1, groups=2)\n        self.avg = torch.nn.AvgPool2d(3, stride=2, padding=0)\n        self.max = torch.nn.MaxPool2d((2, 2), stride=(1, 2))\n        self.min = min\n        self.max = max\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = torch.clamp_max(v1, self.max)\n        v3 = torch.clamp_max(v2, min=self.max)\n        v4 = self.conv1(v3)\n        v5 = torch.clamp_min(v4, self.max)\n        v6 = torch.clamp_min(v5, min=self.min)\n        v7 = self.avg(v6)\n        v8 = torch.clamp_min(v7, max=self.min)\n        v9 = torch.clamp_min(v8, min=self.min)\n        v10 = torch.clamp_max(v9, max=self.max)\n        v11 = self.max(v10)\n        v12 = torch.clamp_min(v11, self.max)\n        v13 = torch.clamp_max(v12, self.min)\n        return v13\nmin = -0.1\nmax = 0.05\n# Inputs to the model\nx1 = torch.randn(1, 20, 98, 97)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min, max, num):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(num, num, 3, stride=1, padding=1)\n        self.pad = torch.nn.ReflectionPad2d((3, 3, 3, 3))\n        self.deconv1 = torch.nn.ConvTranspose2d(num, num, 3, stride=1, padding=1)\n        self.conv2 = torch.nn.Conv2d(num, num, 5, stride=1, padding=2)\n        self.min = min\n        self.max = max\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = self.pad(v1)\n        v3 = self.deconv1(v2)\n        v4 = self.conv2(v3)\n        v5 = torch.clamp_min(v4, self.min)\n        v6 = torch.clamp_max(v5, self.max)\n        return v6\nnum = -9\nmin = 0.7\nmax = -0.6\n# Inputs to the model\nx1 = torch.randn(1, num, 50, 50)\n"
            ],
            "g_time": 12.921264886856079
        }
    }
}
{
    "opt": "unary=11",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(3, 5, 1, stride=[1, 2], dilation=1)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v4 / 6\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 3, 100, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(2, 92, 4, stride=2, output_padding=1)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v4 / 6\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 2, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(2, 3, 1, stride=1, dilation=2, groups=2)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v4 / 6\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 2, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(3, 4, (5, 192), stride=1, groups=3, padding=1)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v4 / 6\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 3, 100, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(21, 3, 5, stride=12, padding=11, groups=14)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v4 / 6\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 21, 1, 1)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(3, 3, 1, stride=(1, 2), padding=[1, 2])\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v4 / 6\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 3, 100, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(2, 4, 1, stride=1, padding=[[0, 1], [2, 0]])\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v4 / 6\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 2, 400, 100)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(3, 4, 1, stride=1, padding='same', output_padding=[0, 2])\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v4 / 6\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 3, 100, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(64, 3, 3, stride=1, dilation=[1, 2])\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v4 / 6\n        return v5\n# Inputs to the model\nx1 = torch.randn(16, 64, 28, 28)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose1d(3, 4, 3, stride=2, padding=1, output_padding=1)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v4 / 6\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 3, 20)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(3, 5, 1, stride=[1, 2], dilation=1)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v4 / 6\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 3, 100, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(2, 92, 4, stride=2, output_padding=1)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v4 / 6\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 2, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(2, 3, 1, stride=1, dilation=2, groups=2)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v4 / 6\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 2, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(3, 4, (5, 192), stride=1, groups=3, padding=1)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v4 / 6\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 3, 100, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(21, 3, 5, stride=12, padding=11, groups=14)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v4 / 6\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 21, 1, 1)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(3, 3, 1, stride=(1, 2), padding=[1, 2])\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v4 / 6\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 3, 100, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(2, 4, 1, stride=1, padding=[[0, 1], [2, 0]])\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v4 / 6\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 2, 400, 100)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(3, 4, 1, stride=1, padding='same', output_padding=[0, 2])\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v4 / 6\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 3, 100, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(64, 3, 3, stride=1, dilation=[1, 2])\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v4 / 6\n        return v5\n# Inputs to the model\nx1 = torch.randn(16, 64, 28, 28)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose1d(3, 4, 3, stride=2, padding=1, output_padding=1)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v4 / 6\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 3, 20)\n"
            ],
            "g_time": 6.536205530166626
        }
    }
}
{
    "opt": "unary=6",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 3, (2,2), (2,2), (5,5))\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp(v2, 0, 6)\n        v4 = v1 * v3\n        v5 = v4 / 6\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 3, 13, 13)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 3, 2, stride=3, padding=4)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp(v2, min=0, max=6)\n        v4 = v1 * v3\n        v5 = v4 / 6\n        return v5\n# Inputs to the model\nx1 = torch.randn(2, 3, 113, 113)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 3, 1, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp(v2, min=0, max=6)\n        v4 = v3 + 3\n        v5 = torch.clamp(v4, min=0, max=6)\n        v6 = v3 * v5\n        v7 = v6 / 6\n        return v7\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 8, 3, stride=2, padding=1)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp(v3, min=0, max=6)\n        v5 = v1 * v4\n        v6 = v5 / 6\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 3, 10, 10)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.relu6 = torch.nn.ReLU6()\n        self.conv = torch.nn.Conv2d(3, 16, 2, stride=2, padding=5)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = self.relu6(v1)\n        v3 = v1 + 3\n        v4 = torch.clamp_min(v3, 0)\n        v5 = torch.clamp_max(v4, 6)\n        v6 = v1 * v5\n        v7 = v6 / 6\n        return v7\n# Input to the model\nx1 = torch.randn(2, 3, 28, 28)\n",
                ".\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 32, 3, stride=2, padding=1)\n        self.conv2 = torch.nn.Conv2d(32, 64, 3, stride=2, padding=1)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v1 * v4\n        v6 = v5 / 6\n        v7 = self.conv2(v6)\n        v8 = v7 + 3\n        v9 = torch.clamp_min(v8, 0)\n        v10 = torch.clamp_max(v9, 6)\n        v11 = v7 * v10\n        v12 = v11 / 6\n        return v12\n# Inputs to the model\nx1 = torch.randn(2, 3, 28, 28)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.avg_pool2d = torch.nn.AvgPool2d(3, stride=1, padding=2, count_include_pad=False)\n        self.conv = torch.nn.Conv2d(256, 16, 7, stride=1, padding=0)\n    def forward(self, x1):\n        v1 = x1\n        v2 = self.avg_pool2d(v1)\n        v3 = self.conv(v2)\n        v4 = v3 + 3\n        v5 = torch.clamp(v4, min=0, max=6)\n        v6 = v3 * v5\n        v7 = v6 / 6\n        return v7\n# Inputs to the model\nx1 = torch.randn(1, 256, 14, 14)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 8, 1, stride=1, padding=0)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = (v1 + 3).reshape(2, 2, 8)\n        v3 = v1.permute(0, 2, 3, 1) + 3\n        v4 = v1 + 3\n        v3 = v1.permute(0, 2, 3, 1) + 3\n        v4 = v1 + 3\n        v4 = v1 + 3\n        v4 = v1.permute(0, 2, 3, 1).permute(1, 0, 2, 3) + 3\n        v5 = torch.clamp(v1 + 3, min=0, max=6)\n        v6 = torch.clamp(v1.permute(0, 2, 3, 1) + 3, min=0, max=6)\n        v7 = torch.clamp(v1 + 3, min=0, max=6)\n        v8 = torch.clamp(v1.permute(0, 2, 3, 1) + 3, min=0, max=6)\n        v9 = torch.clamp(v1 + 3, min=0, max=6)\n        v10 = v1 * v4\n        v11 = v1 * v4\n        v12 = v1 / 6\n        return v7\n# Inputs to the model\nx1 = torch.randn(2, 3, 28, 28)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n        self.tanh = torch.nn.Tanh()\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 + 3\n        v3 = self.tanh(v2)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v1 * v4\n        v6 = v5 / 6\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v4 * v3\n        v6 = v5 / 6\n        return v6\n# Inputs to the model\nx1 = torch.randn(2, 3, 28, 28)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 3, (2,2), (2,2), (5,5))\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp(v2, 0, 6)\n        v4 = v1 * v3\n        v5 = v4 / 6\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 3, 13, 13)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 3, 2, stride=3, padding=4)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp(v2, min=0, max=6)\n        v4 = v1 * v3\n        v5 = v4 / 6\n        return v5\n# Inputs to the model\nx1 = torch.randn(2, 3, 113, 113)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 3, 1, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp(v2, min=0, max=6)\n        v4 = v3 + 3\n        v5 = torch.clamp(v4, min=0, max=6)\n        v6 = v3 * v5\n        v7 = v6 / 6\n        return v7\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 8, 3, stride=2, padding=1)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp(v3, min=0, max=6)\n        v5 = v1 * v4\n        v6 = v5 / 6\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 3, 10, 10)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.relu6 = torch.nn.ReLU6()\n        self.conv = torch.nn.Conv2d(3, 16, 2, stride=2, padding=5)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = self.relu6(v1)\n        v3 = v1 + 3\n        v4 = torch.clamp_min(v3, 0)\n        v5 = torch.clamp_max(v4, 6)\n        v6 = v1 * v5\n        v7 = v6 / 6\n        return v7\n# Input to the model\nx1 = torch.randn(2, 3, 28, 28)\n",
                ".\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 32, 3, stride=2, padding=1)\n        self.conv2 = torch.nn.Conv2d(32, 64, 3, stride=2, padding=1)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v1 * v4\n        v6 = v5 / 6\n        v7 = self.conv2(v6)\n        v8 = v7 + 3\n        v9 = torch.clamp_min(v8, 0)\n        v10 = torch.clamp_max(v9, 6)\n        v11 = v7 * v10\n        v12 = v11 / 6\n        return v12\n# Inputs to the model\nx1 = torch.randn(2, 3, 28, 28)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.avg_pool2d = torch.nn.AvgPool2d(3, stride=1, padding=2, count_include_pad=False)\n        self.conv = torch.nn.Conv2d(256, 16, 7, stride=1, padding=0)\n    def forward(self, x1):\n        v1 = x1\n        v2 = self.avg_pool2d(v1)\n        v3 = self.conv(v2)\n        v4 = v3 + 3\n        v5 = torch.clamp(v4, min=0, max=6)\n        v6 = v3 * v5\n        v7 = v6 / 6\n        return v7\n# Inputs to the model\nx1 = torch.randn(1, 256, 14, 14)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 8, 1, stride=1, padding=0)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = (v1 + 3).reshape(2, 2, 8)\n        v3 = v1.permute(0, 2, 3, 1) + 3\n        v4 = v1 + 3\n        v3 = v1.permute(0, 2, 3, 1) + 3\n        v4 = v1 + 3\n        v4 = v1 + 3\n        v4 = v1.permute(0, 2, 3, 1).permute(1, 0, 2, 3) + 3\n        v5 = torch.clamp(v1 + 3, min=0, max=6)\n        v6 = torch.clamp(v1.permute(0, 2, 3, 1) + 3, min=0, max=6)\n        v7 = torch.clamp(v1 + 3, min=0, max=6)\n        v8 = torch.clamp(v1.permute(0, 2, 3, 1) + 3, min=0, max=6)\n        v9 = torch.clamp(v1 + 3, min=0, max=6)\n        v10 = v1 * v4\n        v11 = v1 * v4\n        v12 = v1 / 6\n        return v7\n# Inputs to the model\nx1 = torch.randn(2, 3, 28, 28)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n        self.tanh = torch.nn.Tanh()\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 + 3\n        v3 = self.tanh(v2)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v1 * v4\n        v6 = v5 / 6\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v4 * v3\n        v6 = v5 / 6\n        return v6\n# Inputs to the model\nx1 = torch.randn(2, 3, 28, 28)\n"
            ],
            "g_time": 13.24141526222229
        }
    }
}
{
    "opt": "sfdp=5",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.heads = 512\n        self.seq_len = 1\n        self.dim = 128 // self.heads\n    def forward(self, query, key, value, attn_mask):\n        qk = query @ key.transpose(-2, -1) / math.sqrt(query.size(-1))\n        qk = qk + attn_mask\n        attn_weight = torch.softmax(qk, dim=-1)\n        attn_weight = torch.dropout(attn_weight, 0.3324105920863156, True)\n        output = attn_weight @ value\n        return output\n# Inputs to the model\nquery = torch.randn(1, 16, 1, 128)\nkey = torch.randn(1, 16, 1, 128)\nvalue = torch.randn(1, 16, 1, 128)\nattn_mask = torch.randn(1, 1, 1, 1)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.heads = 8\n        self.seq_len = 8\n        self.dim = 128 // self.heads\n    def forward(self, query, key, value, attn_mask):\n        qk = query @ key.transpose(-2, -1) / math.sqrt(query.size(-1))\n        qk = qk + attn_mask\n        attn_weight = torch.softmax(qk, dim=-1)\n        attn_weight = torch.dropout(attn_weight, 0.2, True)\n        output = attn_weight @ value\n        return output\n# Inputs to the model\nquery = torch.randn(1, 4, 8, 128)\nkey = torch.randn(1, 4, 8, 128)\nvalue = torch.randn(1, 4, 8, 128)\nattn_mask = torch.randn(1, 1, 8, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.heads = 4\n        self.seq_len = 4096\n        self.dim = 512 // self.heads\n    def forward(self, query, key, value, attn_mask):\n        qk = query @ key.transpose(-2, -1) / math.sqrt(query.size(-1))\n        qk = qk + attn_mask\n        attn_weight = torch.softmax(qk, dim=-1)\n        attn_weight = torch.dropout(attn_weight, 1.2567631423802008e-06, True)\n        output = attn_weight @ value\n        return output\n# Inputs to the model\nquery = torch.randn(1, 16, 128, 512)\nkey = torch.randn(1, 16, 128, 512)\nvalue = torch.randn(1, 16, 128, 512)\nattn_mask = torch.randn(1, 1, 128, 128)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.heads = 32\n        self.seq_len = 2327\n        self.dim = 256 // self.heads\n    def forward(self, query, key, value, attn_mask):\n        qk = query @ key.transpose(-2, -1) / math.sqrt(query.size(-1))\n        qk = qk + attn_mask\n        attn_weight = torch.softmax(qk, dim=-1)\n        attn_weight = torch.dropout(attn_weight, 0.01, True)\n        output = attn_weight @ value\n        return output\n# Inputs to the model\nquery = torch.randn(1, 327, 256, 256)\nkey = torch.randn(1, 327, 256, 256)\nvalue = torch.randn(1, 327, 256, 256)\nattn_mask = torch.randn(1, 1, 256, 256)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.dropout = 0.7854209996613659\n        self.heads = 64\n        self.seq_len = 16384\n        self.dim = 64 // self.heads\n    def forward(self, query, key, value, attn_mask):\n        qk = query @ key.transpose(-2, -1) / math.sqrt(query.size(-1))\n        qk = qk + attn_mask\n        attn_weight = torch.softmax(qk, dim=-1)\n        attn_weight = torch.dropout(attn_weight, self.dropout, True)\n        output = attn_weight @ value\n        return output\n# Inputs to the model\nquery = torch.randn(1, 128, 16384, 64)\nkey = torch.randn(1, 128, 16384, 64)\nvalue = torch.randn(1, 128, 16384, 64)\nattn_mask = torch.randn(1, 1, 16384, 16384)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.attn = nn.MultiheadAttention(1024, 8192)\n        self.gru = nn.GRU(1024, 1024, 1, batch_first = True)\n    def forward(self, x):\n        o, w = self.attn(x, x, x, need_weights = True)\n        o, _ = self.gru(o)\n        return o, w\n# Inputs to the model\nx = torch.randn(1, 16, 1024)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.heads = 64\n        self.seq_len = 1024\n        self.dim = 1024 // self.heads\n    def forward(self, query, key, value, attn_mask):\n        # Compute the attention weights by performing the dot product of the query and keys\n        qk = query @ key.transpose(-2, -1) / math.sqrt(query.size(-1))\n        # Add the attention mask\n        qk = qk + attn_mask\n        # Apply softmax to the scaled dot product of the query and keys\n        attn_weight = torch.softmax(qk, dim=-1)\n        # Apply dropout to the softmax results\n        attn_weight = torch.dropout(attn_weight, 0.1, True)\n        # Perform the dot product of the attention weights and the values\n        output = attn_weight @ value\n        return output\n# Inputs to the model\nquery = torch.randn(1, 256, 1024, 1024)\nkey = torch.randn(1, 256, 1024, 1024)\nvalue = torch.randn(1, 256, 1024, 1024)\nattn_mask = torch.randn(1, 1, 1024, 1024)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.heads = 8\n        self.seq_len = 1415\n        self.dim = 960 // self.heads\n    def forward(self, query, key, value, attn_mask):\n        qk = query @ key.transpose(-2, -1) / math.sqrt(query.size(-1))\n        qk = qk + attn_mask\n        attn_weight = torch.softmax(qk, dim=-1)\n        attn_weight = torch.dropout(attn_weight, 0.7460164712281034, True)\n        output = attn_weight @ value\n        return output\n# Inputs to the model\nquery = torch.randn(1, 280, 1415, 960)\nkey = torch.randn(1, 280, 1415, 960)\nvalue = torch.randn(1, 280, 1415, 960)\nattn_mask = torch.randn(1, 1, 1415, 1415)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.heads = 8\n        self.seq_len = 1\n        self.dim = 128 // self.heads\n    def forward(self, query, key, value, attn_mask):\n        qk = query @ key.transpose(-2, -1) / math.sqrt(query.size(-1))\n        qk = qk + attn_mask\n        attn_weight = torch.softmax(qk, dim=-1)\n        attn_weight = torch.dropout(attn_weight, 0.08907142741139609, True)\n        output = attn_weight @ value\n        return output\n# Inputs to the model\nquery = torch.randn(1, 8, 1, 128)\nkey = torch.randn(1, 8, 1, 128)\nvalue = torch.randn(1, 8, 1, 128)\nattn_mask = torch.randn(1, 1, 1, 1)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.heads = 8\n        self.seq_len = 1\n        self.dim = 6144 // self.heads\n    def forward(self, query, key, value, attn_mask):\n        qk = query @ key.transpose(-2, -1) / math.sqrt(query.size(-1))\n        qk = qk.transpose(-1, -2).transpose(-3, -4)\n        qk = qk + attn_mask\n        attn_weight = torch.softmax(qk, dim=-1)\n        del qk\n        attn_weight = torch.dropout(attn_weight, 2.3897206599917977e-06, True)\n        attn_weight = attn_weight.transpose(-1, -2).transpose(-2, -3)\n        output = attn_weight @ value\n        return output\n# Inputs to the model\nquery = torch.randn(1, 6144, 1, 64)\nkey = torch.randn(1, 6144, 1, 64)\nvalue = torch.randn(1, 6144, 1, 64)\nattn_mask = torch.randn(1, 1, 1, 1)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.heads = 512\n        self.seq_len = 1\n        self.dim = 128 // self.heads\n    def forward(self, query, key, value, attn_mask):\n        qk = query @ key.transpose(-2, -1) / math.sqrt(query.size(-1))\n        qk = qk + attn_mask\n        attn_weight = torch.softmax(qk, dim=-1)\n        attn_weight = torch.dropout(attn_weight, 0.3324105920863156, True)\n        output = attn_weight @ value\n        return output\n# Inputs to the model\nquery = torch.randn(1, 16, 1, 128)\nkey = torch.randn(1, 16, 1, 128)\nvalue = torch.randn(1, 16, 1, 128)\nattn_mask = torch.randn(1, 1, 1, 1)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.heads = 8\n        self.seq_len = 8\n        self.dim = 128 // self.heads\n    def forward(self, query, key, value, attn_mask):\n        qk = query @ key.transpose(-2, -1) / math.sqrt(query.size(-1))\n        qk = qk + attn_mask\n        attn_weight = torch.softmax(qk, dim=-1)\n        attn_weight = torch.dropout(attn_weight, 0.2, True)\n        output = attn_weight @ value\n        return output\n# Inputs to the model\nquery = torch.randn(1, 4, 8, 128)\nkey = torch.randn(1, 4, 8, 128)\nvalue = torch.randn(1, 4, 8, 128)\nattn_mask = torch.randn(1, 1, 8, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.heads = 4\n        self.seq_len = 4096\n        self.dim = 512 // self.heads\n    def forward(self, query, key, value, attn_mask):\n        qk = query @ key.transpose(-2, -1) / math.sqrt(query.size(-1))\n        qk = qk + attn_mask\n        attn_weight = torch.softmax(qk, dim=-1)\n        attn_weight = torch.dropout(attn_weight, 1.2567631423802008e-06, True)\n        output = attn_weight @ value\n        return output\n# Inputs to the model\nquery = torch.randn(1, 16, 128, 512)\nkey = torch.randn(1, 16, 128, 512)\nvalue = torch.randn(1, 16, 128, 512)\nattn_mask = torch.randn(1, 1, 128, 128)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.heads = 32\n        self.seq_len = 2327\n        self.dim = 256 // self.heads\n    def forward(self, query, key, value, attn_mask):\n        qk = query @ key.transpose(-2, -1) / math.sqrt(query.size(-1))\n        qk = qk + attn_mask\n        attn_weight = torch.softmax(qk, dim=-1)\n        attn_weight = torch.dropout(attn_weight, 0.01, True)\n        output = attn_weight @ value\n        return output\n# Inputs to the model\nquery = torch.randn(1, 327, 256, 256)\nkey = torch.randn(1, 327, 256, 256)\nvalue = torch.randn(1, 327, 256, 256)\nattn_mask = torch.randn(1, 1, 256, 256)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.dropout = 0.7854209996613659\n        self.heads = 64\n        self.seq_len = 16384\n        self.dim = 64 // self.heads\n    def forward(self, query, key, value, attn_mask):\n        qk = query @ key.transpose(-2, -1) / math.sqrt(query.size(-1))\n        qk = qk + attn_mask\n        attn_weight = torch.softmax(qk, dim=-1)\n        attn_weight = torch.dropout(attn_weight, self.dropout, True)\n        output = attn_weight @ value\n        return output\n# Inputs to the model\nquery = torch.randn(1, 128, 16384, 64)\nkey = torch.randn(1, 128, 16384, 64)\nvalue = torch.randn(1, 128, 16384, 64)\nattn_mask = torch.randn(1, 1, 16384, 16384)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.attn = nn.MultiheadAttention(1024, 8192)\n        self.gru = nn.GRU(1024, 1024, 1, batch_first = True)\n    def forward(self, x):\n        o, w = self.attn(x, x, x, need_weights = True)\n        o, _ = self.gru(o)\n        return o, w\n# Inputs to the model\nx = torch.randn(1, 16, 1024)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.heads = 64\n        self.seq_len = 1024\n        self.dim = 1024 // self.heads\n    def forward(self, query, key, value, attn_mask):\n        # Compute the attention weights by performing the dot product of the query and keys\n        qk = query @ key.transpose(-2, -1) / math.sqrt(query.size(-1))\n        # Add the attention mask\n        qk = qk + attn_mask\n        # Apply softmax to the scaled dot product of the query and keys\n        attn_weight = torch.softmax(qk, dim=-1)\n        # Apply dropout to the softmax results\n        attn_weight = torch.dropout(attn_weight, 0.1, True)\n        # Perform the dot product of the attention weights and the values\n        output = attn_weight @ value\n        return output\n# Inputs to the model\nquery = torch.randn(1, 256, 1024, 1024)\nkey = torch.randn(1, 256, 1024, 1024)\nvalue = torch.randn(1, 256, 1024, 1024)\nattn_mask = torch.randn(1, 1, 1024, 1024)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.heads = 8\n        self.seq_len = 1415\n        self.dim = 960 // self.heads\n    def forward(self, query, key, value, attn_mask):\n        qk = query @ key.transpose(-2, -1) / math.sqrt(query.size(-1))\n        qk = qk + attn_mask\n        attn_weight = torch.softmax(qk, dim=-1)\n        attn_weight = torch.dropout(attn_weight, 0.7460164712281034, True)\n        output = attn_weight @ value\n        return output\n# Inputs to the model\nquery = torch.randn(1, 280, 1415, 960)\nkey = torch.randn(1, 280, 1415, 960)\nvalue = torch.randn(1, 280, 1415, 960)\nattn_mask = torch.randn(1, 1, 1415, 1415)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.heads = 8\n        self.seq_len = 1\n        self.dim = 128 // self.heads\n    def forward(self, query, key, value, attn_mask):\n        qk = query @ key.transpose(-2, -1) / math.sqrt(query.size(-1))\n        qk = qk + attn_mask\n        attn_weight = torch.softmax(qk, dim=-1)\n        attn_weight = torch.dropout(attn_weight, 0.08907142741139609, True)\n        output = attn_weight @ value\n        return output\n# Inputs to the model\nquery = torch.randn(1, 8, 1, 128)\nkey = torch.randn(1, 8, 1, 128)\nvalue = torch.randn(1, 8, 1, 128)\nattn_mask = torch.randn(1, 1, 1, 1)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.heads = 8\n        self.seq_len = 1\n        self.dim = 6144 // self.heads\n    def forward(self, query, key, value, attn_mask):\n        qk = query @ key.transpose(-2, -1) / math.sqrt(query.size(-1))\n        qk = qk.transpose(-1, -2).transpose(-3, -4)\n        qk = qk + attn_mask\n        attn_weight = torch.softmax(qk, dim=-1)\n        del qk\n        attn_weight = torch.dropout(attn_weight, 2.3897206599917977e-06, True)\n        attn_weight = attn_weight.transpose(-1, -2).transpose(-2, -3)\n        output = attn_weight @ value\n        return output\n# Inputs to the model\nquery = torch.randn(1, 6144, 1, 64)\nkey = torch.randn(1, 6144, 1, 64)\nvalue = torch.randn(1, 6144, 1, 64)\nattn_mask = torch.randn(1, 1, 1, 1)\n"
            ],
            "g_time": 11.251694917678833
        }
    }
}
{
    "opt": "unary=24",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_1 = torch.nn.Conv2d(4, 8, 1, stride=1, padding=1)\n        self.conv_2 = torch.nn.Conv2d(8, 4, 1, stride=1, padding=1)\n    def forward(self, x):\n        negative_slope = 0\n        v1 = self.conv_1(x)\n        v2 = torch.sum(v1, dim=1)\n        v3 = v2 > 0\n        v4 = v2 * negative_slope\n        v5 = torch.where(v3, v2, v4)\n        v6 = v5.shape[2]\n        v7 = v5.shape[3]\n        v8 = self.conv_2(v5)\n        v9 = v8[:,:,:v6,:v7].squeeze()\n        return v9\n# Inputs to the model\nx1 = torch.randn(1, 4, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(15, 5, 2, stride=1, padding=0)\n    def forward(self, x):\n        negative_slope = 0.2\n        v1 = self.conv(x)\n        v2 = v1 > 0\n        v3 = v1 * negative_slope\n        v4 = torch.where(v2, v1, v3)\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 15, 32, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        negative_slope = 0.1\n        self.conv_1 = torch.nn.Conv2d(16, 16, 3, stride=1, padding=1)\n        self.conv_2 = torch.nn.Conv2d(16, 16, 3, stride=1, padding=1)\n    def forward(self, x):\n        v1 = self.conv_1(x)\n        v2 = v1 > 0\n        v3 = v1 * negative_slope\n        v4 = torch.where(v2, v1, v3)\n        v5 = self.conv_2(v4)\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 16, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_1 = torch.nn.Conv2d(24, 24, 1, stride=2, padding=0)\n        self.conv_2 = torch.nn.Conv2d(24, 24, 1, stride=2, padding=0)\n    def forward(self, x):\n        negative_slope = 0.2\n        v1 = self.conv_1(x)\n        v2 = v1 * negative_slope\n        v3 = v2 % 1\n        v4 = self.conv_2(v3)\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 24, 32, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, nchan):\n        super().__init__()\n        self.conv_1 = torch.nn.Conv2d(nchan, nchan, 1, stride=1, padding=1)\n        self.conv_2 = torch.nn.Conv2d(nchan, nchan, 1, stride=1, padding=1)\n    def forward(self, x):\n        negative_slope = 0.2\n        v1 = self.conv_1(x)\n        v2 = v1 > 0\n        v3 = v1 * negative_slope\n        v4 = torch.where(v2, v1, v3)\n        v5 = self.conv_2(v4)\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 8, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 3, 3, stride=1, padding=1)\n        self.conv2 = torch.nn.Conv2d(3, 3, 3, stride=1, padding=1)\n    def forward(self, x):\n        negative_slope = 0.2\n        v1 = self.conv1(x)\n        v2 = v1 > 0\n        v3 = v1 * negative_slope\n        v4 = torch.where(v2, v1, v3)\n        v5 = self.conv2(v4)\n        return v5\nv1 = (torch.randn((1, 3, 5, 5)), torch.randn((1, 3, 5, 5))),\n# Inputs to the model\nx1 = torch.randn(1, 3, 32, 32)\nx2 = torch.randn(1, 3, 5, 5)\nx3 = torch.randn((1, 3, 5, 5)), torch.randn((1, 3, 5, 5))\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_1 = torch.nn.Conv2d(1, 1, 1, stride=1, padding=0)\n        self.conv_2 = torch.nn.Conv2d(1, 1, 1, stride=1, padding=0)\n    def forward(self, x):\n        negative_slope = 0.2\n        v1 = self.conv_1(x)\n        v2 = v1 + x\n        v3 = v2 > 0\n        v4 = v2 * negative_slope\n        v5 = torch.where(v3, v2, v4)\n        v6 = self.conv_2(v5)\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 1, 32, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_1 = torch.nn.Conv2d(16, 16, 1, stride=1, padding=0)\n        self.relu_1 = torch.nn.ReLU(inplace=True)\n        self.conv_2 = torch.nn.Conv2d(16, 16, 1, stride=1, padding=0)\n        self.relu_2 = torch.nn.ReLU(inplace=True)\n    def forward(self, x):\n        negative_slope = 0.3\n        v1 = self.relu_1(self.conv_1(x))\n        v2 = v1 > 0\n        v3 = v1 * negative_slope\n        v4 = torch.where(v2, v1, v3)\n        v5 = self.relu_2(self.conv_2(v4))\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 16, 32, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_1 = torch.nn.Conv2d(8, 8, 1, stride=1, padding=1)\n        self.conv_2 = torch.nn.Conv2d(8, 8, 3, stride=1, padding=1)\n    def forward(self, x):\n        negative_slope = 0.2\n        v1 = self.conv_1(x)\n        v2 = v1 > 0\n        v3 = v1 * negative_slope\n        v4 = torch.where(v2, v1, v3)\n        v5 = self.conv_2(v4)\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 8, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(1, 1, 3, stride=1, padding=1)\n        self.relu = torch.nn.ReLU()\n    def forward(self, x):\n        negative_slope = 0.2\n        t1 = self.conv(x)\n        t2 = t1 > 0\n        t3 = t1 * negative_slope\n        t4 = torch.where(t2, t1, t3)\n        return self.relu(t4)\n# Inputs to the model\nx1 = torch.randn(1, 1, 35, 35)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_1 = torch.nn.Conv2d(4, 8, 1, stride=1, padding=1)\n        self.conv_2 = torch.nn.Conv2d(8, 4, 1, stride=1, padding=1)\n    def forward(self, x):\n        negative_slope = 0\n        v1 = self.conv_1(x)\n        v2 = torch.sum(v1, dim=1)\n        v3 = v2 > 0\n        v4 = v2 * negative_slope\n        v5 = torch.where(v3, v2, v4)\n        v6 = v5.shape[2]\n        v7 = v5.shape[3]\n        v8 = self.conv_2(v5)\n        v9 = v8[:,:,:v6,:v7].squeeze()\n        return v9\n# Inputs to the model\nx1 = torch.randn(1, 4, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(15, 5, 2, stride=1, padding=0)\n    def forward(self, x):\n        negative_slope = 0.2\n        v1 = self.conv(x)\n        v2 = v1 > 0\n        v3 = v1 * negative_slope\n        v4 = torch.where(v2, v1, v3)\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 15, 32, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        negative_slope = 0.1\n        self.conv_1 = torch.nn.Conv2d(16, 16, 3, stride=1, padding=1)\n        self.conv_2 = torch.nn.Conv2d(16, 16, 3, stride=1, padding=1)\n    def forward(self, x):\n        v1 = self.conv_1(x)\n        v2 = v1 > 0\n        v3 = v1 * negative_slope\n        v4 = torch.where(v2, v1, v3)\n        v5 = self.conv_2(v4)\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 16, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_1 = torch.nn.Conv2d(24, 24, 1, stride=2, padding=0)\n        self.conv_2 = torch.nn.Conv2d(24, 24, 1, stride=2, padding=0)\n    def forward(self, x):\n        negative_slope = 0.2\n        v1 = self.conv_1(x)\n        v2 = v1 * negative_slope\n        v3 = v2 % 1\n        v4 = self.conv_2(v3)\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 24, 32, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, nchan):\n        super().__init__()\n        self.conv_1 = torch.nn.Conv2d(nchan, nchan, 1, stride=1, padding=1)\n        self.conv_2 = torch.nn.Conv2d(nchan, nchan, 1, stride=1, padding=1)\n    def forward(self, x):\n        negative_slope = 0.2\n        v1 = self.conv_1(x)\n        v2 = v1 > 0\n        v3 = v1 * negative_slope\n        v4 = torch.where(v2, v1, v3)\n        v5 = self.conv_2(v4)\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 8, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 3, 3, stride=1, padding=1)\n        self.conv2 = torch.nn.Conv2d(3, 3, 3, stride=1, padding=1)\n    def forward(self, x):\n        negative_slope = 0.2\n        v1 = self.conv1(x)\n        v2 = v1 > 0\n        v3 = v1 * negative_slope\n        v4 = torch.where(v2, v1, v3)\n        v5 = self.conv2(v4)\n        return v5\nv1 = (torch.randn((1, 3, 5, 5)), torch.randn((1, 3, 5, 5))),\n# Inputs to the model\nx1 = torch.randn(1, 3, 32, 32)\nx2 = torch.randn(1, 3, 5, 5)\nx3 = torch.randn((1, 3, 5, 5)), torch.randn((1, 3, 5, 5))\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_1 = torch.nn.Conv2d(1, 1, 1, stride=1, padding=0)\n        self.conv_2 = torch.nn.Conv2d(1, 1, 1, stride=1, padding=0)\n    def forward(self, x):\n        negative_slope = 0.2\n        v1 = self.conv_1(x)\n        v2 = v1 + x\n        v3 = v2 > 0\n        v4 = v2 * negative_slope\n        v5 = torch.where(v3, v2, v4)\n        v6 = self.conv_2(v5)\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 1, 32, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_1 = torch.nn.Conv2d(16, 16, 1, stride=1, padding=0)\n        self.relu_1 = torch.nn.ReLU(inplace=True)\n        self.conv_2 = torch.nn.Conv2d(16, 16, 1, stride=1, padding=0)\n        self.relu_2 = torch.nn.ReLU(inplace=True)\n    def forward(self, x):\n        negative_slope = 0.3\n        v1 = self.relu_1(self.conv_1(x))\n        v2 = v1 > 0\n        v3 = v1 * negative_slope\n        v4 = torch.where(v2, v1, v3)\n        v5 = self.relu_2(self.conv_2(v4))\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 16, 32, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_1 = torch.nn.Conv2d(8, 8, 1, stride=1, padding=1)\n        self.conv_2 = torch.nn.Conv2d(8, 8, 3, stride=1, padding=1)\n    def forward(self, x):\n        negative_slope = 0.2\n        v1 = self.conv_1(x)\n        v2 = v1 > 0\n        v3 = v1 * negative_slope\n        v4 = torch.where(v2, v1, v3)\n        v5 = self.conv_2(v4)\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 8, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(1, 1, 3, stride=1, padding=1)\n        self.relu = torch.nn.ReLU()\n    def forward(self, x):\n        negative_slope = 0.2\n        t1 = self.conv(x)\n        t2 = t1 > 0\n        t3 = t1 * negative_slope\n        t4 = torch.where(t2, t1, t3)\n        return self.relu(t4)\n# Inputs to the model\nx1 = torch.randn(1, 1, 35, 35)\n"
            ],
            "g_time": 9.589062690734863
        }
    }
}
{
    "opt": "unary=14",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose_10 = torch.nn.ConvTranspose2d(672, 367, 1, stride=1, padding=0)\n    def forward(self, x1):\n        v1 = self.conv_transpose_10(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 672, 256, 256)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose_3 = torch.nn.ConvTranspose2d(1, 59961, 1, stride=1, padding=0)\n    def forward(self, x1):\n        v1 = self.conv_transpose_3(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 1, 1, 1)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose_4 = torch.nn.ConvTranspose2d(1382, 536, 1, stride=1, padding=0)\n    def forward(self, x1):\n        v1 = self.conv_transpose_4(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 1382, 256, 256)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose_3 = torch.nn.ConvTranspose2d(1655, 213, 1, stride=1, padding=0)\n    def forward(self, x1):\n        v1 = self.conv_transpose_3(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 1655, 128, 128)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose_0 = torch.nn.ConvTranspose2d(1620, 43, 11, stride=1, padding=2)\n        self.conv_transpose_1 = torch.nn.ConvTranspose2d(43, 100, 7, stride=1, padding=3)\n        self.conv_transpose_2 = torch.nn.ConvTranspose2d(100, 21, 11, stride=1, padding=2)\n    def forward(self, x1):\n        v1 = self.conv_transpose_0(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        v4 = self.conv_transpose_1(v3)\n        v5 = torch.sigmoid(v4)\n        v6 = v4 * v5\n        v7 = self.conv_transpose_2(v6)\n        v8 = torch.sigmoid(v7)\n        v9 = v7 * v8\n        return v9\n# Inputs to the model\nx1 = torch.randn(1, 1620, 25, 25)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose_0 = torch.nn.ConvTranspose2d(4, 10, 3, stride=2, padding=1, groups=2)\n    def forward(self, x1):\n        v1 = self.conv_transpose_0(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 4, 8, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose_0 = torch.nn.Conv2d(4, 4, 3, stride=2, padding=1, bias=False)\n        self.relu_1 = torch.nn.ReLU()\n        self.conv_transpose_2 = torch.nn.ConvTranspose2d(4, 1, 3, stride=2, padding=1, output_padding=1, bias=False)\n        self.relu_3 = torch.nn.ReLU()\n        self.conv_transpose_4 = torch.nn.ConvTranspose2d(1, 1, 3, stride=2, padding=1, output_padding=1)\n    def forward(self, x1):\n        v1 = torch.transpose(x1, -3, -1)\n        v2 = torch.transpose(x1, -2, -1)\n        v3 = v1[:]\n        v4 = self.conv_transpose_0(v3)\n        v5 = v2[:]\n        v6 = self.relu_1(v4)\n        v7 = self.conv_transpose_2(v5)\n        v8 = v3[:]\n        v9 = self.relu_3(v7)\n        v10 = self.conv_transpose_4(v8)\n        v11 = torch.transpose(v10, -3, -1)\n        v12 = torch.transpose(v11, -2, -1)\n        v13 = torch.transpose(v12, -1, -3)\n        return v13\n# Inputs to the model\nx1 = torch.randn(1, 4, 6, 6)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose_2 = torch.nn.ConvTranspose2d(813, 512, 1, stride=2, padding=0, dilation=1)\n    def forward(self, x1):\n        v1 = self.conv_transpose_2(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 813, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose_1 = torch.nn.ConvTranspose2d(785, 785, 2, stride=1, padding=0, dilation=1, groups=1)\n    def forward(self, x1):\n        v1 = self.conv_transpose_1(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 785, 32, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose_15 = torch.nn.ConvTranspose2d(3, 7, 1, stride=1, padding=0)\n    def forward(self, x1):\n        v1 = self.conv_transpose_15(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose_10 = torch.nn.ConvTranspose2d(672, 367, 1, stride=1, padding=0)\n    def forward(self, x1):\n        v1 = self.conv_transpose_10(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 672, 256, 256)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose_3 = torch.nn.ConvTranspose2d(1, 59961, 1, stride=1, padding=0)\n    def forward(self, x1):\n        v1 = self.conv_transpose_3(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 1, 1, 1)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose_4 = torch.nn.ConvTranspose2d(1382, 536, 1, stride=1, padding=0)\n    def forward(self, x1):\n        v1 = self.conv_transpose_4(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 1382, 256, 256)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose_3 = torch.nn.ConvTranspose2d(1655, 213, 1, stride=1, padding=0)\n    def forward(self, x1):\n        v1 = self.conv_transpose_3(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 1655, 128, 128)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose_0 = torch.nn.ConvTranspose2d(1620, 43, 11, stride=1, padding=2)\n        self.conv_transpose_1 = torch.nn.ConvTranspose2d(43, 100, 7, stride=1, padding=3)\n        self.conv_transpose_2 = torch.nn.ConvTranspose2d(100, 21, 11, stride=1, padding=2)\n    def forward(self, x1):\n        v1 = self.conv_transpose_0(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        v4 = self.conv_transpose_1(v3)\n        v5 = torch.sigmoid(v4)\n        v6 = v4 * v5\n        v7 = self.conv_transpose_2(v6)\n        v8 = torch.sigmoid(v7)\n        v9 = v7 * v8\n        return v9\n# Inputs to the model\nx1 = torch.randn(1, 1620, 25, 25)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose_0 = torch.nn.ConvTranspose2d(4, 10, 3, stride=2, padding=1, groups=2)\n    def forward(self, x1):\n        v1 = self.conv_transpose_0(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 4, 8, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose_0 = torch.nn.Conv2d(4, 4, 3, stride=2, padding=1, bias=False)\n        self.relu_1 = torch.nn.ReLU()\n        self.conv_transpose_2 = torch.nn.ConvTranspose2d(4, 1, 3, stride=2, padding=1, output_padding=1, bias=False)\n        self.relu_3 = torch.nn.ReLU()\n        self.conv_transpose_4 = torch.nn.ConvTranspose2d(1, 1, 3, stride=2, padding=1, output_padding=1)\n    def forward(self, x1):\n        v1 = torch.transpose(x1, -3, -1)\n        v2 = torch.transpose(x1, -2, -1)\n        v3 = v1[:]\n        v4 = self.conv_transpose_0(v3)\n        v5 = v2[:]\n        v6 = self.relu_1(v4)\n        v7 = self.conv_transpose_2(v5)\n        v8 = v3[:]\n        v9 = self.relu_3(v7)\n        v10 = self.conv_transpose_4(v8)\n        v11 = torch.transpose(v10, -3, -1)\n        v12 = torch.transpose(v11, -2, -1)\n        v13 = torch.transpose(v12, -1, -3)\n        return v13\n# Inputs to the model\nx1 = torch.randn(1, 4, 6, 6)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose_2 = torch.nn.ConvTranspose2d(813, 512, 1, stride=2, padding=0, dilation=1)\n    def forward(self, x1):\n        v1 = self.conv_transpose_2(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 813, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose_1 = torch.nn.ConvTranspose2d(785, 785, 2, stride=1, padding=0, dilation=1, groups=1)\n    def forward(self, x1):\n        v1 = self.conv_transpose_1(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 785, 32, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose_15 = torch.nn.ConvTranspose2d(3, 7, 1, stride=1, padding=0)\n    def forward(self, x1):\n        v1 = self.conv_transpose_15(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n"
            ],
            "g_time": 12.24229097366333
        }
    }
}
{
    "opt": "sfdp=3",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.dropout = torch.nn.Dropout(p=0.1)\n \n    def forward(self, q, k, v):\n        s = torch.matmul(q, k).mul(0.1)\n        a = s.softmax(dim=-1)\n        d = self.dropout(a)\n        r = torch.matmul(d, v)\n        return r\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nq = torch.randn(1, 120, 1024)\nk = torch.randn(1, 120, 1024)\nv = torch.randn(1, 120, 512)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.scale_factor = 1.0/(head_size**0.5)\n        self.dropout_p = 0.2\n        self.dropout = torch.nn.Dropout(p=self.dropout_p)\n \n    def forward(self, query, key, value):\n        qk = torch.matmul(query, key.transpose(-2, -1))\n        scaled_qk = qk.mul(self.scale_factor)\n        softmax_qk = scaled_qk.softmax(dim=-1)\n        dropout_qk = self.dropout(softmax_qk)\n        output = dropout_qk.matmul(value)\n        return output\n\n# Initializing the model\ninput_tensor = torch.randn(1, 8, 64, 64)\nm = Model(input_tensor=input_tensor)\n\n# Inputs to the model\nquery = torch.randn(1, 8, 32, 64)\nkey = torch.randn(1, 8, 64, 32)\nvalue = torch.randn(1, 8, 32, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, scale_factor, dropout_p):\n        super().__init__()\n        self.scale_factor = scale_factor\n        self.dropout_p = dropout_p\n \n    def forward(self, x1, x2):\n        v1 = torch.matmul(x1, x2.transpose(-2, -1))\n        v2 = v1.mul(self.scale_factor)\n        v3 = v2.softmax(dim=-1)\n        v4 = torch.nn.functional.dropout(v3, p=self.dropout_p)\n        v5 = torch.matmul(v4, x2)\n        return v5\n\n# Initializing the model\nscale_factor = 1.0\ndropout_p = 0.3\nm = Model(scale_factor, dropout_p)\n\n# Inputs to the model\nx1 = torch.randn(10, 64)\nx2 = torch.randn(20, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.dropout = torch.nn.Dropout(dropout_p)\n \n    def forward(self, x1, x2):\n        v1 = torch.matmul(x1, x2.transpose(-2, -1))\n        v2 = v1 * scale_factor\n        v3 = torch.nn.functional.softmax(v2, dim=-1)\n        v4 = self.dropout(v3)\n        output = torch.matmul(v4, x2)\n        return output\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 2, 3)\nx2 = torch.randn(1, 3, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.dropout_p = torch.nn.Parameter(torch.ones(1) * 0.9)\n        self.scale_factor = torch.nn.Parameter(torch.ones(1))\n\n    def forward(self, q, k, v):\n        qk = torch.matmul(q, k.transpose(-2, -1))\n        scaled_qk = qk.mul(self.scale_factor)\n        softmax_qk = scaled_qk.softmax(dim=-1)\n        dropout_qk = torch.nn.functional.dropout(softmax_qk, p=self.dropout_p)\n        output = dropout_qk.matmul(v)\n        return output\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nQ = torch.randn(1, 16, 64, 64)\nK = torch.randn(1, 16, 64, 64)\nV = torch.randn(1, 16, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, query, key, value, scale_factor=1./(10**2), dropout_p=.2):\n        super().__init__()\n        qk = torch.matmul(query, key.transpose(-2, -1))\n        scaled_qk = qk.mul(scale_factor)\n        softmax_qk = scaled_qk.softmax(dim=-1)\n        dropout_qk = torch.nn.functional.dropout(softmax_qk, p=dropout_p)\n        output = dropout_qk.matmul(value)\n        self.register_buffer('dropout_qk', dropout_qk)\n        self.register_buffer('value', value)\n \n    def forward(self, x):\n        scaled_qk = torch.matmul(x, self.dropout_qk.transpose(-2, -1))\n        output = scaled_qk.matmul(self.value)\n        return output\n\n# Inplace operation on constant\na = torch.tensor([1, 2, 3], requires_grad=True)\nb = torch.tensor([1.2, 2.3, 3.4], requires_grad=True)\nc = b.clone()\nb_id = id(b)\nc_id = id(c)\nb += 1\nc += float(1)\nprint('a:', a, 'b:', b, 'c:', c)\n\n# Inputs to the model\nx = torch.randn(1, 1, 16, 8)\nquery = torch.randn(1, 1, 16, 8)\nkey = torch.randn(1, 1, 8, 8)\nvalue = torch.randn(1, 1, 16, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.softmax = torch.nn.Softmax(dim=-1)\n \n    def forward(self, q1, k1):\n        s1 = torch.matmul(q1, k1.transpose(-2, -1))\n        s2 = s1 * 0.5\n        s3 = s2.transpose(-2, -1)\n        s4 = self.softmax(s3)\n        s5 = torch.nn.functional.dropout(s4, p=0.5)\n        v1 = torch.matmul(s5, v1)\n        return v1\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nq1 = torch.randn(8, 256, 64, 64)\nk1 = torch.randn(8, 256, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n \n    def forward(self, input1, input2, input3):\n        v1 = torch.matmul(input1, input2.transpose(-2, -1))\n        v2 = v1.mul(10)\n        v3 = x3.softmax(-1)\n        v4 = torch.nn.functional.dropout(v3, p=0.1)\n        return v4.matmul(input3)\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\ninput1 = torch.randn(50, 64)\ninput2 = torch.randn(64, 80)\ninput3 = torch.randn(80, 10)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n \n    def forward(self, query, key, value, dropout_p):\n        qk = torch.matmul(query, key.transpose(-2, -1))\n        scale_factor = float(query.size(-1)) ** -0.5\n        scaled_qk = qk.mul(scale_factor)\n        softmax_qk = scaled_qk.softmax(dim=-1)\n        dropout_qk = torch.nn.functional.dropout(softmax_qk, p=dropout_p)\n        output = dropout_qk.matmul(value)\n        return output\n \n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx3 = torch.randn(4, 512, 768)\nx4 = torch.randn(4, 512, 768)\nx5 = torch.randn(4, 512, 768)\ndropout_p = 0.98\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.scale_factor = #\n        self.dropout_p = #\n  \n    def forward(query, key, value):\n        v1 = torch.matmul(query, key.transpose(-2, -1))\n        v2 = v1 * self.scale_factor\n        v3 = torch.nn.functional.softmax(v2, dim=-1)\n        v4 = torch.nn.functional.dropout(v3, p=self.dropout_p)\n        v5 = torch.matmul(v4, value)\n        return v5\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nquery = torch.randn(1, 16, 9)\nkey = torch.randn(1, 16, 10)\nvalue = torch.randn(1, 16, 10)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.dropout = torch.nn.Dropout(p=0.1)\n \n    def forward(self, q, k, v):\n        s = torch.matmul(q, k).mul(0.1)\n        a = s.softmax(dim=-1)\n        d = self.dropout(a)\n        r = torch.matmul(d, v)\n        return r\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nq = torch.randn(1, 120, 1024)\nk = torch.randn(1, 120, 1024)\nv = torch.randn(1, 120, 512)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.scale_factor = 1.0/(head_size**0.5)\n        self.dropout_p = 0.2\n        self.dropout = torch.nn.Dropout(p=self.dropout_p)\n \n    def forward(self, query, key, value):\n        qk = torch.matmul(query, key.transpose(-2, -1))\n        scaled_qk = qk.mul(self.scale_factor)\n        softmax_qk = scaled_qk.softmax(dim=-1)\n        dropout_qk = self.dropout(softmax_qk)\n        output = dropout_qk.matmul(value)\n        return output\n\n# Initializing the model\ninput_tensor = torch.randn(1, 8, 64, 64)\nm = Model(input_tensor=input_tensor)\n\n# Inputs to the model\nquery = torch.randn(1, 8, 32, 64)\nkey = torch.randn(1, 8, 64, 32)\nvalue = torch.randn(1, 8, 32, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, scale_factor, dropout_p):\n        super().__init__()\n        self.scale_factor = scale_factor\n        self.dropout_p = dropout_p\n \n    def forward(self, x1, x2):\n        v1 = torch.matmul(x1, x2.transpose(-2, -1))\n        v2 = v1.mul(self.scale_factor)\n        v3 = v2.softmax(dim=-1)\n        v4 = torch.nn.functional.dropout(v3, p=self.dropout_p)\n        v5 = torch.matmul(v4, x2)\n        return v5\n\n# Initializing the model\nscale_factor = 1.0\ndropout_p = 0.3\nm = Model(scale_factor, dropout_p)\n\n# Inputs to the model\nx1 = torch.randn(10, 64)\nx2 = torch.randn(20, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.dropout = torch.nn.Dropout(dropout_p)\n \n    def forward(self, x1, x2):\n        v1 = torch.matmul(x1, x2.transpose(-2, -1))\n        v2 = v1 * scale_factor\n        v3 = torch.nn.functional.softmax(v2, dim=-1)\n        v4 = self.dropout(v3)\n        output = torch.matmul(v4, x2)\n        return output\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 2, 3)\nx2 = torch.randn(1, 3, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.dropout_p = torch.nn.Parameter(torch.ones(1) * 0.9)\n        self.scale_factor = torch.nn.Parameter(torch.ones(1))\n\n    def forward(self, q, k, v):\n        qk = torch.matmul(q, k.transpose(-2, -1))\n        scaled_qk = qk.mul(self.scale_factor)\n        softmax_qk = scaled_qk.softmax(dim=-1)\n        dropout_qk = torch.nn.functional.dropout(softmax_qk, p=self.dropout_p)\n        output = dropout_qk.matmul(v)\n        return output\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nQ = torch.randn(1, 16, 64, 64)\nK = torch.randn(1, 16, 64, 64)\nV = torch.randn(1, 16, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, query, key, value, scale_factor=1./(10**2), dropout_p=.2):\n        super().__init__()\n        qk = torch.matmul(query, key.transpose(-2, -1))\n        scaled_qk = qk.mul(scale_factor)\n        softmax_qk = scaled_qk.softmax(dim=-1)\n        dropout_qk = torch.nn.functional.dropout(softmax_qk, p=dropout_p)\n        output = dropout_qk.matmul(value)\n        self.register_buffer('dropout_qk', dropout_qk)\n        self.register_buffer('value', value)\n \n    def forward(self, x):\n        scaled_qk = torch.matmul(x, self.dropout_qk.transpose(-2, -1))\n        output = scaled_qk.matmul(self.value)\n        return output\n\n# Inplace operation on constant\na = torch.tensor([1, 2, 3], requires_grad=True)\nb = torch.tensor([1.2, 2.3, 3.4], requires_grad=True)\nc = b.clone()\nb_id = id(b)\nc_id = id(c)\nb += 1\nc += float(1)\nprint('a:', a, 'b:', b, 'c:', c)\n\n# Inputs to the model\nx = torch.randn(1, 1, 16, 8)\nquery = torch.randn(1, 1, 16, 8)\nkey = torch.randn(1, 1, 8, 8)\nvalue = torch.randn(1, 1, 16, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.softmax = torch.nn.Softmax(dim=-1)\n \n    def forward(self, q1, k1):\n        s1 = torch.matmul(q1, k1.transpose(-2, -1))\n        s2 = s1 * 0.5\n        s3 = s2.transpose(-2, -1)\n        s4 = self.softmax(s3)\n        s5 = torch.nn.functional.dropout(s4, p=0.5)\n        v1 = torch.matmul(s5, v1)\n        return v1\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nq1 = torch.randn(8, 256, 64, 64)\nk1 = torch.randn(8, 256, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n \n    def forward(self, input1, input2, input3):\n        v1 = torch.matmul(input1, input2.transpose(-2, -1))\n        v2 = v1.mul(10)\n        v3 = x3.softmax(-1)\n        v4 = torch.nn.functional.dropout(v3, p=0.1)\n        return v4.matmul(input3)\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\ninput1 = torch.randn(50, 64)\ninput2 = torch.randn(64, 80)\ninput3 = torch.randn(80, 10)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n \n    def forward(self, query, key, value, dropout_p):\n        qk = torch.matmul(query, key.transpose(-2, -1))\n        scale_factor = float(query.size(-1)) ** -0.5\n        scaled_qk = qk.mul(scale_factor)\n        softmax_qk = scaled_qk.softmax(dim=-1)\n        dropout_qk = torch.nn.functional.dropout(softmax_qk, p=dropout_p)\n        output = dropout_qk.matmul(value)\n        return output\n \n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx3 = torch.randn(4, 512, 768)\nx4 = torch.randn(4, 512, 768)\nx5 = torch.randn(4, 512, 768)\ndropout_p = 0.98\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.scale_factor = #\n        self.dropout_p = #\n  \n    def forward(query, key, value):\n        v1 = torch.matmul(query, key.transpose(-2, -1))\n        v2 = v1 * self.scale_factor\n        v3 = torch.nn.functional.softmax(v2, dim=-1)\n        v4 = torch.nn.functional.dropout(v3, p=self.dropout_p)\n        v5 = torch.matmul(v4, value)\n        return v5\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nquery = torch.randn(1, 16, 9)\nkey = torch.randn(1, 16, 10)\nvalue = torch.randn(1, 16, 10)\n"
            ],
            "g_time": 12.790364265441895
        }
    }
}
{
    "opt": "unary=29",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min_value=-5.79773, max_value=-3.4646):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(32, 8, 3, stride=1, padding=1)\n        self.min_value = min_value\n        self.max_value = max_value\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = torch.clamp_min(v1, self.min_value)\n        v3 = torch.clamp_max(v2, self.max_value)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 32, 256, 512)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min_value=-2.1542, max_value=0):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(7, 15, 1, stride=1, padding=1, dilation=1, output_padding=0)\n        self.min_value = min_value\n        self.max_value = max_value\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = torch.clamp_min(v1, self.min_value)\n        v3 = torch.clamp_max(v2, self.max_value)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 7, 513, 512)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min_value=-1.3117, max_value=0):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(7, 10, 1, stride=1, padding=0)\n        self.min_value = min_value\n        self.max_value = max_value\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = torch.clamp_min(v1, self.min_value)\n        v3 = torch.clamp_max(v2, self.max_value)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 7, 512, 512)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min_value=-3.57328, max_value=4.38135):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(3, 6, 1, stride=3, padding=0)\n        self.min_value = min_value\n        self.max_value = max_value\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = torch.clamp_min(v1, self.min_value)\n        v3 = torch.clamp_max(v2, self.max_value)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 3, 63, 109)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min_value=-2, max_value=0):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(18, 10, 1, stride=3, padding=1)\n        self.min_value = min_value\n        self.max_value = max_value\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = torch.clamp_min(v1, self.min_value)\n        v3 = torch.clamp_max(v2, self.max_value)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 18, 1501, 1503)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min_value=-178, max_value=184):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(2, 7, 3, stride=3, padding=0)\n        self.min_value = min_value\n        self.max_value = max_value\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = torch.clamp_min(v1, self.min_value)\n        v3 = torch.clamp_max(v2, self.max_value)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 2, 7, 7)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min_value=-2.3, max_value=4.16525):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(1, 3, 1, stride=1, padding=1, dilation=3)\n        self.min_value = min_value\n        self.max_value = max_value\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = torch.clamp_min(v1, self.min_value)\n        v3 = torch.clamp_max(v2, self.max_value)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 1, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min_value=2.884316, max_value=3):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 8, 3, stride=1, padding=1)\n        self.conv2 = torch.nn.Conv2d(8, 13, 3, stride=2, padding=0)\n        self.min_value = min_value\n        self.max_value = max_value\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = self.conv2(v1)\n        v3 = torch.clamp_min(v2, self.min_value)\n        v4 = torch.clamp_max(v3, self.max_value)\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min_value=-0.059082031, max_value=1.021):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose1d(13, 15, 1, stride=1, padding=0, groups=13, bias=True)\n        self.min_value = min_value\n        self.max_value = max_value\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = torch.clamp_min(v1, self.min_value)\n        v3 = torch.clamp_max(v2, self.max_value)\n        return v3\n# Inputs to the model\nx1 = torch.randn(13, 1, 5)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min_value=0, max_value=3):\n        super().__init__()\n        self.min_value = min_value\n        self.max_value = max_value\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = torch.clamp_min(v1, self.min_value)\n        v3 = torch.clamp_max(v2, self.max_value)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 3, 8, 8)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min_value=-5.79773, max_value=-3.4646):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(32, 8, 3, stride=1, padding=1)\n        self.min_value = min_value\n        self.max_value = max_value\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = torch.clamp_min(v1, self.min_value)\n        v3 = torch.clamp_max(v2, self.max_value)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 32, 256, 512)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min_value=-2.1542, max_value=0):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(7, 15, 1, stride=1, padding=1, dilation=1, output_padding=0)\n        self.min_value = min_value\n        self.max_value = max_value\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = torch.clamp_min(v1, self.min_value)\n        v3 = torch.clamp_max(v2, self.max_value)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 7, 513, 512)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min_value=-1.3117, max_value=0):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(7, 10, 1, stride=1, padding=0)\n        self.min_value = min_value\n        self.max_value = max_value\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = torch.clamp_min(v1, self.min_value)\n        v3 = torch.clamp_max(v2, self.max_value)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 7, 512, 512)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min_value=-3.57328, max_value=4.38135):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(3, 6, 1, stride=3, padding=0)\n        self.min_value = min_value\n        self.max_value = max_value\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = torch.clamp_min(v1, self.min_value)\n        v3 = torch.clamp_max(v2, self.max_value)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 3, 63, 109)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min_value=-2, max_value=0):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(18, 10, 1, stride=3, padding=1)\n        self.min_value = min_value\n        self.max_value = max_value\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = torch.clamp_min(v1, self.min_value)\n        v3 = torch.clamp_max(v2, self.max_value)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 18, 1501, 1503)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min_value=-178, max_value=184):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(2, 7, 3, stride=3, padding=0)\n        self.min_value = min_value\n        self.max_value = max_value\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = torch.clamp_min(v1, self.min_value)\n        v3 = torch.clamp_max(v2, self.max_value)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 2, 7, 7)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min_value=-2.3, max_value=4.16525):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(1, 3, 1, stride=1, padding=1, dilation=3)\n        self.min_value = min_value\n        self.max_value = max_value\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = torch.clamp_min(v1, self.min_value)\n        v3 = torch.clamp_max(v2, self.max_value)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 1, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min_value=2.884316, max_value=3):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 8, 3, stride=1, padding=1)\n        self.conv2 = torch.nn.Conv2d(8, 13, 3, stride=2, padding=0)\n        self.min_value = min_value\n        self.max_value = max_value\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = self.conv2(v1)\n        v3 = torch.clamp_min(v2, self.min_value)\n        v4 = torch.clamp_max(v3, self.max_value)\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min_value=-0.059082031, max_value=1.021):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose1d(13, 15, 1, stride=1, padding=0, groups=13, bias=True)\n        self.min_value = min_value\n        self.max_value = max_value\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = torch.clamp_min(v1, self.min_value)\n        v3 = torch.clamp_max(v2, self.max_value)\n        return v3\n# Inputs to the model\nx1 = torch.randn(13, 1, 5)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min_value=0, max_value=3):\n        super().__init__()\n        self.min_value = min_value\n        self.max_value = max_value\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = torch.clamp_min(v1, self.min_value)\n        v3 = torch.clamp_max(v2, self.max_value)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 3, 8, 8)\n"
            ],
            "g_time": 8.179353713989258
        }
    }
}
{
    "opt": "unary=26",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_t = torch.nn.ConvTranspose2d(1, 3, 8, stride=4, dilation=2, padding=13)\n        self.negative_slope = 1.0\n    def forward(self, x1):\n        v1 = self.conv_t(x1)\n        v2 = v1 > 0\n        v3 = v1 * self.negative_slope\n        v4 = torch.where(v2, v1, v3)\n        return v4\n# Inputs to the model\nx1 = torch.randn(4, 1, 56, 56)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_t = torch.nn.ConvTranspose2d(35, 1, 4, stride=1, padding=1, bias=False)\n    def forward(self, x1):\n        v1 = self.conv_t(x1)\n        v2 = v1 > 0\n        v3 = v1 * -10.398\n        v4 = torch.where(v2, v1, v3)\n        return v4\n# Inputs to the model\nx1 = torch.randn(4, 35, 22, 99)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_t = torch.nn.ConvTranspose2d(1, 1, 3, stride=1, padding=2, bias=False)\n    def forward(self, x):\n        x1 = self.conv_t(x)\n        x2 = x1 > 0\n        x3 = x1 * -0.398\n        x4 = torch.where(x2, x1, x3)\n        return x4\n# Inputs to the model\nx = torch.randn(8, 1, 90, 80)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.conv = nn.ConvTranspose2d(1, 3, 3, stride=1, padding=0, output_padding=0, groups=1, bias=False)\n    def forward(self, x):\n        x = self.conv(x)\n        x = x > 0\n        x = x * -0.009009742276873541\n        x = torch.where(x, x, x)\n        return x\n# Inputs to the model\nx = torch.randn(2, 1, 4, 4)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_t = torch.nn.ConvTranspose2d(4, 128, 7, stride=2, padding=2, bias=False)\n    def forward(self, x3):\n        x1 = self.conv_t(x3)\n        x2 = x1 > 0\n        x3 = x1 * -4.94\n        x4 = torch.where(x2, x1, x3)\n        return x4\n# Inputs to the model\nx3 = torch.randn(2, 4, 35, 42)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_t = torch.nn.ConvTranspose2d(11, 17, 7, padding=2)\n    def forward(self, x):\n        v1 = self.conv_t(x)\n        v2 = v1 > 0\n        v3 = v1 * 127.12\n        v4 = torch.where(v2, v1, v3)\n        return v4\n# Inputs to the model\nx = torch.randn(4, 11, 20, 59)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_t = torch.nn.ConvTranspose2d(165, 163, 8, stride=1, padding=0, bias=False)\n    def forward(self, x2):\n        v1 = self.conv_t(x2)\n        v2 = v1 > 0\n        v3 = v1 * -0.5\n        v4 = torch.where(v2, v1, v3)\n        return v4\n# Inputs to the model\nx2 = torch.randn(45, 165, 8, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_t = torch.nn.ConvTranspose2d(33, 23, 3, stride=7, padding=1)\n        self.negative_slope = 0.0625\n    def forward(self, x2):\n        x1 = self.conv_t(x2)\n        x2 = x1 > 0\n        x3 = x1 * self.negative_slope\n        x4 = torch.where(x2, x1, x3)\n        return x4\n# Inputs to the model\nx2 = torch.randn(7, 33, 20, 25, device='cuda')\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.negative_slope = 0.42\n        self.conv_t = torch.nn.ConvTranspose2d(1, 5, 9, stride=4)\n    def forward(self, x):\n        v1 = self.conv_t(x)\n        v2 = v1 > 0\n        v3 = v1 * self.negative_slope\n        v4 = torch.where(v2, v1, v3)\n        return v4\n# Inputs to the model\nx = torch.randn(4, 1, 1, 1, device='cuda')\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(1, 32, 5, stride=1)\n        self.conv_t = torch.nn.ConvTranspose2d(32, 16, 5, stride=1)\n    def forward(self, x1):\n        x2 = self.conv(x1)\n        x3 = self.conv_t(x2)\n        x4 = x3 > 0\n        x5 = x3 * -10.398\n        x6 = torch.where(x4, x3, x5)\n        return x6\n# Inputs to the model\nx1 = torch.randn(2, 1, 56, 56)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_t = torch.nn.ConvTranspose2d(1, 3, 8, stride=4, dilation=2, padding=13)\n        self.negative_slope = 1.0\n    def forward(self, x1):\n        v1 = self.conv_t(x1)\n        v2 = v1 > 0\n        v3 = v1 * self.negative_slope\n        v4 = torch.where(v2, v1, v3)\n        return v4\n# Inputs to the model\nx1 = torch.randn(4, 1, 56, 56)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_t = torch.nn.ConvTranspose2d(35, 1, 4, stride=1, padding=1, bias=False)\n    def forward(self, x1):\n        v1 = self.conv_t(x1)\n        v2 = v1 > 0\n        v3 = v1 * -10.398\n        v4 = torch.where(v2, v1, v3)\n        return v4\n# Inputs to the model\nx1 = torch.randn(4, 35, 22, 99)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_t = torch.nn.ConvTranspose2d(1, 1, 3, stride=1, padding=2, bias=False)\n    def forward(self, x):\n        x1 = self.conv_t(x)\n        x2 = x1 > 0\n        x3 = x1 * -0.398\n        x4 = torch.where(x2, x1, x3)\n        return x4\n# Inputs to the model\nx = torch.randn(8, 1, 90, 80)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.conv = nn.ConvTranspose2d(1, 3, 3, stride=1, padding=0, output_padding=0, groups=1, bias=False)\n    def forward(self, x):\n        x = self.conv(x)\n        x = x > 0\n        x = x * -0.009009742276873541\n        x = torch.where(x, x, x)\n        return x\n# Inputs to the model\nx = torch.randn(2, 1, 4, 4)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_t = torch.nn.ConvTranspose2d(4, 128, 7, stride=2, padding=2, bias=False)\n    def forward(self, x3):\n        x1 = self.conv_t(x3)\n        x2 = x1 > 0\n        x3 = x1 * -4.94\n        x4 = torch.where(x2, x1, x3)\n        return x4\n# Inputs to the model\nx3 = torch.randn(2, 4, 35, 42)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_t = torch.nn.ConvTranspose2d(11, 17, 7, padding=2)\n    def forward(self, x):\n        v1 = self.conv_t(x)\n        v2 = v1 > 0\n        v3 = v1 * 127.12\n        v4 = torch.where(v2, v1, v3)\n        return v4\n# Inputs to the model\nx = torch.randn(4, 11, 20, 59)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_t = torch.nn.ConvTranspose2d(165, 163, 8, stride=1, padding=0, bias=False)\n    def forward(self, x2):\n        v1 = self.conv_t(x2)\n        v2 = v1 > 0\n        v3 = v1 * -0.5\n        v4 = torch.where(v2, v1, v3)\n        return v4\n# Inputs to the model\nx2 = torch.randn(45, 165, 8, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_t = torch.nn.ConvTranspose2d(33, 23, 3, stride=7, padding=1)\n        self.negative_slope = 0.0625\n    def forward(self, x2):\n        x1 = self.conv_t(x2)\n        x2 = x1 > 0\n        x3 = x1 * self.negative_slope\n        x4 = torch.where(x2, x1, x3)\n        return x4\n# Inputs to the model\nx2 = torch.randn(7, 33, 20, 25, device='cuda')\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.negative_slope = 0.42\n        self.conv_t = torch.nn.ConvTranspose2d(1, 5, 9, stride=4)\n    def forward(self, x):\n        v1 = self.conv_t(x)\n        v2 = v1 > 0\n        v3 = v1 * self.negative_slope\n        v4 = torch.where(v2, v1, v3)\n        return v4\n# Inputs to the model\nx = torch.randn(4, 1, 1, 1, device='cuda')\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(1, 32, 5, stride=1)\n        self.conv_t = torch.nn.ConvTranspose2d(32, 16, 5, stride=1)\n    def forward(self, x1):\n        x2 = self.conv(x1)\n        x3 = self.conv_t(x2)\n        x4 = x3 > 0\n        x5 = x3 * -10.398\n        x6 = torch.where(x4, x3, x5)\n        return x6\n# Inputs to the model\nx1 = torch.randn(2, 1, 56, 56)\n"
            ],
            "g_time": 6.992393493652344
        }
    }
}
{
    "opt": "replace_fx",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1):\n        x2 = F.dropout(x1, p=0.5)\n        x3 = torch.rand_like(x1)\n        x4 = F.dropout(x3, training=False)\n        return x4\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1):\n        x2 = F.dropout(x1, p=0.5)\n        x3 = torch.rand_like(x1)\n        x4 = F.dropout(x3)\n        return x4\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x:torch.Tensor):\n        return x.view(1, 2, 3),\n# Inputs to the model\nx = torch.randn(2, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1):\n        x2 = F.dropout(x1, p=0.5)\n        x3 = x1\n        return x3\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1):\n        x2 = F.dropout(x1, p=0.5)\n        tmp = x2\n        return x2\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\n",
                ", inputs are arbitrary\nclass MyModel(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, t1):\n        t0 = torch.nn.functional.dropout(t1)\n        t2 = torch.rand_like(t0)\n        t3 = torch.nn.functional.dropout(t2)\n        return t3\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layer1 = nn.Linear(2, 2)\n        self.layer2 = nn.Linear(2, 2)\n    \n    def forward(self, x):\n        x = F.dropout(F.gelu(self.layer1(x)))\n        x = F.dropout(self.layer2(x), training=self.training)\n        return x\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1):\n        x2 = F.dropout(x1, p=0.5)\n        t1 = torch.rand_like(x1)\n        t2 = torch.rand_like(t1)\n        x4 = F.dropout(t2)\n        return x4\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1):\n        x2 = F.dropout(x1, p=0.5)\n        t = torch.rand_like(x1)\n        return t\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1):\n        x2 = F.dropout(x1, p=0.5)\n        x3 = F.dropout(x1)\n        x3 = F.dropout(x1)\n        x4 = torch.rand_like(x2)\n        x5 = torch.rand_like(x3)\n        x6 = F.dropout(x5)\n        x7 = F.dropout(x5)\n        return x6, x7\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1):\n        x2 = F.dropout(x1, p=0.5)\n        x3 = torch.rand_like(x2)\n        x4 = F.dropout(x3)\n        return x4\n# Inputs to the model\nx1 = torch.randn(1, 2, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x):\n        x = torch.nn.functional.dropout(x, p=0.5)\n        t =  F.conv1d(x, x)\n        x = torch.rand_like(x)\n        x = torch.nn.functional.dropout(x, p=0.3)\n        return x\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\n",
                "\nimport torch.nn.functional as F\nclass MyModule(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x):\n        x = F.dropout(x, p=0.2, training=False)\n        x = torch.rand_like(x)\n        return x\n# Inputs to the model\nx1 = torch.randn(2, 3)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1):\n        x2 = F.dropout(x1, p=0.5)\n        x3 = torch.rand_like(x1)\n        x4 = F.dropout(x3, training=False)\n        return x4\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1):\n        x2 = F.dropout(x1, p=0.5)\n        x3 = torch.rand_like(x1)\n        x4 = F.dropout(x3)\n        return x4\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x:torch.Tensor):\n        return x.view(1, 2, 3),\n# Inputs to the model\nx = torch.randn(2, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1):\n        x2 = F.dropout(x1, p=0.5)\n        x3 = x1\n        return x3\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1):\n        x2 = F.dropout(x1, p=0.5)\n        tmp = x2\n        return x2\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\n",
                ", inputs are arbitrary\nclass MyModel(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, t1):\n        t0 = torch.nn.functional.dropout(t1)\n        t2 = torch.rand_like(t0)\n        t3 = torch.nn.functional.dropout(t2)\n        return t3\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layer1 = nn.Linear(2, 2)\n        self.layer2 = nn.Linear(2, 2)\n    \n    def forward(self, x):\n        x = F.dropout(F.gelu(self.layer1(x)))\n        x = F.dropout(self.layer2(x), training=self.training)\n        return x\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1):\n        x2 = F.dropout(x1, p=0.5)\n        t1 = torch.rand_like(x1)\n        t2 = torch.rand_like(t1)\n        x4 = F.dropout(t2)\n        return x4\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1):\n        x2 = F.dropout(x1, p=0.5)\n        t = torch.rand_like(x1)\n        return t\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1):\n        x2 = F.dropout(x1, p=0.5)\n        x3 = F.dropout(x1)\n        x3 = F.dropout(x1)\n        x4 = torch.rand_like(x2)\n        x5 = torch.rand_like(x3)\n        x6 = F.dropout(x5)\n        x7 = F.dropout(x5)\n        return x6, x7\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1):\n        x2 = F.dropout(x1, p=0.5)\n        x3 = torch.rand_like(x2)\n        x4 = F.dropout(x3)\n        return x4\n# Inputs to the model\nx1 = torch.randn(1, 2, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x):\n        x = torch.nn.functional.dropout(x, p=0.5)\n        t =  F.conv1d(x, x)\n        x = torch.rand_like(x)\n        x = torch.nn.functional.dropout(x, p=0.3)\n        return x\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\n",
                "\nimport torch.nn.functional as F\nclass MyModule(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x):\n        x = F.dropout(x, p=0.2, training=False)\n        x = torch.rand_like(x)\n        return x\n# Inputs to the model\nx1 = torch.randn(2, 3)\n"
            ],
            "g_time": 6.263725280761719
        }
    }
}
{
    "opt": "unary=19",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(123, 456)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.sigmoid(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 123)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(224, 1000)\n \n    def forward(self, x2):\n        v1 = self.linear(x2)\n        v2 = torch.sigmoid(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx2 = torch.randn(1, 224)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.sigmoid(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(10, 1)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.sigmoid(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(100, 10)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(10, 1)\n        self.dropout = torch.nn.Dropout(0.8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.sigmoid(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 10)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(128, 32)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.sigmoid(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 128)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(4, 1)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.sigmoid(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 4)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 5)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.sigmoid(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.linear = torch.nn.Linear(3, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.sigmoid(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(100, 100)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.sigmoid(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 100)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(123, 456)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.sigmoid(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 123)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(224, 1000)\n \n    def forward(self, x2):\n        v1 = self.linear(x2)\n        v2 = torch.sigmoid(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx2 = torch.randn(1, 224)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.sigmoid(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(10, 1)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.sigmoid(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(100, 10)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(10, 1)\n        self.dropout = torch.nn.Dropout(0.8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.sigmoid(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 10)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(128, 32)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.sigmoid(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 128)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(4, 1)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.sigmoid(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 4)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 5)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.sigmoid(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.linear = torch.nn.Linear(3, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.sigmoid(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(100, 100)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.sigmoid(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 100)\n"
            ],
            "g_time": 4.834307909011841
        }
    }
}
{
    "opt": "linear_permute_fusion",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(32, 32)\n    def forward(self, x1):\n        y = x1\n        v5 = torch.nn.functional.linear(y, self.linear.weight, self.linear.bias)\n        v4 = v5.permute(0, 2, 1)\n        v3 = torch.nn.functional.batch_norm(v4, None, None, self.layernorm.weight, self.layernorm.bias, True, 0.019999999552965164, 1.0000000000000002e-05).contiguous()\n        v1 = torch.nn.functional.layer_norm(v3, v3.size(1), self.layernorm.weight, self.layernorm.bias, 1e-05, True).contiguous()\n        v2 = v1.permute(0, 2, 1)\n        return v1\n\n# Inputs to the model\nx1 = torch.randn(3, 2, 2, device='cpu')\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear1 = torch.nn.Linear(2, 2) # Input size and hidden dimension of the layer 1\n        self.linear2 = torch.nn.Linear(2, 2) # Hidden dimension of the layer 1 and hidden dimension of the layer 2\n    def forward(self, x1, x2):\n        v1 = torch.nn.functional.linear(x1, self.linear1.weight, self.linear1.bias)\n        v2 = v1.permute(0, 2, 1)\n        t1 = torch.nn.functional.linear(v2, self.linear2.weight, self.linear2.bias)\n        out = t1 + x2\n        return out\n# Inputs to the model\nw = 2;\nx1 = torch.randn(2, 2, w, w)\nx2 = torch.randn(2, 2, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        #self.linear = torch.nn.Linear(2, 2, 1e-3)\n    def forward(self, x1, x2):\n        v1a = torch.nn.functional.linear(x1, x2)\n        v1b = torch.nn.functional.linear(x2, x1[:1])\n        v1c = v1a.expand(v1b.size()) + v1b\n        return v1c\n# Inputs to the model\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 3, 2, 2, 1, groups=2)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        return torch.mean(v1, dim=(2, 3))\n# Inputs to the model\nx1 = torch.randn(1, 3, 8, 8, device='cpu')\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(2, 2, bias=False)\n    def forward(self, x1):\n        v1 = torch.cat([torch.stack([torch.zeros_like(x1) for i in range(torch.numel(x1))], dim=0) for i in range(torch.numel(x1))])\n        v1 = torch.stack([v1 for i in range(torch.numel(x1))])\n        v1 = torch.rand(2 * torch.numel(x1), device='cpu')\n        v1 = v1 - v1\n        v2 = v1.view(2, 2, torch.numel(x1), 1)\n        v2 = torch.stack([v2.permute(1, 0, 2, 3) for i in range(2)])\n        v3 = torch.stack([v2.view(4, torch.numel(x1), 1) for i in range(2)])\n        v4 = v3.permute(2, 1, 0)\n        v4 = v4.permute(0, 2, 1)\n        return v4\n# Inputs to the model\nx1 = torch.randn(2, 2, 2, device='cpu')\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(10, 10)\n    def forward(self, x1):\n        v1 = torch.tanh(x1)\n        v2 = torch.nn.functional.relu(v1).permute(0, 3, 2, 1)\n        v3 = torch.clamp(v2, min=-2, max=2)\n        return torch.nn.functional.sigmoid(torch.nn.functional.linear(v3, self.linear.weight, self.linear.bias))\n# Inputs to the model\nx1 = torch.randn(2, 10, 10, 10)\n",
                "\nw = 3\nx = torch.randn(w, 2, w)\ny = torch.nn.functional.linear(x, torch.zeros([w, w], device='cpu'), bias=None)\n# Inputs to the model\nx1 = torch.randn(w, w, w)\ny = torch.nn.functional.linear(x, x1, bias=None)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(2, 2)\n    def forward(self, x1):\n        y = x1\n        v1 = torch.nn.functional.linear(y, self.linear.weight, self.linear.bias)\n        v2 = v1.permute(0, 2, 1)\n        w = v2.size(1)\n        h = v2.size(2)\n        v3 = v2.reshape(v2.size(0), w * h)\n        return v1\n# Inputs to the model\nx1 = torch.randn(3, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear1 = torch.nn.Linear(2, 2)\n        self.linear2 = torch.nn.Linear(2, 2)\n    def forward(self, x1):\n        v2 = x1\n        v1 = torch.nn.functional.linear(v2, self.linear1.weight, self.linear1.bias)\n        v2 = v1.permute(0, 2, 1)\n        v2 = torch.nn.functional.linear(v2, self.linear2.weight, self.linear2.bias)\n        return v1\n# Inputs to the model\nx1 = torch.randn(3, 2, 2, device='cpu')\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(2, 2)\n    def forward(self, x1):\n        y = x1\n        v1 = torch.nn.functional.linear(y, self.linear.weight, self.linear.bias)\n        v2 = v1.permute(0, 2, 1)\n        w = v2.size(1)\n        h = v2.size(2)\n        v3 = v2.contiguous().view(v2.size(0), w * h)\n        v4 = torch.nn.functional.linear(v3, self.linear.weight, self.linear.bias)\n        w = v4.size(1)\n        h = v4.size(2)\n        v5 = v4.contiguous().view(v4.size(0), w * h)\n        return v5\n# Inputs to the model\nx1 = torch.randn(3, 2, 2)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(32, 32)\n    def forward(self, x1):\n        y = x1\n        v5 = torch.nn.functional.linear(y, self.linear.weight, self.linear.bias)\n        v4 = v5.permute(0, 2, 1)\n        v3 = torch.nn.functional.batch_norm(v4, None, None, self.layernorm.weight, self.layernorm.bias, True, 0.019999999552965164, 1.0000000000000002e-05).contiguous()\n        v1 = torch.nn.functional.layer_norm(v3, v3.size(1), self.layernorm.weight, self.layernorm.bias, 1e-05, True).contiguous()\n        v2 = v1.permute(0, 2, 1)\n        return v1\n\n# Inputs to the model\nx1 = torch.randn(3, 2, 2, device='cpu')\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear1 = torch.nn.Linear(2, 2) # Input size and hidden dimension of the layer 1\n        self.linear2 = torch.nn.Linear(2, 2) # Hidden dimension of the layer 1 and hidden dimension of the layer 2\n    def forward(self, x1, x2):\n        v1 = torch.nn.functional.linear(x1, self.linear1.weight, self.linear1.bias)\n        v2 = v1.permute(0, 2, 1)\n        t1 = torch.nn.functional.linear(v2, self.linear2.weight, self.linear2.bias)\n        out = t1 + x2\n        return out\n# Inputs to the model\nw = 2;\nx1 = torch.randn(2, 2, w, w)\nx2 = torch.randn(2, 2, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        #self.linear = torch.nn.Linear(2, 2, 1e-3)\n    def forward(self, x1, x2):\n        v1a = torch.nn.functional.linear(x1, x2)\n        v1b = torch.nn.functional.linear(x2, x1[:1])\n        v1c = v1a.expand(v1b.size()) + v1b\n        return v1c\n# Inputs to the model\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 3, 2, 2, 1, groups=2)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        return torch.mean(v1, dim=(2, 3))\n# Inputs to the model\nx1 = torch.randn(1, 3, 8, 8, device='cpu')\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(2, 2, bias=False)\n    def forward(self, x1):\n        v1 = torch.cat([torch.stack([torch.zeros_like(x1) for i in range(torch.numel(x1))], dim=0) for i in range(torch.numel(x1))])\n        v1 = torch.stack([v1 for i in range(torch.numel(x1))])\n        v1 = torch.rand(2 * torch.numel(x1), device='cpu')\n        v1 = v1 - v1\n        v2 = v1.view(2, 2, torch.numel(x1), 1)\n        v2 = torch.stack([v2.permute(1, 0, 2, 3) for i in range(2)])\n        v3 = torch.stack([v2.view(4, torch.numel(x1), 1) for i in range(2)])\n        v4 = v3.permute(2, 1, 0)\n        v4 = v4.permute(0, 2, 1)\n        return v4\n# Inputs to the model\nx1 = torch.randn(2, 2, 2, device='cpu')\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(10, 10)\n    def forward(self, x1):\n        v1 = torch.tanh(x1)\n        v2 = torch.nn.functional.relu(v1).permute(0, 3, 2, 1)\n        v3 = torch.clamp(v2, min=-2, max=2)\n        return torch.nn.functional.sigmoid(torch.nn.functional.linear(v3, self.linear.weight, self.linear.bias))\n# Inputs to the model\nx1 = torch.randn(2, 10, 10, 10)\n",
                "\nw = 3\nx = torch.randn(w, 2, w)\ny = torch.nn.functional.linear(x, torch.zeros([w, w], device='cpu'), bias=None)\n# Inputs to the model\nx1 = torch.randn(w, w, w)\ny = torch.nn.functional.linear(x, x1, bias=None)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(2, 2)\n    def forward(self, x1):\n        y = x1\n        v1 = torch.nn.functional.linear(y, self.linear.weight, self.linear.bias)\n        v2 = v1.permute(0, 2, 1)\n        w = v2.size(1)\n        h = v2.size(2)\n        v3 = v2.reshape(v2.size(0), w * h)\n        return v1\n# Inputs to the model\nx1 = torch.randn(3, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear1 = torch.nn.Linear(2, 2)\n        self.linear2 = torch.nn.Linear(2, 2)\n    def forward(self, x1):\n        v2 = x1\n        v1 = torch.nn.functional.linear(v2, self.linear1.weight, self.linear1.bias)\n        v2 = v1.permute(0, 2, 1)\n        v2 = torch.nn.functional.linear(v2, self.linear2.weight, self.linear2.bias)\n        return v1\n# Inputs to the model\nx1 = torch.randn(3, 2, 2, device='cpu')\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(2, 2)\n    def forward(self, x1):\n        y = x1\n        v1 = torch.nn.functional.linear(y, self.linear.weight, self.linear.bias)\n        v2 = v1.permute(0, 2, 1)\n        w = v2.size(1)\n        h = v2.size(2)\n        v3 = v2.contiguous().view(v2.size(0), w * h)\n        v4 = torch.nn.functional.linear(v3, self.linear.weight, self.linear.bias)\n        w = v4.size(1)\n        h = v4.size(2)\n        v5 = v4.contiguous().view(v4.size(0), w * h)\n        return v5\n# Inputs to the model\nx1 = torch.randn(3, 2, 2)\n"
            ],
            "g_time": 9.898905754089355
        }
    }
}
{
    "opt": "unary=20",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_t = torch.nn.ConvTranspose3d(3, 3, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), output_padding=(1, 1, 1))\n    def forward(self, x1):\n        v1 = self.conv_t(x1)\n        v2 = torch.sigmoid(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 3, 224, 224, 224)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.softmax = torch.nn.Softmax(dim=-1)\n        self.conv2d = torch.nn.Conv2d(in_channels=3, out_channels=3, kernel_size=3, stride=1, padding=0)\n        self.conv_transpose_1 = torch.nn.ConvTranspose2d(in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1, output_padding=0)\n        self.conv_transpose_2 = torch.nn.ConvTranspose2d(in_channels=32, out_channels=32, kernel_size=3, stride=2, padding=1, output_padding=1)\n        self.conv_transpose_3 = torch.nn.ConvTranspose2d(in_channels=32, out_channels=32, kernel_size=3, stride=2, padding=1, output_padding=1)\n    def forward(self, x1):\n        v1 = self.softmax(x1)\n        v2 = self.conv2d(v1)\n        v3 = self.conv_transpose_1(v2)\n        v4 = self.conv_transpose_2(v3)\n        v5 = self.conv_transpose_3(v4)\n        v6 = self.softmax(v5)\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 3, 108, 108)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.bn = torch.nn.BatchNorm2d(num_features=6)\n        self.conv_transpose = torch.nn.ConvTranspose2d(in_channels=6, out_channels=6, kernel_size=(3, 3), stride=(1, 2), padding=(2, 1), groups=1)\n    def forward(self, x1):\n        v1 = self.bn(x1)\n        v2 = self.conv_transpose(v1)\n        v3 = torch.sigmoid(v2)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 6, 32, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv2d = torch.nn.Conv2d(3, 8, kernel_size=(1, 1), stride=(1, 1))\n        self.conv_t = torch.nn.ConvTranspose2d(8, 10, kernel_size=(3, 3), stride=(2, 2))\n    def forward(self, x1):\n        v1 = self.conv2d(x1)\n        v2 = self.conv_t(v1)\n        v3 = torch.sigmoid(v2)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 3, 10, 10)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_t = torch.nn.ConvTranspose2d(3, 3, kernel_size=(3, 3), bias=True, stride=1)\n    def forward(self, x1):\n        v1 = self.conv_t(x1)\n        v2 = torch.sigmoid(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 3, 24, 24)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv2d = torch.nn.Conv2d(in_channels=3, out_channels=3, kernel_size=3, stride=1, padding=0)\n        self.conv_transpose = torch.nn.ConvTranspose2d(in_channels=3, out_channels=3, kernel_size=3, stride=2, padding=0, output_padding=0)\n    def forward(self, x1):\n        v1 = self.conv2d(x1)\n        v2 = self.conv_transpose(v1)\n        v3 = torch.sigmoid(v2)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 3, 224, 256)\n",
                "\nclass Model(torch.nn.Module):\n    def forward(self, x1):\n        v1 = torch.nn.functional.conv_transpose2d(x1, None, kernel_size=(9, 9), stride=(2, 2), padding=(1, 1))\n        v2 = torch.sigmoid(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 16, 14, 14)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv2d = torch.nn.Conv2d(in_channels=1,out_channels=8,kernel_size=(3,3),stride=1,padding=0)\n        self.conv_transpose = torch.nn.ConvTranspose2d(in_channels=8,out_channels=16,kernel_size=3,stride=2,padding=1,output_padding=0,bias=False)\n    def forward(self, x1):\n        v1 = self.conv2d(x1)\n        v2 = self.conv_transpose(v1)\n        v3 = torch.sigmoid(v2)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 1, 128, 128)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.t = torch.nn.ConvTranspose2d(in_channels=3, out_channels=3, bias=False, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n        self.relu = torch.nn.ReLU(inplace=True)\n    def forward(self, x1):\n        v1 = self.t(x1)\n        v2 = self.relu(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 3, 224, 256)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(in_channels=3, out_channels=3, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = torch.softmax(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 3, 224, 256)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_t = torch.nn.ConvTranspose3d(3, 3, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), output_padding=(1, 1, 1))\n    def forward(self, x1):\n        v1 = self.conv_t(x1)\n        v2 = torch.sigmoid(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 3, 224, 224, 224)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.softmax = torch.nn.Softmax(dim=-1)\n        self.conv2d = torch.nn.Conv2d(in_channels=3, out_channels=3, kernel_size=3, stride=1, padding=0)\n        self.conv_transpose_1 = torch.nn.ConvTranspose2d(in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1, output_padding=0)\n        self.conv_transpose_2 = torch.nn.ConvTranspose2d(in_channels=32, out_channels=32, kernel_size=3, stride=2, padding=1, output_padding=1)\n        self.conv_transpose_3 = torch.nn.ConvTranspose2d(in_channels=32, out_channels=32, kernel_size=3, stride=2, padding=1, output_padding=1)\n    def forward(self, x1):\n        v1 = self.softmax(x1)\n        v2 = self.conv2d(v1)\n        v3 = self.conv_transpose_1(v2)\n        v4 = self.conv_transpose_2(v3)\n        v5 = self.conv_transpose_3(v4)\n        v6 = self.softmax(v5)\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 3, 108, 108)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.bn = torch.nn.BatchNorm2d(num_features=6)\n        self.conv_transpose = torch.nn.ConvTranspose2d(in_channels=6, out_channels=6, kernel_size=(3, 3), stride=(1, 2), padding=(2, 1), groups=1)\n    def forward(self, x1):\n        v1 = self.bn(x1)\n        v2 = self.conv_transpose(v1)\n        v3 = torch.sigmoid(v2)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 6, 32, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv2d = torch.nn.Conv2d(3, 8, kernel_size=(1, 1), stride=(1, 1))\n        self.conv_t = torch.nn.ConvTranspose2d(8, 10, kernel_size=(3, 3), stride=(2, 2))\n    def forward(self, x1):\n        v1 = self.conv2d(x1)\n        v2 = self.conv_t(v1)\n        v3 = torch.sigmoid(v2)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 3, 10, 10)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_t = torch.nn.ConvTranspose2d(3, 3, kernel_size=(3, 3), bias=True, stride=1)\n    def forward(self, x1):\n        v1 = self.conv_t(x1)\n        v2 = torch.sigmoid(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 3, 24, 24)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv2d = torch.nn.Conv2d(in_channels=3, out_channels=3, kernel_size=3, stride=1, padding=0)\n        self.conv_transpose = torch.nn.ConvTranspose2d(in_channels=3, out_channels=3, kernel_size=3, stride=2, padding=0, output_padding=0)\n    def forward(self, x1):\n        v1 = self.conv2d(x1)\n        v2 = self.conv_transpose(v1)\n        v3 = torch.sigmoid(v2)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 3, 224, 256)\n",
                "\nclass Model(torch.nn.Module):\n    def forward(self, x1):\n        v1 = torch.nn.functional.conv_transpose2d(x1, None, kernel_size=(9, 9), stride=(2, 2), padding=(1, 1))\n        v2 = torch.sigmoid(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 16, 14, 14)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv2d = torch.nn.Conv2d(in_channels=1,out_channels=8,kernel_size=(3,3),stride=1,padding=0)\n        self.conv_transpose = torch.nn.ConvTranspose2d(in_channels=8,out_channels=16,kernel_size=3,stride=2,padding=1,output_padding=0,bias=False)\n    def forward(self, x1):\n        v1 = self.conv2d(x1)\n        v2 = self.conv_transpose(v1)\n        v3 = torch.sigmoid(v2)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 1, 128, 128)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.t = torch.nn.ConvTranspose2d(in_channels=3, out_channels=3, bias=False, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n        self.relu = torch.nn.ReLU(inplace=True)\n    def forward(self, x1):\n        v1 = self.t(x1)\n        v2 = self.relu(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 3, 224, 256)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(in_channels=3, out_channels=3, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = torch.softmax(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 3, 224, 256)\n"
            ],
            "g_time": 11.25589108467102
        }
    }
}
{
    "opt": "permute_linear_fusion",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(2, 2)\n    def forward(self, x1):\n        v1 = x1.permute(0, 2, 1)\n        v2 = torch.nn.functional.linear(v1, self.linear.weight, self.linear.bias)\n        v3 = v2.expand(1, 2, 2)\n        v3 = torch.nn.functional.elu(v3)\n        x2 = torch.nn.functional.dropout(v3)\n        x2 = torch.max(x2, dim=-1, keepdim=True)[0]\n        return x2\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(2, 2)\n        self.linear = lambda x: torch.nn.functional.linear(x, self.linear.weight, self.linear.bias)\n    def forward(self, x1):\n        v1 = x1.permute(0, 2, 1)\n        v2 = self.linear(v1)\n        v2 = v2.detach()\n        v3 = torch.max(v2, dim=-1)[0]\n        v4 = torch.max(v2, dim=-1)[1]\n        v4 = v4.unsqueeze(dim=0)\n        v5 = v2.shape[0]\n        v4 = v4.repeat(v5, 1)\n        v3 = v3.unsqueeze(dim=-1)\n        v3 = v3.repeat(1, 1, 2)\n        v3 = v3 + v4\n        v3 = v3 > 0\n        v4 = v3.to(x1.dtype)\n        return (v2 * v4)\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(2, 2)\n        self.linear1 = torch.nn.Linear(2, 1)\n        self.linear2 = torch.nn.Linear(1, 1)\n        self.linear3 = torch.nn.Linear(2, 2)\n        self.linear4 = torch.nn.Linear(2, 1)\n        self.linear5 = torch.nn.Linear(1, 1)\n    def forward(self, x1):\n        v1 = x1.permute(0, 2, 1)\n        v2 = torch.nn.functional.linear(v1, self.linear.weight, self.linear.bias)\n        v2 = v2.tanh()\n        v3 = torch.sigmoid(torch.nn.functional.linear(v2, self.linear1.weight, self.linear1.bias))\n        v4 = torch.nn.functional.linear(v2, self.linear2.weight, torch.cat((self.linear2.bias, torch.tensor([1]).to(self.linear2.bias)), dim=0))\n        v4 = torch.sigmoid(v4)\n        v5 = torch.cat((v3, v4), dim=-1)\n        v5 = v5.unsqueeze(dim=-1)\n        v5 = v5.expand(1, 1, 8)\n        v6 = v2.expand(1, 2, 2) * v5\n        v7 = torch.sigmoid(torch.nn.functional.linear(v6, self.linear3.weight, self.linear3.bias))\n        v6 = torch.sigmoid(torch.nn.functional.linear(v6, self.linear4.weight, torch.cat((self.linear4.bias, torch.tensor([1]).to(self.linear4.bias)), dim=0)))\n        v8 = v6 * v7\n        v8 = torch.nn.functional.linear(v8, self.linear5.weight, self.linear5.bias)\n        return v8\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(2, 2)\n    def forward(self, x1):\n        v1 = x1.permute(0, 2, 1)\n        v2 = torch.nn.functional.linear(v1, self.linear.weight, self.linear.bias)\n        v3 = v2.reshape([1, 2, 2])\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(2, 2)\n    def forward(self, x1):\n        v1 = x1.permute(0, 2, 1)\n        v2 = torch.nn.functional.linear(v1, self.linear.weight, self.linear.bias)\n        v3 = torch.max(v2, dim=-1)[1]\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 4)\n        self.linear1 = torch.nn.Linear(4, 2)\n    def forward(self, x1):\n\tv1 = x1.permute(0, 2, 1)\n\tv2 = torch.nn.functional.linear(v1, self.linear.weight, self.linear.bias)\n\tv2 = torch.nn.functional.relu(v2)\n\tv3 = torch.arange(v2.shape[0]).to(dtype=torch.long)\n\tv4 = v2[v3, :]\n\tv5 = self.linear1(v4)\n\tv5 = torch.min(v5, dim=-1)[0]\n\tv5 = v5.unsqueeze(dim=-1)\n        return v5\n# Inputs to the model\nx1 = torch.randn(32, 3, 4)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(2, 2)\n    def forward(self, x1):\n        v1 = x1.permute(0, 2, 1)\n        v2 = torch.nn.functional.linear(v1, self.linear.weight, self.linear.bias)\n        x2 = torch.nn.functional.relu(x1 + v2)\n        v3 = x2.detach()\n        v4 = torch.max(v3, dim=-1)[1]\n        v4 = v4.unsqueeze(dim=-1)\n        v3 = v3 + v4.to(v3.dtype)\n        v4 = (v3 == -1).to(v3.dtype)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1):\n        v2 = torch.nn.functional.grid_sample(x1, torch.rand(1, 2, 2, 2))\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 1, 2, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(2, 2)\n    def forward(self, x1):\n        v1 = x1.permute(0, 2, 1)\n        v2 = torch.nn.functional.linear(v1, self.linear.weight, self.linear.bias)\n        x2 = torch.nn.functional.relu(v2)\n        v3 = x2.detach()\n        v4 = torch.max(v3, dim=-1, keepdim=True)[1]\n        v4 = v4.expand(1, 1, 2)\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(2, 2)\n    def forward(self, x1):\n        v1 = torch.nn.functional.linear(x1, self.linear.weight, self.linear.bias)\n        x2 = torch.nn.functional.relu(v1)\n        v2 = x2.detach()\n        v3 = v2.unsqueeze(dim=-1)\n        return v2 + v3\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(2, 2)\n    def forward(self, x1):\n        v1 = x1.permute(0, 2, 1)\n        v2 = torch.nn.functional.linear(v1, self.linear.weight, self.linear.bias)\n        v3 = v2.expand(1, 2, 2)\n        v3 = torch.nn.functional.elu(v3)\n        x2 = torch.nn.functional.dropout(v3)\n        x2 = torch.max(x2, dim=-1, keepdim=True)[0]\n        return x2\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(2, 2)\n        self.linear = lambda x: torch.nn.functional.linear(x, self.linear.weight, self.linear.bias)\n    def forward(self, x1):\n        v1 = x1.permute(0, 2, 1)\n        v2 = self.linear(v1)\n        v2 = v2.detach()\n        v3 = torch.max(v2, dim=-1)[0]\n        v4 = torch.max(v2, dim=-1)[1]\n        v4 = v4.unsqueeze(dim=0)\n        v5 = v2.shape[0]\n        v4 = v4.repeat(v5, 1)\n        v3 = v3.unsqueeze(dim=-1)\n        v3 = v3.repeat(1, 1, 2)\n        v3 = v3 + v4\n        v3 = v3 > 0\n        v4 = v3.to(x1.dtype)\n        return (v2 * v4)\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(2, 2)\n        self.linear1 = torch.nn.Linear(2, 1)\n        self.linear2 = torch.nn.Linear(1, 1)\n        self.linear3 = torch.nn.Linear(2, 2)\n        self.linear4 = torch.nn.Linear(2, 1)\n        self.linear5 = torch.nn.Linear(1, 1)\n    def forward(self, x1):\n        v1 = x1.permute(0, 2, 1)\n        v2 = torch.nn.functional.linear(v1, self.linear.weight, self.linear.bias)\n        v2 = v2.tanh()\n        v3 = torch.sigmoid(torch.nn.functional.linear(v2, self.linear1.weight, self.linear1.bias))\n        v4 = torch.nn.functional.linear(v2, self.linear2.weight, torch.cat((self.linear2.bias, torch.tensor([1]).to(self.linear2.bias)), dim=0))\n        v4 = torch.sigmoid(v4)\n        v5 = torch.cat((v3, v4), dim=-1)\n        v5 = v5.unsqueeze(dim=-1)\n        v5 = v5.expand(1, 1, 8)\n        v6 = v2.expand(1, 2, 2) * v5\n        v7 = torch.sigmoid(torch.nn.functional.linear(v6, self.linear3.weight, self.linear3.bias))\n        v6 = torch.sigmoid(torch.nn.functional.linear(v6, self.linear4.weight, torch.cat((self.linear4.bias, torch.tensor([1]).to(self.linear4.bias)), dim=0)))\n        v8 = v6 * v7\n        v8 = torch.nn.functional.linear(v8, self.linear5.weight, self.linear5.bias)\n        return v8\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(2, 2)\n    def forward(self, x1):\n        v1 = x1.permute(0, 2, 1)\n        v2 = torch.nn.functional.linear(v1, self.linear.weight, self.linear.bias)\n        v3 = v2.reshape([1, 2, 2])\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(2, 2)\n    def forward(self, x1):\n        v1 = x1.permute(0, 2, 1)\n        v2 = torch.nn.functional.linear(v1, self.linear.weight, self.linear.bias)\n        v3 = torch.max(v2, dim=-1)[1]\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 4)\n        self.linear1 = torch.nn.Linear(4, 2)\n    def forward(self, x1):\n\tv1 = x1.permute(0, 2, 1)\n\tv2 = torch.nn.functional.linear(v1, self.linear.weight, self.linear.bias)\n\tv2 = torch.nn.functional.relu(v2)\n\tv3 = torch.arange(v2.shape[0]).to(dtype=torch.long)\n\tv4 = v2[v3, :]\n\tv5 = self.linear1(v4)\n\tv5 = torch.min(v5, dim=-1)[0]\n\tv5 = v5.unsqueeze(dim=-1)\n        return v5\n# Inputs to the model\nx1 = torch.randn(32, 3, 4)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(2, 2)\n    def forward(self, x1):\n        v1 = x1.permute(0, 2, 1)\n        v2 = torch.nn.functional.linear(v1, self.linear.weight, self.linear.bias)\n        x2 = torch.nn.functional.relu(x1 + v2)\n        v3 = x2.detach()\n        v4 = torch.max(v3, dim=-1)[1]\n        v4 = v4.unsqueeze(dim=-1)\n        v3 = v3 + v4.to(v3.dtype)\n        v4 = (v3 == -1).to(v3.dtype)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1):\n        v2 = torch.nn.functional.grid_sample(x1, torch.rand(1, 2, 2, 2))\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 1, 2, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(2, 2)\n    def forward(self, x1):\n        v1 = x1.permute(0, 2, 1)\n        v2 = torch.nn.functional.linear(v1, self.linear.weight, self.linear.bias)\n        x2 = torch.nn.functional.relu(v2)\n        v3 = x2.detach()\n        v4 = torch.max(v3, dim=-1, keepdim=True)[1]\n        v4 = v4.expand(1, 1, 2)\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(2, 2)\n    def forward(self, x1):\n        v1 = torch.nn.functional.linear(x1, self.linear.weight, self.linear.bias)\n        x2 = torch.nn.functional.relu(v1)\n        v2 = x2.detach()\n        v3 = v2.unsqueeze(dim=-1)\n        return v2 + v3\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\n"
            ],
            "g_time": 15.8658607006073
        }
    }
}
{
    "opt": "binary_unary=2",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(1, 80, 3, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = v1 - 100\n        v3 = F.relu(v2)\n        v4 = self.conv1(v3)\n        v5 = v4 - 200\n        v6 = F.relu(v5)\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 1, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 4, 1, stride=2)\n        self.conv2 = torch.nn.Conv2d(4, 2, 1, stride=2)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = self.conv2(v1)\n        v3 = v2 - 10\n        v4 = F.relu(v3)\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(10, 20, 5, stride=2, padding=1)\n        self.conv2 = torch.nn.Conv2d(20, 50, 5, stride=2, padding=1)\n    def forward(self, x1):\n        v1 = F.relu6(inputs)\n        v2 = F.relu(self.conv1(v1))\n        v3 = F.relu(self.conv2(v2))\n        return v3\n# Inputs to the model\nx1 = torch.randn(16, 10, 56, 56)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.conv = torch.nn.Conv2d(64, 128, 3, stride=2, padding=1)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 - 127\n        v3 = F.relu(v2)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 64, 56, 56)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(16, 64, 3, stride=2, padding=1)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = v1 - 10\n        v3 = F.relu(v2)\n        v4 = v3.unsqueeze(0)\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 16, 56, 56)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(1, 15, 3, stride=5)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = v1 - 0.48\n        v3 = F.relu(v2)\n        v4 = torch.squeeze(v3, 2)\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 1, 25, 25)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(10, 32, 5, stride=2, padding=1)\n        self.conv2 = torch.nn.ConvTranspose2d(32, 10, 5, stride=2, padding=1)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = v1 - -10.22\n        v3 = torch.relu(v2)\n        v4 = self.conv2(v3)\n        v5 = v4 - 0.9\n        v6 = torch.tanh(v5)\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 10, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 8, 1)\n        self.conv2 = torch.nn.Conv2d(8, 32, 3, stride=2, padding=1)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = v1 - 30.6636\n        v3 = F.relu(v2)\n        v4 = self.conv2(v3)\n        v5 = v4 - -24.4705\n        v6 = F.relu(v5)\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 8, 5, stride=2, padding=1)\n        self.conv2 = torch.nn.Conv2d(8, 16, 3, stride=2, padding=1)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = v1 - 127\n        v3 = F.relu(v2)\n        v4 = self.conv2(v3)\n        v5 = v4 - 127\n        v6 = F.relu(v5)\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 32, 1)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 - 15\n        v3 = F.relu(v2)\n        v4 = v3 - self.conv(x1)\n        v5 = F.relu(v4)\n        v6 = torch.transpose(v3, 0, 2)\n        v7 = F.relu(v6)\n        return v7\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(1, 80, 3, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = v1 - 100\n        v3 = F.relu(v2)\n        v4 = self.conv1(v3)\n        v5 = v4 - 200\n        v6 = F.relu(v5)\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 1, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 4, 1, stride=2)\n        self.conv2 = torch.nn.Conv2d(4, 2, 1, stride=2)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = self.conv2(v1)\n        v3 = v2 - 10\n        v4 = F.relu(v3)\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(10, 20, 5, stride=2, padding=1)\n        self.conv2 = torch.nn.Conv2d(20, 50, 5, stride=2, padding=1)\n    def forward(self, x1):\n        v1 = F.relu6(inputs)\n        v2 = F.relu(self.conv1(v1))\n        v3 = F.relu(self.conv2(v2))\n        return v3\n# Inputs to the model\nx1 = torch.randn(16, 10, 56, 56)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.conv = torch.nn.Conv2d(64, 128, 3, stride=2, padding=1)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 - 127\n        v3 = F.relu(v2)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 64, 56, 56)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(16, 64, 3, stride=2, padding=1)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = v1 - 10\n        v3 = F.relu(v2)\n        v4 = v3.unsqueeze(0)\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 16, 56, 56)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(1, 15, 3, stride=5)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = v1 - 0.48\n        v3 = F.relu(v2)\n        v4 = torch.squeeze(v3, 2)\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 1, 25, 25)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(10, 32, 5, stride=2, padding=1)\n        self.conv2 = torch.nn.ConvTranspose2d(32, 10, 5, stride=2, padding=1)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = v1 - -10.22\n        v3 = torch.relu(v2)\n        v4 = self.conv2(v3)\n        v5 = v4 - 0.9\n        v6 = torch.tanh(v5)\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 10, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 8, 1)\n        self.conv2 = torch.nn.Conv2d(8, 32, 3, stride=2, padding=1)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = v1 - 30.6636\n        v3 = F.relu(v2)\n        v4 = self.conv2(v3)\n        v5 = v4 - -24.4705\n        v6 = F.relu(v5)\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 8, 5, stride=2, padding=1)\n        self.conv2 = torch.nn.Conv2d(8, 16, 3, stride=2, padding=1)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = v1 - 127\n        v3 = F.relu(v2)\n        v4 = self.conv2(v3)\n        v5 = v4 - 127\n        v6 = F.relu(v5)\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 32, 1)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 - 15\n        v3 = F.relu(v2)\n        v4 = v3 - self.conv(x1)\n        v5 = F.relu(v4)\n        v6 = torch.transpose(v3, 0, 2)\n        v7 = F.relu(v6)\n        return v7\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n"
            ],
            "g_time": 6.802262544631958
        }
    }
}
{
    "opt": "binary=4",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(100, 100, False)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 + torch.randn(100)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(100)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(8, 10)\n \n    def forward(self, x1, x2):\n        v1 = self.linear(x1)\n        v2 = v1 + x2\n        return v2\n \n# Initializing the model\nm = Model()\n\n# Inputs to the model, \"x1\" and \"x2\" can be different\nx1 = torch.randn(1, 8)\nx2 = torch.randn(1, 10)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(32, 64, bias=True)  # 32-dimensional input, 64-dimensional output. You can add bia\n    \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        ",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(16, 32)\n \n    def forward(self, x1, other):\n        v1 = self.linear(x1)\n        v2 = v1 + other\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 16)\nother = torch.randn(1, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.l1 = torch.nn.Linear(8, 8)\n \n    def forward(self, x1, x2):\n        i1 = self.l1(x1)\n        o = i1 + x2\n        return o\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 8)\nx2 = torch.randn(1, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8, bias=True)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        ",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(10, 5)\n \n    def forward(self, x1, x2):\n        v1 = self.linear(x1)\n        v2 = v1 + x2\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 10)\nx2 = torch.randn(1, 5)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(20, 8, bias=False)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 + x2 # where \"x2\" represents a specific tensor\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 20)\nx2 = torch.randn(1, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, other):\n        super().__init__()\n        self.linear1 = torch.nn.Linear(64, 32)\n        self.linear2 = torch.nn.Linear(32, 8)\n        self.other = other\n \n    def forward(self, x1):\n        v1 = self.linear1(x1)\n        v2 = v1 + self.other\n        v3 = self.linear2(v2)\n        return v3\n\n# Initializing the model\nm1 = Model(torch.randn(1))\nm2 = Model(torch.randn(5))\n\n# Inputs to the model\nx1 = torch.randn(1, 64)\n__output_1__ = m1(x1)\n__output_2__ = m2(x1)\n\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8, bias=True)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 + torch.ones(3, 8)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\nx2 = torch.randn(1, 3)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(100, 100, False)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 + torch.randn(100)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(100)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(8, 10)\n \n    def forward(self, x1, x2):\n        v1 = self.linear(x1)\n        v2 = v1 + x2\n        return v2\n \n# Initializing the model\nm = Model()\n\n# Inputs to the model, \"x1\" and \"x2\" can be different\nx1 = torch.randn(1, 8)\nx2 = torch.randn(1, 10)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(32, 64, bias=True)  # 32-dimensional input, 64-dimensional output. You can add bia\n    \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        ",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(16, 32)\n \n    def forward(self, x1, other):\n        v1 = self.linear(x1)\n        v2 = v1 + other\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 16)\nother = torch.randn(1, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.l1 = torch.nn.Linear(8, 8)\n \n    def forward(self, x1, x2):\n        i1 = self.l1(x1)\n        o = i1 + x2\n        return o\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 8)\nx2 = torch.randn(1, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8, bias=True)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        ",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(10, 5)\n \n    def forward(self, x1, x2):\n        v1 = self.linear(x1)\n        v2 = v1 + x2\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 10)\nx2 = torch.randn(1, 5)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(20, 8, bias=False)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 + x2 # where \"x2\" represents a specific tensor\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 20)\nx2 = torch.randn(1, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, other):\n        super().__init__()\n        self.linear1 = torch.nn.Linear(64, 32)\n        self.linear2 = torch.nn.Linear(32, 8)\n        self.other = other\n \n    def forward(self, x1):\n        v1 = self.linear1(x1)\n        v2 = v1 + self.other\n        v3 = self.linear2(v2)\n        return v3\n\n# Initializing the model\nm1 = Model(torch.randn(1))\nm2 = Model(torch.randn(5))\n\n# Inputs to the model\nx1 = torch.randn(1, 64)\n__output_1__ = m1(x1)\n__output_2__ = m2(x1)\n\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8, bias=True)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 + torch.ones(3, 8)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\nx2 = torch.randn(1, 3)\n"
            ],
            "g_time": 6.666109085083008
        }
    }
}
{
    "opt": "unary=10",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(640, 8, bias=True)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v4 / 6\n        return v5\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 8, 640)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(10, 5)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v4 / 6\n        return v5\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(2, 10)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc = torch.nn.Linear(32 * 32, 1)\n \n    def forward(self, x1):\n        v1 = self.fc(x1.view(x1.shape[0], -1))\n        v2 = v1 + 3\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v4 / 6\n        return v5\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 32, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(64, 64)\n \n    def forward(self, x1):\n        x2 = self.linear(x1)\n        x3 = x2 + 3\n        x4 = torch.clamp_min(x3, 0)\n        x5 = torch.clamp_max(x4, 6)\n        x6 = x5 / 6\n        return x6\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(8, 8)\n \n    def forward(self, x1):\n        l1 = self.linear(x1)\n        l2 = l1 + 3\n        l3 = torch.clamp_min(l2, 0)\n        l4 = torch.clamp_max(l3, 6)\n        l5 = l4 / 6\n        return l5\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(64, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(13, 15)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v4 / 6\n        return v5\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 13)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 4, 3)\n \n    def forward(self, x1):\n        l1 = self.linear(x1)\n        l2 = l1 + 3\n        l3 = torch.clamp_min(l2, 0)\n        l4 = torch.clamp_max(l3, 6)\n        l5 = l4 / 6\n        return l5\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(2, 3)\nif torch.cuda.is_available():\n    x1 = x1.cuda()\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 3)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp(v2, 0, 6)\n        v4 = v3 / 6\n        return v4\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(8, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v4 / 6\n        return v5\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(6, 4)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v4 / 6\n        return v5\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 6)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(640, 8, bias=True)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v4 / 6\n        return v5\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 8, 640)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(10, 5)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v4 / 6\n        return v5\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(2, 10)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc = torch.nn.Linear(32 * 32, 1)\n \n    def forward(self, x1):\n        v1 = self.fc(x1.view(x1.shape[0], -1))\n        v2 = v1 + 3\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v4 / 6\n        return v5\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 32, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(64, 64)\n \n    def forward(self, x1):\n        x2 = self.linear(x1)\n        x3 = x2 + 3\n        x4 = torch.clamp_min(x3, 0)\n        x5 = torch.clamp_max(x4, 6)\n        x6 = x5 / 6\n        return x6\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(8, 8)\n \n    def forward(self, x1):\n        l1 = self.linear(x1)\n        l2 = l1 + 3\n        l3 = torch.clamp_min(l2, 0)\n        l4 = torch.clamp_max(l3, 6)\n        l5 = l4 / 6\n        return l5\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(64, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(13, 15)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v4 / 6\n        return v5\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 13)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 4, 3)\n \n    def forward(self, x1):\n        l1 = self.linear(x1)\n        l2 = l1 + 3\n        l3 = torch.clamp_min(l2, 0)\n        l4 = torch.clamp_max(l3, 6)\n        l5 = l4 / 6\n        return l5\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(2, 3)\nif torch.cuda.is_available():\n    x1 = x1.cuda()\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 3)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp(v2, 0, 6)\n        v4 = v3 / 6\n        return v4\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(8, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v4 / 6\n        return v5\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(6, 4)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v4 / 6\n        return v5\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 6)\n"
            ],
            "g_time": 6.533720016479492
        }
    }
}
{
    "opt": "unary=28",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(16, 16)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.clamp_min(v1, 0.5)\n        v3 = torch.clamp_max(v2, 0.7071067811865476)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min_val, max_val):\n        super().__init__()\n        self.linear = torch.nn.Linear(10, 5)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.clamp_min(v1, min=min_val)\n        v3 = torch.clamp_max(v2, max=max_val)\n        return v3\n\n# Initializing the model\nm = Model(min_val=-1, max_val=0)\n\n# Inputs to the model\nx1 = torch.randn(1, 10)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(18, 9)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.clamp_min(v1, -1000)\n        v3 = torch.clamp_max(v2, 1000)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# The minimum value is provided as a keyword argument\nmin_value = -1000\n\n# The maximum value is provided as a keyword argument\nmax_value = 1000\n\n# The input tensor is generated randomly\nx1 = torch.randn(1, 18)\n\n# The generated model is tested against some random inputs\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min_value=0.0, max_value=0.0):\n        super().__init__()\n        self.linear = torch.nn.Linear(64, 16)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.clamp_min(v1, self.min_value)\n        v3 = torch.clamp_max(v2, self.max_value)\n        return v3\n\n# Initializing the model\nm = Model(min_value=0.0, max_value=1.0)\n\n# Input to the model\nx1 = torch.randn(1, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(224, 6, bias=True)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.clamp_min(v1, -0.630984)\n        v3 = torch.clamp_max(v2, 3.0)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 224)\n",
                "\nclass Model(nn.Module):\n    def __init__(self, min_value=-2, max_value=2):\n        super().__init__()\n        self.linear = nn.Linear(3, 8)\n        self.min_value = min_value\n        self.max_value = max_value\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.clamp_min(v1, self.min_value)\n        v3 = torch.clamp_max(v2, self.max_value)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(32, 32)\n \n    def forward(self, x1, min_value, max_value):\n        v1 = self.linear(x1)\n        v2 = torch.clamp_min(v1, min_value)\n        v3 = torch.clamp_max(v2, max_value)\n        return v3\n \ninit = Initializer(min_value=-1, max_value=1)\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(10, 8, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(8, 16)\n        self.linear_bias = torch.nn.Parameter(torch.ones(16))\n \n    def forward(self, x1, min_value, max_value):\n        v1 = self.linear(x1)\n        v2 = torch.clamp_min(v1, min_value)\n        v3 = torch.clamp_max(v2, max_value)\n        return v3 + self.linear_bias\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 4, 8)\nmin_value = 1.0\nmax_value = 4.0\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min_value, max_value):\n        super().__init__()\n        self.linear = torch.nn.Linear(320, 64)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.clamp_min(v1, min_value)\n        v3 = torch.clamp_max(v2, max_value)\n        return v3\n\n# Initializing the model\nm = Model(0.1, 0.3)\n\n# Inputs to the model\nx1 = torch.randn(1, 320)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min_value=-1.0, max_value=10):\n        super().__init__()\n        self.linear = torch.nn.Linear(64, 32)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.clamp_min(v1, min_value)\n        v3 = torch.clamp_max(v2, max_value)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1000)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(16, 16)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.clamp_min(v1, 0.5)\n        v3 = torch.clamp_max(v2, 0.7071067811865476)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min_val, max_val):\n        super().__init__()\n        self.linear = torch.nn.Linear(10, 5)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.clamp_min(v1, min=min_val)\n        v3 = torch.clamp_max(v2, max=max_val)\n        return v3\n\n# Initializing the model\nm = Model(min_val=-1, max_val=0)\n\n# Inputs to the model\nx1 = torch.randn(1, 10)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(18, 9)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.clamp_min(v1, -1000)\n        v3 = torch.clamp_max(v2, 1000)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# The minimum value is provided as a keyword argument\nmin_value = -1000\n\n# The maximum value is provided as a keyword argument\nmax_value = 1000\n\n# The input tensor is generated randomly\nx1 = torch.randn(1, 18)\n\n# The generated model is tested against some random inputs\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min_value=0.0, max_value=0.0):\n        super().__init__()\n        self.linear = torch.nn.Linear(64, 16)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.clamp_min(v1, self.min_value)\n        v3 = torch.clamp_max(v2, self.max_value)\n        return v3\n\n# Initializing the model\nm = Model(min_value=0.0, max_value=1.0)\n\n# Input to the model\nx1 = torch.randn(1, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(224, 6, bias=True)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.clamp_min(v1, -0.630984)\n        v3 = torch.clamp_max(v2, 3.0)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 224)\n",
                "\nclass Model(nn.Module):\n    def __init__(self, min_value=-2, max_value=2):\n        super().__init__()\n        self.linear = nn.Linear(3, 8)\n        self.min_value = min_value\n        self.max_value = max_value\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.clamp_min(v1, self.min_value)\n        v3 = torch.clamp_max(v2, self.max_value)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(32, 32)\n \n    def forward(self, x1, min_value, max_value):\n        v1 = self.linear(x1)\n        v2 = torch.clamp_min(v1, min_value)\n        v3 = torch.clamp_max(v2, max_value)\n        return v3\n \ninit = Initializer(min_value=-1, max_value=1)\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(10, 8, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(8, 16)\n        self.linear_bias = torch.nn.Parameter(torch.ones(16))\n \n    def forward(self, x1, min_value, max_value):\n        v1 = self.linear(x1)\n        v2 = torch.clamp_min(v1, min_value)\n        v3 = torch.clamp_max(v2, max_value)\n        return v3 + self.linear_bias\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 4, 8)\nmin_value = 1.0\nmax_value = 4.0\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min_value, max_value):\n        super().__init__()\n        self.linear = torch.nn.Linear(320, 64)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.clamp_min(v1, min_value)\n        v3 = torch.clamp_max(v2, max_value)\n        return v3\n\n# Initializing the model\nm = Model(0.1, 0.3)\n\n# Inputs to the model\nx1 = torch.randn(1, 320)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min_value=-1.0, max_value=10):\n        super().__init__()\n        self.linear = torch.nn.Linear(64, 32)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.clamp_min(v1, min_value)\n        v3 = torch.clamp_max(v2, max_value)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1000)\n"
            ],
            "g_time": 6.973611354827881
        }
    }
}
{
    "opt": "binary=10",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self, a):\n        super().__init__()\n        self.linear = torch.nn.Linear(4, 8)\n        self.a = a\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 + self.a\n        return v2\n\n# Initializing the model\na = torch.nn.Parameter(torch.randn(8), requires_grad=True)\nm = Model(a)\n\n# Inputs to the model\nx1 = torch.randn(4)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(16, 16)\n \n    def forward(self, x1, x2):\n        v1 = self.linear(x1)\n        v2 = v1 + x2\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(32, 16)\nx2 = torch.randn(32, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(8, 16)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 + other\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8, bias=True)\n \n    def forward(self, x1, x2):\n        v1 = self.linear(x1)\n        v2 = v1 + x2\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\nx2 = torch.rand(1, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(128, 256)\n \n    def forward(self, x1, x2):\n        v1 = self.linear(x1)\n        v2 = v1 + x2\n        return v2\n \n# Initializing the model\nm = Model()\n \n# Inputs to the model\nx1 = torch.randn(1, 128)\nx2 = torch.zeros(256)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n \n    def forward(self, x1, x2):\n        v1 = self.linear(x1)\n        v2 = v1 + x2\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\nx2 = torch.randn(1, 3, 64, 64)\n",
                "1\nclass Model1_0(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(2, 3, bias=False)\n \n    def forward(self, x1, x2):\n        v1 = self.linear(x1)\n        v2 = v1 + x2\n        return v2\n\n# Model1\nclass Model1_1(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(2, 3, bias=False)\n \n    def forward(self, x1, x2):\n        v1 = self.linear(x1)\n        v2 = v1 + x2\n        v3 = torch.tanh(v2)\n        return v3\n\n# Model1\nclass Model1_2(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(2, 3, bias=False)\n \n    def forward(self, x1, x2):\n        v1 = self.linear(x1)\n        v2 = v1 + x2\n        v3 = torch.relu(v2)\n        return v3\n\n# Initializing the model\nm_0 = Model1_0()\nm_1 = Model1_1()\nm_2 = Model1_2()\n\n# Inputs to the model\nx1 = torch.randn(1, 2)\n__output__m_0  = m_0(x1, x1)\n__output__m_1  = m_1(x1, x1)\n__output__m_2  = m_2(x1, x1)\n\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n        self.other = torch.randn(8, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 + self.other\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, other_tensor):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n        self.other_tensor = other_tensor\n \n    def forward(self, x1):\n        t1 = self.linear(x1)\n        t2 = t1 + self.other_tensor\n        return t2\n\n# Initializing the model\nother_tensor = torch.randn(8, 3)\nm = Model(other_tensor)\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    other = None\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(16, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 + self.other\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Initializing the tensor \"other\" to be added\nm.other = torch.randn(8, 8)\n\n# Inputs to the model\nx1 = torch.randn(1, 16)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self, a):\n        super().__init__()\n        self.linear = torch.nn.Linear(4, 8)\n        self.a = a\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 + self.a\n        return v2\n\n# Initializing the model\na = torch.nn.Parameter(torch.randn(8), requires_grad=True)\nm = Model(a)\n\n# Inputs to the model\nx1 = torch.randn(4)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(16, 16)\n \n    def forward(self, x1, x2):\n        v1 = self.linear(x1)\n        v2 = v1 + x2\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(32, 16)\nx2 = torch.randn(32, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(8, 16)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 + other\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8, bias=True)\n \n    def forward(self, x1, x2):\n        v1 = self.linear(x1)\n        v2 = v1 + x2\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\nx2 = torch.rand(1, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(128, 256)\n \n    def forward(self, x1, x2):\n        v1 = self.linear(x1)\n        v2 = v1 + x2\n        return v2\n \n# Initializing the model\nm = Model()\n \n# Inputs to the model\nx1 = torch.randn(1, 128)\nx2 = torch.zeros(256)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n \n    def forward(self, x1, x2):\n        v1 = self.linear(x1)\n        v2 = v1 + x2\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\nx2 = torch.randn(1, 3, 64, 64)\n",
                "1\nclass Model1_0(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(2, 3, bias=False)\n \n    def forward(self, x1, x2):\n        v1 = self.linear(x1)\n        v2 = v1 + x2\n        return v2\n\n# Model1\nclass Model1_1(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(2, 3, bias=False)\n \n    def forward(self, x1, x2):\n        v1 = self.linear(x1)\n        v2 = v1 + x2\n        v3 = torch.tanh(v2)\n        return v3\n\n# Model1\nclass Model1_2(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(2, 3, bias=False)\n \n    def forward(self, x1, x2):\n        v1 = self.linear(x1)\n        v2 = v1 + x2\n        v3 = torch.relu(v2)\n        return v3\n\n# Initializing the model\nm_0 = Model1_0()\nm_1 = Model1_1()\nm_2 = Model1_2()\n\n# Inputs to the model\nx1 = torch.randn(1, 2)\n__output__m_0  = m_0(x1, x1)\n__output__m_1  = m_1(x1, x1)\n__output__m_2  = m_2(x1, x1)\n\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n        self.other = torch.randn(8, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 + self.other\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, other_tensor):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n        self.other_tensor = other_tensor\n \n    def forward(self, x1):\n        t1 = self.linear(x1)\n        t2 = t1 + self.other_tensor\n        return t2\n\n# Initializing the model\nother_tensor = torch.randn(8, 3)\nm = Model(other_tensor)\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    other = None\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(16, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 + self.other\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Initializing the tensor \"other\" to be added\nm.other = torch.randn(8, 8)\n\n# Inputs to the model\nx1 = torch.randn(1, 16)\n"
            ],
            "g_time": 12.247633934020996
        }
    }
}
{
    "opt": "unary=3",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(64, 6, 15, stride=1, padding=7)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        v7 = v6 * 0.5\n        v8 = v6 * 0.7071067811865476\n        v9 = torch.erf(v8)\n        v10 = v9 + 1\n        v11 = v7 * v10\n        v12 = v11 * 0.5\n        return v12\n# Inputs to the model\nx1 = torch.randn(1, 64, 60, 60)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(1, 128, 7, stride=1, padding=3)\n        self.conv2 = torch.nn.Conv2d(128, 128, 1, stride=1, padding=0)\n        self.conv3 = torch.nn.Conv2d(128, 64, 3, stride=1, padding=1)\n        self.conv4 = torch.nn.Conv2d(64, 1, 7, stride=1, padding=3)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        v7 = v6 * 0.5\n        v8 = v6 * 0.7071067811865476\n        v9 = torch.erf(v8)\n        v10 = v9 + 1\n        v11 = v7 * v10\n        v12 = self.conv2(v6)\n        v13 = v11 * v12\n        v14 = v13 * 0.5\n        v15 = v13 * 0.7071067811865476\n        v16 = torch.erf(v15)\n        v17 = v16 + 1\n        v18 = v14 * v17\n        v19 = self.conv3(v10)\n        v20 = v18 * v19\n        v21 = v20 * 0.5\n        v22 = v20 * 0.7071067811865476\n        v23 = torch.erf(v22)\n        v24 = v23 + 1\n        v25 = v21 * v24\n        v26 = v25 * 0.5\n        return v25\n# Inputs to the model\nx1 = torch.randn(1, 1, 39, 39)\n",
                "\ninput = torch.randn(1, 1, 64, 64)\n\nclass Model1(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(1, 2, 1, stride=1, padding=0)\n        self.conv2 = torch.nn.Conv2d(2, 3, 1, stride=1, padding=0)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        v7 = self.conv2(v6)\n        return v7\n    \nclass Model(torch.nn.Module):\n    def forward(self, input):\n        m1 = Model1()\n        out = m1(input)\n        return out\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(2, 12, 1, stride=1, padding=0)\n        self.conv2 = torch.nn.Conv2d(12, 3, 1, stride=1, padding=0)\n    def forward(self, x1):\n        v1 = self.conv2(self.conv1(x1))\n        return v1\n# Inputs to the model\nx1 = torch.randn(1, 2, 32, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc1 = torch.nn.Linear(64, 128)\n        self.fc2 = torch.nn.Linear(128, 512)\n    def forward(self, x1):\n        v1 = self.fc1(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        v7 = self.fc2(v6)\n        return v7\n# Inputs to the model\nx1 = torch.randn(1, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(64, 16, 3, stride=2, padding=1)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        v7 = self.conv2(v6)\n        return v7\n# Inputs to the model\nx1 = torch.randn(1, 64, 40, 40)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(16, 8, 3, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        v7 = self.conv1(v6)\n        v8 = v7 * 0.5\n        v9 = v7 * 0.7071067811865476\n        v10 = torch.erf(v9)\n        return v10\n# Inputs to the model\nx1 = torch.randn(1, 16, 50, 50)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(32, 64, 7, stride=1, padding=3)\n        self.conv2 = torch.nn.Conv2d(64, 48, 2, stride=2, padding=0)\n        self.conv3 = torch.nn.Conv2d(48, 32, 3, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        v7 = self.conv2(v6)\n        v8 = v7 * 0.5\n        v9 = v7 * 0.7071067811865476\n        v10 = torch.erf(v9)\n        v11 = v10 + 1\n        v12 = v8 * v11\n        v13 = v12 * 0.5\n        v14 = v12 * 0.7071067811865476\n        v15 = torch.erf(v14)\n        v16 = v15 + 1\n        v17 = v13 * v16\n        v18 = v17 * 0.5\n        v19 = self.conv3(v6)\n        return v19\n# Inputs to the model\nx1 = torch.randn(1, 32, 56, 56)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(16, 32, 3, stride=1, padding=1)\n        self.conv2 = torch.nn.Conv2d(32, 56, 9, stride=1, padding=4)\n        self.conv3 = torch.nn.Conv2d(56, 1, 7, stride=1, padding=3)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        v7 = self.conv2(v6)\n        v8 = v7 * 0.5\n        v9 = v7 * 0.7071067811865476\n        v10 = torch.erf(v9)\n        v11 = v10 + 1\n        v12 = v8 * v11\n        return v12\n# Inputs to the model\nx1 = torch.randn(1, 16, 80, 80)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(64, 1, 3, stride=1, padding=1)\n        self.conv2 = torch.nn.Conv2d(1, 8, 7, stride=1, padding=3)\n        self.conv3 = torch.nn.Conv2d(8, 4, 11, stride=1, padding=5)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        v7 = self.conv2(v6)\n        v8 = v7 * 0.5\n        v9 = v7 * 0.7071067811865476\n        v10 = torch.erf(v9)\n        v11 = v10 + 1\n        v12 = v8 * v11\n        return v12\n# Inputs to the model\nx1 = torch.randn(1, 64, 53, 53)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(64, 6, 15, stride=1, padding=7)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        v7 = v6 * 0.5\n        v8 = v6 * 0.7071067811865476\n        v9 = torch.erf(v8)\n        v10 = v9 + 1\n        v11 = v7 * v10\n        v12 = v11 * 0.5\n        return v12\n# Inputs to the model\nx1 = torch.randn(1, 64, 60, 60)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(1, 128, 7, stride=1, padding=3)\n        self.conv2 = torch.nn.Conv2d(128, 128, 1, stride=1, padding=0)\n        self.conv3 = torch.nn.Conv2d(128, 64, 3, stride=1, padding=1)\n        self.conv4 = torch.nn.Conv2d(64, 1, 7, stride=1, padding=3)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        v7 = v6 * 0.5\n        v8 = v6 * 0.7071067811865476\n        v9 = torch.erf(v8)\n        v10 = v9 + 1\n        v11 = v7 * v10\n        v12 = self.conv2(v6)\n        v13 = v11 * v12\n        v14 = v13 * 0.5\n        v15 = v13 * 0.7071067811865476\n        v16 = torch.erf(v15)\n        v17 = v16 + 1\n        v18 = v14 * v17\n        v19 = self.conv3(v10)\n        v20 = v18 * v19\n        v21 = v20 * 0.5\n        v22 = v20 * 0.7071067811865476\n        v23 = torch.erf(v22)\n        v24 = v23 + 1\n        v25 = v21 * v24\n        v26 = v25 * 0.5\n        return v25\n# Inputs to the model\nx1 = torch.randn(1, 1, 39, 39)\n",
                "\ninput = torch.randn(1, 1, 64, 64)\n\nclass Model1(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(1, 2, 1, stride=1, padding=0)\n        self.conv2 = torch.nn.Conv2d(2, 3, 1, stride=1, padding=0)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        v7 = self.conv2(v6)\n        return v7\n    \nclass Model(torch.nn.Module):\n    def forward(self, input):\n        m1 = Model1()\n        out = m1(input)\n        return out\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(2, 12, 1, stride=1, padding=0)\n        self.conv2 = torch.nn.Conv2d(12, 3, 1, stride=1, padding=0)\n    def forward(self, x1):\n        v1 = self.conv2(self.conv1(x1))\n        return v1\n# Inputs to the model\nx1 = torch.randn(1, 2, 32, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc1 = torch.nn.Linear(64, 128)\n        self.fc2 = torch.nn.Linear(128, 512)\n    def forward(self, x1):\n        v1 = self.fc1(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        v7 = self.fc2(v6)\n        return v7\n# Inputs to the model\nx1 = torch.randn(1, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(64, 16, 3, stride=2, padding=1)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        v7 = self.conv2(v6)\n        return v7\n# Inputs to the model\nx1 = torch.randn(1, 64, 40, 40)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(16, 8, 3, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        v7 = self.conv1(v6)\n        v8 = v7 * 0.5\n        v9 = v7 * 0.7071067811865476\n        v10 = torch.erf(v9)\n        return v10\n# Inputs to the model\nx1 = torch.randn(1, 16, 50, 50)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(32, 64, 7, stride=1, padding=3)\n        self.conv2 = torch.nn.Conv2d(64, 48, 2, stride=2, padding=0)\n        self.conv3 = torch.nn.Conv2d(48, 32, 3, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        v7 = self.conv2(v6)\n        v8 = v7 * 0.5\n        v9 = v7 * 0.7071067811865476\n        v10 = torch.erf(v9)\n        v11 = v10 + 1\n        v12 = v8 * v11\n        v13 = v12 * 0.5\n        v14 = v12 * 0.7071067811865476\n        v15 = torch.erf(v14)\n        v16 = v15 + 1\n        v17 = v13 * v16\n        v18 = v17 * 0.5\n        v19 = self.conv3(v6)\n        return v19\n# Inputs to the model\nx1 = torch.randn(1, 32, 56, 56)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(16, 32, 3, stride=1, padding=1)\n        self.conv2 = torch.nn.Conv2d(32, 56, 9, stride=1, padding=4)\n        self.conv3 = torch.nn.Conv2d(56, 1, 7, stride=1, padding=3)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        v7 = self.conv2(v6)\n        v8 = v7 * 0.5\n        v9 = v7 * 0.7071067811865476\n        v10 = torch.erf(v9)\n        v11 = v10 + 1\n        v12 = v8 * v11\n        return v12\n# Inputs to the model\nx1 = torch.randn(1, 16, 80, 80)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(64, 1, 3, stride=1, padding=1)\n        self.conv2 = torch.nn.Conv2d(1, 8, 7, stride=1, padding=3)\n        self.conv3 = torch.nn.Conv2d(8, 4, 11, stride=1, padding=5)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        v7 = self.conv2(v6)\n        v8 = v7 * 0.5\n        v9 = v7 * 0.7071067811865476\n        v10 = torch.erf(v9)\n        v11 = v10 + 1\n        v12 = v8 * v11\n        return v12\n# Inputs to the model\nx1 = torch.randn(1, 64, 53, 53)\n"
            ],
            "g_time": 19.374632835388184
        }
    }
}
{
    "opt": "unary=2",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(64, 16, 4)\n        self.conv2d = torch.nn.Conv2d(16, 8, 4)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = self.conv2d(v1)\n        v3 = v2 * 0.5\n        v4 = v2 * v2 * v2\n        v5 = v2 + v4\n        v6 = v5 * 0.044715\n        v7 = torch.tanh(v6)\n        v8 = v7 + 1\n        v9 = v3 * v8\n        return v9\n# Inputs to the model\nx1 = torch.randn(3, 64, 16, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(16, 4, 2, stride=2)\n    def forward(self, x1):\n        v1 = F.elu(self.conv_transpose(x1))\n        v2 = torch.sigmoid(v1)\n        return v2\n# Inputs to the model.\nx1 = torch.randn(1, 16, 16, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(7, 2, 1)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * v1 * v1\n        v4 = v3 * 0.044715\n        v5 = v1 + v4\n        v6 = v5 * 0.7978845608028654\n        v7 = torch.tanh(v6)\n        v8 = v7 + 1\n        v9 = v2 * v8\n        return v9\n# Inputs to the model\nx1 = torch.randn(5, 7, 28, 28)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(2, 16, kernel_size=(3, 3), stride=2, padding=1, dilation=1)\n        self.conv_transpose = torch.nn.ConvTranspose2d(15, 16, 14, stride=2, padding=1, output_padding=3)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = self.conv_transpose(v1)\n        v3 = v2 * 0.5\n        v4 = v2 * v2 * v2\n        v5 = v4 * 0.044715\n        v6 = v2 + v5\n        v7 = v6 * 0.7978845608028654\n        v8 = torch.tanh(v7)\n        v9 = v8 + 1\n        v10 = v3 * v8\n        return v10\n# Inputs to the model\nx1 = torch.randn(3, 2, 16, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(9, 9, 2, stride=2, padding=1, output_padding=1)\n    def forward(self, x1, x2):\n        v1 = torch.cat((x1, x2), 1)\n        v2 = self.conv_transpose(v1)\n        v3 = v2 * 0.5\n        v4 = v2 * v2 * v2\n        v5 = v4 * 0.044715\n        v6 = v2 + v5\n        v7 = v6 * 0.7978845608028654\n        v8 = torch.tanh(v7)\n        v9 = v8 + 1\n        v10 = torch.cat((x2, v9), 1)\n        v11 = v3 * v10\n        return v11\n# Inputs to the model\nx1 = torch.randn(2, 9, 10, 2)\nx2 = torch.randn(7, 9, 14, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(11, 11, 1, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * v1 * v1\n        v4 = v3 * 0.044715\n        v5 = v1 + v4\n        v6 = v5 * 0.7978845608028654\n        v7 = torch.tanh(v6)\n        v8 = v7 + 1\n        v9 = v2 * v8\n        return v9\n# Inputs to the model\nx1 = torch.randn(10, 11, 4, 4)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(32, 8, 2, stride=1, output_padding=2)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * v1 * v1\n        v4 = v3 * 0.044715\n        v5 = v1 + v4\n        v6 = v5 * 0.7978845608028654\n        v7 = torch.tanh(v6)\n        v8 = v7 + 1\n        v9 = v2 * v8\n        return v9\n# Inputs to the model\nx1 = torch.randn(7, 32, 4, 4)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(1, 8, 1, stride=1, padding=1, dilation=1, output_padding=1)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * v1 * v1\n        v4 = v3 * 0.044715\n        v5 = v1 + v4\n        v6 = v5 * 0.7978845608028654\n        v7 = torch.tanh(v6)\n        v8 = v7 + 1\n        v9 = v2 * v8\n        return v9\n# Inputs to the model\nx1 = torch.randn(1, 1, 1, 1)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(3, 16, 3, padding=1)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * v1 * v1\n        v4 = v3 * 0.044715\n        v5 = v1 + v4\n        v6 = v5 * 0.7978845608028654\n        v7 = torch.tanh(v6)\n        v8 = v7 + 1\n        v9 = v2 * v8\n        return v9\n# Inputs to the model\nx1 = torch.randn(8, 3, 3, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(3, 13, 1, stride=1)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * v1 * v1\n        v4 = v3 * 0.044715\n        v5 = v1 + v4\n        v6 = v5 * 0.7978845608028654\n        v7 = torch.tanh(v6)\n        v8 = v7 + 1\n        v9 = v2 * v8\n        return v9\n# Inputs to the model\nx1 = torch.randn(2, 3, 6, 6)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(64, 16, 4)\n        self.conv2d = torch.nn.Conv2d(16, 8, 4)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = self.conv2d(v1)\n        v3 = v2 * 0.5\n        v4 = v2 * v2 * v2\n        v5 = v2 + v4\n        v6 = v5 * 0.044715\n        v7 = torch.tanh(v6)\n        v8 = v7 + 1\n        v9 = v3 * v8\n        return v9\n# Inputs to the model\nx1 = torch.randn(3, 64, 16, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(16, 4, 2, stride=2)\n    def forward(self, x1):\n        v1 = F.elu(self.conv_transpose(x1))\n        v2 = torch.sigmoid(v1)\n        return v2\n# Inputs to the model.\nx1 = torch.randn(1, 16, 16, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(7, 2, 1)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * v1 * v1\n        v4 = v3 * 0.044715\n        v5 = v1 + v4\n        v6 = v5 * 0.7978845608028654\n        v7 = torch.tanh(v6)\n        v8 = v7 + 1\n        v9 = v2 * v8\n        return v9\n# Inputs to the model\nx1 = torch.randn(5, 7, 28, 28)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(2, 16, kernel_size=(3, 3), stride=2, padding=1, dilation=1)\n        self.conv_transpose = torch.nn.ConvTranspose2d(15, 16, 14, stride=2, padding=1, output_padding=3)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = self.conv_transpose(v1)\n        v3 = v2 * 0.5\n        v4 = v2 * v2 * v2\n        v5 = v4 * 0.044715\n        v6 = v2 + v5\n        v7 = v6 * 0.7978845608028654\n        v8 = torch.tanh(v7)\n        v9 = v8 + 1\n        v10 = v3 * v8\n        return v10\n# Inputs to the model\nx1 = torch.randn(3, 2, 16, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(9, 9, 2, stride=2, padding=1, output_padding=1)\n    def forward(self, x1, x2):\n        v1 = torch.cat((x1, x2), 1)\n        v2 = self.conv_transpose(v1)\n        v3 = v2 * 0.5\n        v4 = v2 * v2 * v2\n        v5 = v4 * 0.044715\n        v6 = v2 + v5\n        v7 = v6 * 0.7978845608028654\n        v8 = torch.tanh(v7)\n        v9 = v8 + 1\n        v10 = torch.cat((x2, v9), 1)\n        v11 = v3 * v10\n        return v11\n# Inputs to the model\nx1 = torch.randn(2, 9, 10, 2)\nx2 = torch.randn(7, 9, 14, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(11, 11, 1, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * v1 * v1\n        v4 = v3 * 0.044715\n        v5 = v1 + v4\n        v6 = v5 * 0.7978845608028654\n        v7 = torch.tanh(v6)\n        v8 = v7 + 1\n        v9 = v2 * v8\n        return v9\n# Inputs to the model\nx1 = torch.randn(10, 11, 4, 4)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(32, 8, 2, stride=1, output_padding=2)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * v1 * v1\n        v4 = v3 * 0.044715\n        v5 = v1 + v4\n        v6 = v5 * 0.7978845608028654\n        v7 = torch.tanh(v6)\n        v8 = v7 + 1\n        v9 = v2 * v8\n        return v9\n# Inputs to the model\nx1 = torch.randn(7, 32, 4, 4)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(1, 8, 1, stride=1, padding=1, dilation=1, output_padding=1)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * v1 * v1\n        v4 = v3 * 0.044715\n        v5 = v1 + v4\n        v6 = v5 * 0.7978845608028654\n        v7 = torch.tanh(v6)\n        v8 = v7 + 1\n        v9 = v2 * v8\n        return v9\n# Inputs to the model\nx1 = torch.randn(1, 1, 1, 1)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(3, 16, 3, padding=1)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * v1 * v1\n        v4 = v3 * 0.044715\n        v5 = v1 + v4\n        v6 = v5 * 0.7978845608028654\n        v7 = torch.tanh(v6)\n        v8 = v7 + 1\n        v9 = v2 * v8\n        return v9\n# Inputs to the model\nx1 = torch.randn(8, 3, 3, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(3, 13, 1, stride=1)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * v1 * v1\n        v4 = v3 * 0.044715\n        v5 = v1 + v4\n        v6 = v5 * 0.7978845608028654\n        v7 = torch.tanh(v6)\n        v8 = v7 + 1\n        v9 = v2 * v8\n        return v9\n# Inputs to the model\nx1 = torch.randn(2, 3, 6, 6)\n"
            ],
            "g_time": 11.389038801193237
        }
    }
}
{
    "opt": "mm_plus_mm",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def forward(self, input1):\n        t1 = torch.mm(input1, input1)\n        t2 = torch.mm(input1.transpose(2,1), input1)\n        return t1 + t2\n# Inputs to the model\ninput1 = torch.randn(216, 27, 10)\n",
                "\nclass Model(torch.nn.Module):\n    def forward(self, input1, input2, input3, input4, input5):\n        t1 = torch.mm(input1, input2)\n        t2 = torch.mm(input3, input4)\n        t3 = torch.mm(input5, input1)\n        return t1 + t2 + t3\n# Inputs to the model\ninput1 = torch.randn(1, 12)\ninput2 = torch.randn(12,8)\ninput3 = torch.randn(11, 12)\ninput4 = torch.randn(126, 128)\ninput5 = torch.randn(128, 1)\n# Model input ends\n",
                "\nclass Model(torch.nn.Module):\n    def forward(self, input1, input2, input3, input4):\n        t1 = torch.mm(input1, input2)\n        t2 = torch.mm(input3, input4)\n        t3 = t1 + t2\n        return t3\n# Inputs to the model\ninput1 = torch.randn(4, 4)\ninput2 = torch.randn(4, 4)\ninput3 = torch.randn(4, 4)\ninput4 = torch.randn(4, 4)\n",
                "\nclass Model(torch.nn.Module):\n    def forward(self, input1, input2, input3, input4):\n        t1 = torch.mm(input2, input3)\n        t2 = torch.mm(input2, input1)\n        t3 = torch.mm(input4, input3)\n        t4 = torch.mm(input4, input1)\n        t5 = torch.mm(input2, input3)\n        return t1 + t2 + t3 + t4 + t5\n# Inputs to the model\ninput1 = torch.randn(3, 3)\ninput2 = torch.randn(3, 3)\ninput3 = torch.randn(3, 3)\ninput4 = torch.randn(3, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def forward(self, input1):\n        t1 = torch.mm(input1, input1)\n        return t1\n# Inputs to the model\ninput1 = torch.randn(3, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def forward(self, input1, input2, input3, input4, input5, input6):\n        t0 = torch.mm(input2, input1)\n        t1 = torch.mm(input4, input3)\n        t2 = torch.mm(input4, input5)\n        t3 = torch.mm(input4, input6)\n        t4 = torch.mm(t3, t1)\n        t5 = t0 + t2\n        t6 = t0 * t5\n        t7 = t4 * t6\n        return t7\n# Inputs to the model\ninput1 = torch.randn(5, 5)\ninput2 = torch.randn(5, 5)\ninput3 = torch.randn(5, 5)\ninput4 = torch.randn(5, 5)\ninput5 = torch.randn(5, 5)\ninput6 = torch.randn(5, 5)\n",
                "\nclass Model(torch.nn.Module):\n    def forward(self, input1, input2, input3):\n        t1 = torch.mm(input1, input2)\n        t2 = torch.sigmoid(input1)\n        t3 = torch.mm(t1, t2)\n        t4 = torch.mm(input1, input3)\n        t5 = torch.mm(input2, input3)\n        t6 = torch.mm(input3, input1)\n        t7 = t6 + t1 + t5 + t4\n        return t3\n# Inputs to the model\ninput1 = torch.randn(2, 2)\ninput2 = torch.randn(2, 2)\ninput3 = torch.randn(2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def forward(self, input1, input2, input3, input4):\n        t0 = torch.mm(input1, input2)\n        t1 = torch.mm(input3, input4)\n        t2 = torch.mm(input1, input3)\n        t3 = torch.mm(input2, input4)\n        return t0 + t1 + t2 + t3\n# Inputs to the model\ninput1 = torch.randn(5, 5)\ninput2 = torch.randn(5, 5)\ninput3 = torch.randn(5, 5)\ninput4 = torch.randn(5, 5)\n",
                "\nclass Model(torch.nn.Module):\n    def forward(self, input1, input2, input3):\n        t1 = torch.mm(input1, input2)\n        t2 = torch.mm(input1, input3)\n        t3 = torch.mm(input2, input3)\n        t4 = torch.mm(input1, input2)\n        t5 = torch.mm(input1, input2)\n        return t1 + t2 + t3 + t4 + t5\n# Inputs to the model\ninput1 = torch.randn(1, 1)\ninput2 = torch.randn(1, 1)\ninput3 = torch.randn(1, 1)\n",
                "\nclass Model(torch.nn.Module):\n    def forward(self, input1, input2, input3, input4):\n        t1 = torch.mm(input1, input3)\n        t2 = torch.mm(input1, input4)\n        t3 = torch.mm(input2, input3)\n        t4 = torch.mm(input2, input4)\n        t5 = t1 + t2\n        t6 = t3 + t4\n        t6.add_(t5)\n        return t5\n# Inputs to the model\ninput1 = torch.randn(5, 5)\ninput2 = torch.randn(5, 5)\ninput3 = torch.randn(5, 5)\ninput4 = torch.randn(5, 5)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def forward(self, input1):\n        t1 = torch.mm(input1, input1)\n        t2 = torch.mm(input1.transpose(2,1), input1)\n        return t1 + t2\n# Inputs to the model\ninput1 = torch.randn(216, 27, 10)\n",
                "\nclass Model(torch.nn.Module):\n    def forward(self, input1, input2, input3, input4, input5):\n        t1 = torch.mm(input1, input2)\n        t2 = torch.mm(input3, input4)\n        t3 = torch.mm(input5, input1)\n        return t1 + t2 + t3\n# Inputs to the model\ninput1 = torch.randn(1, 12)\ninput2 = torch.randn(12,8)\ninput3 = torch.randn(11, 12)\ninput4 = torch.randn(126, 128)\ninput5 = torch.randn(128, 1)\n# Model input ends\n",
                "\nclass Model(torch.nn.Module):\n    def forward(self, input1, input2, input3, input4):\n        t1 = torch.mm(input1, input2)\n        t2 = torch.mm(input3, input4)\n        t3 = t1 + t2\n        return t3\n# Inputs to the model\ninput1 = torch.randn(4, 4)\ninput2 = torch.randn(4, 4)\ninput3 = torch.randn(4, 4)\ninput4 = torch.randn(4, 4)\n",
                "\nclass Model(torch.nn.Module):\n    def forward(self, input1, input2, input3, input4):\n        t1 = torch.mm(input2, input3)\n        t2 = torch.mm(input2, input1)\n        t3 = torch.mm(input4, input3)\n        t4 = torch.mm(input4, input1)\n        t5 = torch.mm(input2, input3)\n        return t1 + t2 + t3 + t4 + t5\n# Inputs to the model\ninput1 = torch.randn(3, 3)\ninput2 = torch.randn(3, 3)\ninput3 = torch.randn(3, 3)\ninput4 = torch.randn(3, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def forward(self, input1):\n        t1 = torch.mm(input1, input1)\n        return t1\n# Inputs to the model\ninput1 = torch.randn(3, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def forward(self, input1, input2, input3, input4, input5, input6):\n        t0 = torch.mm(input2, input1)\n        t1 = torch.mm(input4, input3)\n        t2 = torch.mm(input4, input5)\n        t3 = torch.mm(input4, input6)\n        t4 = torch.mm(t3, t1)\n        t5 = t0 + t2\n        t6 = t0 * t5\n        t7 = t4 * t6\n        return t7\n# Inputs to the model\ninput1 = torch.randn(5, 5)\ninput2 = torch.randn(5, 5)\ninput3 = torch.randn(5, 5)\ninput4 = torch.randn(5, 5)\ninput5 = torch.randn(5, 5)\ninput6 = torch.randn(5, 5)\n",
                "\nclass Model(torch.nn.Module):\n    def forward(self, input1, input2, input3):\n        t1 = torch.mm(input1, input2)\n        t2 = torch.sigmoid(input1)\n        t3 = torch.mm(t1, t2)\n        t4 = torch.mm(input1, input3)\n        t5 = torch.mm(input2, input3)\n        t6 = torch.mm(input3, input1)\n        t7 = t6 + t1 + t5 + t4\n        return t3\n# Inputs to the model\ninput1 = torch.randn(2, 2)\ninput2 = torch.randn(2, 2)\ninput3 = torch.randn(2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def forward(self, input1, input2, input3, input4):\n        t0 = torch.mm(input1, input2)\n        t1 = torch.mm(input3, input4)\n        t2 = torch.mm(input1, input3)\n        t3 = torch.mm(input2, input4)\n        return t0 + t1 + t2 + t3\n# Inputs to the model\ninput1 = torch.randn(5, 5)\ninput2 = torch.randn(5, 5)\ninput3 = torch.randn(5, 5)\ninput4 = torch.randn(5, 5)\n",
                "\nclass Model(torch.nn.Module):\n    def forward(self, input1, input2, input3):\n        t1 = torch.mm(input1, input2)\n        t2 = torch.mm(input1, input3)\n        t3 = torch.mm(input2, input3)\n        t4 = torch.mm(input1, input2)\n        t5 = torch.mm(input1, input2)\n        return t1 + t2 + t3 + t4 + t5\n# Inputs to the model\ninput1 = torch.randn(1, 1)\ninput2 = torch.randn(1, 1)\ninput3 = torch.randn(1, 1)\n",
                "\nclass Model(torch.nn.Module):\n    def forward(self, input1, input2, input3, input4):\n        t1 = torch.mm(input1, input3)\n        t2 = torch.mm(input1, input4)\n        t3 = torch.mm(input2, input3)\n        t4 = torch.mm(input2, input4)\n        t5 = t1 + t2\n        t6 = t3 + t4\n        t6.add_(t5)\n        return t5\n# Inputs to the model\ninput1 = torch.randn(5, 5)\ninput2 = torch.randn(5, 5)\ninput3 = torch.randn(5, 5)\ninput4 = torch.randn(5, 5)\n"
            ],
            "g_time": 7.438124179840088
        }
    }
}
{
    "opt": "addmm",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x, inp):\n        v2 = torch.mm(x, x)\n        v1 = v2 + inp\n        return v1\n# Inputs to the model\nx = torch.randn(3, 3)\ninp = torch.randn(3, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        # [torch.Tensor(8,224,224)]\n        self.fc1 = torch.nn.Linear(224, 224)\n        # [torch.Tensor(8,224,224)]\n        self.fc2 = torch.nn.Linear(224, 224)\n    def forward(self, x):\n        v1 = self.fc1(x)\n        v2 = self.fc2(v1)\n        return v2\n# Inputs to the model\nx = torch.randn(8, 224, 224)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, x2, inp):\n        v1 = torch.mm(x2, x1) + torch.mm(inp, x2)\n        return v1\n# Inputs to the model\nx1 = torch.randn(3, 3)\nx2 = torch.randn(3, 3)\ninp = torch.randn(3, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.bn = torch.nn.BatchNorm2d(1, affine=False)\n    def forward(self, x):\n        return self.bn(x)\n# Inputs to the model\nx = torch.randn(1, 1, 1, 1)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, x2, inp):\n        v1 = torch.mm(x1, x2) + inp\n        return v1\n# Inputs to the model\nx1 = torch.randn(3, 3)\nx2 = torch.randn(3, 3)\ninp = torch.randn(3, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x):\n        v1 = torch.mm(x, x)\n        v2 = v1 + x\n        return v2\n# Input to the model\nx = torch.randn(3, 3)\n",
                "\nclass Model(torch.nn.Module):\n  def __init__(self):\n    super().__init__()\n    self.bn = torch.nn.BatchNorm1d(3, affine=False)\n    self.linear = torch.nn.Linear(3, 3, False)\n    self.linear2 = torch.nn.Linear(3, 3, False)\n  def forward(self, x, inp):\n    x1 = self.bn(x)\n    v1 = self.linear(x1)\n    v2 = self.linear2(v1)\n    v3 = v1 + x1\n    v4 = torch.mm(v3, v3)\n    t1 = v2 + v3\n    t1 = v4 + t1\n    t2 = t1 + v4\n    t2 = torch.mm(t1, t1)\n    t2 = t2 + inp\n    return (v1,v2,t2, v3.detach() + x)\n# Inputs to the model\nx = torch.randn(3, 3)\ninp = torch.randn(3, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x, inp):\n        v1 = torch.mm(x, x)\n        v2 = torch.mm(v1, v1)\n        v3 = v2\n        return v1 * v3 + inp\n# Inputs to the model\nx = torch.randn(3, 3)\ninp = torch.randn(3, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.relu = torch.nn.ReLU(inplace=True)\n    def forward(self, x, inp):\n        v1 = self.relu(x)\n        v2 = v1.mean()\n        v3 = torch.mm(x, x)\n        v4 = torch.matmul(v3, v3) + v2\n        v5 = v4 + inp\n        return v5\n# Inputs to the model\nx = torch.randn(3, 3)\ninp = torch.randn(3, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, x2):\n        v3 = torch.mm(x2, x2)\n        v2 = torch.mm(v3, v3)\n        v1 = torch.mm(v2, v2)\n        return v1\n# Inputs to the model\nx1 = torch.randn(3, 3)\nx2 = torch.randn(3, 3)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x, inp):\n        v2 = torch.mm(x, x)\n        v1 = v2 + inp\n        return v1\n# Inputs to the model\nx = torch.randn(3, 3)\ninp = torch.randn(3, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        # [torch.Tensor(8,224,224)]\n        self.fc1 = torch.nn.Linear(224, 224)\n        # [torch.Tensor(8,224,224)]\n        self.fc2 = torch.nn.Linear(224, 224)\n    def forward(self, x):\n        v1 = self.fc1(x)\n        v2 = self.fc2(v1)\n        return v2\n# Inputs to the model\nx = torch.randn(8, 224, 224)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, x2, inp):\n        v1 = torch.mm(x2, x1) + torch.mm(inp, x2)\n        return v1\n# Inputs to the model\nx1 = torch.randn(3, 3)\nx2 = torch.randn(3, 3)\ninp = torch.randn(3, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.bn = torch.nn.BatchNorm2d(1, affine=False)\n    def forward(self, x):\n        return self.bn(x)\n# Inputs to the model\nx = torch.randn(1, 1, 1, 1)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, x2, inp):\n        v1 = torch.mm(x1, x2) + inp\n        return v1\n# Inputs to the model\nx1 = torch.randn(3, 3)\nx2 = torch.randn(3, 3)\ninp = torch.randn(3, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x):\n        v1 = torch.mm(x, x)\n        v2 = v1 + x\n        return v2\n# Input to the model\nx = torch.randn(3, 3)\n",
                "\nclass Model(torch.nn.Module):\n  def __init__(self):\n    super().__init__()\n    self.bn = torch.nn.BatchNorm1d(3, affine=False)\n    self.linear = torch.nn.Linear(3, 3, False)\n    self.linear2 = torch.nn.Linear(3, 3, False)\n  def forward(self, x, inp):\n    x1 = self.bn(x)\n    v1 = self.linear(x1)\n    v2 = self.linear2(v1)\n    v3 = v1 + x1\n    v4 = torch.mm(v3, v3)\n    t1 = v2 + v3\n    t1 = v4 + t1\n    t2 = t1 + v4\n    t2 = torch.mm(t1, t1)\n    t2 = t2 + inp\n    return (v1,v2,t2, v3.detach() + x)\n# Inputs to the model\nx = torch.randn(3, 3)\ninp = torch.randn(3, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x, inp):\n        v1 = torch.mm(x, x)\n        v2 = torch.mm(v1, v1)\n        v3 = v2\n        return v1 * v3 + inp\n# Inputs to the model\nx = torch.randn(3, 3)\ninp = torch.randn(3, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.relu = torch.nn.ReLU(inplace=True)\n    def forward(self, x, inp):\n        v1 = self.relu(x)\n        v2 = v1.mean()\n        v3 = torch.mm(x, x)\n        v4 = torch.matmul(v3, v3) + v2\n        v5 = v4 + inp\n        return v5\n# Inputs to the model\nx = torch.randn(3, 3)\ninp = torch.randn(3, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, x2):\n        v3 = torch.mm(x2, x2)\n        v2 = torch.mm(v3, v3)\n        v1 = torch.mm(v2, v2)\n        return v1\n# Inputs to the model\nx1 = torch.randn(3, 3)\nx2 = torch.randn(3, 3)\n"
            ],
            "g_time": 7.925538063049316
        }
    }
}
{
    "opt": "unary=12",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 2, 1, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = torch.tanh(v1)\n        v3 = v1 * v2\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 6, 4, stride=1, padding=2)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(8, 4, 5, stride=2, padding=2, dilation=1)\n        self.relu = torch.nn.ReLU()\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = self.relu(v1)\n        v2 = v2.mean(dim=1, keepdim=True)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 8, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.relu = torch.nn.ReLU()\n        self.conv = torch.nn.Conv2d(3, 8, 7, stride=1, padding=3)\n    def forward(self, x1):\n        v1 = self.relu(x1)\n        v2 = self.conv(v1)\n        v3 = v2.sum(dim=-1).sum(dim=-1).expand_as(v1)\n        v4 = self.relu(v1)\n        v5 = self.conv(v4)\n        v6 = v5 * v3\n        v7 = v6.sum(dim=-1).sum(dim=-1).expand_as(v1)\n        return v7\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1):\n        v1 = torch.sigmoid(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 1, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 6, 3, stride=1, padding=2, dilation=1)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        return v1\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(2, 7, 6, stride=1, padding=4)\n    def forward(self, x1):\n        v1 = self.sigmoid(x1)\n        v2 = self.conv(v1)\n        return v2.sigmoid()\n# Inputs to the model\nx1 = torch.randn(1, 2, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 6, 3, stride=1, padding=1, dilation=2)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = torch.sigmod(v1)\n        v3 = v1 * v2\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 8, 7, stride=2, padding=3)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv1d(4, 8, 1, stride=4, padding=1)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 4, 32)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 2, 1, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = torch.tanh(v1)\n        v3 = v1 * v2\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 6, 4, stride=1, padding=2)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(8, 4, 5, stride=2, padding=2, dilation=1)\n        self.relu = torch.nn.ReLU()\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = self.relu(v1)\n        v2 = v2.mean(dim=1, keepdim=True)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 8, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.relu = torch.nn.ReLU()\n        self.conv = torch.nn.Conv2d(3, 8, 7, stride=1, padding=3)\n    def forward(self, x1):\n        v1 = self.relu(x1)\n        v2 = self.conv(v1)\n        v3 = v2.sum(dim=-1).sum(dim=-1).expand_as(v1)\n        v4 = self.relu(v1)\n        v5 = self.conv(v4)\n        v6 = v5 * v3\n        v7 = v6.sum(dim=-1).sum(dim=-1).expand_as(v1)\n        return v7\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1):\n        v1 = torch.sigmoid(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 1, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 6, 3, stride=1, padding=2, dilation=1)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        return v1\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(2, 7, 6, stride=1, padding=4)\n    def forward(self, x1):\n        v1 = self.sigmoid(x1)\n        v2 = self.conv(v1)\n        return v2.sigmoid()\n# Inputs to the model\nx1 = torch.randn(1, 2, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 6, 3, stride=1, padding=1, dilation=2)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = torch.sigmod(v1)\n        v3 = v1 * v2\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 8, 7, stride=2, padding=3)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv1d(4, 8, 1, stride=4, padding=1)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 4, 32)\n"
            ],
            "g_time": 7.112858295440674
        }
    }
}
{
    "opt": "unary=9",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n    def forward(self, x1):\n        v2 = self.conv(x1)\n        v2 = v2 + 3\n        v3 = v2.clamp_min(0)\n        v4 = v3.clamp_max(6)\n        v5 = v4 / 6\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = torch.sum(v1)\n        v3 = v2 / 3\n        v4 = torch.clamp_min(v3, 0)\n        v5 = torch.clamp_max(v4, 6)\n        v6 = v5.div(6)\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, input):\n        x = torch.nn.ReLU(inplace=True)(input)\n        x = x - 3\n        x = torch.nn.functional.relu6(x)\n        x = x / 6\n        return x\n# Inputs to the model\nx = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n    def forward(self, x1):\n        v2 = 3 + self.conv(x1)\n        v3 = v2.clamp(0, 6)\n        v4 = v3.div(6)\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n    def forward(self, x1):\n        v2 = self.conv(x1) + 3\n        v3 = v2.clamp_min(0)\n        v4 = v3.clamp_max(6)\n        v5 = v4 / 6\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = 3 + self.conv(x1)\n        v2 = v1.clamp_min(0)\n        v3 = v2.clamp_max(6)\n        v4 = v3 / 6\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        t0 = 3\n        v2 = v1 + t0\n        v3 = v2.clamp_min(0)\n        v4 = v3.clamp_max(6)\n        v5 = v4 / 6\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n    def forward(self, x1):\n        v2 = self.conv(x1) + 3\n        v3 = v2.clamp_min(0)\n        v4 = v3.clamp_max(6)\n        v5 = v4.div(6)\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n    def forward(self, x1):\n        v2 = 3 + self.conv(x1)\n        v3 = v2.clamp(0, 6)\n        v4 = v3.div(6)\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = torch.clamp(self.conv(x1), 0, 6)\n        v2 = v1.add(3)\n        v3 = v2.div(6)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n    def forward(self, x1):\n        v2 = self.conv(x1)\n        v2 = v2 + 3\n        v3 = v2.clamp_min(0)\n        v4 = v3.clamp_max(6)\n        v5 = v4 / 6\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = torch.sum(v1)\n        v3 = v2 / 3\n        v4 = torch.clamp_min(v3, 0)\n        v5 = torch.clamp_max(v4, 6)\n        v6 = v5.div(6)\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, input):\n        x = torch.nn.ReLU(inplace=True)(input)\n        x = x - 3\n        x = torch.nn.functional.relu6(x)\n        x = x / 6\n        return x\n# Inputs to the model\nx = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n    def forward(self, x1):\n        v2 = 3 + self.conv(x1)\n        v3 = v2.clamp(0, 6)\n        v4 = v3.div(6)\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n    def forward(self, x1):\n        v2 = self.conv(x1) + 3\n        v3 = v2.clamp_min(0)\n        v4 = v3.clamp_max(6)\n        v5 = v4 / 6\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = 3 + self.conv(x1)\n        v2 = v1.clamp_min(0)\n        v3 = v2.clamp_max(6)\n        v4 = v3 / 6\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        t0 = 3\n        v2 = v1 + t0\n        v3 = v2.clamp_min(0)\n        v4 = v3.clamp_max(6)\n        v5 = v4 / 6\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n    def forward(self, x1):\n        v2 = self.conv(x1) + 3\n        v3 = v2.clamp_min(0)\n        v4 = v3.clamp_max(6)\n        v5 = v4.div(6)\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n    def forward(self, x1):\n        v2 = 3 + self.conv(x1)\n        v3 = v2.clamp(0, 6)\n        v4 = v3.div(6)\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = torch.clamp(self.conv(x1), 0, 6)\n        v2 = v1.add(3)\n        v3 = v2.div(6)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n"
            ],
            "g_time": 6.105549573898315
        }
    }
}
{
    "opt": "unary=25",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(4, 4)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 > 0\n        v3 = v1 * 0.01\n        v4 = torch.where(v2, v1, v3)\n        return v4\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 4)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, -1)\n\n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 > 0\n        v3 = v1 * -0.1\n        v4 = torch.where(v2, v1, v3)\n        return v4\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, negative_slope=0.01):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n        self.negative_slope = negative_slope\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 > 0\n        v3 = v1 * self.negative_slope\n        v4 = torch.where(v2, v1, v3)\n        return v4\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(9, 3)\n \n    def forward(self, x1, negative_slope):\n        v1 = self.linear(x1)\n        v2 = v1 > 0\n        v3 = v1 * negative_slope\n        v4 = torch.where(v2, v1, v3)\n        return v4\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(2, 9)\nnegative_slope = 0.1\nl = x1.size(1)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc = torch.nn.Linear(1024, 512)\n \n    def forward(self, x1):\n        v1 = self.fc(x1)\n        v2 = v1 > 0\n        v3 = v1 * 0.2\n        v4 = torch.where(v2, v1, v3)\n        return v4\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(256, 1024)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(8, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 > 0\n        v3 = v1 * 0.01\n        v4 = torch.where(v2, v1, v3)\n        return v4\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, negative_slope=0.01):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n        self.negative_slope = negative_slope\n \n    def forward(self, features):\n        intermediate = self.linear(features)\n        intermediate = intermediate > 0\n        return torch.where(intermediate, intermediate, intermediate * self.negative_slope)\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 > 0\n        v3 = v1 * 0.01\n        v4 = torch.where(v2, v1, v3)\n        return v4\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(nn.Module):\n    def __init__(self, negative_slope):\n        super().__init__()\n        self.linear = nn.Linear(5, 10)\n        self.negative_slope = negative_slope\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 > 0\n        v3 = v1 * self.negative_slope\n        v4 = torch.where(v2, v1, v3)\n        return v4\n \n # Initializing the model\nm = Model(negative_slope=0.5)\n\n# Inputs to the model\nx1 = torch.randn(50, 5)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(64, 64),\n        self.negative_slope = 0.01\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 > 0\n        v3 = v1 * self.negative_slope\n        v4 = torch.where(v2, v1, v3)\n        return v4\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(64, 64)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(4, 4)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 > 0\n        v3 = v1 * 0.01\n        v4 = torch.where(v2, v1, v3)\n        return v4\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 4)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, -1)\n\n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 > 0\n        v3 = v1 * -0.1\n        v4 = torch.where(v2, v1, v3)\n        return v4\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, negative_slope=0.01):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n        self.negative_slope = negative_slope\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 > 0\n        v3 = v1 * self.negative_slope\n        v4 = torch.where(v2, v1, v3)\n        return v4\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(9, 3)\n \n    def forward(self, x1, negative_slope):\n        v1 = self.linear(x1)\n        v2 = v1 > 0\n        v3 = v1 * negative_slope\n        v4 = torch.where(v2, v1, v3)\n        return v4\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(2, 9)\nnegative_slope = 0.1\nl = x1.size(1)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc = torch.nn.Linear(1024, 512)\n \n    def forward(self, x1):\n        v1 = self.fc(x1)\n        v2 = v1 > 0\n        v3 = v1 * 0.2\n        v4 = torch.where(v2, v1, v3)\n        return v4\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(256, 1024)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(8, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 > 0\n        v3 = v1 * 0.01\n        v4 = torch.where(v2, v1, v3)\n        return v4\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, negative_slope=0.01):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n        self.negative_slope = negative_slope\n \n    def forward(self, features):\n        intermediate = self.linear(features)\n        intermediate = intermediate > 0\n        return torch.where(intermediate, intermediate, intermediate * self.negative_slope)\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 > 0\n        v3 = v1 * 0.01\n        v4 = torch.where(v2, v1, v3)\n        return v4\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(nn.Module):\n    def __init__(self, negative_slope):\n        super().__init__()\n        self.linear = nn.Linear(5, 10)\n        self.negative_slope = negative_slope\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 > 0\n        v3 = v1 * self.negative_slope\n        v4 = torch.where(v2, v1, v3)\n        return v4\n \n # Initializing the model\nm = Model(negative_slope=0.5)\n\n# Inputs to the model\nx1 = torch.randn(50, 5)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(64, 64),\n        self.negative_slope = 0.01\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 > 0\n        v3 = v1 * self.negative_slope\n        v4 = torch.where(v2, v1, v3)\n        return v4\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(64, 64)\n"
            ],
            "g_time": 6.235023498535156
        }
    }
}
{
    "opt": "sfdp=1",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    class Config(NamedTuple):\n        dropout_p: float\n        in_features: int\n        num_heads: int\n \n    def __init__(self, dropout_p, in_features, num_heads):\n        super().__init__()\n        self.dropout_p = dropout_p\n        self.in_features = in_features\n        self.num_heads = num_heads\n        self.head_dim = in_features // num_heads\n \n        self.dropout = torch.nn.Dropout(dropout_p)\n        self.query = torch.nn.Parameter(torch.randn(in_features, ))\n        self.key = torch.nn.Parameter(torch.randn(num_heads, self.head_dim))\n        self.value = torch.nn.Parameter(torch.randn(num_heads, self.head_dim))\n        self.scaled_dot_product = ScaledDotProduct()\n \n    def forward(self, x):\n        q = self.query.view(self.num_heads, 1, self.head_dim)\n        k = self.key\n        v = self.value\n \n        q = torch.matmul(q, k)\n        q = q / np.sqrt(self.head_dim)\n        q = self.dropout(q)\n \n        y = self.scaled_dot_product(q, x)\n        return y\n \n# Initializing the model\nconfig = Model.Config(dropout_p=0.0, in_features=10, num_heads=3)\nmodel = Model(**dataclasses.asdict(config))\n\n# Inputs to the model\nx = torch.randn(3, 1, 10)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, batch_size, dim_q, dim_k, dim_v, num_heads, dropout_p):\n        super().__init__()\n        # Parameters\n        self.dropout_p = dropout_p\n \n        # Query, key, and value linear transformations\n        self.q = torch.nn.Linear(dim_q, dim_q)\n        self.k = torch.nn.Linear(dim_k, dim_k)\n        self.v = torch.nn.Linear(dim_v, dim_v)\n \n        # Dropout\n        self.dropout = torch.nn.Dropout(p=dropout_p)\n \n    def forward(self, x1):\n        x2 = torch.nn.functional.relu(self.q(x1))\n        x3 = torch.nn.functional.relu(self.k(x1))\n        x4 = torch.nn.functional.relu(self.v(x1))\n \n        # Compute the dot product of the query and key tensors\n        qk = torch.matmul(x2, x3.transpose(-2, -1))\n \n        # Scale the dot product by the inverse scale factor\n        scaled_qk = qk.div(np.sqrt(self.dim_k))\n \n        # Apply softmax to the dot product\n        softmax_qk = scaled_qk.softmax(dim=-1)\n \n        # Apply dropout to the softmax output\n        dropout_qk = torch.nn.functional.dropout(softmax_qk, p=self.dropout_p)\n \n        # Compute the dot product of the dropout output and the value tensor\n        output = dropout_qk.matmul(x4)\n \n        return output\n \n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.mat = torch.nn.Linear(3, 3)\n        self.dropout = torch.nn.Dropout(0.8)\n \n    def forward(self, x1, x2, x3, x4):\n        q = self.mat(x1)\n        k = self.mat(x2)\n        k = k.T\n        v = self.mat(x3)\n        qk = torch.matmul(q, k)\n        inv_scale_factor = torch.sqrt(torch.as_tensor(q.size(-1)).float()).reciprocal()\n        scaled_qk = qk.div(inv_scale_factor)\n        softmax_qk = scaled_qk.softmax(dim=-1)\n        drop_qk = self.dropout(softmax_qk)\n        o = torch.matmul(drop_qk, v)\n        return o, o\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(3, 3)\nx2 = torch.randn(3, 3)\nx3 = torch.randn(3, 3)\nx4 = torch.randn(3, 3)\n\n\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, p):\n        super().__init__()\n        self.p = p\n \n    def forward(self, q, k, v, inf=1e9):\n        t1 = torch.matmul(q, k.transpose(-2, -1))\n        s1 = t1.div(self.p.i1)\n        t2 = torch.nn.functional.softmax(s1, dim=-1)\n        t3 = torch.nn.functional.dropout(t2, p=self.p.d0)\n        t4 = torch.matmul(t3, v)\n        return t4\n\n# Initializing the model\np = Param()\nm = Model(p)\n\n# Inputs to the model\nq = torch.randn(1, 40, 20)\nk = torch.randn(1, 60, 20)\nv = torch.randn(1, 60, 5)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, query, key, value, inv_scale_factor, dropout_p):\n        super().__init__()\n        self.query = query\n        self.key = key\n        self.value = value\n        self.inv_scale_factor = inv_scale_factor\n        self.dropout_p = dropout_p\n \n    def forward(self):\n        qk = torch.matmul(self.query, self.key.transpose(-2, -1))\n        scaled_qk = qk.div(self.inv_scale_factor)\n        softmax_qk = scaled_qk.softmax(dim=-1)\n        dropout_qk = torch.nn.functional.dropout(softmax_qk, p=self.dropout_p)\n        output = dropout_qk.matmul(self.value)\n        return output\n\n# Initializing the model parameters\nquery = torch.randn(2, 2, 2)\nkey = torch.randn(2, 1, 2)\nvalue = torch.randn(2, 3, 2)\ninv_scale_factor = 2.4\ndropout_p = 0.03\n\n# Initializing the model\nm = Model(query, key, value, inv_scale_factor, dropout_p)\n\n# Inputs to the model\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n \n    def forward(self, v1, v2):\n        v3 = torch.matmul(v1, v2.transpose(-2, -1))\n        v4 = v3.div(9.765625)\n        v5 = v4.softmax(dim=-1)\n        v6 = torch.nn.functional.dropout(v5, p=0.3062484076194763)\n        v7 = torch.matmul(v6, v2)\n        return v7\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nv1 = torch.randn(1, 16, 256)\nv2 = torch.randn(1, 2, 256)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    \n    def forward(self, query, key, value, inv_scale_factor, dropout_p):\n        qk = torch.matmul(query, key.transpose(-2, -1))\n        scaled_qk = qk.div(inv_scale_factor)\n        softmax_qk = scaled_qk.softmax(dim=-1)\n        dropout_qk = torch.nn.functional.dropout(softmax_qk, p=dropout_p)\n        output = dropout_qk.matmul(value)\n        return output\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nbatch, heads, seq_len_q, seq_len_k, dim = 4, 2, 8, 8, 16\nquery = torch.randn(batch, heads, seq_len_q, seq_len_k, dim)\nkey = torch.randn(batch, heads, seq_len_k, seq_len_q, dim)\nvalue = torch.randn(batch, heads, seq_len_k, seq_len_q, dim)\ninv_scale_factor = torch.randn(1,1)\ndropout_p = 0.5\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, embed_dim, num_heads, dropout_p):\n        super().__init__()\n        self.head = SingleHeadAttention(embed_dim, num_heads, dropout_p)\n        self.layernorm = torch.nn.LayerNorm([embed_dim])\n \n    def forward(self, query, key, value):\n        output = self.head(query, key, value)\n        output = self.layernorm(output)\n        return output\n\n# Initializing the model\nm = Model(embed_dim=128, num_heads=4, dropout_p=0.5)\n\n# Inputs to the model\nquery = torch.randn(2, 3, 128)\nkey = torch.randn(2, 4, 128)\nvalue = torch.randn(2, 4, 128)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.output_size = 256\n\n    def forward(self, query, key, value, query_scale_factor=1.0, key_scale_factor=1.0, value_scale_factor=1.0):\n        qk = torch.matmul(query, key.transpose(-2, -1)) * query_scale_factor * key_scale_factor\n        scaled_qk = torch.div(qk, value_scale_factor)\n        softmax_qk = torch.nn.functional.softmax(scaled_qk, dim=-1)\n        dropout_qk = torch.nn.functional.dropout(softmax_qk, p=0.4)\n        output = dropout_qk.matmul(value)\n        return output\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nquery = torch.randn(8, 256, 30)\nkey = torch.randn(8, 30, 256)\nvalue = torch.randn(8, 256, 256)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc_qk = torch.nn.Linear(dim_hidden, dim_hidden, bias=False)\n        self.fc_v  = torch.nn.Linear(dim_hidden, dim_hidden, bias=False)\n        self.fc_o  = torch.nn.Linear(dim_hidden, dim_hidden, bias=False)\n \n    def forward(self, input):\n        q   = self.fc_qk(input)\n        k   = self.fc_qk(input)\n        v   = self.fc_v(input)\n        qk  = torch.matmul(q, k.transpose(-2, -1))\n        inv_scale_factor = math.sqrt(dim_hidden)\n        scaled_qk = qk.div(inv_scale_factor)\n        softmax_qk = scaled_qk.softmax(dim=-1)\n        dropout_qk = torch.nn.functional.dropout(softmax_qk, p=dropout_p)\n        output = dropout_qk.matmul(v)\n        return self.fc_o(output)\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx  = torch.randn(16, dim_hidden, 4)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    class Config(NamedTuple):\n        dropout_p: float\n        in_features: int\n        num_heads: int\n \n    def __init__(self, dropout_p, in_features, num_heads):\n        super().__init__()\n        self.dropout_p = dropout_p\n        self.in_features = in_features\n        self.num_heads = num_heads\n        self.head_dim = in_features // num_heads\n \n        self.dropout = torch.nn.Dropout(dropout_p)\n        self.query = torch.nn.Parameter(torch.randn(in_features, ))\n        self.key = torch.nn.Parameter(torch.randn(num_heads, self.head_dim))\n        self.value = torch.nn.Parameter(torch.randn(num_heads, self.head_dim))\n        self.scaled_dot_product = ScaledDotProduct()\n \n    def forward(self, x):\n        q = self.query.view(self.num_heads, 1, self.head_dim)\n        k = self.key\n        v = self.value\n \n        q = torch.matmul(q, k)\n        q = q / np.sqrt(self.head_dim)\n        q = self.dropout(q)\n \n        y = self.scaled_dot_product(q, x)\n        return y\n \n# Initializing the model\nconfig = Model.Config(dropout_p=0.0, in_features=10, num_heads=3)\nmodel = Model(**dataclasses.asdict(config))\n\n# Inputs to the model\nx = torch.randn(3, 1, 10)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, batch_size, dim_q, dim_k, dim_v, num_heads, dropout_p):\n        super().__init__()\n        # Parameters\n        self.dropout_p = dropout_p\n \n        # Query, key, and value linear transformations\n        self.q = torch.nn.Linear(dim_q, dim_q)\n        self.k = torch.nn.Linear(dim_k, dim_k)\n        self.v = torch.nn.Linear(dim_v, dim_v)\n \n        # Dropout\n        self.dropout = torch.nn.Dropout(p=dropout_p)\n \n    def forward(self, x1):\n        x2 = torch.nn.functional.relu(self.q(x1))\n        x3 = torch.nn.functional.relu(self.k(x1))\n        x4 = torch.nn.functional.relu(self.v(x1))\n \n        # Compute the dot product of the query and key tensors\n        qk = torch.matmul(x2, x3.transpose(-2, -1))\n \n        # Scale the dot product by the inverse scale factor\n        scaled_qk = qk.div(np.sqrt(self.dim_k))\n \n        # Apply softmax to the dot product\n        softmax_qk = scaled_qk.softmax(dim=-1)\n \n        # Apply dropout to the softmax output\n        dropout_qk = torch.nn.functional.dropout(softmax_qk, p=self.dropout_p)\n \n        # Compute the dot product of the dropout output and the value tensor\n        output = dropout_qk.matmul(x4)\n \n        return output\n \n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.mat = torch.nn.Linear(3, 3)\n        self.dropout = torch.nn.Dropout(0.8)\n \n    def forward(self, x1, x2, x3, x4):\n        q = self.mat(x1)\n        k = self.mat(x2)\n        k = k.T\n        v = self.mat(x3)\n        qk = torch.matmul(q, k)\n        inv_scale_factor = torch.sqrt(torch.as_tensor(q.size(-1)).float()).reciprocal()\n        scaled_qk = qk.div(inv_scale_factor)\n        softmax_qk = scaled_qk.softmax(dim=-1)\n        drop_qk = self.dropout(softmax_qk)\n        o = torch.matmul(drop_qk, v)\n        return o, o\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(3, 3)\nx2 = torch.randn(3, 3)\nx3 = torch.randn(3, 3)\nx4 = torch.randn(3, 3)\n\n\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, p):\n        super().__init__()\n        self.p = p\n \n    def forward(self, q, k, v, inf=1e9):\n        t1 = torch.matmul(q, k.transpose(-2, -1))\n        s1 = t1.div(self.p.i1)\n        t2 = torch.nn.functional.softmax(s1, dim=-1)\n        t3 = torch.nn.functional.dropout(t2, p=self.p.d0)\n        t4 = torch.matmul(t3, v)\n        return t4\n\n# Initializing the model\np = Param()\nm = Model(p)\n\n# Inputs to the model\nq = torch.randn(1, 40, 20)\nk = torch.randn(1, 60, 20)\nv = torch.randn(1, 60, 5)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, query, key, value, inv_scale_factor, dropout_p):\n        super().__init__()\n        self.query = query\n        self.key = key\n        self.value = value\n        self.inv_scale_factor = inv_scale_factor\n        self.dropout_p = dropout_p\n \n    def forward(self):\n        qk = torch.matmul(self.query, self.key.transpose(-2, -1))\n        scaled_qk = qk.div(self.inv_scale_factor)\n        softmax_qk = scaled_qk.softmax(dim=-1)\n        dropout_qk = torch.nn.functional.dropout(softmax_qk, p=self.dropout_p)\n        output = dropout_qk.matmul(self.value)\n        return output\n\n# Initializing the model parameters\nquery = torch.randn(2, 2, 2)\nkey = torch.randn(2, 1, 2)\nvalue = torch.randn(2, 3, 2)\ninv_scale_factor = 2.4\ndropout_p = 0.03\n\n# Initializing the model\nm = Model(query, key, value, inv_scale_factor, dropout_p)\n\n# Inputs to the model\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n \n    def forward(self, v1, v2):\n        v3 = torch.matmul(v1, v2.transpose(-2, -1))\n        v4 = v3.div(9.765625)\n        v5 = v4.softmax(dim=-1)\n        v6 = torch.nn.functional.dropout(v5, p=0.3062484076194763)\n        v7 = torch.matmul(v6, v2)\n        return v7\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nv1 = torch.randn(1, 16, 256)\nv2 = torch.randn(1, 2, 256)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    \n    def forward(self, query, key, value, inv_scale_factor, dropout_p):\n        qk = torch.matmul(query, key.transpose(-2, -1))\n        scaled_qk = qk.div(inv_scale_factor)\n        softmax_qk = scaled_qk.softmax(dim=-1)\n        dropout_qk = torch.nn.functional.dropout(softmax_qk, p=dropout_p)\n        output = dropout_qk.matmul(value)\n        return output\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nbatch, heads, seq_len_q, seq_len_k, dim = 4, 2, 8, 8, 16\nquery = torch.randn(batch, heads, seq_len_q, seq_len_k, dim)\nkey = torch.randn(batch, heads, seq_len_k, seq_len_q, dim)\nvalue = torch.randn(batch, heads, seq_len_k, seq_len_q, dim)\ninv_scale_factor = torch.randn(1,1)\ndropout_p = 0.5\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, embed_dim, num_heads, dropout_p):\n        super().__init__()\n        self.head = SingleHeadAttention(embed_dim, num_heads, dropout_p)\n        self.layernorm = torch.nn.LayerNorm([embed_dim])\n \n    def forward(self, query, key, value):\n        output = self.head(query, key, value)\n        output = self.layernorm(output)\n        return output\n\n# Initializing the model\nm = Model(embed_dim=128, num_heads=4, dropout_p=0.5)\n\n# Inputs to the model\nquery = torch.randn(2, 3, 128)\nkey = torch.randn(2, 4, 128)\nvalue = torch.randn(2, 4, 128)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.output_size = 256\n\n    def forward(self, query, key, value, query_scale_factor=1.0, key_scale_factor=1.0, value_scale_factor=1.0):\n        qk = torch.matmul(query, key.transpose(-2, -1)) * query_scale_factor * key_scale_factor\n        scaled_qk = torch.div(qk, value_scale_factor)\n        softmax_qk = torch.nn.functional.softmax(scaled_qk, dim=-1)\n        dropout_qk = torch.nn.functional.dropout(softmax_qk, p=0.4)\n        output = dropout_qk.matmul(value)\n        return output\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nquery = torch.randn(8, 256, 30)\nkey = torch.randn(8, 30, 256)\nvalue = torch.randn(8, 256, 256)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc_qk = torch.nn.Linear(dim_hidden, dim_hidden, bias=False)\n        self.fc_v  = torch.nn.Linear(dim_hidden, dim_hidden, bias=False)\n        self.fc_o  = torch.nn.Linear(dim_hidden, dim_hidden, bias=False)\n \n    def forward(self, input):\n        q   = self.fc_qk(input)\n        k   = self.fc_qk(input)\n        v   = self.fc_v(input)\n        qk  = torch.matmul(q, k.transpose(-2, -1))\n        inv_scale_factor = math.sqrt(dim_hidden)\n        scaled_qk = qk.div(inv_scale_factor)\n        softmax_qk = scaled_qk.softmax(dim=-1)\n        dropout_qk = torch.nn.functional.dropout(softmax_qk, p=dropout_p)\n        output = dropout_qk.matmul(v)\n        return self.fc_o(output)\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx  = torch.randn(16, dim_hidden, 4)\n"
            ],
            "g_time": 12.286707162857056
        }
    }
}
{
    "opt": "unary=0",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 32, 1, stride=1, padding=0, dilation=1, groups=3)\n        self.conv2 = torch.nn.Conv2d(32, 2, 5, stride=1, padding=1, dilation=2, groups=3)\n        self.conv3 = torch.nn.Conv2d(6, 8, 5, stride=1, padding=0, dilation=1, groups=3)\n    def forward(self, x3):\n        v1 = self.conv1(x3)\n        v2 = self.conv2(v1)\n        v3 = self.conv3(x3)\n        v4 = torch.cat((v2, v3), dim=1)\n        return v4\n# Inputs to the model\nx3 = torch.randn(2, 6, 128, 128)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(1, 15, 7, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * v1\n        v4 = v3 * v1\n        v5 = v4 * 0.044715\n        v6 = v1 + v5\n        v7 = v6 * 0.7978845608028654\n        v8 = torch.tanh(v7)\n        v9 = v8 + 1\n        v10 = v2 * v9\n        return v10\n# Inputs to the model\nx1 = torch.randn(1, 1, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(15, 20, 5, stride=1, padding=1)\n    def forward(self, x3):\n        v1 = self.conv(x3)\n        v2 = v1 * 0.5\n        v3 = v1 * v1\n        v4 = v3 * v1\n        v5 = v4 * 0.044715\n        v6 = v1 + v5\n        v7 = v6 * 0.7978845608028654\n        v8 = torch.tanh(v7)\n        v9 = v8 + 1\n        v10 = v2 * v9\n        return v10\n# Inputs to the model\nx3 = torch.randn(7, 15, 53, 101)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(1, 23, 7, stride=1, padding=0)\n        self.bn1 = torch.nn.BatchNorm2d(23)\n        self.conv2 = torch.nn.Conv2d(23, 15, 7, stride=1, padding=0)\n        self.bn2 = torch.nn.BatchNorm2d(15)\n    def forward(self, x3):\n        v1 = self.conv1(x3)\n        v2 = self.bn1(v1)\n        v3 = self.conv2(v2)\n        v4 = self.bn2(v3)\n        v5 = v4 * 0.5\n        v6 = v4 * v4\n        v7 = v6 * v4\n        v8 = v7 * 0.044715\n        v9 = v4 + v8\n        v10 = v9 * 0.7978845608028654\n        v11 = torch.tanh(v10)\n        v12 = v11 + 1\n        v13 = v5 * v12\n        return v13\n# Inputs to the model\nx3 = torch.randn(4, 1, 23, 23)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(1, 12, 1, stride=1, padding=1, dilation=2)\n    def forward(self, x2):\n        v1 = self.conv(x2)\n        v2 = v1 * 0.5\n        v3 = v1 * v1\n        v4 = v3 * v1\n        v5 = v4 * 0.044715\n        v6 = v1 + v5\n        v7 = v6 * 0.7978845608028654\n        v8 = torch.tanh(v7)\n        v9 = v8 + 1\n        v10 = v2 * v9\n        return v10\n# Inputs to the model\nx2 = torch.randn(2, 1, 27, 27)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(9, 6, 7, stride=12, padding=1, dilation=12)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * v1\n        v4 = v3 * v1\n        v5 = v4 * 0.044715\n        v6 = v1 + v5\n        v7 = v6 * 0.7978845608028654\n        v8 = torch.tanh(v7)\n        v9 = v8 + 1\n        v10 = v2 * v9\n        return v10\n# Inputs to the model\nx1 = torch.randn(4, 9, 23, 23)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(1, 14, 1, stride=1, padding=0)\n        self.conv2 = torch.nn.Conv2d(14, 27, 1, stride=1, padding=0)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = self.conv2(v1)\n        return v2\n# Inputs to the model\nx3 = torch.randn(1, 1, 23, 23)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(19, 8, kernel_size=(11, 7), stride=(1, 2), padding=(2, 1))\n    def forward(self, x2):\n        v1 = self.conv(x2)\n        v2 = v1 * 0.5\n        v3 = v1 * v1\n        v4 = v3 * v1\n        v5 = v4 * 0.044715\n        v6 = v1 + v5\n        v7 = v6 * 0.7978845608028654\n        v8 = torch.tanh(v7)\n        v9 = v8 + 1\n        v10 = v2 * v9\n        return v10\n# Inputs to the model\nx2 = torch.randn(9, 19, 55, 17)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(15, 5, 7, stride=1, padding=1)\n    def forward(self, x2):\n        v1 = self.conv(x2)\n        v2 = v1 * 0.5\n        v3 = v1 * v1\n        v4 = v3 * v1\n        v5 = v4 * 0.044715\n        v6 = v1 + v5\n        v7 = v6 * 0.7978845608028654\n        v8 = torch.tanh(v7)\n        v9 = v8 + 1\n        v10 = v2 * v9\n        return v10\n# Inputs to the model\nx2 = torch.randn(4, 15, 20, 20)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(1, 30, 1, stride=1, padding=0)\n    def forward(self, x2):\n        v1 = self.conv(x2)\n        v4 = v1 * 0.044715\n        v5 = v1 + v4\n        v8 = v5 * 0.7978845608028654\n        v9 = torch.tanh(v8)\n        v10 = v9 + 1\n        return v10\n# Inputs to the model\nx2 = torch.randn(1, 1, 15, 15)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 32, 1, stride=1, padding=0, dilation=1, groups=3)\n        self.conv2 = torch.nn.Conv2d(32, 2, 5, stride=1, padding=1, dilation=2, groups=3)\n        self.conv3 = torch.nn.Conv2d(6, 8, 5, stride=1, padding=0, dilation=1, groups=3)\n    def forward(self, x3):\n        v1 = self.conv1(x3)\n        v2 = self.conv2(v1)\n        v3 = self.conv3(x3)\n        v4 = torch.cat((v2, v3), dim=1)\n        return v4\n# Inputs to the model\nx3 = torch.randn(2, 6, 128, 128)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(1, 15, 7, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * v1\n        v4 = v3 * v1\n        v5 = v4 * 0.044715\n        v6 = v1 + v5\n        v7 = v6 * 0.7978845608028654\n        v8 = torch.tanh(v7)\n        v9 = v8 + 1\n        v10 = v2 * v9\n        return v10\n# Inputs to the model\nx1 = torch.randn(1, 1, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(15, 20, 5, stride=1, padding=1)\n    def forward(self, x3):\n        v1 = self.conv(x3)\n        v2 = v1 * 0.5\n        v3 = v1 * v1\n        v4 = v3 * v1\n        v5 = v4 * 0.044715\n        v6 = v1 + v5\n        v7 = v6 * 0.7978845608028654\n        v8 = torch.tanh(v7)\n        v9 = v8 + 1\n        v10 = v2 * v9\n        return v10\n# Inputs to the model\nx3 = torch.randn(7, 15, 53, 101)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(1, 23, 7, stride=1, padding=0)\n        self.bn1 = torch.nn.BatchNorm2d(23)\n        self.conv2 = torch.nn.Conv2d(23, 15, 7, stride=1, padding=0)\n        self.bn2 = torch.nn.BatchNorm2d(15)\n    def forward(self, x3):\n        v1 = self.conv1(x3)\n        v2 = self.bn1(v1)\n        v3 = self.conv2(v2)\n        v4 = self.bn2(v3)\n        v5 = v4 * 0.5\n        v6 = v4 * v4\n        v7 = v6 * v4\n        v8 = v7 * 0.044715\n        v9 = v4 + v8\n        v10 = v9 * 0.7978845608028654\n        v11 = torch.tanh(v10)\n        v12 = v11 + 1\n        v13 = v5 * v12\n        return v13\n# Inputs to the model\nx3 = torch.randn(4, 1, 23, 23)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(1, 12, 1, stride=1, padding=1, dilation=2)\n    def forward(self, x2):\n        v1 = self.conv(x2)\n        v2 = v1 * 0.5\n        v3 = v1 * v1\n        v4 = v3 * v1\n        v5 = v4 * 0.044715\n        v6 = v1 + v5\n        v7 = v6 * 0.7978845608028654\n        v8 = torch.tanh(v7)\n        v9 = v8 + 1\n        v10 = v2 * v9\n        return v10\n# Inputs to the model\nx2 = torch.randn(2, 1, 27, 27)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(9, 6, 7, stride=12, padding=1, dilation=12)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * v1\n        v4 = v3 * v1\n        v5 = v4 * 0.044715\n        v6 = v1 + v5\n        v7 = v6 * 0.7978845608028654\n        v8 = torch.tanh(v7)\n        v9 = v8 + 1\n        v10 = v2 * v9\n        return v10\n# Inputs to the model\nx1 = torch.randn(4, 9, 23, 23)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(1, 14, 1, stride=1, padding=0)\n        self.conv2 = torch.nn.Conv2d(14, 27, 1, stride=1, padding=0)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = self.conv2(v1)\n        return v2\n# Inputs to the model\nx3 = torch.randn(1, 1, 23, 23)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(19, 8, kernel_size=(11, 7), stride=(1, 2), padding=(2, 1))\n    def forward(self, x2):\n        v1 = self.conv(x2)\n        v2 = v1 * 0.5\n        v3 = v1 * v1\n        v4 = v3 * v1\n        v5 = v4 * 0.044715\n        v6 = v1 + v5\n        v7 = v6 * 0.7978845608028654\n        v8 = torch.tanh(v7)\n        v9 = v8 + 1\n        v10 = v2 * v9\n        return v10\n# Inputs to the model\nx2 = torch.randn(9, 19, 55, 17)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(15, 5, 7, stride=1, padding=1)\n    def forward(self, x2):\n        v1 = self.conv(x2)\n        v2 = v1 * 0.5\n        v3 = v1 * v1\n        v4 = v3 * v1\n        v5 = v4 * 0.044715\n        v6 = v1 + v5\n        v7 = v6 * 0.7978845608028654\n        v8 = torch.tanh(v7)\n        v9 = v8 + 1\n        v10 = v2 * v9\n        return v10\n# Inputs to the model\nx2 = torch.randn(4, 15, 20, 20)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(1, 30, 1, stride=1, padding=0)\n    def forward(self, x2):\n        v1 = self.conv(x2)\n        v4 = v1 * 0.044715\n        v5 = v1 + v4\n        v8 = v5 * 0.7978845608028654\n        v9 = torch.tanh(v8)\n        v10 = v9 + 1\n        return v10\n# Inputs to the model\nx2 = torch.randn(1, 1, 15, 15)\n"
            ],
            "g_time": 12.95714020729065
        }
    }
}
{
    "opt": "binary=6",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 - 1.0\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 1, 1)\n",
                "\nclass Model2(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(1, 2)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 - x1\n        return v2\n\n# Initializing the model\nm2 = Model2()\n\n# Inputs to the model\nx1 = torch.randn(1, 1, 1, 1)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(5, 10)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 - other\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 5)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(64, 16)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 - other\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(19, 21)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 - other\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 19)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(12, 16)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 - other\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 12)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8, bias=False)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 - other\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(10, 4)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 - 3\n        return v2\n\n# Initializing the model\nm = Model()\nv = torch.randn(1, 10)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.Linear = torch.nn.Linear(67, 33)\n \n    def forward(self, x1):\n        v1 = self.Linear(x1)\n        v2 = v1 - other\n        return \nx1 = torch.randn(1, 67)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 4, bias=False)\n \n    def forward(self, x1, x2):\n        v1 = self.linear(x1)\n        v2 = v1 - x2\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 4, 4)\nx2 = torch.randn(1, 4, 4)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 - 1.0\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 1, 1)\n",
                "\nclass Model2(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(1, 2)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 - x1\n        return v2\n\n# Initializing the model\nm2 = Model2()\n\n# Inputs to the model\nx1 = torch.randn(1, 1, 1, 1)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(5, 10)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 - other\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 5)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(64, 16)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 - other\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(19, 21)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 - other\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 19)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(12, 16)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 - other\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 12)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8, bias=False)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 - other\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(10, 4)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 - 3\n        return v2\n\n# Initializing the model\nm = Model()\nv = torch.randn(1, 10)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.Linear = torch.nn.Linear(67, 33)\n \n    def forward(self, x1):\n        v1 = self.Linear(x1)\n        v2 = v1 - other\n        return \nx1 = torch.randn(1, 67)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 4, bias=False)\n \n    def forward(self, x1, x2):\n        v1 = self.linear(x1)\n        v2 = v1 - x2\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 4, 4)\nx2 = torch.randn(1, 4, 4)\n"
            ],
            "g_time": 5.138075113296509
        }
    }
}
{
    "opt": "unary=1",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(5, 2)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * 0.5\n        v3 = (v1 * v1 * v1) * 0.044715\n        v4 = v3 + v2\n        v5 = v4 * 0.7978845608028654\n        v6 = torch.tanh(v5)\n        v7 = v6 + 1\n        v8 = v7 * v2\n        return v8\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 5)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(32, 64, bias=True)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * 0.5\n        c1 = (v1 * v1 * v1) * 0.044715\n        v3 = c1 + v2\n        v4 = v3 * 0.7978845608028654\n        v5 = torch.tanh(v4)\n        v6 = v5 + 1\n        v7 = v2 * v6\n        return v7\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 32, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(2, 4)\n \n    def forward(self, x2):\n        v1 = self.linear(x2)\n        v2 = v1 * 0.5\n        v3 = v1 + (v1 * v1 * v1) * 0.044715\n        v4 = v3 * 0.7978845608028654\n        v5 = torch.tanh(v4)\n        v6 = v5 + 1\n        v7 = v2 * v6\n        return v7\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx2 = torch.randn(4, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(8, 8)\n \n    def forward(self, __input__):\n        v1 = self.linear(__input__)\n        v2 = v1 * 0.5\n        v3 = v1 + (v1 * v1 * v1) * 0.044715\n        v4 = v3 * 0.7978845608028654\n        v5 = torch.tanh(v4)\n        v6 = v5 + 1\n        v7 = v2 * v6\n        return v7\n        \n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(16, 32)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * 0.5\n        v3 = v1 + (v1 * v1 * v1) * 0.044715\n        v4 = v3 * 0.7978845608028654\n        v5 = torch.tanh(v4)\n        v6 = v5 + 1\n        v7 = v2 * v6\n        return v7\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear1 = torch.nn.Linear(1, 8, bias=False)\n        self.linear2 = torch.nn.Linear(8, 8, bias=False)\n \n    def forward(self, x1):\n        v1 = self.linear1(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * v1 * v1 * 0.044715\n        v4 = v3 * 0.7978845608028654\n        v5 = torch.tanh(v4)\n        v6 = v5 + 1\n        v7 = v2 * v6\n        return v7\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 1)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * 0.5\n        v3 = v1 + (v1 * v1 * v1) * 0.044715\n        v4 = v3 * 0.7978845608028654\n        v5 = torch.tanh(v4)\n        v6 = v5 + 1\n        v7 = v2 * v6\n        return v7\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n   \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * 0.5\n        v3 = v1 + (v1 * v1 * v1) * 0.044715\n        v4 = v3 * 0.7978845608028654\n        v5 = torch.tanh(v4)\n        v6 = v5 + 1\n        v7 = v2 * v6\n        return v7\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(8, 16, bias=False)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * 0.5\n        v3 = v1 + (v1 * v1 * v1) * 0.044715\n        v4 = v3 * 0.7978845608028654\n        v5 = torch.tanh(v4)\n        v6 = v5 + 1\n        v7 = v2 * v6\n        return v7\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(2, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 10)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * 0.5\n        v3 = v1 + (v1 * v1 * v1) * 0.044715\n        v4 = v3 * 0.7978845608028654\n        v5 = torch.tanh(v4)\n        v6 = v5 + 1\n        v7 = v2 * v6\n        return v7"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(5, 2)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * 0.5\n        v3 = (v1 * v1 * v1) * 0.044715\n        v4 = v3 + v2\n        v5 = v4 * 0.7978845608028654\n        v6 = torch.tanh(v5)\n        v7 = v6 + 1\n        v8 = v7 * v2\n        return v8\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 5)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(32, 64, bias=True)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * 0.5\n        c1 = (v1 * v1 * v1) * 0.044715\n        v3 = c1 + v2\n        v4 = v3 * 0.7978845608028654\n        v5 = torch.tanh(v4)\n        v6 = v5 + 1\n        v7 = v2 * v6\n        return v7\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 32, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(2, 4)\n \n    def forward(self, x2):\n        v1 = self.linear(x2)\n        v2 = v1 * 0.5\n        v3 = v1 + (v1 * v1 * v1) * 0.044715\n        v4 = v3 * 0.7978845608028654\n        v5 = torch.tanh(v4)\n        v6 = v5 + 1\n        v7 = v2 * v6\n        return v7\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx2 = torch.randn(4, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(8, 8)\n \n    def forward(self, __input__):\n        v1 = self.linear(__input__)\n        v2 = v1 * 0.5\n        v3 = v1 + (v1 * v1 * v1) * 0.044715\n        v4 = v3 * 0.7978845608028654\n        v5 = torch.tanh(v4)\n        v6 = v5 + 1\n        v7 = v2 * v6\n        return v7\n        \n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(16, 32)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * 0.5\n        v3 = v1 + (v1 * v1 * v1) * 0.044715\n        v4 = v3 * 0.7978845608028654\n        v5 = torch.tanh(v4)\n        v6 = v5 + 1\n        v7 = v2 * v6\n        return v7\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear1 = torch.nn.Linear(1, 8, bias=False)\n        self.linear2 = torch.nn.Linear(8, 8, bias=False)\n \n    def forward(self, x1):\n        v1 = self.linear1(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * v1 * v1 * 0.044715\n        v4 = v3 * 0.7978845608028654\n        v5 = torch.tanh(v4)\n        v6 = v5 + 1\n        v7 = v2 * v6\n        return v7\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 1)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * 0.5\n        v3 = v1 + (v1 * v1 * v1) * 0.044715\n        v4 = v3 * 0.7978845608028654\n        v5 = torch.tanh(v4)\n        v6 = v5 + 1\n        v7 = v2 * v6\n        return v7\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n   \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * 0.5\n        v3 = v1 + (v1 * v1 * v1) * 0.044715\n        v4 = v3 * 0.7978845608028654\n        v5 = torch.tanh(v4)\n        v6 = v5 + 1\n        v7 = v2 * v6\n        return v7\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(8, 16, bias=False)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * 0.5\n        v3 = v1 + (v1 * v1 * v1) * 0.044715\n        v4 = v3 * 0.7978845608028654\n        v5 = torch.tanh(v4)\n        v6 = v5 + 1\n        v7 = v2 * v6\n        return v7\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(2, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 10)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * 0.5\n        v3 = v1 + (v1 * v1 * v1) * 0.044715\n        v4 = v3 * 0.7978845608028654\n        v5 = torch.tanh(v4)\n        v6 = v5 + 1\n        v7 = v2 * v6\n        return v7"
            ],
            "g_time": 8.439988374710083
        }
    }
}
{
    "opt": "unary=8",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(18, 32, 7, stride=3, padding=3, output_padding=0)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp(v2, min=0)\n        v4 = torch.clamp(v3, max=6)\n        v5 = v1 * v4\n        v6 = v5 / 6\n        return v6\n# Input to the model\nx1 = torch.randn(1, 18, 76, 76)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(12000, 4096, 1, stride=1, padding=0, output_padding=0)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp(v2, min=0)\n        v4 = torch.clamp(v3, max=6)\n        v5 = v1 * v4\n        v6 = v5 / 6\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 12000, 1, 1)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(12, 18, kernel_size=(3, 4), stride=(4, 1), padding=2, output_padding=0)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp(v2, min=0)\n        v4 = torch.clamp(v3, max=6)\n        v5 = v1 * v4\n        v6 = v5 / 6\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 12, 48, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(32, 32, 3, stride=1)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp(v2, min=0)\n        v4 = torch.clamp(v3, max=6)\n        v5 = v1 * v4\n        v6 = v5 / 6\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 32, 100, 150)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(8, 12, 6, 3, padding=2, output_padding=0)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp(v2, min=0)\n        v4 = torch.clamp(v3, max=6)\n        v5 = v1 * v4\n        v6 = v5 / 6\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 8, 40, 40)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(32, 32, 3, stride=2, padding=1, output_padding=1)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp(v2, min=0)\n        v4 = torch.clamp(v3, max=6)\n        v5 = v1 * v4\n        v6 = v5 / 6\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 32, 36, 36)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(45, 18, 3, stride=1, padding=1, output_padding=0)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp(v2, min=0)\n        v4 = torch.clamp(v3, max=6)\n        v5 = v1 * v4\n        v6 = v5 / 6\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 45, 36, 36)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(2, 32, (2, 4), 3, output_padding=(1, 2))\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp(v2, min=0)\n        v4 = torch.clamp(v3, max=6)\n        v5 = v1 * v4\n        v6 = v5 / 6\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 2, 16, 24)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(3, 32, (2, 4), 3, output_padding=(1, 2))\n        self.linear = torch.nn.Linear(1344, 16)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp(v2, min=0)\n        v4 = torch.clamp(v3, max=6)\n        v5 = v1 * v4\n        v6 = v5 / 6\n        v7 = v6.flatten(start_dim=1)\n        v8 = self.linear(v7)\n        return v8\n# Inputs to the model\nx1 = torch.randn(1, 3, 12, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(1, 512, 4, 2)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp(v2, min=0)\n        v4 = torch.clamp(v3, max=6)\n        v5 = v1 * v4\n        v6 = v5 / 6\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 1, 196, 256)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(18, 32, 7, stride=3, padding=3, output_padding=0)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp(v2, min=0)\n        v4 = torch.clamp(v3, max=6)\n        v5 = v1 * v4\n        v6 = v5 / 6\n        return v6\n# Input to the model\nx1 = torch.randn(1, 18, 76, 76)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(12000, 4096, 1, stride=1, padding=0, output_padding=0)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp(v2, min=0)\n        v4 = torch.clamp(v3, max=6)\n        v5 = v1 * v4\n        v6 = v5 / 6\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 12000, 1, 1)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(12, 18, kernel_size=(3, 4), stride=(4, 1), padding=2, output_padding=0)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp(v2, min=0)\n        v4 = torch.clamp(v3, max=6)\n        v5 = v1 * v4\n        v6 = v5 / 6\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 12, 48, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(32, 32, 3, stride=1)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp(v2, min=0)\n        v4 = torch.clamp(v3, max=6)\n        v5 = v1 * v4\n        v6 = v5 / 6\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 32, 100, 150)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(8, 12, 6, 3, padding=2, output_padding=0)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp(v2, min=0)\n        v4 = torch.clamp(v3, max=6)\n        v5 = v1 * v4\n        v6 = v5 / 6\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 8, 40, 40)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(32, 32, 3, stride=2, padding=1, output_padding=1)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp(v2, min=0)\n        v4 = torch.clamp(v3, max=6)\n        v5 = v1 * v4\n        v6 = v5 / 6\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 32, 36, 36)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(45, 18, 3, stride=1, padding=1, output_padding=0)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp(v2, min=0)\n        v4 = torch.clamp(v3, max=6)\n        v5 = v1 * v4\n        v6 = v5 / 6\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 45, 36, 36)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(2, 32, (2, 4), 3, output_padding=(1, 2))\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp(v2, min=0)\n        v4 = torch.clamp(v3, max=6)\n        v5 = v1 * v4\n        v6 = v5 / 6\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 2, 16, 24)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(3, 32, (2, 4), 3, output_padding=(1, 2))\n        self.linear = torch.nn.Linear(1344, 16)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp(v2, min=0)\n        v4 = torch.clamp(v3, max=6)\n        v5 = v1 * v4\n        v6 = v5 / 6\n        v7 = v6.flatten(start_dim=1)\n        v8 = self.linear(v7)\n        return v8\n# Inputs to the model\nx1 = torch.randn(1, 3, 12, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(1, 512, 4, 2)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp(v2, min=0)\n        v4 = torch.clamp(v3, max=6)\n        v5 = v1 * v4\n        v6 = v5 / 6\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 1, 196, 256)\n"
            ],
            "g_time": 8.244323968887329
        }
    }
}
{
    "opt": "cat_slice_cat",
    "response": {
        "0": {
            "raw": [
                "\ndef func(f0, f1, f2, x4):\n    v2 = f0(x4)\n    v3 = x4 * f1(x4)\n    v5 = x4 - f0(x4)\n    v6 = f2(x4)\n    v7 = f2(x4)\n    v10 = f2(v2)\n    v11 = v5 * v6\n    v12 = v5 - v10\n    v15 = v6 * v7\n    v16 = v12 + v15\n    v17 = v11 * v16\n    return v17\n\n# Initializing the model\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n \n    def forward(self, x1, x2, x6, x7):\n        v1 = torch.cat([x1, x2, x6, x7], dim=1)\n        v2 = v1[:, 0:9223372036854775807]\n        v3 = v2[:, 0:34]\n        v4 = torch.cat([v1, v3], dim=1)\n        return v4\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 34, 64, 64)\nx2 = torch.randn(1, 2000, 56, 56)\nx6 = torch.randn(1, 256, 28, 28)\nx7 = torch.randn(1, 512, 14, 14)\n",
                " \nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n \n    def forward(self, x1):\n        v1 = torch.cat(input_tensors, dim=1)\n        v2 = v1[:, 0:9223372036854775807]\n        v3 = v2[:, 0:size]\n        v4 = torch.cat([v1, v3], dim=1)\n        return v4\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\nx2 = torch.randn(1, 3, 64, 64)\nx3 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n \n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = torch.cat([v1, v1, v1], dim=1)\n        v3 = v2[:, 0:1073741823]\n        v4 = v2[:, 0:2147483647]\n        v5 = torch.cat([v2, v3, v4], dim=1)\n        return v5\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n \n    def forward(self, x1, x2, x3):\n        v1 = torch.cat([x1, x2, x3], dim=1)\n        v2 = v1[:, 0:9223372036854775807]\n        v3 = v2[:, 0:v1.size(1)]\n        v4 = torch.cat([v1, v3], dim=1)\n        return v4\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 6, 5, 5)\nx2 = torch.randn(1, 7, 5, 5)\nx3 = torch.randn(1, 8, 5, 5)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, x1):\n        v1 = torch.cat([x1, x2])\n        v2 = v1[:, 0:9223372036854775807]\n        v3 = v2[:, 0:8]\n        v4 = torch.cat([v1, v3], dim=1)\n        return v4\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 8, 16, 16)\nx2 = torch.randn(1, 8, 16, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, size):\n        super().__init__()\n        self.size = size\n \n    def forward(self, x1):\n        v1 = torch.cat(input_tensors, dim=1)\n        v2 = v1[:, 0:9223372036854775807]\n        v3 = v2[:, 0:self.size]\n        v4 = torch.cat([v1, v3], dim=1)\n        return v4\n\n# Initializing the model\nm = Model(size=5)\n\n# Inputs to the model\nx1 = torch.randn(1, 10, 512, 512)\nx2 = torch.randn(1, 10, 512, 512)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n \n    def forward(self, x1, x2, x3):\n        v1 = torch.cat([x1, x2, x3], dim=1)\n        v2 = v1[:, 0:9223372036854775807]\n        v3 = v2[:, 0:64]\n        v4 = torch.cat([v1, v3], dim=1)\n        return v4\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 64, 8, 8)\nx2 = torch.randn(1, 64, 8, 8)\nx3 = torch.randn(1, 64, 8, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def forward(self, x1):\n        v1 = torch.cat([x1, x1], dim=1)\n        v2 = v1[:, 0:9223372036854775807]\n        v3 = v2[:, 0:v1.size(3)-1]\n        v4 = torch.cat([v1, v3], dim=1)\n        return v4\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 10, 8, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, x1, size):\n        v1 = torch.cat([x1, x1, x1],dim=1)\n        v2 = v1[:, 0:10000000000000]\n        v3 = v2[:, 0:size]\n        v4 = torch.cat([v1, v3], dim=1)\n        return v4\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\nprint(\"Input size of the convolution: {}\".format(x1.shape))\nx2 = 27\n"
            ],
            "code": [
                "\ndef func(f0, f1, f2, x4):\n    v2 = f0(x4)\n    v3 = x4 * f1(x4)\n    v5 = x4 - f0(x4)\n    v6 = f2(x4)\n    v7 = f2(x4)\n    v10 = f2(v2)\n    v11 = v5 * v6\n    v12 = v5 - v10\n    v15 = v6 * v7\n    v16 = v12 + v15\n    v17 = v11 * v16\n    return v17\n\n# Initializing the model\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n \n    def forward(self, x1, x2, x6, x7):\n        v1 = torch.cat([x1, x2, x6, x7], dim=1)\n        v2 = v1[:, 0:9223372036854775807]\n        v3 = v2[:, 0:34]\n        v4 = torch.cat([v1, v3], dim=1)\n        return v4\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 34, 64, 64)\nx2 = torch.randn(1, 2000, 56, 56)\nx6 = torch.randn(1, 256, 28, 28)\nx7 = torch.randn(1, 512, 14, 14)\n",
                " \nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n \n    def forward(self, x1):\n        v1 = torch.cat(input_tensors, dim=1)\n        v2 = v1[:, 0:9223372036854775807]\n        v3 = v2[:, 0:size]\n        v4 = torch.cat([v1, v3], dim=1)\n        return v4\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\nx2 = torch.randn(1, 3, 64, 64)\nx3 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n \n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = torch.cat([v1, v1, v1], dim=1)\n        v3 = v2[:, 0:1073741823]\n        v4 = v2[:, 0:2147483647]\n        v5 = torch.cat([v2, v3, v4], dim=1)\n        return v5\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n \n    def forward(self, x1, x2, x3):\n        v1 = torch.cat([x1, x2, x3], dim=1)\n        v2 = v1[:, 0:9223372036854775807]\n        v3 = v2[:, 0:v1.size(1)]\n        v4 = torch.cat([v1, v3], dim=1)\n        return v4\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 6, 5, 5)\nx2 = torch.randn(1, 7, 5, 5)\nx3 = torch.randn(1, 8, 5, 5)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, x1):\n        v1 = torch.cat([x1, x2])\n        v2 = v1[:, 0:9223372036854775807]\n        v3 = v2[:, 0:8]\n        v4 = torch.cat([v1, v3], dim=1)\n        return v4\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 8, 16, 16)\nx2 = torch.randn(1, 8, 16, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, size):\n        super().__init__()\n        self.size = size\n \n    def forward(self, x1):\n        v1 = torch.cat(input_tensors, dim=1)\n        v2 = v1[:, 0:9223372036854775807]\n        v3 = v2[:, 0:self.size]\n        v4 = torch.cat([v1, v3], dim=1)\n        return v4\n\n# Initializing the model\nm = Model(size=5)\n\n# Inputs to the model\nx1 = torch.randn(1, 10, 512, 512)\nx2 = torch.randn(1, 10, 512, 512)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n \n    def forward(self, x1, x2, x3):\n        v1 = torch.cat([x1, x2, x3], dim=1)\n        v2 = v1[:, 0:9223372036854775807]\n        v3 = v2[:, 0:64]\n        v4 = torch.cat([v1, v3], dim=1)\n        return v4\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 64, 8, 8)\nx2 = torch.randn(1, 64, 8, 8)\nx3 = torch.randn(1, 64, 8, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def forward(self, x1):\n        v1 = torch.cat([x1, x1], dim=1)\n        v2 = v1[:, 0:9223372036854775807]\n        v3 = v2[:, 0:v1.size(3)-1]\n        v4 = torch.cat([v1, v3], dim=1)\n        return v4\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 10, 8, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, x1, size):\n        v1 = torch.cat([x1, x1, x1],dim=1)\n        v2 = v1[:, 0:10000000000000]\n        v3 = v2[:, 0:size]\n        v4 = torch.cat([v1, v3], dim=1)\n        return v4\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\nprint(\"Input size of the convolution: {}\".format(x1.shape))\nx2 = 27\n"
            ],
            "g_time": 7.900230169296265
        }
    }
}
{
    "opt": "binary_unary=4",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(6, 2)\n \n    def forward(self, x1, output_activation=True):\n        v1 = self.linear(x1)\n        v2 = v1 + x1\n        if output_activation:\n            v3 = v2.relu()\n        else:\n            v3 = v2\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 6)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n\n    def forward(self, x):\n        v1 = self.linear(x)\n        v2 = v1 + x\n        v3 = F.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.weight = torch.randn(1000, 512)\n        self.bias = torch.randn(512)\n \n    def forward(self, x1, other=None):\n        v1 = torch.matmul(x1, self.weight.t())\n        v2 = v1 + other\n        v3 = torch.nn.functional.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 1000)\nother = torch.randn(1, 512)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(64, 1)\n \n    def forward(self, x1, x2):\n        v1 = self.linear(x1)\n        v2 = v1 + x2\n        v3 = torch.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\n# The second input tensor can be any random tensor with the same number of rows with the output of linear transformation of the first tensor.\nx1 = torch.randn(1, 3)\nx2 = torch.randn(1, 1)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, other):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n        self.other = other\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 + self.other\n        v3 = torch.nn.functional.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model(torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]]))\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(21, 5)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 + other\n        v3 = torch.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(2, 21)\nother = torch.randn(2, 5)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(4, 4)\n \n    def forward(self, x1, other):\n        v1 = self.linear(x1)\n        v2 = v1 + other\n        v3 = torch.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model, note that the other tensor is passed as a keyword argument\nx1 = torch.randn(4)\nother = torch.randn(4)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(16, 32)\n \n    def forward(self, x1, other=None):\n        v1 = self.linear(x1)\n        v2 = v1 + other\n        v3 = torch.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(535, 379)\n \n    def forward(self, x, weight=None):\n        out = self.linear(x) if weight is None else self.linear(x) + weight\n        return torch.nn.functional.relu(out)\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 13, 19)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 3)\n \n    def forward(self, x1, other=None):\n        y = self.linear(x1)\n        if not other is None:\n            y = y + other\n        y = torch.relu(y)\n        return y\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\nx2 = torch.randn(1, 3, 64, 64)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(6, 2)\n \n    def forward(self, x1, output_activation=True):\n        v1 = self.linear(x1)\n        v2 = v1 + x1\n        if output_activation:\n            v3 = v2.relu()\n        else:\n            v3 = v2\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 6)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n\n    def forward(self, x):\n        v1 = self.linear(x)\n        v2 = v1 + x\n        v3 = F.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.weight = torch.randn(1000, 512)\n        self.bias = torch.randn(512)\n \n    def forward(self, x1, other=None):\n        v1 = torch.matmul(x1, self.weight.t())\n        v2 = v1 + other\n        v3 = torch.nn.functional.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 1000)\nother = torch.randn(1, 512)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(64, 1)\n \n    def forward(self, x1, x2):\n        v1 = self.linear(x1)\n        v2 = v1 + x2\n        v3 = torch.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\n# The second input tensor can be any random tensor with the same number of rows with the output of linear transformation of the first tensor.\nx1 = torch.randn(1, 3)\nx2 = torch.randn(1, 1)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, other):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n        self.other = other\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 + self.other\n        v3 = torch.nn.functional.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model(torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]]))\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(21, 5)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 + other\n        v3 = torch.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(2, 21)\nother = torch.randn(2, 5)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(4, 4)\n \n    def forward(self, x1, other):\n        v1 = self.linear(x1)\n        v2 = v1 + other\n        v3 = torch.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model, note that the other tensor is passed as a keyword argument\nx1 = torch.randn(4)\nother = torch.randn(4)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(16, 32)\n \n    def forward(self, x1, other=None):\n        v1 = self.linear(x1)\n        v2 = v1 + other\n        v3 = torch.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(535, 379)\n \n    def forward(self, x, weight=None):\n        out = self.linear(x) if weight is None else self.linear(x) + weight\n        return torch.nn.functional.relu(out)\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 13, 19)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 3)\n \n    def forward(self, x1, other=None):\n        y = self.linear(x1)\n        if not other is None:\n            y = y + other\n        y = torch.relu(y)\n        return y\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\nx2 = torch.randn(1, 3, 64, 64)\n"
            ],
            "g_time": 6.484536170959473
        }
    }
}
{
    "opt": "unary=7",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(23, 42)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * torch.clamp(torch.min(v1), torch.max(v1), v1 + 3)\n        v3 = v2 / 6\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 23)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(10, 10)\n \n    def forward(self, x1):\n        y1 = self.linear(x1)\n        y2 = y1 * torch.clamp(y1 + 3, 0, 6)\n        return y2 / 6\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(16, 10)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(1, 8)\n\n    def forward(self, x1):\n        l1 = self.linear(x1)\n        l2 = l1 * torch.clamp(torch.tensor(l1 + 3), min=0, max=6)\n        l3 = l2 / 6\n        return l3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 1)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(8, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * torch.clamp(min=0, max=6, v1 + 3)\n        v3 = v2 / 6\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * torch.clamp(torch.min(v1 + 3, torch.full_like(v1, 6)), max=6)\n        v3 = v2 / 6\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(16, 16, bias=False)\n        self.min = torch.tensor(0.0)\n        self.max = torch.tensor(6.0)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        return v1 * (v1.clamp(min=self.min, max=self.max) + self.min) / 6\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(8, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n \n    def forward(self, x1):\n        t1 = self.linear(x1)\n        t2 = t1 * torch.clamp(torch.minimum(torch.maximum(t1+3,0), 6))\n        t3 = t2 / 6\n        return t3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(16, 8)\n \n    def forward(self, x1):\n        t1 = self.linear(x1)\n        t2 = t1 * torch.clamp(t1+3, max=6)\n        t3 = t2 / 6\n        return t3\n# Initializing the model\nd1 = Model()\n# Inputs to the model\nx1 = torch.randn(1, 16)\nprint(d1(x1))\n\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(32, 80)\n \n    def forward(self, x):\n        v1 = self.linear(x)\n        v2 = v1 * torch.clamp(torch.clamp(v1 + 3, min=0), max=6)\n        v3 = v2 / 6\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx = torch.randn(20, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(64, 64, bias=False)\n \n    def forward(self, x):\n        v1 = self.act(x)\n        v2 = v1 * torch.clamp(v1 + 3, 0, 6)\n        return v2 / 6\n\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(23, 42)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * torch.clamp(torch.min(v1), torch.max(v1), v1 + 3)\n        v3 = v2 / 6\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 23)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(10, 10)\n \n    def forward(self, x1):\n        y1 = self.linear(x1)\n        y2 = y1 * torch.clamp(y1 + 3, 0, 6)\n        return y2 / 6\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(16, 10)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(1, 8)\n\n    def forward(self, x1):\n        l1 = self.linear(x1)\n        l2 = l1 * torch.clamp(torch.tensor(l1 + 3), min=0, max=6)\n        l3 = l2 / 6\n        return l3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 1)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(8, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * torch.clamp(min=0, max=6, v1 + 3)\n        v3 = v2 / 6\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * torch.clamp(torch.min(v1 + 3, torch.full_like(v1, 6)), max=6)\n        v3 = v2 / 6\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(16, 16, bias=False)\n        self.min = torch.tensor(0.0)\n        self.max = torch.tensor(6.0)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        return v1 * (v1.clamp(min=self.min, max=self.max) + self.min) / 6\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(8, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n \n    def forward(self, x1):\n        t1 = self.linear(x1)\n        t2 = t1 * torch.clamp(torch.minimum(torch.maximum(t1+3,0), 6))\n        t3 = t2 / 6\n        return t3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(16, 8)\n \n    def forward(self, x1):\n        t1 = self.linear(x1)\n        t2 = t1 * torch.clamp(t1+3, max=6)\n        t3 = t2 / 6\n        return t3\n# Initializing the model\nd1 = Model()\n# Inputs to the model\nx1 = torch.randn(1, 16)\nprint(d1(x1))\n\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(32, 80)\n \n    def forward(self, x):\n        v1 = self.linear(x)\n        v2 = v1 * torch.clamp(torch.clamp(v1 + 3, min=0), max=6)\n        v3 = v2 / 6\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx = torch.randn(20, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(64, 64, bias=False)\n \n    def forward(self, x):\n        v1 = self.act(x)\n        v2 = v1 * torch.clamp(v1 + 3, 0, 6)\n        return v2 / 6\n\n"
            ],
            "g_time": 5.831315994262695
        }
    }
}
{
    "opt": "unary=23",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose1 = torch.nn.ConvTranspose2d(512, 576, kernel_size=(7, 1), stride=(1, 1), padding=(521, 0), dilation=1, groups=1, bias=False)\n    def forward(self, x1):\n        v1 = self.conv_transpose1(x1)\n        v2 = torch.tanh(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 512, 1, 1)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose1 = torch.nn.ConvTranspose2d(9, 16, kernel_size=5, stride=2, padding=1, dilation=1, groups=1, bias=False)\n        self.conv_transpose2 = torch.nn.ConvTranspose2d(16, 9, kernel_size=5, stride=2, padding=1, dilation=1, groups=1, bias=False)\n    def forward(self, x1):\n        v1 = self.conv_transpose1(x1)\n        v3 = self.conv_transpose2(v1)\n        v2 = torch.tanh(v3)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 9, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(1, 1, 1, stride=2, dilation=1, bias=False)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = torch.tanh(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 1, 128, 128)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose1 = torch.nn.ConvTranspose2d(16, 9, kernel_size=7, stride=1, padding=1, bias=None)\n        self.conv_transpose2 = torch.nn.ConvTranspose2d(9, 6, kernel_size=1, stride=1, padding=0)\n        self.conv_transpose3 = torch.nn.ConvTranspose2d(6, 2, kernel_size=1, stride=1, padding=0, bias=None)\n        self.conv_transpose4 = torch.nn.ConvTranspose2d(2, 3, kernel_size=3, stride=1, padding=1, bias=None)\n    def forward(self, x1):\n        v1 = self.conv_transpose1(x1)\n        v2 = self.conv_transpose2(v1)\n        v3 = self.conv_transpose3(v2)\n        v4 = self.conv_transpose4(v3)\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 16, 232, 321)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose1 = torch.nn.ConvTranspose2d(6, 50, kernel_size=1, stride=1, padding=0, dilation=1, groups=1, bias=True)\n        self.conv_transpose2 = torch.nn.ConvTranspose2d(50, 6, kernel_size=1, stride=1, padding=0, dilation=1, groups=1, bias=True)\n    def forward(self, x1):\n        v1 = self.conv_transpose1(x1)\n        v3 = torch.abs(v1)\n        v2 = self.conv_transpose2(v3)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 6, 224, 224)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose1 = torch.nn.ConvTranspose2d(9, 36, kernel_size=2, stride=2, padding=0, dilation=1, groups=1, bias=True)\n        self.conv_transpose2 = torch.nn.ConvTranspose2d(36, 18, kernel_size=2, stride=2, padding=0, dilation=1, groups=1, bias=True)\n        self.conv_transpose3 = torch.nn.ConvTranspose2d(18, 9, kernel_size=2, stride=2, padding=0, dilation=1, groups=1, bias=True)\n    def forward(self, x1):\n        v1 = self.conv_transpose1(x1)\n        v3 = self.conv_transpose2(v1)\n        v5 = self.conv_transpose3(v3)\n        v2 = torch.tanh(v5)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 9, 224, 224)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(64, 64, kernel_size=1, stride=1, padding=0, dilation=1, groups=8, bias=False)\n        self.softmax = torch.nn.Softmax(dim=-1)\n        self.linear = torch.nn.Linear(1, 1)\n        self.conv = torch.nn.Conv2d(1, 1, 1, stride=1, padding=0, dilation=1, groups=1, bias=False)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = self.softmax(v1)\n        v3 = self.linear(v2)\n        v4 = self.conv(v2)\n        v5 = v3 + v4\n        v6 = torch.tanh(v5)\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 64, 224, 224)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(6, 6, kernel_size=(5, 5), stride=(1, 1), padding=(4, 4), dilation=(2, 2), groups=1, bias=True)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = torch.tanh(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 6, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose1 = torch.nn.ConvTranspose2d(3, 16, kernel_size=3, stride=1, padding=1, dilation=1, groups=1, bias=False)\n        self.avg1 = torch.nn.AvgPool2d(kernel_size=3, stride=1, padding=1, ceil_mode=True, count_include_pad=True)\n        self.conv_transpose2 = torch.nn.ConvTranspose2d(16, 24, kernel_size=3, stride=1, padding=1, dilation=1, groups=1, bias=False)\n        self.batch2 = torch.nn.BatchNorm2d(24)\n        self.fc = torch.nn.Linear(1408, 3)\n    def forward(self, x1):\n        v0 = self.conv_transpose1(x1)\n        v4 = self.avg1(v0)\n        v1 = self.conv_transpose2(v4)\n        v5 = self.batch2(v1)\n        v2 = v5.reshape(1, 1408)\n        v3 = self.fc(v2)\n        v6 = torch.tanh(v3)\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 3, 256, 256)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose1 = torch.nn.ConvTranspose2d(6, 50, kernel_size=1, stride=1, padding=0, dilation=1, groups=1, bias=True)\n        self.conv_transpose2 = torch.nn.ConvTranspose2d(50, 6, kernel_size=1, stride=1, padding=0, dilation=1, groups=1, bias=True)\n    def forward(self, x1):\n        v1 = self.conv_transpose1(x1)\n        v3 = self.conv_transpose2(v1)\n        v2 = torch.tanh(v3)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 6, 224, 224)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose1 = torch.nn.ConvTranspose2d(512, 576, kernel_size=(7, 1), stride=(1, 1), padding=(521, 0), dilation=1, groups=1, bias=False)\n    def forward(self, x1):\n        v1 = self.conv_transpose1(x1)\n        v2 = torch.tanh(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 512, 1, 1)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose1 = torch.nn.ConvTranspose2d(9, 16, kernel_size=5, stride=2, padding=1, dilation=1, groups=1, bias=False)\n        self.conv_transpose2 = torch.nn.ConvTranspose2d(16, 9, kernel_size=5, stride=2, padding=1, dilation=1, groups=1, bias=False)\n    def forward(self, x1):\n        v1 = self.conv_transpose1(x1)\n        v3 = self.conv_transpose2(v1)\n        v2 = torch.tanh(v3)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 9, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(1, 1, 1, stride=2, dilation=1, bias=False)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = torch.tanh(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 1, 128, 128)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose1 = torch.nn.ConvTranspose2d(16, 9, kernel_size=7, stride=1, padding=1, bias=None)\n        self.conv_transpose2 = torch.nn.ConvTranspose2d(9, 6, kernel_size=1, stride=1, padding=0)\n        self.conv_transpose3 = torch.nn.ConvTranspose2d(6, 2, kernel_size=1, stride=1, padding=0, bias=None)\n        self.conv_transpose4 = torch.nn.ConvTranspose2d(2, 3, kernel_size=3, stride=1, padding=1, bias=None)\n    def forward(self, x1):\n        v1 = self.conv_transpose1(x1)\n        v2 = self.conv_transpose2(v1)\n        v3 = self.conv_transpose3(v2)\n        v4 = self.conv_transpose4(v3)\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 16, 232, 321)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose1 = torch.nn.ConvTranspose2d(6, 50, kernel_size=1, stride=1, padding=0, dilation=1, groups=1, bias=True)\n        self.conv_transpose2 = torch.nn.ConvTranspose2d(50, 6, kernel_size=1, stride=1, padding=0, dilation=1, groups=1, bias=True)\n    def forward(self, x1):\n        v1 = self.conv_transpose1(x1)\n        v3 = torch.abs(v1)\n        v2 = self.conv_transpose2(v3)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 6, 224, 224)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose1 = torch.nn.ConvTranspose2d(9, 36, kernel_size=2, stride=2, padding=0, dilation=1, groups=1, bias=True)\n        self.conv_transpose2 = torch.nn.ConvTranspose2d(36, 18, kernel_size=2, stride=2, padding=0, dilation=1, groups=1, bias=True)\n        self.conv_transpose3 = torch.nn.ConvTranspose2d(18, 9, kernel_size=2, stride=2, padding=0, dilation=1, groups=1, bias=True)\n    def forward(self, x1):\n        v1 = self.conv_transpose1(x1)\n        v3 = self.conv_transpose2(v1)\n        v5 = self.conv_transpose3(v3)\n        v2 = torch.tanh(v5)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 9, 224, 224)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(64, 64, kernel_size=1, stride=1, padding=0, dilation=1, groups=8, bias=False)\n        self.softmax = torch.nn.Softmax(dim=-1)\n        self.linear = torch.nn.Linear(1, 1)\n        self.conv = torch.nn.Conv2d(1, 1, 1, stride=1, padding=0, dilation=1, groups=1, bias=False)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = self.softmax(v1)\n        v3 = self.linear(v2)\n        v4 = self.conv(v2)\n        v5 = v3 + v4\n        v6 = torch.tanh(v5)\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 64, 224, 224)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(6, 6, kernel_size=(5, 5), stride=(1, 1), padding=(4, 4), dilation=(2, 2), groups=1, bias=True)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = torch.tanh(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 6, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose1 = torch.nn.ConvTranspose2d(3, 16, kernel_size=3, stride=1, padding=1, dilation=1, groups=1, bias=False)\n        self.avg1 = torch.nn.AvgPool2d(kernel_size=3, stride=1, padding=1, ceil_mode=True, count_include_pad=True)\n        self.conv_transpose2 = torch.nn.ConvTranspose2d(16, 24, kernel_size=3, stride=1, padding=1, dilation=1, groups=1, bias=False)\n        self.batch2 = torch.nn.BatchNorm2d(24)\n        self.fc = torch.nn.Linear(1408, 3)\n    def forward(self, x1):\n        v0 = self.conv_transpose1(x1)\n        v4 = self.avg1(v0)\n        v1 = self.conv_transpose2(v4)\n        v5 = self.batch2(v1)\n        v2 = v5.reshape(1, 1408)\n        v3 = self.fc(v2)\n        v6 = torch.tanh(v3)\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 3, 256, 256)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose1 = torch.nn.ConvTranspose2d(6, 50, kernel_size=1, stride=1, padding=0, dilation=1, groups=1, bias=True)\n        self.conv_transpose2 = torch.nn.ConvTranspose2d(50, 6, kernel_size=1, stride=1, padding=0, dilation=1, groups=1, bias=True)\n    def forward(self, x1):\n        v1 = self.conv_transpose1(x1)\n        v3 = self.conv_transpose2(v1)\n        v2 = torch.tanh(v3)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 6, 224, 224)\n"
            ],
            "g_time": 10.24750018119812
        }
    }
}
{
    "opt": "sink_cat_after_pointwise",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x):\n        y = x.view(x.shape[0], -1)\n        return y.view(y.shape[0], 1, -1) if y.shape!= (0, 0) else y\n# Inputs to the model\nx = torch.randn(2, 3, 4)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x):\n        x = torch.sigmoid(x)\n        x = x.view(-1, x.shape[-1])\n        y = x * x\n        x = x.tanh()\n        x = y * x * x\n        x = torch.sum(x)\n        return x\n# Inputs to the model\nx = torch.randn(2, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x):\n        y = 2*x\n        y = y.view(x.shape[0], -1)\n        y = y.tanh()\n        y = torch.cat((y, y), dim=1)\n        x = y.view(y.shape[0], -1).tanh()\n        return x\n# Inputs to the model\nx = torch.randn(2, 3, 4)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x):\n        y = torch.arange(3)\n        y = y.repeat(x.shape[0],)\n        z = torch.cat((x, x), dim=1)\n        y = z.view(z.shape[0], -1)\n        y = y.tanh()\n        x = y.relu()\n        return x\n# Inputs to the model\nx = torch.randn(3, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x):\n        y = x.view(x.shape[0], -1)\n        y = y.tanh()\n        y = torch.cat((y, y), dim=1)\n        x = y.view(y.shape[0], -1).tanh()\n        return x\n# Inputs to the model\nx = torch.randn(2, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x):\n        y = torch.cat((x, x + 1.0), dim=1)\n        return y.view(y.shape[0], -1).tanh() if y.shape!= (1, 3) else y.view(y.shape[0], -1).relu()\n# Inputs to the model\nx = torch.randn(2, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x):\n        y, z = x.view(x.shape[0], -1), x.view(x.shape[0], -1)\n        w = torch.relu(z)\n        return torch.cat((x,y), dim=1).tanh()\n# Inputs to the model\nx = torch.randn(2, 32, 32, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x):\n        return torch.cat((x, x, x), dim=0).tanh() if torch.sum(x).item()!= 0 else torch.cat((x, x, x), dim=0).relu()\n# Inputs to the model\nx = torch.randn(2, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x):\n        y = x.view(x.shape[0], -1)\n        x = y.view(y.shape[0], -1).tanh() if torch.numel(y) == 1 else y.tanh()\n        return x\n# Inputs to the model\nx = torch.randn(2, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x):\n        y = torch.cat((x, x, x), dim=1)\n        y = y.view(-1)\n        return y.tanh()\n# Inputs to the model\nx = torch.randn(2, 3, 4)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x):\n        y = x.view(x.shape[0], -1)\n        return y.view(y.shape[0], 1, -1) if y.shape!= (0, 0) else y\n# Inputs to the model\nx = torch.randn(2, 3, 4)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x):\n        x = torch.sigmoid(x)\n        x = x.view(-1, x.shape[-1])\n        y = x * x\n        x = x.tanh()\n        x = y * x * x\n        x = torch.sum(x)\n        return x\n# Inputs to the model\nx = torch.randn(2, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x):\n        y = 2*x\n        y = y.view(x.shape[0], -1)\n        y = y.tanh()\n        y = torch.cat((y, y), dim=1)\n        x = y.view(y.shape[0], -1).tanh()\n        return x\n# Inputs to the model\nx = torch.randn(2, 3, 4)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x):\n        y = torch.arange(3)\n        y = y.repeat(x.shape[0],)\n        z = torch.cat((x, x), dim=1)\n        y = z.view(z.shape[0], -1)\n        y = y.tanh()\n        x = y.relu()\n        return x\n# Inputs to the model\nx = torch.randn(3, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x):\n        y = x.view(x.shape[0], -1)\n        y = y.tanh()\n        y = torch.cat((y, y), dim=1)\n        x = y.view(y.shape[0], -1).tanh()\n        return x\n# Inputs to the model\nx = torch.randn(2, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x):\n        y = torch.cat((x, x + 1.0), dim=1)\n        return y.view(y.shape[0], -1).tanh() if y.shape!= (1, 3) else y.view(y.shape[0], -1).relu()\n# Inputs to the model\nx = torch.randn(2, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x):\n        y, z = x.view(x.shape[0], -1), x.view(x.shape[0], -1)\n        w = torch.relu(z)\n        return torch.cat((x,y), dim=1).tanh()\n# Inputs to the model\nx = torch.randn(2, 32, 32, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x):\n        return torch.cat((x, x, x), dim=0).tanh() if torch.sum(x).item()!= 0 else torch.cat((x, x, x), dim=0).relu()\n# Inputs to the model\nx = torch.randn(2, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x):\n        y = x.view(x.shape[0], -1)\n        x = y.view(y.shape[0], -1).tanh() if torch.numel(y) == 1 else y.tanh()\n        return x\n# Inputs to the model\nx = torch.randn(2, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x):\n        y = torch.cat((x, x, x), dim=1)\n        y = y.view(-1)\n        return y.tanh()\n# Inputs to the model\nx = torch.randn(2, 3, 4)\n"
            ],
            "g_time": 4.610032320022583
        }
    }
}
{
    "opt": "binary=2",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 1, 1, stride=1, padding=1)\n    def forward(self, x):\n        v1 = self.conv(x)\n        v2 = x - 0.2\n        v3 = v1 - v2\n        v4 = -v3\n        return v4\n# Inputs to the model\nx = torch.randn(1, 3, 57, 57)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(2, 4, 3, stride=2, padding=1)\n    def forward(self, x):\n        v3 = self.conv(x)\n        v4 = 1.0 - v3\n        v1 = v4 / 3.0\n        return v1\n# Inputs to the model\nx = torch.randn(1, 2, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(1, 8, 1, stride=2, padding=1)\n    def forward(self, x):\n        v1 = self.conv(x)\n        s = 1.0\n        v2 = (v1 - s) * 12.0\n        return v2\n# Inputs to the model\nx = torch.randn(1, 1, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super(Model).__init__()\n        self.conv = torch.nn.Conv2d(2, 4, 3, stride=2, padding=1)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 + 2.0\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 2, 35, 35)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n    def forward(self, x):\n        v = self.conv(x)\n        return v + -1.570796\n# Inputs to the model\nx = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(2, 8, 1, stride=2, padding=1)\n    def forward(self, x):\n        v1 = self.conv(x)\n        v2 = v1 - 1.0\n        return v2\n# Inputs to the model\nx = torch.randn(1, 2, 35, 35)\n",
                "\ninput_tensor = None\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(2, 4, 3, stride=2, padding=1)\n    def forward(self, x):\n        x = x * 1.2\n        output = self.conv(x)\n        return output\n# Inputs to the model\nx = torch.randn(1, 2, 35, 35)\ninput_tensor = x\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 4, 3, stride=2, padding=1)\n        self.relu = torch.nn.ReLU()\n    def forward(self, x1):\n        v3 = self.conv(x1)\n        v1 = v3 - 0.0003\n        v2 = self.relu(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 3, 75, 75)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(15, 1, 3, stride=2, padding=1)\n    def forward(self, x1):\n        v4 = self.conv(x1)\n        v1 = v4 - 1.5707963267948966\n        v2 = v1 - 0.12\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 15, 125, 125)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(1, 8, 1, stride=1, padding=1)\n        self.conv2 = torch.nn.Conv2d(2, 4, 3, stride=2, padding=1)\n    def forward(self, x):\n        v1 = self.conv1(x)\n        v3 = self.conv2(v1)\n        v2 = v3 - 0.0\n        return v2\n# Inputs to the model\nx = torch.randn(1, 1, 64, 64)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 1, 1, stride=1, padding=1)\n    def forward(self, x):\n        v1 = self.conv(x)\n        v2 = x - 0.2\n        v3 = v1 - v2\n        v4 = -v3\n        return v4\n# Inputs to the model\nx = torch.randn(1, 3, 57, 57)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(2, 4, 3, stride=2, padding=1)\n    def forward(self, x):\n        v3 = self.conv(x)\n        v4 = 1.0 - v3\n        v1 = v4 / 3.0\n        return v1\n# Inputs to the model\nx = torch.randn(1, 2, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(1, 8, 1, stride=2, padding=1)\n    def forward(self, x):\n        v1 = self.conv(x)\n        s = 1.0\n        v2 = (v1 - s) * 12.0\n        return v2\n# Inputs to the model\nx = torch.randn(1, 1, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super(Model).__init__()\n        self.conv = torch.nn.Conv2d(2, 4, 3, stride=2, padding=1)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 + 2.0\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 2, 35, 35)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n    def forward(self, x):\n        v = self.conv(x)\n        return v + -1.570796\n# Inputs to the model\nx = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(2, 8, 1, stride=2, padding=1)\n    def forward(self, x):\n        v1 = self.conv(x)\n        v2 = v1 - 1.0\n        return v2\n# Inputs to the model\nx = torch.randn(1, 2, 35, 35)\n",
                "\ninput_tensor = None\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(2, 4, 3, stride=2, padding=1)\n    def forward(self, x):\n        x = x * 1.2\n        output = self.conv(x)\n        return output\n# Inputs to the model\nx = torch.randn(1, 2, 35, 35)\ninput_tensor = x\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 4, 3, stride=2, padding=1)\n        self.relu = torch.nn.ReLU()\n    def forward(self, x1):\n        v3 = self.conv(x1)\n        v1 = v3 - 0.0003\n        v2 = self.relu(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 3, 75, 75)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(15, 1, 3, stride=2, padding=1)\n    def forward(self, x1):\n        v4 = self.conv(x1)\n        v1 = v4 - 1.5707963267948966\n        v2 = v1 - 0.12\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 15, 125, 125)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(1, 8, 1, stride=1, padding=1)\n        self.conv2 = torch.nn.Conv2d(2, 4, 3, stride=2, padding=1)\n    def forward(self, x):\n        v1 = self.conv1(x)\n        v3 = self.conv2(v1)\n        v2 = v3 - 0.0\n        return v2\n# Inputs to the model\nx = torch.randn(1, 1, 64, 64)\n"
            ],
            "g_time": 5.37627387046814
        }
    }
}
{
    "opt": "unary=18",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(1, 8, 3, stride=2, padding=1)\n        self.conv2 = torch.nn.Conv2d(8, 16, 1, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = self.conv2(v2)\n        v4 = torch.sigmoid(v3)\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 1, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(in_channels=3, out_channels=64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n        self.conv2 = torch.nn.Conv2d(in_channels=64, out_channels=512, kernel_size=(1, 1), stride=(1, 1), padding=(0, 0))\n        self.bn1 = torch.nn.BatchNorm2d(512)\n        self.conv3 = torch.nn.Conv2d(in_channels=512, out_channels=256, kernel_size=(1, 1), stride=(1, 1), padding=(0, 0))\n        self.bn2 = torch.nn.BatchNorm2d(256) \n        self.max_pooling1 = torch.nn.MaxPool2d(kernel_size=(2, 2), padding=(0, 1), stride=(2, 2), dilation=(1, 1))\n        self.conv4 = torch.nn.Conv2d(in_channels=256, out_channels=128, kernel_size=(1, 1), stride=(1, 1), padding=(0, 0))\n        self.conv5 = torch.nn.Conv2d(in_channels=128, out_channels=192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        self.bn3 = torch.nn.BatchNorm2d(192) \n        self.conv6 = torch.nn.Conv2d(in_channels=192, out_channels=128, kernel_size=(1, 1), stride=(1, 1), padding=(0, 0))\n        self.conv7 = torch.nn.Conv2d(in_channels=128, out_channels=128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        self.bn4 = torch.nn.BatchNorm2d(128) \n        self.max_pooling2 = torch.nn.MaxPool2d(kernel_size=(2, 2), padding=(0, 1), stride=(2, 2), dilation=(1, 1))\n        self.flatten = torch.nn.Flatten()\n        self.linear1 = torch.nn.Linear(in_features=128, out_features=64, bias=True) \n        self.linear2 = torch.nn.Linear(in_features=64, out_features=1, bias=True)\n\n    def forward(self, input_image):\n        x1 = self.conv1(input_image)\n        x2 = gum_sigmoid_function(x1)\n        x3 = self.conv2(x2)\n        x4 = self.bn1(x3)\n        x5 = torch.sigmoid(x4)\n        x6 = self.conv3(x5)\n        x7 = self.bn2(x6)\n        x8 = gum_sigmoid_function(x7)\n        x9 = self.max_pooling1(x8)\n        x10 = self.conv4(x9)\n        x11 = self.conv5(x10)\n        x12 = self.bn3(x11)\n        x13 = gum_sigmoid_function(x12)\n        x14 = self.conv6(x13)\n        x15 = self.conv7(x14)\n        x16 = self.bn4(x15)\n        x17 = gum_sigmoid_function(x16)\n        x18 = self.max_pooling2(x17)\n        x19 = self.flatten(x18)\n        x20 = self.linear1(x19)\n        x21 = self.linear2(x20)\n        x22 = torch.sigmoid(x21)\n        return x22\n# Inputs to the model\ninput_image = torch.randn(1, 3, 16, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(1, 16, 3, stride=1, padding=1)\n        self.conv2 = torch.nn.Conv2d(16, 8, 1, stride=2, padding=0)\n        self.conv3 = torch.nn.Conv2d(8, 8, 3, stride=1, padding=1)\n        self.conv4 = torch.nn.Conv2d(8, 8, 1, stride=1, padding=0)\n        self.conv5 = torch.nn.Conv2d(8, 1, 3, stride=1, padding=1)\n        self.relu1 = torch.nn.ReLU()\n        self.relu2 = torch.nn.ReLU()\n        self.relu3 = torch.nn.ReLU()\n    def forward(self, x1):\n        v1 = self.relu1(self.conv1(x1))\n        v2 = self.relu2(self.conv2(v1))\n        v3 = self.relu3(self.conv3(v2))\n        v4 = self.conv4(v3)\n        v5 = torch.sigmoid(self.conv5(v4))\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 1, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(1, 16, 3, stride=1, padding=1)\n        self.conv2 = torch.nn.Conv2d(16, 32, 1, stride=1, padding=1)\n        self.conv3 = torch.nn.Conv2d(32, 1, 3, stride=1, padding=1)\n    def forward(self, x1):\n\n        v1 = self.conv1(x1)\n        v2 = self.conv2(v1)\n        v3 = torch.sigmoid(v2)\n        v4 = self.conv3(v3)\n        v5 = torch.sigmoid(v4)\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 1, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(in_channels=1, out_channels=8, kernel_size=3, stride=1, padding=1)\n        self.conv2 = torch.nn.Conv2d(in_channels=8, out_channels=8, kernel_size=1, stride=1, padding=0)\n        self.conv3 = torch.nn.Conv2d(in_channels=8, out_channels=1, kernel_size=1, stride=1, padding=0)\n    def forward(self, x3):\n        v1 = self.conv1(x3)\n        v2 = self.conv2(v1)\n        v3 = self.conv3(v2)\n        v4 = torch.sigmoid(v3)\n        return v4\n# Inputs to the model\nx3 = torch.randn(1, 1, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(1, 32, 7, stride=1, padding=3)\n        self.conv2 = torch.nn.Conv2d(32, 64, 1, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = self.conv2(v2)\n        v4 = torch.sigmoid(v3)\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 1, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(1, 8, 3, stride=1, padding=1)\n        self.conv2 = torch.nn.Conv2d(8, 1, 1, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = self.conv2(v1)\n        v3 = torch.sigmoid(v2)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 1, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 1, 3, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = torch.sigmoid(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 3, 224, 224)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(32, 32, 1, stride=2, padding=0)\n        self.conv2 = torch.nn.Conv2d(32, 64, 1, stride=1, padding=0)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = self.conv2(v2)\n        v4 = torch.sigmoid(v3)\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 32, 32, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(in_channels=32, out_channels=32, kernel_size=[1, 252], stride=1, padding=0)\n        self.conv2 = torch.nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=0)\n        self.conv3 = torch.nn.Conv2d(in_channels=64, out_channels=64, kernel_size=1, stride=1, padding=0)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = self.conv2(v2)\n        v4 = torch.sigmoid(v3)\n        v5 = self.conv3(v4)\n        v6 = torch.sigmoid(v5)\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 32, 1, 252)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(1, 8, 3, stride=2, padding=1)\n        self.conv2 = torch.nn.Conv2d(8, 16, 1, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = self.conv2(v2)\n        v4 = torch.sigmoid(v3)\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 1, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(in_channels=3, out_channels=64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n        self.conv2 = torch.nn.Conv2d(in_channels=64, out_channels=512, kernel_size=(1, 1), stride=(1, 1), padding=(0, 0))\n        self.bn1 = torch.nn.BatchNorm2d(512)\n        self.conv3 = torch.nn.Conv2d(in_channels=512, out_channels=256, kernel_size=(1, 1), stride=(1, 1), padding=(0, 0))\n        self.bn2 = torch.nn.BatchNorm2d(256) \n        self.max_pooling1 = torch.nn.MaxPool2d(kernel_size=(2, 2), padding=(0, 1), stride=(2, 2), dilation=(1, 1))\n        self.conv4 = torch.nn.Conv2d(in_channels=256, out_channels=128, kernel_size=(1, 1), stride=(1, 1), padding=(0, 0))\n        self.conv5 = torch.nn.Conv2d(in_channels=128, out_channels=192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        self.bn3 = torch.nn.BatchNorm2d(192) \n        self.conv6 = torch.nn.Conv2d(in_channels=192, out_channels=128, kernel_size=(1, 1), stride=(1, 1), padding=(0, 0))\n        self.conv7 = torch.nn.Conv2d(in_channels=128, out_channels=128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        self.bn4 = torch.nn.BatchNorm2d(128) \n        self.max_pooling2 = torch.nn.MaxPool2d(kernel_size=(2, 2), padding=(0, 1), stride=(2, 2), dilation=(1, 1))\n        self.flatten = torch.nn.Flatten()\n        self.linear1 = torch.nn.Linear(in_features=128, out_features=64, bias=True) \n        self.linear2 = torch.nn.Linear(in_features=64, out_features=1, bias=True)\n\n    def forward(self, input_image):\n        x1 = self.conv1(input_image)\n        x2 = gum_sigmoid_function(x1)\n        x3 = self.conv2(x2)\n        x4 = self.bn1(x3)\n        x5 = torch.sigmoid(x4)\n        x6 = self.conv3(x5)\n        x7 = self.bn2(x6)\n        x8 = gum_sigmoid_function(x7)\n        x9 = self.max_pooling1(x8)\n        x10 = self.conv4(x9)\n        x11 = self.conv5(x10)\n        x12 = self.bn3(x11)\n        x13 = gum_sigmoid_function(x12)\n        x14 = self.conv6(x13)\n        x15 = self.conv7(x14)\n        x16 = self.bn4(x15)\n        x17 = gum_sigmoid_function(x16)\n        x18 = self.max_pooling2(x17)\n        x19 = self.flatten(x18)\n        x20 = self.linear1(x19)\n        x21 = self.linear2(x20)\n        x22 = torch.sigmoid(x21)\n        return x22\n# Inputs to the model\ninput_image = torch.randn(1, 3, 16, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(1, 16, 3, stride=1, padding=1)\n        self.conv2 = torch.nn.Conv2d(16, 8, 1, stride=2, padding=0)\n        self.conv3 = torch.nn.Conv2d(8, 8, 3, stride=1, padding=1)\n        self.conv4 = torch.nn.Conv2d(8, 8, 1, stride=1, padding=0)\n        self.conv5 = torch.nn.Conv2d(8, 1, 3, stride=1, padding=1)\n        self.relu1 = torch.nn.ReLU()\n        self.relu2 = torch.nn.ReLU()\n        self.relu3 = torch.nn.ReLU()\n    def forward(self, x1):\n        v1 = self.relu1(self.conv1(x1))\n        v2 = self.relu2(self.conv2(v1))\n        v3 = self.relu3(self.conv3(v2))\n        v4 = self.conv4(v3)\n        v5 = torch.sigmoid(self.conv5(v4))\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 1, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(1, 16, 3, stride=1, padding=1)\n        self.conv2 = torch.nn.Conv2d(16, 32, 1, stride=1, padding=1)\n        self.conv3 = torch.nn.Conv2d(32, 1, 3, stride=1, padding=1)\n    def forward(self, x1):\n\n        v1 = self.conv1(x1)\n        v2 = self.conv2(v1)\n        v3 = torch.sigmoid(v2)\n        v4 = self.conv3(v3)\n        v5 = torch.sigmoid(v4)\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 1, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(in_channels=1, out_channels=8, kernel_size=3, stride=1, padding=1)\n        self.conv2 = torch.nn.Conv2d(in_channels=8, out_channels=8, kernel_size=1, stride=1, padding=0)\n        self.conv3 = torch.nn.Conv2d(in_channels=8, out_channels=1, kernel_size=1, stride=1, padding=0)\n    def forward(self, x3):\n        v1 = self.conv1(x3)\n        v2 = self.conv2(v1)\n        v3 = self.conv3(v2)\n        v4 = torch.sigmoid(v3)\n        return v4\n# Inputs to the model\nx3 = torch.randn(1, 1, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(1, 32, 7, stride=1, padding=3)\n        self.conv2 = torch.nn.Conv2d(32, 64, 1, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = self.conv2(v2)\n        v4 = torch.sigmoid(v3)\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 1, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(1, 8, 3, stride=1, padding=1)\n        self.conv2 = torch.nn.Conv2d(8, 1, 1, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = self.conv2(v1)\n        v3 = torch.sigmoid(v2)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 1, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 1, 3, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = torch.sigmoid(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 3, 224, 224)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(32, 32, 1, stride=2, padding=0)\n        self.conv2 = torch.nn.Conv2d(32, 64, 1, stride=1, padding=0)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = self.conv2(v2)\n        v4 = torch.sigmoid(v3)\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 32, 32, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(in_channels=32, out_channels=32, kernel_size=[1, 252], stride=1, padding=0)\n        self.conv2 = torch.nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=0)\n        self.conv3 = torch.nn.Conv2d(in_channels=64, out_channels=64, kernel_size=1, stride=1, padding=0)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = self.conv2(v2)\n        v4 = torch.sigmoid(v3)\n        v5 = self.conv3(v4)\n        v6 = torch.sigmoid(v5)\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 32, 1, 252)\n"
            ],
            "g_time": 29.869800329208374
        }
    }
}
{
    "opt": "permute_matmul_fusion",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, x2):\n        v1 = x1.permute(2, 1, 0)\n        v2 = x2.transpose(0, 1)\n        v3 = torch.matmul(v1, v2)\n        return v3\n# Inputs to the model\nx1 = torch.randn(3, 2, 5)\nx2 = torch.randn(2, 5)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, x2):\n        x = torch.cat((x1, x2), dim = 1)\n        return x\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\nx2 = torch.randn(1, 2, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, x2):\n        t0 = torch.bmm(x1, x2)\n        t1 = x1.permute(0, 2, 1)\n        t2 = t0.permute(0, 2, 1)\n        t3 = t0.permute(0, 2, 1)\n        t4 = t2 * t3\n        return t4\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\nx2 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, x2):\n        v1 = x1.permute(0, 2, 1)\n        v2 = torch.bmm(x2, v1)\n        v3 = x1 * v2\n        v4 = v1.permute(0, 2, 1)\n        return v3.permute(0, 2, 1)\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\nx2 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, x2):\n        t0 = x1.permute(0, 2, 1)\n        t1 = torch.bmm(x2, t0)\n        t2 = t1.permute(0, 2, 1)\n        return torch.bmm(x2, t2)\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\nx2 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, x2):\n        v1 = x1.permute(0, 2, 1)\n        v2 = x2.permute(0, 2, 1)\n        v3 = torch.matmul(v1, input_tensor_B)\n        return torch.bmm(x2, v2)\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\nx2 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.t1 = torch.tensor([ 1.,  2.,\n                                  3.,  4.])\n        self.r0 = torch.tensor([ 5.,  6.])\n    def forward(self, x1, x2):\n        t0 = x1.permute(0, 2, 1)\n        t1 = x2.permute(0, 2, 1)\n        t2 = torch.bmm(t0, t1)\n        t3 = self.t1.unsqueeze(2) * t2\n        t4 = self.t1.unsqueeze(2) + t3\n        t5 = t4 * t3\n        t6 = torch.bmm(x1, t5)\n        t7 = self.r0.unsqueeze(2) + x2\n        return t7\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\nx2 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, x2):\n        v1 = x1.permute(1, 0, 2)\n        v2 = x2.permute(1, 0, 2)\n        v3 = torch.matmul(v1, v2)\n        v4 = v3.permute(0, 2, 1)\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\nx2 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, x2):\n        t2 = x2.permute(0, 3, 1, 2)\n        t3 = x1.permute(0, 3, 1, 2)\n        t5 = torch.bmm(t3, t2)\n        v8 = t2.permute(0, 1, 3, 2)\n        v9 = torch.bmm(t2, v8)\n        t4 = x1.permute(0, 3, 1, 2)\n        v6 = torch.bmm(t4, v8)\n        return v9\n# Inputs to the model\nx1 = torch.randn(1, 3, 2, 2)\nx2 = torch.randn(1, 3, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, x2):\n        v1 = x2.permute(0, 2, 1)\n        v2 = x1.permute(0, 2, 1)\n        v3 = torch.matmul(v1, v2)\n        v4 = torch.matmul(v2, x2)\n        v5 = v2.permute(0, 2, 1)\n        w1 = v4 * x1\n        return (v3 - torch.bmm(v5, w1))\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\nx2 = torch.randn(1, 2, 2)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, x2):\n        v1 = x1.permute(2, 1, 0)\n        v2 = x2.transpose(0, 1)\n        v3 = torch.matmul(v1, v2)\n        return v3\n# Inputs to the model\nx1 = torch.randn(3, 2, 5)\nx2 = torch.randn(2, 5)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, x2):\n        x = torch.cat((x1, x2), dim = 1)\n        return x\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\nx2 = torch.randn(1, 2, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, x2):\n        t0 = torch.bmm(x1, x2)\n        t1 = x1.permute(0, 2, 1)\n        t2 = t0.permute(0, 2, 1)\n        t3 = t0.permute(0, 2, 1)\n        t4 = t2 * t3\n        return t4\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\nx2 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, x2):\n        v1 = x1.permute(0, 2, 1)\n        v2 = torch.bmm(x2, v1)\n        v3 = x1 * v2\n        v4 = v1.permute(0, 2, 1)\n        return v3.permute(0, 2, 1)\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\nx2 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, x2):\n        t0 = x1.permute(0, 2, 1)\n        t1 = torch.bmm(x2, t0)\n        t2 = t1.permute(0, 2, 1)\n        return torch.bmm(x2, t2)\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\nx2 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, x2):\n        v1 = x1.permute(0, 2, 1)\n        v2 = x2.permute(0, 2, 1)\n        v3 = torch.matmul(v1, input_tensor_B)\n        return torch.bmm(x2, v2)\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\nx2 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.t1 = torch.tensor([ 1.,  2.,\n                                  3.,  4.])\n        self.r0 = torch.tensor([ 5.,  6.])\n    def forward(self, x1, x2):\n        t0 = x1.permute(0, 2, 1)\n        t1 = x2.permute(0, 2, 1)\n        t2 = torch.bmm(t0, t1)\n        t3 = self.t1.unsqueeze(2) * t2\n        t4 = self.t1.unsqueeze(2) + t3\n        t5 = t4 * t3\n        t6 = torch.bmm(x1, t5)\n        t7 = self.r0.unsqueeze(2) + x2\n        return t7\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\nx2 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, x2):\n        v1 = x1.permute(1, 0, 2)\n        v2 = x2.permute(1, 0, 2)\n        v3 = torch.matmul(v1, v2)\n        v4 = v3.permute(0, 2, 1)\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\nx2 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, x2):\n        t2 = x2.permute(0, 3, 1, 2)\n        t3 = x1.permute(0, 3, 1, 2)\n        t5 = torch.bmm(t3, t2)\n        v8 = t2.permute(0, 1, 3, 2)\n        v9 = torch.bmm(t2, v8)\n        t4 = x1.permute(0, 3, 1, 2)\n        v6 = torch.bmm(t4, v8)\n        return v9\n# Inputs to the model\nx1 = torch.randn(1, 3, 2, 2)\nx2 = torch.randn(1, 3, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, x2):\n        v1 = x2.permute(0, 2, 1)\n        v2 = x1.permute(0, 2, 1)\n        v3 = torch.matmul(v1, v2)\n        v4 = torch.matmul(v2, x2)\n        v5 = v2.permute(0, 2, 1)\n        w1 = v4 * x1\n        return (v3 - torch.bmm(v5, w1))\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\nx2 = torch.randn(1, 2, 2)\n"
            ],
            "g_time": 8.603440999984741
        }
    }
}
{
    "opt": "cat_mm",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x, y, z):\n        v1 = torch.mm(x, y)\n        v2 = torch.mm(y, z)\n        v4 = torch.mm(z, x)\n        v6 = torch.mm(x, y)\n        return torch.cat([v1, v1, v1, v2, v2, v4], 1)\n# Inputs to the model\nx = torch.randn(2, 2)\ny = torch.randn(2, 2)\nz = torch.randn(2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x):\n        v0 = torch.mm(x, x)\n        v1 = torch.mm(x, x)\n        return torch.cat([v0, v1, v1, v0, v1, v0], 1)\n# Inputs to the model\nx = torch.randn(1, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, x2):\n        v0 = torch.mm(x1, x2)\n        v1 = torch.mm(x1, x2)\n        v2 = torch.mm(x2, x1)\n        return torch.cat([v0, v1, v2], 1)\n# Inputs to the model\nx1 = torch.randn(1, 2)\nx2 = torch.randn(1, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.t1 = torch.randn(2, 2)\n        self.t2 = torch.randn(2, 2)\n        self.t3 = torch.randn(2, 2)\n    def forward(self, x1, x2):\n        v1 = torch.mm(x1, self.t1)\n        v2 = torch.mm(x1, self.t2)\n        v3 = torch.mm(x1, self.t3)\n        v4 = torch.mm(x2, self.t1)\n        return torch.cat([v1, v2, v3, v4], 0)\n# Inputs to the model\nx1 = torch.randn(2, 2)\nx2 = torch.randn(2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, x2):\n        v0 = torch.mm(x1, x2)\n        v1 = torch.mm(x1, x2)\n        v2 = torch.mm(x1, x2)\n        v3 = torch.mm(x1, x2)\n        x2.add_(12)\n        v4 = torch.mm(x1, x2)\n        x1.add_(12)\n        v5 = torch.mm(x1, x2)\n        return torch.cat([v0, v1, v2, v3, v4, v5], 0)\n# Inputs to the model\nx1 = torch.randn(1, 2)\nx2 = torch.randn(1, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, x2):\n        v0 = torch.mm(x1, x2)\n        v1 = torch.mm(x1, x2)\n        return torch.cat([v0, v1], 1)\n# Inputs to the model\nx1 = torch.randn(2, 3)\nx2 = torch.randn(3, 2)\n",
                "\nclass CustomModule(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear1 = torch.nn.Linear(2, 2)\n        self.linear2 = torch.nn.Linear(2, 2)\n    def forward(self, x, z):\n        h = self.linear1(x)\n        h.add_(self.linear2(x))\n        h.sub_(self.linear1(z))\n        h.add_(self.linear1(x))\n        return h\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear1 = torch.nn.Linear(3, 5)\n        self.linear2 = torch.nn.Linear(5, 2)\n        self.custom = CustomModule()\n    def forward(self, u, v, w):\n        x = self.linear1(u)\n        y = self.linear1(v)\n        z = self.linear1(w)\n        return self.custom(x, y)\n# Inputs to the model\nu = torch.randn(3, 2)\nv = torch.randn(3, 2)\nw = torch.randn(3, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, x2):\n        x1 = x1.detach()\n        x2 = x2.detach()\n        v1 = torch.mm(x1, x2)\n        v2 = torch.mm(x1, x2)\n        v3 = torch.mm(x1, x2)\n        return torch.cat([v1, v2, v3], 1)\n# Inputs to the model\nx1 = torch.randn(2, 2)\nx2 = torch.randn(2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, input):\n        t = torch.mm(input, input)\n        return torch.cat([t, t])\n# Inputs to the model\ninput = torch.randn(3, 4)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, x2):\n        v1 = torch.mm(x2, x1)\n        v2 = torch.mm(x2, x1)\n        v3 = torch.mm(x1, x2)\n        v4 = torch.mm(x2, x1)\n        v5 = torch.mm(x2, x1)\n        return torch.cat([v1, v2, v3, v4, v5], 0)\n# Inputs to the model\nx1 = torch.randn(2, 2)\nx2 = torch.randn(2, 2)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x, y, z):\n        v1 = torch.mm(x, y)\n        v2 = torch.mm(y, z)\n        v4 = torch.mm(z, x)\n        v6 = torch.mm(x, y)\n        return torch.cat([v1, v1, v1, v2, v2, v4], 1)\n# Inputs to the model\nx = torch.randn(2, 2)\ny = torch.randn(2, 2)\nz = torch.randn(2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x):\n        v0 = torch.mm(x, x)\n        v1 = torch.mm(x, x)\n        return torch.cat([v0, v1, v1, v0, v1, v0], 1)\n# Inputs to the model\nx = torch.randn(1, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, x2):\n        v0 = torch.mm(x1, x2)\n        v1 = torch.mm(x1, x2)\n        v2 = torch.mm(x2, x1)\n        return torch.cat([v0, v1, v2], 1)\n# Inputs to the model\nx1 = torch.randn(1, 2)\nx2 = torch.randn(1, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.t1 = torch.randn(2, 2)\n        self.t2 = torch.randn(2, 2)\n        self.t3 = torch.randn(2, 2)\n    def forward(self, x1, x2):\n        v1 = torch.mm(x1, self.t1)\n        v2 = torch.mm(x1, self.t2)\n        v3 = torch.mm(x1, self.t3)\n        v4 = torch.mm(x2, self.t1)\n        return torch.cat([v1, v2, v3, v4], 0)\n# Inputs to the model\nx1 = torch.randn(2, 2)\nx2 = torch.randn(2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, x2):\n        v0 = torch.mm(x1, x2)\n        v1 = torch.mm(x1, x2)\n        v2 = torch.mm(x1, x2)\n        v3 = torch.mm(x1, x2)\n        x2.add_(12)\n        v4 = torch.mm(x1, x2)\n        x1.add_(12)\n        v5 = torch.mm(x1, x2)\n        return torch.cat([v0, v1, v2, v3, v4, v5], 0)\n# Inputs to the model\nx1 = torch.randn(1, 2)\nx2 = torch.randn(1, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, x2):\n        v0 = torch.mm(x1, x2)\n        v1 = torch.mm(x1, x2)\n        return torch.cat([v0, v1], 1)\n# Inputs to the model\nx1 = torch.randn(2, 3)\nx2 = torch.randn(3, 2)\n",
                "\nclass CustomModule(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear1 = torch.nn.Linear(2, 2)\n        self.linear2 = torch.nn.Linear(2, 2)\n    def forward(self, x, z):\n        h = self.linear1(x)\n        h.add_(self.linear2(x))\n        h.sub_(self.linear1(z))\n        h.add_(self.linear1(x))\n        return h\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear1 = torch.nn.Linear(3, 5)\n        self.linear2 = torch.nn.Linear(5, 2)\n        self.custom = CustomModule()\n    def forward(self, u, v, w):\n        x = self.linear1(u)\n        y = self.linear1(v)\n        z = self.linear1(w)\n        return self.custom(x, y)\n# Inputs to the model\nu = torch.randn(3, 2)\nv = torch.randn(3, 2)\nw = torch.randn(3, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, x2):\n        x1 = x1.detach()\n        x2 = x2.detach()\n        v1 = torch.mm(x1, x2)\n        v2 = torch.mm(x1, x2)\n        v3 = torch.mm(x1, x2)\n        return torch.cat([v1, v2, v3], 1)\n# Inputs to the model\nx1 = torch.randn(2, 2)\nx2 = torch.randn(2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, input):\n        t = torch.mm(input, input)\n        return torch.cat([t, t])\n# Inputs to the model\ninput = torch.randn(3, 4)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, x2):\n        v1 = torch.mm(x2, x1)\n        v2 = torch.mm(x2, x1)\n        v3 = torch.mm(x1, x2)\n        v4 = torch.mm(x2, x1)\n        v5 = torch.mm(x2, x1)\n        return torch.cat([v1, v2, v3, v4, v5], 0)\n# Inputs to the model\nx1 = torch.randn(2, 2)\nx2 = torch.randn(2, 2)\n"
            ],
            "g_time": 9.125104188919067
        }
    }
}
{
    "opt": "binary_unary=10",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self, in_features=25088, out_features=4096):\n        super().__init__()\n        self.linear = torch.nn.Linear(in_features, out_features)\n \n    def forward(self, x1, x2):\n        x = self.linear(x1)\n        x = x + x2\n        x = torch.relu(x)\n        return x\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 25088)\nx2 = torch.randn(1, 4096)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.linear = torch.nn.Linear(8, 16)\n\n    def forward(self, x1, x2):\n        v1 = self.linear(x1)\n        v2 = v1 + x2\n        v3 = F.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(10, 8)\nx2 = torch.randn(10, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc = torch.nn.Linear(56, 32, bias=False)\n\n    def forward(self, x1):\n        v1 = self.fc(x1)\n        v2 = v1 + torch.ones_like(v1) / 56\n        v3 = torch.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(2, 56)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n \n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = other\n        w1 = v1 + v2\n        v3 = torch.relu(w1)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8, bias=False)\n \n    def forward(self, x1, x2):\n        v1 = self.linear(x1)\n        v2 = v1 + x2\n        v3 = F.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\nx2 = torch.randn(1, 8, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(128, 80)\n \n    def forward(self, x1, x2):\n        v1 = self.linear(x1)\n        v2 = v1 + x2\n        v3 = torch.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 128)\nx2 = torch.randn(1, 80)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 16)\n \n    def forward(self, x):\n        x1 = self.linear(x)\n        x2 = x1 + x\n        out = torch.nn.functional.relu(x2)\n        return out\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx = torch.randn(2, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.lin = torch.nn.Linear(10, 6)\n \n    def forward(self, x1):\n        v1 = self.lin(x1)\n        v2 = v1 + torch.sinh(x1)\n        v3 = torch.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(2, 10)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, dim_in, dim_out):\n        super().__init__()\n        self.linear = torch.nn.Linear(dim_in, dim_out)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 + other\n        v3 = torch.nn.functional.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model(dim_in=32, dim_out=64)\n\n# Inputs to the model\nx1 = torch.randn(1, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, __input_size1, __input_size2, __output_size):\n        super().__init__()\n        self.linear1 = torch.nn.Linear(__input_size1, __output_size)\n        self.linear2 = torch.nn.Linear(__input_size2, __output_size)\n \n    def forward(self, x1, x2):\n        v1 = self.linear1(x1)\n        v2 = v1 + x2\n        v3 = v2.relu()\n        return v3\n\n# Initializing the model\nm = Model(32, 16, 8)\n\n# Inputs to the model\nx1 = torch.randn(128, 32)\nx2 = torch.randn(128, 16)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self, in_features=25088, out_features=4096):\n        super().__init__()\n        self.linear = torch.nn.Linear(in_features, out_features)\n \n    def forward(self, x1, x2):\n        x = self.linear(x1)\n        x = x + x2\n        x = torch.relu(x)\n        return x\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 25088)\nx2 = torch.randn(1, 4096)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.linear = torch.nn.Linear(8, 16)\n\n    def forward(self, x1, x2):\n        v1 = self.linear(x1)\n        v2 = v1 + x2\n        v3 = F.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(10, 8)\nx2 = torch.randn(10, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc = torch.nn.Linear(56, 32, bias=False)\n\n    def forward(self, x1):\n        v1 = self.fc(x1)\n        v2 = v1 + torch.ones_like(v1) / 56\n        v3 = torch.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(2, 56)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n \n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = other\n        w1 = v1 + v2\n        v3 = torch.relu(w1)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8, bias=False)\n \n    def forward(self, x1, x2):\n        v1 = self.linear(x1)\n        v2 = v1 + x2\n        v3 = F.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\nx2 = torch.randn(1, 8, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(128, 80)\n \n    def forward(self, x1, x2):\n        v1 = self.linear(x1)\n        v2 = v1 + x2\n        v3 = torch.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 128)\nx2 = torch.randn(1, 80)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 16)\n \n    def forward(self, x):\n        x1 = self.linear(x)\n        x2 = x1 + x\n        out = torch.nn.functional.relu(x2)\n        return out\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx = torch.randn(2, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.lin = torch.nn.Linear(10, 6)\n \n    def forward(self, x1):\n        v1 = self.lin(x1)\n        v2 = v1 + torch.sinh(x1)\n        v3 = torch.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(2, 10)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, dim_in, dim_out):\n        super().__init__()\n        self.linear = torch.nn.Linear(dim_in, dim_out)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 + other\n        v3 = torch.nn.functional.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model(dim_in=32, dim_out=64)\n\n# Inputs to the model\nx1 = torch.randn(1, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, __input_size1, __input_size2, __output_size):\n        super().__init__()\n        self.linear1 = torch.nn.Linear(__input_size1, __output_size)\n        self.linear2 = torch.nn.Linear(__input_size2, __output_size)\n \n    def forward(self, x1, x2):\n        v1 = self.linear1(x1)\n        v2 = v1 + x2\n        v3 = v2.relu()\n        return v3\n\n# Initializing the model\nm = Model(32, 16, 8)\n\n# Inputs to the model\nx1 = torch.randn(128, 32)\nx2 = torch.randn(128, 16)\n"
            ],
            "g_time": 6.62427282333374
        }
    }
}
{
    "opt": "fuse_conv_bn",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.conv0 = nn.Conv2d(1,3,28,1)\n        self.bn0 = nn.BatchNorm2d(3, affine=False)\n        self.conv1 = nn.Conv2d(3,32,1,1)\n        self.conv2 = nn.Conv2d(3,32,3,1)\n        self.conv3 = nn.Conv2d(32,32,1,1)\n        self.bn1 = nn.BatchNorm2d(32, affine=False)\n        self.conv4 = nn.Conv2d(32,32,5,1)\n        self.conv5 = nn.Conv2d(32, 32, 1, 1)\n        self.bn2 = nn.BatchNorm2d(32, affine=True)\n        self.relu = nn.ReLU(inplace=True)\n        self.bn3 = nn.BatchNorm2d(32, affine=False)\n        self.conv6 = nn.Conv2d(32, 32, 3, 1)\n        self.conv7 = nn.Conv2d(32,32,1,1)\n        self.bn4 = nn.BatchNorm2d(32, affine=False)\n        self.conv8 = nn.Conv2d(32,32,3,1)\n        self.conv9 = nn.Conv2d(32,32,1,1)\n        self.bn5 = nn.BatchNorm2d(32, affine=True)\n        self.relu1 = nn.ReLU(inplace=True)\n        self.bn6 = nn.BatchNorm2d(32, affine=False)\n        self.conv10 = nn.Conv2d(32,32,3,1)\n        self.conv11 = nn.Conv2d(32,32,1,1)\n        self.bn7 = nn.BatchNorm2d(32, affine=False)\n        self.conv12 = nn.Conv2d(32,32,3,1)\n        self.conv13 = nn.Conv2d(32,10,1,1)\n    def forward(self, x0):\n        x1 = self.conv0(x0)\n        x1 = self.bn0(x1)\n        x1 = self.conv1(x1)\n        x2 = self.conv2(x1)\n        x3 = self.conv3(x2)\n        x3 = self.bn1(x3)\n        x4 = self.conv4(x3)\n        x5 = self.conv5(x4)\n        x5 = self.bn2(x5)\n        x5 = self.conv6(x5)\n        x6 = self.conv7(x5)\n        x6 = self.bn3(x6)\n        x7 = self.conv8(x6)\n        x8 = self.conv9(x7)\n        x8 = self.bn4(x8)\n        x9 = self.conv10(x8)\n        x10 = self.conv11(x9)\n        x10 = self.bn5(x10)\n        x11 = self.conv12(x10)\n        x12 = self.conv13(x11)\n        return (x12, x12)\n# Inputs to the model\nx0 = torch.randn(10, 1, 28, 28)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.a = torch.nn.BatchNorm1d(16)\n    def forward(self, x):\n        x = self.a(x)\n        return x\n# Inputs to the model\nx = torch.randn(2, 16, 1, 1)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.a = torch.nn.Conv2d(6, 3, 1)\n        self.relu = torch.nn.ReLU()\n        self.b = torch.nn.BatchNorm2d(6)\n        self.maxpool2d = torch.nn.MaxPool2d((1, 1))\n    def forward(self, x4):\n        x4 = self.b(x4)\n        x4 = self.maxpool2d(x4)\n        return self.relu(self.a(x4))\n# Inputs to the model\nx4 = torch.randn(10, 6, 38, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        torch.manual_seed(2)\n        self.conv1 = torch.nn.Conv2d(3, 3, 3)\n        torch.manual_seed(2)\n        self.bn = torch.nn.BatchNorm2d(3)\n        self.conv2 = torch.nn.Conv2d(3, 3, 2)\n        self.activation = torch.nn.Sigmoid()\n    def forward(self, x1):\n        s = self.conv1(x1)\n        t = self.bn(s)\n        y = self.conv2(t)\n        return y\n# Inputs to the model\nx1 = torch.randn(1, 3, 6, 6)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 3, 3, bias=False)\n        self.bn = torch.nn.BatchNorm2d(3)\n    def forward(self, x):\n        y = self.conv(x)\n        y = self.bn(y)\n        return y\n# Inputs to the model\nx = torch.randn(1, 3, 6, 6)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        conv = torch.nn.Conv1d\n        bn = torch.nn.BatchNorm1d\n        self.in_channels = 3\n        self.conv = conv(self.in_channels, 16, kernel_size=7, bias=False)\n        self.bn = bn(16, momentum=0.5)\n        self.relu = torch.nn.ReLU(inplace=True)\n    def forward(self, x2):\n        x = self.conv(x2)\n        x = self.bn(x)\n        x = self.relu(x)\n        return x\n# Inputs to the model\nx2 = torch.randn(1, 3, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        torch.manual_seed(1)\n        self.conv = torch.nn.Conv1d(6, 6, 6)\n        torch.manual_seed(1)\n        self.bn = torch.nn.BatchNorm1d(6)\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.bn(x)\n        return x\n# Inputs to the model\nx = torch.randn(1, 6, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 3)\n        self.activation = torch.nn.ReLU()\n        self.bn = torch.nn.BatchNorm1d(3)\n        self.linear2 = torch.nn.Linear(3, 3)\n    def forward(self, x0):\n        y0 = self.linear(x0)\n        y1 = self.activation(y0)\n        y1 = self.bn(y1)\n        y1 = self.linear2(y1)\n        return y1\n# Inputs to the model\nx0 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.a = torch.nn.Conv2d(1, 1, 3, groups=1, bias=False)\n        self.b = torch.nn.BatchNorm2d(1, momentum=0.1, affine=True, track_running_stats=True)\n        self.c = torch.nn.Conv2d(1, 1, 3, stride=(1), dilation=(1), groups=1, bias=False, padding=(1))\n    def forward(self, x):\n        o1 = self.a(x)\n        o2 = self.b(o1)\n        o3 = self.c(o2)\n        return o3\n# Inputs to the model\nx = torch.randn(1, 1, 3, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 3, 3)\n        self.conv2 = torch.nn.Conv2d(3, 3, 2)\n        self.bn = torch.nn.BatchNorm2d(3)\n        self.activation = torch.nn.Sigmoid()\n    def forward(self, x1):\n        s = self.conv1(x1)\n        t = self.conv2(s)\n        t = self.bn(t)\n        y = self.activation(t)\n        return s\n# Inputs to the model\nx1 = torch.randn(1, 3, 6, 6)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.conv0 = nn.Conv2d(1,3,28,1)\n        self.bn0 = nn.BatchNorm2d(3, affine=False)\n        self.conv1 = nn.Conv2d(3,32,1,1)\n        self.conv2 = nn.Conv2d(3,32,3,1)\n        self.conv3 = nn.Conv2d(32,32,1,1)\n        self.bn1 = nn.BatchNorm2d(32, affine=False)\n        self.conv4 = nn.Conv2d(32,32,5,1)\n        self.conv5 = nn.Conv2d(32, 32, 1, 1)\n        self.bn2 = nn.BatchNorm2d(32, affine=True)\n        self.relu = nn.ReLU(inplace=True)\n        self.bn3 = nn.BatchNorm2d(32, affine=False)\n        self.conv6 = nn.Conv2d(32, 32, 3, 1)\n        self.conv7 = nn.Conv2d(32,32,1,1)\n        self.bn4 = nn.BatchNorm2d(32, affine=False)\n        self.conv8 = nn.Conv2d(32,32,3,1)\n        self.conv9 = nn.Conv2d(32,32,1,1)\n        self.bn5 = nn.BatchNorm2d(32, affine=True)\n        self.relu1 = nn.ReLU(inplace=True)\n        self.bn6 = nn.BatchNorm2d(32, affine=False)\n        self.conv10 = nn.Conv2d(32,32,3,1)\n        self.conv11 = nn.Conv2d(32,32,1,1)\n        self.bn7 = nn.BatchNorm2d(32, affine=False)\n        self.conv12 = nn.Conv2d(32,32,3,1)\n        self.conv13 = nn.Conv2d(32,10,1,1)\n    def forward(self, x0):\n        x1 = self.conv0(x0)\n        x1 = self.bn0(x1)\n        x1 = self.conv1(x1)\n        x2 = self.conv2(x1)\n        x3 = self.conv3(x2)\n        x3 = self.bn1(x3)\n        x4 = self.conv4(x3)\n        x5 = self.conv5(x4)\n        x5 = self.bn2(x5)\n        x5 = self.conv6(x5)\n        x6 = self.conv7(x5)\n        x6 = self.bn3(x6)\n        x7 = self.conv8(x6)\n        x8 = self.conv9(x7)\n        x8 = self.bn4(x8)\n        x9 = self.conv10(x8)\n        x10 = self.conv11(x9)\n        x10 = self.bn5(x10)\n        x11 = self.conv12(x10)\n        x12 = self.conv13(x11)\n        return (x12, x12)\n# Inputs to the model\nx0 = torch.randn(10, 1, 28, 28)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.a = torch.nn.BatchNorm1d(16)\n    def forward(self, x):\n        x = self.a(x)\n        return x\n# Inputs to the model\nx = torch.randn(2, 16, 1, 1)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.a = torch.nn.Conv2d(6, 3, 1)\n        self.relu = torch.nn.ReLU()\n        self.b = torch.nn.BatchNorm2d(6)\n        self.maxpool2d = torch.nn.MaxPool2d((1, 1))\n    def forward(self, x4):\n        x4 = self.b(x4)\n        x4 = self.maxpool2d(x4)\n        return self.relu(self.a(x4))\n# Inputs to the model\nx4 = torch.randn(10, 6, 38, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        torch.manual_seed(2)\n        self.conv1 = torch.nn.Conv2d(3, 3, 3)\n        torch.manual_seed(2)\n        self.bn = torch.nn.BatchNorm2d(3)\n        self.conv2 = torch.nn.Conv2d(3, 3, 2)\n        self.activation = torch.nn.Sigmoid()\n    def forward(self, x1):\n        s = self.conv1(x1)\n        t = self.bn(s)\n        y = self.conv2(t)\n        return y\n# Inputs to the model\nx1 = torch.randn(1, 3, 6, 6)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 3, 3, bias=False)\n        self.bn = torch.nn.BatchNorm2d(3)\n    def forward(self, x):\n        y = self.conv(x)\n        y = self.bn(y)\n        return y\n# Inputs to the model\nx = torch.randn(1, 3, 6, 6)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        conv = torch.nn.Conv1d\n        bn = torch.nn.BatchNorm1d\n        self.in_channels = 3\n        self.conv = conv(self.in_channels, 16, kernel_size=7, bias=False)\n        self.bn = bn(16, momentum=0.5)\n        self.relu = torch.nn.ReLU(inplace=True)\n    def forward(self, x2):\n        x = self.conv(x2)\n        x = self.bn(x)\n        x = self.relu(x)\n        return x\n# Inputs to the model\nx2 = torch.randn(1, 3, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        torch.manual_seed(1)\n        self.conv = torch.nn.Conv1d(6, 6, 6)\n        torch.manual_seed(1)\n        self.bn = torch.nn.BatchNorm1d(6)\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.bn(x)\n        return x\n# Inputs to the model\nx = torch.randn(1, 6, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 3)\n        self.activation = torch.nn.ReLU()\n        self.bn = torch.nn.BatchNorm1d(3)\n        self.linear2 = torch.nn.Linear(3, 3)\n    def forward(self, x0):\n        y0 = self.linear(x0)\n        y1 = self.activation(y0)\n        y1 = self.bn(y1)\n        y1 = self.linear2(y1)\n        return y1\n# Inputs to the model\nx0 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.a = torch.nn.Conv2d(1, 1, 3, groups=1, bias=False)\n        self.b = torch.nn.BatchNorm2d(1, momentum=0.1, affine=True, track_running_stats=True)\n        self.c = torch.nn.Conv2d(1, 1, 3, stride=(1), dilation=(1), groups=1, bias=False, padding=(1))\n    def forward(self, x):\n        o1 = self.a(x)\n        o2 = self.b(o1)\n        o3 = self.c(o2)\n        return o3\n# Inputs to the model\nx = torch.randn(1, 1, 3, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 3, 3)\n        self.conv2 = torch.nn.Conv2d(3, 3, 2)\n        self.bn = torch.nn.BatchNorm2d(3)\n        self.activation = torch.nn.Sigmoid()\n    def forward(self, x1):\n        s = self.conv1(x1)\n        t = self.conv2(s)\n        t = self.bn(t)\n        y = self.activation(t)\n        return s\n# Inputs to the model\nx1 = torch.randn(1, 3, 6, 6)\n"
            ],
            "g_time": 26.066051959991455
        }
    }
}
{
    "opt": "unary=13",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8, bias=False)\n\n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(16, 16)\n \n    def forward(self, x1):    \n        v1 = self.linear(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 20)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n \n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 4)\n\n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(8, 9)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = F.sigmoid(v1)\n        v3 = v1*v2\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 8, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(64, 32)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(56, 20)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 56)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(2, 1)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(1536, 1000)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 1536)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8, bias=False)\n\n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(16, 16)\n \n    def forward(self, x1):    \n        v1 = self.linear(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 20)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n \n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 4)\n\n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(8, 9)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = F.sigmoid(v1)\n        v3 = v1*v2\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 8, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(64, 32)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(56, 20)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 56)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(2, 1)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(1536, 1000)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 1536)\n"
            ],
            "g_time": 5.42256236076355
        }
    }
}
{
    "opt": "binary_unary=0",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(16, 16, 7, stride=1, padding=3)\n        self.conv2 = torch.nn.Conv2d(16, 16, 7, stride=1, padding=3)\n    def forward(self, x1, x2):\n        v1 = self.conv1(x1)\n        v2 = self.conv2(x2)\n        v3 = torch.relu(v1)\n        v4 = self.conv1(v3)\n        v5 = v4 + v2\n        v6 = torch.relu(v5)\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 16, 64, 64)\nx2 = torch.randn(1, 16, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(16, 16, 5, stride=1, padding=2)\n        self.conv2 = torch.nn.Conv2d(16, 16, 5, stride=1, padding=2)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = v1 + x1\n        v3 = torch.relu(v2)\n        v4 = self.conv2(v3)\n        v5 = v4 + v3\n        v6 = torch.relu(v5)\n        v7 = self.conv1(v6)\n        v8 = v7 + v6\n        v9 = torch.relu(v8)\n        v10 = self.conv2(v9)\n        v11 = v10 + v9\n        v12 = torch.relu(v11)\n        return v12\n# Inputs to the model\nx1 = torch.randn(2, 16, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(24, 48, (1,5), stride=(4,1), padding=(1,2))\n        self.conv2 = torch.nn.Conv2d(48, 96, (1,10), stride=(1,5), padding=(0,7))\n        self.conv3 = torch.nn.Conv2d(96, 192, (1,20), stride=(1,10), padding=(0,6))\n    def forward(self, x):\n        v1 = self.conv1(x)\n        v2 = self.conv2(v1)\n        v3 = self.conv3(v2)\n        return v3\n# Inputs to the model\nx = torch.randn(1, 24, 47, 26)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(16, 16, 3, stride=1, padding=1)\n        self.conv2 = torch.nn.Conv2d(16, 16, 1, stride=1, padding=0)\n        self.conv3 = torch.nn.Conv2d(16, 16, 3, stride=1, padding=1)\n    def forward(self, x):\n        v1 = self.conv1(x)\n        v2 = self.conv2(v1)\n        v3 = self.conv3(v2)\n        return v3\n# Inputs to the model\nx = torch.randn(1, 16, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(16, 16, 7, stride=1, padding=3)\n    def forward(self, x):\n        v1 = self.conv(x) + x\n        v2 = torch.relu(v1)\n        return v2\n# Inputs to the model\nx = torch.randn(1, 16, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(16, 16, 7, stride=1, padding=3)\n        self.conv2 = torch.nn.Conv2d(16, 16, 7, stride=1, padding=3)\n        self.conv3 = torch.nn.Conv2d(16, 16, 7, stride=1, padding=3)\n    def forward(self, x1, x2, x3):\n        v1 = self.conv1(x1)\n        v2 = torch.relu(v1)\n        v3 = v2 + x1\n        v4 = torch.relu(v3)\n        v5 = self.conv2(v4)\n        v6 = v5 + x2\n        v7 = torch.relu(v6)\n        v8 = self.conv3(v6)\n        v9 = v8 + v7\n        return v9\n# Inputs to the model\nx1 = torch.randn(3, 16, 64, 64),\nx2 = torch.randn(2, 16, 64, 64)\nx3 = torch.randn(2, 16, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(16, 16, 7, stride=1, padding=3)\n    def forward(self, x1, x2, x3):\n        v1 = self.conv(x1)\n        v2 = self.conv(x1)\n        v3 = v2 * v1\n        v4 = v1 ^ v3\n        v5 = v4 | v2\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 16, 64, 64)\nx2 = torch.randn(1, 16, 64, 64)\nx3 = torch.randn(1, 16, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(16, 16, 7, stride=1, padding=3)\n        self.conv2 = torch.nn.Conv2d(16, 16, 7, stride=1, padding=3)\n        self.conv3 = torch.nn.Conv2d(16, 16, 7, stride=1, padding=3)\n        self.conv4 = torch.nn.Conv2d(32, 16, 7, stride=1, padding=3)\n    def forward(self, x):\n        v1 = self.conv1(x)\n        v2 = self.conv2(v1)\n        v3 = torch.relu(v2 + x)\n        v4 = self.conv3(v3)\n        v5 = torch.cat([v3, v4], 1)\n        v6 = self.conv4(v5)\n        v7 = v4 + v6\n        v8 = torch.relu(v7)\n        return v8\n# Inputs to the model\nx1 = torch.randn(1, 32, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(16, 16, 7, stride=1, padding=3)\n        self.conv2 = torch.nn.Conv2d(16, 16, 7, stride=1, padding=3)\n        self.conv3 = torch.nn.Conv2d(16, 16, 7, stride=1, padding=3)\n        self.conv4 = torch.nn.Conv2d(16, 16, 3, stride=1, padding=1)\n        self.conv5 = torch.nn.Conv2d(16, 16, 3, stride=1, padding=1)\n        self.conv6 = torch.nn.Conv2d(16, 16, 3, stride=1, padding=1)\n        self.conv7 = torch.nn.Conv2d(16, 16, 3, stride=1, padding=1)\n        self.conv8 = torch.nn.Conv2d(16, 16, 1, stride=1, padding=0)\n    def forward(self, x1, x2, x3):\n        v1 = self.conv1(x3)\n        v2 = v1 + x2\n        v3 = torch.relu(v2)\n        v4 = self.conv2(v3)\n        v5 = v4 * 2\n        v6 = v3 + x2\n        v7 = x1 + v6\n        v8 = self.conv3(v7)\n        v9 = v8 * v5\n        v10 = v8 + v5\n        v11 = self.conv4(v6)\n        v12 = self.conv5(v9)\n        v13 = self.conv6(v10)\n        v14 = self.conv7(v12)\n        v15 = v14 * v11\n        v16 = v15 * v13\n        v17 = v15 * v16\n        v18 = self.conv8(v17)\n        v19 = v18 * v11\n        v20 = v19 + v11\n        return v20\n# Inputs to the model\nx1 = torch.randn(50, 16, 32, 32)\nx2 = torch.randn(50, 16, 32, 32)\nx3 = torch.randn(50, 16, 32, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(16, 16, 7, stride=1, padding=3)\n        self.conv2 = torch.nn.Conv2d(16, 16, 7, stride=1, padding=3)\n    def forward(self, x1, x2, x3):\n        v1 = self.conv1(x1)\n        v2 = v1 * 2.0\n        v3 = v2 + x2\n        v4 = self.conv2(v3)\n        v5 = v3 + x3\n        v6 = torch.relu(v4 + v5)\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 16, 64, 64)\nx2 = torch.randn(1, 16, 64, 64)\nx3 = torch.randn(1, 16, 64, 64)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(16, 16, 7, stride=1, padding=3)\n        self.conv2 = torch.nn.Conv2d(16, 16, 7, stride=1, padding=3)\n    def forward(self, x1, x2):\n        v1 = self.conv1(x1)\n        v2 = self.conv2(x2)\n        v3 = torch.relu(v1)\n        v4 = self.conv1(v3)\n        v5 = v4 + v2\n        v6 = torch.relu(v5)\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 16, 64, 64)\nx2 = torch.randn(1, 16, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(16, 16, 5, stride=1, padding=2)\n        self.conv2 = torch.nn.Conv2d(16, 16, 5, stride=1, padding=2)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = v1 + x1\n        v3 = torch.relu(v2)\n        v4 = self.conv2(v3)\n        v5 = v4 + v3\n        v6 = torch.relu(v5)\n        v7 = self.conv1(v6)\n        v8 = v7 + v6\n        v9 = torch.relu(v8)\n        v10 = self.conv2(v9)\n        v11 = v10 + v9\n        v12 = torch.relu(v11)\n        return v12\n# Inputs to the model\nx1 = torch.randn(2, 16, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(24, 48, (1,5), stride=(4,1), padding=(1,2))\n        self.conv2 = torch.nn.Conv2d(48, 96, (1,10), stride=(1,5), padding=(0,7))\n        self.conv3 = torch.nn.Conv2d(96, 192, (1,20), stride=(1,10), padding=(0,6))\n    def forward(self, x):\n        v1 = self.conv1(x)\n        v2 = self.conv2(v1)\n        v3 = self.conv3(v2)\n        return v3\n# Inputs to the model\nx = torch.randn(1, 24, 47, 26)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(16, 16, 3, stride=1, padding=1)\n        self.conv2 = torch.nn.Conv2d(16, 16, 1, stride=1, padding=0)\n        self.conv3 = torch.nn.Conv2d(16, 16, 3, stride=1, padding=1)\n    def forward(self, x):\n        v1 = self.conv1(x)\n        v2 = self.conv2(v1)\n        v3 = self.conv3(v2)\n        return v3\n# Inputs to the model\nx = torch.randn(1, 16, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(16, 16, 7, stride=1, padding=3)\n    def forward(self, x):\n        v1 = self.conv(x) + x\n        v2 = torch.relu(v1)\n        return v2\n# Inputs to the model\nx = torch.randn(1, 16, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(16, 16, 7, stride=1, padding=3)\n        self.conv2 = torch.nn.Conv2d(16, 16, 7, stride=1, padding=3)\n        self.conv3 = torch.nn.Conv2d(16, 16, 7, stride=1, padding=3)\n    def forward(self, x1, x2, x3):\n        v1 = self.conv1(x1)\n        v2 = torch.relu(v1)\n        v3 = v2 + x1\n        v4 = torch.relu(v3)\n        v5 = self.conv2(v4)\n        v6 = v5 + x2\n        v7 = torch.relu(v6)\n        v8 = self.conv3(v6)\n        v9 = v8 + v7\n        return v9\n# Inputs to the model\nx1 = torch.randn(3, 16, 64, 64),\nx2 = torch.randn(2, 16, 64, 64)\nx3 = torch.randn(2, 16, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(16, 16, 7, stride=1, padding=3)\n    def forward(self, x1, x2, x3):\n        v1 = self.conv(x1)\n        v2 = self.conv(x1)\n        v3 = v2 * v1\n        v4 = v1 ^ v3\n        v5 = v4 | v2\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 16, 64, 64)\nx2 = torch.randn(1, 16, 64, 64)\nx3 = torch.randn(1, 16, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(16, 16, 7, stride=1, padding=3)\n        self.conv2 = torch.nn.Conv2d(16, 16, 7, stride=1, padding=3)\n        self.conv3 = torch.nn.Conv2d(16, 16, 7, stride=1, padding=3)\n        self.conv4 = torch.nn.Conv2d(32, 16, 7, stride=1, padding=3)\n    def forward(self, x):\n        v1 = self.conv1(x)\n        v2 = self.conv2(v1)\n        v3 = torch.relu(v2 + x)\n        v4 = self.conv3(v3)\n        v5 = torch.cat([v3, v4], 1)\n        v6 = self.conv4(v5)\n        v7 = v4 + v6\n        v8 = torch.relu(v7)\n        return v8\n# Inputs to the model\nx1 = torch.randn(1, 32, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(16, 16, 7, stride=1, padding=3)\n        self.conv2 = torch.nn.Conv2d(16, 16, 7, stride=1, padding=3)\n        self.conv3 = torch.nn.Conv2d(16, 16, 7, stride=1, padding=3)\n        self.conv4 = torch.nn.Conv2d(16, 16, 3, stride=1, padding=1)\n        self.conv5 = torch.nn.Conv2d(16, 16, 3, stride=1, padding=1)\n        self.conv6 = torch.nn.Conv2d(16, 16, 3, stride=1, padding=1)\n        self.conv7 = torch.nn.Conv2d(16, 16, 3, stride=1, padding=1)\n        self.conv8 = torch.nn.Conv2d(16, 16, 1, stride=1, padding=0)\n    def forward(self, x1, x2, x3):\n        v1 = self.conv1(x3)\n        v2 = v1 + x2\n        v3 = torch.relu(v2)\n        v4 = self.conv2(v3)\n        v5 = v4 * 2\n        v6 = v3 + x2\n        v7 = x1 + v6\n        v8 = self.conv3(v7)\n        v9 = v8 * v5\n        v10 = v8 + v5\n        v11 = self.conv4(v6)\n        v12 = self.conv5(v9)\n        v13 = self.conv6(v10)\n        v14 = self.conv7(v12)\n        v15 = v14 * v11\n        v16 = v15 * v13\n        v17 = v15 * v16\n        v18 = self.conv8(v17)\n        v19 = v18 * v11\n        v20 = v19 + v11\n        return v20\n# Inputs to the model\nx1 = torch.randn(50, 16, 32, 32)\nx2 = torch.randn(50, 16, 32, 32)\nx3 = torch.randn(50, 16, 32, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(16, 16, 7, stride=1, padding=3)\n        self.conv2 = torch.nn.Conv2d(16, 16, 7, stride=1, padding=3)\n    def forward(self, x1, x2, x3):\n        v1 = self.conv1(x1)\n        v2 = v1 * 2.0\n        v3 = v2 + x2\n        v4 = self.conv2(v3)\n        v5 = v3 + x3\n        v6 = torch.relu(v4 + v5)\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 16, 64, 64)\nx2 = torch.randn(1, 16, 64, 64)\nx3 = torch.randn(1, 16, 64, 64)\n"
            ],
            "g_time": 19.440324306488037
        }
    }
}
{
    "opt": "unary=5",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(1, 1, (13, 14), stride=2, padding=1)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 1, 128, 128)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(1, 32, 8, stride=1, padding=8)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.where(v3 > 0, torch.ones_like(v3, dtype=torch.float32), torch.zeros_like(v3, dtype=torch.float32))\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 1, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(4, 32, 4, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 4, 32, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose_1 = torch.nn.ConvTranspose2d(4, 64, 8, stride=8, padding=4)\n        self.conv_transpose_2 = torch.nn.ConvTranspose2d(1, 32, 8, stride=1, padding=8)\n    def forward(self, x1):\n        t1 = self.conv_transpose_2(self.conv_transpose_1(x1))\n        v1 = t1 * 0.5\n        v2 = t1 * 0.7071067811865476\n        v3 = torch.erf(v2)\n        v4 = v3 + 1\n        v5 = v1 * v4\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 1, 255, 80)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(16, 32, 5, stride=1, padding=2)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 16, 128, 128)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(16, 16, 3, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 16, 128, 128)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(8, 32, 5, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 8, 128, 128)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(32, 16, 2, stride=2, padding=0)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 32, 512, 512)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(4, 1, 3, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 4, 512, 512)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(64, 32, 5, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 64, 256, 256)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(1, 1, (13, 14), stride=2, padding=1)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 1, 128, 128)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(1, 32, 8, stride=1, padding=8)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.where(v3 > 0, torch.ones_like(v3, dtype=torch.float32), torch.zeros_like(v3, dtype=torch.float32))\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 1, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(4, 32, 4, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 4, 32, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose_1 = torch.nn.ConvTranspose2d(4, 64, 8, stride=8, padding=4)\n        self.conv_transpose_2 = torch.nn.ConvTranspose2d(1, 32, 8, stride=1, padding=8)\n    def forward(self, x1):\n        t1 = self.conv_transpose_2(self.conv_transpose_1(x1))\n        v1 = t1 * 0.5\n        v2 = t1 * 0.7071067811865476\n        v3 = torch.erf(v2)\n        v4 = v3 + 1\n        v5 = v1 * v4\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 1, 255, 80)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(16, 32, 5, stride=1, padding=2)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 16, 128, 128)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(16, 16, 3, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 16, 128, 128)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(8, 32, 5, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 8, 128, 128)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(32, 16, 2, stride=2, padding=0)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 32, 512, 512)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(4, 1, 3, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 4, 512, 512)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(64, 32, 5, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 64, 256, 256)\n"
            ],
            "g_time": 8.436896562576294
        }
    }
}
{
    "opt": "cat_addmm",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layers = nn.Linear(2, 1)\n        self.stack1 = torch.stack\n        self.stack2 = torch.stack\n    def forward(self, x):\n        x = x.unsqueeze(1)\n        x = self.stack1(x)\n        x = self.layers(x)\n        x = self.stack2(x)\n        return x\n# Inputs to the model\nx = torch.randn(4, 2)\n",
                "\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layers = nn.Linear(2, 6)\n    def forward(self, x):\n        x = self.layers(x)\n        x = torch.stack((x, x), dim=0)\n        return x\n# Inputs to the model\nx = torch.randn(1, 2)\n",
                "\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layers = nn.Linear(16, 2)\n        self.stack = torch.stack\n    def forward(self, x):\n        x = self.layers(x)\n        x = self.stack([x], dim=1)\n        return x\n# Inputs to the model\nx = torch.randn(2, 16)\n",
                "\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.cat = torch.cat\n    def forward(self, x):\n        x = x.flatten(1)\n        return self.cat((x, x.t()), dim=1)\n# Inputs to the model\nx = torch.randn(3, 5)\n",
                "\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layers = torch.addmm\n    def forward(self, x):\n        x = self.layers(x, x, x)\n        return x\n# Inputs to the model\nx = torch.randn(2, 2)\n",
                "\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layers = nn.Linear(2, 2)\n        self.cat = torch.cat\n    def forward(self, x):\n        x = self.layers(x)\n        x = self.cat([x, x], dim=1)\n        return x\n# Inputs to the model\nx = torch.randn(2, 2)\n",
                "\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layers = nn.Linear(2, 4)\n    def forward(self, x):\n        x = self.layers(x)\n        x = torch.stack((x, x), dim=-1)\n        return x\n# Inputs to the model\nx = torch.randn(2, 2)\n",
                "\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layers = nn.Sequential(nn.Linear(2, 4), nn.Linear(4, 4))\n    def forward(self, x):\n        x = self.layers(x)\n        x = torch.stack((x, x), dim=1)\n        return x\n# Inputs to the model\nx = torch.randn(2, 2)\n",
                "\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layers = nn.Linear(2, 4)\n    def forward(self, x):\n        x = self.layers(x)\n        x = torch.stack(t1_, dim=1)\n        return x\n# Inputs to the model\nx_ = torch.randn(2, 2)\n",
                "\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layers = nn.Linear(2, 2)\n        self.stack = torch.stack\n    def forward(self, x):\n        x = self.layers(x)\n        x = self.stack([x, x], dim=1)\n        return x\n# Inputs to the model\nx = torch.randn(1, 2)\n"
            ],
            "code": [
                "\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layers = nn.Linear(2, 1)\n        self.stack1 = torch.stack\n        self.stack2 = torch.stack\n    def forward(self, x):\n        x = x.unsqueeze(1)\n        x = self.stack1(x)\n        x = self.layers(x)\n        x = self.stack2(x)\n        return x\n# Inputs to the model\nx = torch.randn(4, 2)\n",
                "\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layers = nn.Linear(2, 6)\n    def forward(self, x):\n        x = self.layers(x)\n        x = torch.stack((x, x), dim=0)\n        return x\n# Inputs to the model\nx = torch.randn(1, 2)\n",
                "\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layers = nn.Linear(16, 2)\n        self.stack = torch.stack\n    def forward(self, x):\n        x = self.layers(x)\n        x = self.stack([x], dim=1)\n        return x\n# Inputs to the model\nx = torch.randn(2, 16)\n",
                "\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.cat = torch.cat\n    def forward(self, x):\n        x = x.flatten(1)\n        return self.cat((x, x.t()), dim=1)\n# Inputs to the model\nx = torch.randn(3, 5)\n",
                "\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layers = torch.addmm\n    def forward(self, x):\n        x = self.layers(x, x, x)\n        return x\n# Inputs to the model\nx = torch.randn(2, 2)\n",
                "\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layers = nn.Linear(2, 2)\n        self.cat = torch.cat\n    def forward(self, x):\n        x = self.layers(x)\n        x = self.cat([x, x], dim=1)\n        return x\n# Inputs to the model\nx = torch.randn(2, 2)\n",
                "\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layers = nn.Linear(2, 4)\n    def forward(self, x):\n        x = self.layers(x)\n        x = torch.stack((x, x), dim=-1)\n        return x\n# Inputs to the model\nx = torch.randn(2, 2)\n",
                "\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layers = nn.Sequential(nn.Linear(2, 4), nn.Linear(4, 4))\n    def forward(self, x):\n        x = self.layers(x)\n        x = torch.stack((x, x), dim=1)\n        return x\n# Inputs to the model\nx = torch.randn(2, 2)\n",
                "\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layers = nn.Linear(2, 4)\n    def forward(self, x):\n        x = self.layers(x)\n        x = torch.stack(t1_, dim=1)\n        return x\n# Inputs to the model\nx_ = torch.randn(2, 2)\n",
                "\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layers = nn.Linear(2, 2)\n        self.stack = torch.stack\n    def forward(self, x):\n        x = self.layers(x)\n        x = self.stack([x, x], dim=1)\n        return x\n# Inputs to the model\nx = torch.randn(1, 2)\n"
            ],
            "g_time": 4.0302886962890625
        }
    }
}
{
    "opt": "binary=8",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n \n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 + x1\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 4, 1, stride=1, padding=1)\n        self.fc = torch.nn.Linear(4, 16)\n \n    def forward(self, x1, x2):\n        v1 = self.conv(x1)\n        v2 =  self.fc(x2)\n\n        v3 = v1 + v2\n\n        return v3\n\n# Initializing the model\nm = Model()\nm.eval()\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 480, 640)\nx2 = torch.randn(1, 4, 160, 1920)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n \n    def forward(self, x1, x2):\n        v1 = self.conv(x1)\n        v2 = v1 + x2\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\nx2 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, input1):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n        self.input1 = input1\n \n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 + self.input1\n        return v2\n\n# Initializing the model\ninput1 = torch.randn(1, 3, 64, 64)\nm = Model(input1)\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n \n    def forward(self, x2, x3):\n        v1 = self.conv(x2)\n        v2 = v1 + x3\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx2 = torch.randn(1, 3, 64, 64)\nx3 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, other):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n        self.other = other\n \n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 + self.other\n        return v2\n\nother_tensor = torch.randn(1, 8, 64, 64, requires_grad=True)\nm = Model(other=other_tensor)\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, other):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n        self.other = other\n \n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 + self.other\n        return v2\n\n# Initializing the model\nm = Model(torch.randn(1, 8, 64, 64))\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n \n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 + other\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n \n    def forward(self, x1, o1):\n        v1 = self.conv(x1)\n        v2 = v1 + o1\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\no1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n \n    def forward(self, x1, x2, alpha):\n        v1 = self.conv(x1)\n        v2 = v1 + x2\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\nx2 = torch.randn(1, 3, 64, 64)\nalpha = 0.3\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n \n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 + x1\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 4, 1, stride=1, padding=1)\n        self.fc = torch.nn.Linear(4, 16)\n \n    def forward(self, x1, x2):\n        v1 = self.conv(x1)\n        v2 =  self.fc(x2)\n\n        v3 = v1 + v2\n\n        return v3\n\n# Initializing the model\nm = Model()\nm.eval()\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 480, 640)\nx2 = torch.randn(1, 4, 160, 1920)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n \n    def forward(self, x1, x2):\n        v1 = self.conv(x1)\n        v2 = v1 + x2\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\nx2 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, input1):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n        self.input1 = input1\n \n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 + self.input1\n        return v2\n\n# Initializing the model\ninput1 = torch.randn(1, 3, 64, 64)\nm = Model(input1)\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n \n    def forward(self, x2, x3):\n        v1 = self.conv(x2)\n        v2 = v1 + x3\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx2 = torch.randn(1, 3, 64, 64)\nx3 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, other):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n        self.other = other\n \n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 + self.other\n        return v2\n\nother_tensor = torch.randn(1, 8, 64, 64, requires_grad=True)\nm = Model(other=other_tensor)\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, other):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n        self.other = other\n \n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 + self.other\n        return v2\n\n# Initializing the model\nm = Model(torch.randn(1, 8, 64, 64))\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n \n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 + other\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n \n    def forward(self, x1, o1):\n        v1 = self.conv(x1)\n        v2 = v1 + o1\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\no1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n \n    def forward(self, x1, x2, alpha):\n        v1 = self.conv(x1)\n        v2 = v1 + x2\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\nx2 = torch.randn(1, 3, 64, 64)\nalpha = 0.3\n"
            ],
            "g_time": 6.648193120956421
        }
    }
}
{
    "opt": "binary_unary=8",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(1, 8, (3,3))\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = self.conv1(x1)\n        v3 = self.conv1(x1)\n        v4 = v1 + v2 + v3\n        v5 = torch.relu(v4)\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 1, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1):\n        v1 = x1.permute(0, 2, 3, 1)\n        v2 = x1.permute(0, 2, 3, 1)\n        v3 = x1.permute(0, 2, 3, 1)\n        v4 = v1 + v2 + v3\n        v5 = torch.relu(v4)\n        v6 = v5.permute(0, 3, 1, 2)\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = self.conv1(x1)\n        v3 = self.conv1(x1)\n        v4 = v1 + v2 + v3\n        v5 = torch.relu(v4)\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(1, 8, 1, stride=1, padding=1)\n        self.conv2 = torch.nn.Conv2d(1, 8, 1, stride=1, padding=1)\n        self.conv3 = torch.nn.Conv2d(1, 8, 1, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = self.conv2(x1)\n        v3 = self.conv3(x1)\n        v4 = torch.relu(v1 + v2 + v3)\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 1, 16, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(1, 8, 1, stride=1, padding=1)\n        self.conv2 = torch.nn.Conv2d(8, 8, 3, stride=1, padding=1)\n    def forward(self, x1):\n        x2 = self.conv1(x1)\n        t1 = self.conv2(x2)\n        x4 = self.conv1(x1)\n        t2 = self.conv2(x4)\n        x6 = self.conv1(x1)\n        t3 = self.conv2(x6)\n        x7 = self.conv1(x1)\n        t4 = self.conv2(x7)\n        x9 = self.conv1(x1)\n        t5 = self.conv2(x9)\n        x8 = self.conv1(x1)\n        t6 = self.conv2(x8)\n        x10 = self.conv1(x1)\n        t7 = self.conv2(x10)\n        t1001 = torch.relu(t1 + t2 + t3 + t4 + t5 + t6 + t7)\n        t1002 = self.conv1(x1)\n        t1003 = torch.relu(t1001 + t1002)\n        t1004 = torch.relu(t1003 + t1002)\n        return t1004\n# Inputs to the model\nx1 = torch.randn(1, 1, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.v1 = torch.randn(16, 2, 3, 3)\n    def forward(self, x1):\n        v2 = self.v1\n        v3 = torch.nn.functional.conv2d(x1, v2, padding=1, stride=1)\n        v4 = torch.relu(v3)\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 1, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(1, 8, 1, stride=1, padding=1)\n        self.conv2 = torch.nn.Conv2d(1, 8, 3, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = self.conv2(x1)\n        v3 = self.conv1(x1)\n        v4 = v1 + v2 + v3\n        v5 = torch.relu(v4)\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 1, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(1, 8, 1, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = self.conv1(x1)\n        v3 = self.conv1(x1)\n        v4 = v1 + v2 + v3\n        t1 = torch.relu(v4)\n        v5 = self.conv1(t1)\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 1, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 8, 3, stride=2, padding=0)\n        self.conv_batchn = torch.nn.BatchNorm2d(8, affine=False, momentum=None, eps=1)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = self.conv_batchn(v1)\n        v3 = torch.relu(v2)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(1, 8, 3, stride=2, padding=1)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = self.conv(x1)\n        v3 = torch.relu(v1 + v2)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 1, 64, 64)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(1, 8, (3,3))\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = self.conv1(x1)\n        v3 = self.conv1(x1)\n        v4 = v1 + v2 + v3\n        v5 = torch.relu(v4)\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 1, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1):\n        v1 = x1.permute(0, 2, 3, 1)\n        v2 = x1.permute(0, 2, 3, 1)\n        v3 = x1.permute(0, 2, 3, 1)\n        v4 = v1 + v2 + v3\n        v5 = torch.relu(v4)\n        v6 = v5.permute(0, 3, 1, 2)\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = self.conv1(x1)\n        v3 = self.conv1(x1)\n        v4 = v1 + v2 + v3\n        v5 = torch.relu(v4)\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(1, 8, 1, stride=1, padding=1)\n        self.conv2 = torch.nn.Conv2d(1, 8, 1, stride=1, padding=1)\n        self.conv3 = torch.nn.Conv2d(1, 8, 1, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = self.conv2(x1)\n        v3 = self.conv3(x1)\n        v4 = torch.relu(v1 + v2 + v3)\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 1, 16, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(1, 8, 1, stride=1, padding=1)\n        self.conv2 = torch.nn.Conv2d(8, 8, 3, stride=1, padding=1)\n    def forward(self, x1):\n        x2 = self.conv1(x1)\n        t1 = self.conv2(x2)\n        x4 = self.conv1(x1)\n        t2 = self.conv2(x4)\n        x6 = self.conv1(x1)\n        t3 = self.conv2(x6)\n        x7 = self.conv1(x1)\n        t4 = self.conv2(x7)\n        x9 = self.conv1(x1)\n        t5 = self.conv2(x9)\n        x8 = self.conv1(x1)\n        t6 = self.conv2(x8)\n        x10 = self.conv1(x1)\n        t7 = self.conv2(x10)\n        t1001 = torch.relu(t1 + t2 + t3 + t4 + t5 + t6 + t7)\n        t1002 = self.conv1(x1)\n        t1003 = torch.relu(t1001 + t1002)\n        t1004 = torch.relu(t1003 + t1002)\n        return t1004\n# Inputs to the model\nx1 = torch.randn(1, 1, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.v1 = torch.randn(16, 2, 3, 3)\n    def forward(self, x1):\n        v2 = self.v1\n        v3 = torch.nn.functional.conv2d(x1, v2, padding=1, stride=1)\n        v4 = torch.relu(v3)\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 1, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(1, 8, 1, stride=1, padding=1)\n        self.conv2 = torch.nn.Conv2d(1, 8, 3, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = self.conv2(x1)\n        v3 = self.conv1(x1)\n        v4 = v1 + v2 + v3\n        v5 = torch.relu(v4)\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 1, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(1, 8, 1, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = self.conv1(x1)\n        v3 = self.conv1(x1)\n        v4 = v1 + v2 + v3\n        t1 = torch.relu(v4)\n        v5 = self.conv1(t1)\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 1, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 8, 3, stride=2, padding=0)\n        self.conv_batchn = torch.nn.BatchNorm2d(8, affine=False, momentum=None, eps=1)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = self.conv_batchn(v1)\n        v3 = torch.relu(v2)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(1, 8, 3, stride=2, padding=1)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = self.conv(x1)\n        v3 = torch.relu(v1 + v2)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 1, 64, 64)\n"
            ],
            "g_time": 12.106167554855347
        }
    }
}
{
    "opt": "sfdp=0",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.key = torch.nn.Parameter(torch.randn(12, 22, 20, 15))\n    def forward(self, x1):\n        q = x1\n        k = x1\n        v = x1\n        inv_scale = math.sqrt(k.size(1))\n        scaled_dot_product = torch.matmul(q, k.transpose(-2, -1)) / inv_scale\n        attention_weights = scaled_dot_product.softmax(dim=-1)\n        output = attention_weights.matmul(v)\n        return output\n# Inputs to the model\nx1 = torch.randn(12, 29, 8, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.key = torch.nn.Parameter(torch.randn(19, 100, 47, 32))\n    def forward(self, x1):\n        q = x1\n        k = x1\n        v = x1\n        inv_scale = math.sqrt(k.size(1))\n        scaled_dot_product = torch.matmul(q, k.transpose(-2, -1)) / inv_scale\n        attention_weights = scaled_dot_product.softmax(dim=-1)\n        output = attention_weights.matmul(v)\n        return output\n# Inputs to the model\nx1 = torch.randn(1, 12, 14, 23)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.key = torch.nn.Parameter(torch.randn(1, 7, 3, 26))\n    def forward(self, x1):\n        q = x1\n        k = x1\n        v = x1\n        inv_scale = math.sqrt(k.size(1))\n        scaled_dot_product = torch.matmul(q, k.transpose(-2, -1)) / inv_scale\n        attention_weights = scaled_dot_product.softmax(dim=-1)\n        output = attention_weights.matmul(v)\n        return output\n# Inputs to the model\nx1 = torch.randn(3, 1, 7, 17)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.key = torch.nn.Parameter(torch.randn(30, 1, 6, 6))\n    def forward(self, x1):\n        q = x1\n        k = x1\n        v = x1\n        inv_scale = math.sqrt(k.size(1))\n        scaled_dot_product = torch.matmul(q, k.transpose(-2, -1)) / inv_scale\n        attention_weights = scaled_dot_product.softmax(dim=-1)\n        output = attention_weights.matmul(v)\n        return output\n# Inputs to the model\nx1 = torch.randn(4, 30, 23, 30)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.key = torch.nn.Parameter(torch.randn(1, 64, 49465, 28686))\n    def forward(self, x1):\n        q = x1\n        k = x1\n        v = x1\n        inv_scale = math.sqrt(k.size(1))\n        scaled_dot_product = torch.matmul(q, k.transpose(-2, -1)) / inv_scale\n        attention_weights = scaled_dot_product.softmax(dim=-1)\n        output = attention_weights.matmul(v)\n        return output\n# Inputs to the model\nx1 = torch.randn(1, 64, 17, 17)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.key = torch.nn.Parameter(torch.randn(19, 2, 4, 8))\n    def forward(self, x1):\n        q = x1\n        k = x1\n        v = x1\n        inv_scale = math.sqrt(k.size(1))\n        scaled_dot_product = torch.matmul(q, k.transpose(-2, -1)) / inv_scale\n        attention_weights = scaled_dot_product.softmax(dim=-1)\n        output = attention_weights.matmul(v)\n        return output\n# Inputs to the model\nx1 = torch.randn(1, 7, 23, 27)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.key = torch.nn.Parameter(torch.randn(3, 1, 14, 78))\n    def forward(self, x1):\n        q = x1\n        k = x1\n        v = x1\n        inv_scale = math.sqrt(k.size(1))\n        scaled_dot_product = torch.matmul(q, k.transpose(-2, -1)) / inv_scale\n        attention_weights = scaled_dot_product.softmax(dim=-1)\n        output = attention_weights.matmul(v)\n        return output\n# Inputs to the model\nx1 = torch.randn(68, 1, 1, 12)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.key = torch.nn.Parameter(torch.randn(1, 400, 1, 128))\n    def forward(self, x1):\n        q = x1\n        k = x1\n        v = x1\n        inv_scale = math.sqrt(k.size(1))\n        scaled_dot_product = torch.matmul(q, k.transpose(-2, -1)) / inv_scale\n        attention_weights = scaled_dot_product.softmax(dim=-1)\n        output = attention_weights.matmul(v)\n        return output\n# Inputs to the model\nx1 = torch.randn(1, 400, 500, 811)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.key = torch.nn.Parameter(torch.randn(14, 30, 2))\n    def forward(self, x1):\n        q = x1\n        k = x1\n        v = x1\n        inv_scale = math.sqrt(k.size(1))\n        scaled_dot_product = torch.matmul(q, k.transpose(-2, -1)) / inv_scale\n        attention_weights = scaled_dot_product.softmax(dim=-1)\n        output = attention_weights.matmul(v)\n        return output\n# Inputs to the model\nx1 = torch.randn(1, 2, 30, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.key = torch.nn.Parameter(torch.randn(227, 7, 13))\n    def forward(self, x1):\n        q = x1\n        k = x1\n        v = x1\n        inv_scale = math.sqrt(k.size(1))\n        scaled_dot_product = torch.matmul(q, k.transpose(-2, -1)) / inv_scale\n        attention_weights = scaled_dot_product.softmax(dim=-1)\n        output = attention_weights.matmul(v)\n        return output\n# Inputs to the model\nx1 = torch.randn(6, 227, 3)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.key = torch.nn.Parameter(torch.randn(12, 22, 20, 15))\n    def forward(self, x1):\n        q = x1\n        k = x1\n        v = x1\n        inv_scale = math.sqrt(k.size(1))\n        scaled_dot_product = torch.matmul(q, k.transpose(-2, -1)) / inv_scale\n        attention_weights = scaled_dot_product.softmax(dim=-1)\n        output = attention_weights.matmul(v)\n        return output\n# Inputs to the model\nx1 = torch.randn(12, 29, 8, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.key = torch.nn.Parameter(torch.randn(19, 100, 47, 32))\n    def forward(self, x1):\n        q = x1\n        k = x1\n        v = x1\n        inv_scale = math.sqrt(k.size(1))\n        scaled_dot_product = torch.matmul(q, k.transpose(-2, -1)) / inv_scale\n        attention_weights = scaled_dot_product.softmax(dim=-1)\n        output = attention_weights.matmul(v)\n        return output\n# Inputs to the model\nx1 = torch.randn(1, 12, 14, 23)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.key = torch.nn.Parameter(torch.randn(1, 7, 3, 26))\n    def forward(self, x1):\n        q = x1\n        k = x1\n        v = x1\n        inv_scale = math.sqrt(k.size(1))\n        scaled_dot_product = torch.matmul(q, k.transpose(-2, -1)) / inv_scale\n        attention_weights = scaled_dot_product.softmax(dim=-1)\n        output = attention_weights.matmul(v)\n        return output\n# Inputs to the model\nx1 = torch.randn(3, 1, 7, 17)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.key = torch.nn.Parameter(torch.randn(30, 1, 6, 6))\n    def forward(self, x1):\n        q = x1\n        k = x1\n        v = x1\n        inv_scale = math.sqrt(k.size(1))\n        scaled_dot_product = torch.matmul(q, k.transpose(-2, -1)) / inv_scale\n        attention_weights = scaled_dot_product.softmax(dim=-1)\n        output = attention_weights.matmul(v)\n        return output\n# Inputs to the model\nx1 = torch.randn(4, 30, 23, 30)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.key = torch.nn.Parameter(torch.randn(1, 64, 49465, 28686))\n    def forward(self, x1):\n        q = x1\n        k = x1\n        v = x1\n        inv_scale = math.sqrt(k.size(1))\n        scaled_dot_product = torch.matmul(q, k.transpose(-2, -1)) / inv_scale\n        attention_weights = scaled_dot_product.softmax(dim=-1)\n        output = attention_weights.matmul(v)\n        return output\n# Inputs to the model\nx1 = torch.randn(1, 64, 17, 17)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.key = torch.nn.Parameter(torch.randn(19, 2, 4, 8))\n    def forward(self, x1):\n        q = x1\n        k = x1\n        v = x1\n        inv_scale = math.sqrt(k.size(1))\n        scaled_dot_product = torch.matmul(q, k.transpose(-2, -1)) / inv_scale\n        attention_weights = scaled_dot_product.softmax(dim=-1)\n        output = attention_weights.matmul(v)\n        return output\n# Inputs to the model\nx1 = torch.randn(1, 7, 23, 27)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.key = torch.nn.Parameter(torch.randn(3, 1, 14, 78))\n    def forward(self, x1):\n        q = x1\n        k = x1\n        v = x1\n        inv_scale = math.sqrt(k.size(1))\n        scaled_dot_product = torch.matmul(q, k.transpose(-2, -1)) / inv_scale\n        attention_weights = scaled_dot_product.softmax(dim=-1)\n        output = attention_weights.matmul(v)\n        return output\n# Inputs to the model\nx1 = torch.randn(68, 1, 1, 12)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.key = torch.nn.Parameter(torch.randn(1, 400, 1, 128))\n    def forward(self, x1):\n        q = x1\n        k = x1\n        v = x1\n        inv_scale = math.sqrt(k.size(1))\n        scaled_dot_product = torch.matmul(q, k.transpose(-2, -1)) / inv_scale\n        attention_weights = scaled_dot_product.softmax(dim=-1)\n        output = attention_weights.matmul(v)\n        return output\n# Inputs to the model\nx1 = torch.randn(1, 400, 500, 811)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.key = torch.nn.Parameter(torch.randn(14, 30, 2))\n    def forward(self, x1):\n        q = x1\n        k = x1\n        v = x1\n        inv_scale = math.sqrt(k.size(1))\n        scaled_dot_product = torch.matmul(q, k.transpose(-2, -1)) / inv_scale\n        attention_weights = scaled_dot_product.softmax(dim=-1)\n        output = attention_weights.matmul(v)\n        return output\n# Inputs to the model\nx1 = torch.randn(1, 2, 30, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.key = torch.nn.Parameter(torch.randn(227, 7, 13))\n    def forward(self, x1):\n        q = x1\n        k = x1\n        v = x1\n        inv_scale = math.sqrt(k.size(1))\n        scaled_dot_product = torch.matmul(q, k.transpose(-2, -1)) / inv_scale\n        attention_weights = scaled_dot_product.softmax(dim=-1)\n        output = attention_weights.matmul(v)\n        return output\n# Inputs to the model\nx1 = torch.randn(6, 227, 3)\n"
            ],
            "g_time": 6.774328708648682
        }
    }
}
{
    "opt": "sfdp=4",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, q, k2, v, mask):\n        qk = q @ k2.transpose(-2, -1) / math.sqrt(q.size(-1))\n        qk = qk + mask\n        attn_weight = torch.softmax(qk, dim=-1)\n        output = attn_weight @ v\n        return output\n# Inputs to the model\nQ = torch.randn(1, 64, 56, 56)\nK = torch.randn(1, 64, 56, 56)\nV = torch.randn(1, 64, 56, 56)\nmask = (torch.rand(1, 56, 56) > 0.7).fill_(-1000000000.0)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, q2, k, v2, mask):\n        qk = q2 @ k.transpose(-2, -1) / math.sqrt(q2.size(-1))\n        qk = qk + mask\n        attn_weight = torch.softmax(qk, dim=-1)\n        output = attn_weight @ v2\n        return output\n# Inputs to the model\nQ = torch.randn(1, 64, 56, 56)\nK = torch.randn(1, 64, 56, 56)\nV = torch.randn(1, 64, 56, 56)\nmask = (torch.rand(1, 56, 56) > 0.7).fill_(-1000000000.0)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, Q3, k, v, mask):\n        qk = Q3 @ k.transpose(-2, -1) / math.sqrt(Q3.size(-1))\n        qk = qk + mask\n        attn_weight = torch.softmax(qk, dim=-1)\n        output = attn_weight @ v\n        return output\n# Inputs to the model\nQ = torch.randn(1, 64, 56, 56)\nK = torch.randn(1, 64, 56, 56)\nV = torch.randn(1, 64, 56, 56)\nmask = (torch.rand(1, 56, 56) > 0.7).fill_(-1000000000.0)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, q, k, v4, mask):\n        qk = q @ k.transpose(-2, -1) / math.sqrt(q.size(-1))\n        qk = qk + mask\n        attn_weight = torch.softmax(qk, dim=-1)\n        output = attn_weight @ v4\n        return output\n# Inputs to the model\nQ = torch.randn(1, 64, 56, 56)\nK = torch.randn(1, 64, 56, 56)\nV = torch.randn(1, 64, 56, 56)\nmask = (torch.rand(1, 56, 56) > 0.7).fill_(-1000000000.0)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, q3, k, v4, mask):\n        qk = q3 @ k.transpose(-2, -1) / math.sqrt(q3.size(-1))\n        qk = qk + mask\n        attn_weight = torch.softmax(qk, dim=-1)\n        output = attn_weight @ v4\n        return output\n# Inputs to the model\nQ = torch.randn(1, 64, 56, 56)\nK = torch.randn(1, 64, 56, 56)\nV = torch.randn(1, 64, 56, 56)\nmask = (torch.rand(1, 56, 56) > 0.7).fill_(-1000000000.0)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, q, k, v4, mask):\n        qk = q @ k.transpose(-2, -1) / math.sqrt(q.size(-1))\n        qk = qk + mask\n        attn_weight = torch.softmax(qk, dim=-1)\n        output = attn_weight @ v4\n        return output\n# Inputs to the model\nQ = torch.randn(1, 64, 56, 56)\nK = torch.randn(1, 64, 56, 56)\nV = torch.randn(1, 64, 56, 56)\nmask = (torch.rand(1, 56, 56) > 0.7).fill_(-1000000000.0)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, q, k, v6, mask):\n        qk = q @ k.transpose(-2, -1) / math.sqrt(q.size(-1))\n        qk = qk + mask\n        attn_weight = torch.softmax(qk, dim=-1)\n        output = attn_weight @ v6\n        return output\n# Inputs to the model\nQ = torch.randn(1, 64, 56, 56)\nK = torch.randn(1, 64, 56, 56)\nV = torch.randn(1, 64, 56, 56)\nmask = (torch.rand(1, 56, 56) > 0.7).fill_(-1000000000.0)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, q, k, v4, mask):\n        qk = q @ k.transpose(-2, -1) / math.sqrt(q.size(-1))\n        qk = qk + mask\n        attn_weight = torch.softmax(qk, dim=-1)\n        output = attn_weight @ v4\n        return output\n# Inputs to the model\nQ = torch.randn(1, 64, 56, 56)\nK = torch.randn(1, 64, 56, 56)\nV = torch.randn(1, 64, 56, 56)\nmask = (torch.rand(1, 56, 56) > 0.7).fill_(-1000000000.0)\n",
                "\nclass ModelWithCustomMethod(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layer1 = torch.nn.Linear(64, 56)\n    def my_custom_linear_op(self, x):\n        y = self.layer1(x)\n        return y + 10\n    def forward(self, q, k, v4, mask):\n        q = self.my_custom_linear_op(q)\n        k = self.my_custom_linear_op(k)\n        v = self.my_custom_linear_op(v4)\n        qk = q @ k.transpose(-2, -1) / math.sqrt(q.size(-1))\n        qk = qk + mask\n        attn_weight = torch.softmax(qk, dim=-1)\n        output = attn_weight @ v4\n        return output\n# Inputs to the model\nQ = torch.randn(1, 64)\nK = torch.randn(1, 64)\nV = torch.randn(1, 64)\nmask = (torch.rand(1, 56, 56) > 0.7).fill_(-1000000000.0)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, q3, k, v3, mask):\n        qk = q3 @ k.transpose(-2, -1) / math.sqrt(q3.size(-1))\n        qk = qk + mask\n        attn_weight = torch.softmax(qk, dim=-1)\n        output = attn_weight @ v3\n        return output\n# Inputs to the model\nQ = torch.randn(1, 64, 56, 56)\nK = torch.randn(1, 64, 56, 56)\nV = torch.randn(1, 64, 56, 56)\nmask = (torch.rand(1, 56, 56) > 0.7).fill_(-1000000000.0)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, q, k2, v, mask):\n        qk = q @ k2.transpose(-2, -1) / math.sqrt(q.size(-1))\n        qk = qk + mask\n        attn_weight = torch.softmax(qk, dim=-1)\n        output = attn_weight @ v\n        return output\n# Inputs to the model\nQ = torch.randn(1, 64, 56, 56)\nK = torch.randn(1, 64, 56, 56)\nV = torch.randn(1, 64, 56, 56)\nmask = (torch.rand(1, 56, 56) > 0.7).fill_(-1000000000.0)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, q2, k, v2, mask):\n        qk = q2 @ k.transpose(-2, -1) / math.sqrt(q2.size(-1))\n        qk = qk + mask\n        attn_weight = torch.softmax(qk, dim=-1)\n        output = attn_weight @ v2\n        return output\n# Inputs to the model\nQ = torch.randn(1, 64, 56, 56)\nK = torch.randn(1, 64, 56, 56)\nV = torch.randn(1, 64, 56, 56)\nmask = (torch.rand(1, 56, 56) > 0.7).fill_(-1000000000.0)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, Q3, k, v, mask):\n        qk = Q3 @ k.transpose(-2, -1) / math.sqrt(Q3.size(-1))\n        qk = qk + mask\n        attn_weight = torch.softmax(qk, dim=-1)\n        output = attn_weight @ v\n        return output\n# Inputs to the model\nQ = torch.randn(1, 64, 56, 56)\nK = torch.randn(1, 64, 56, 56)\nV = torch.randn(1, 64, 56, 56)\nmask = (torch.rand(1, 56, 56) > 0.7).fill_(-1000000000.0)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, q, k, v4, mask):\n        qk = q @ k.transpose(-2, -1) / math.sqrt(q.size(-1))\n        qk = qk + mask\n        attn_weight = torch.softmax(qk, dim=-1)\n        output = attn_weight @ v4\n        return output\n# Inputs to the model\nQ = torch.randn(1, 64, 56, 56)\nK = torch.randn(1, 64, 56, 56)\nV = torch.randn(1, 64, 56, 56)\nmask = (torch.rand(1, 56, 56) > 0.7).fill_(-1000000000.0)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, q3, k, v4, mask):\n        qk = q3 @ k.transpose(-2, -1) / math.sqrt(q3.size(-1))\n        qk = qk + mask\n        attn_weight = torch.softmax(qk, dim=-1)\n        output = attn_weight @ v4\n        return output\n# Inputs to the model\nQ = torch.randn(1, 64, 56, 56)\nK = torch.randn(1, 64, 56, 56)\nV = torch.randn(1, 64, 56, 56)\nmask = (torch.rand(1, 56, 56) > 0.7).fill_(-1000000000.0)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, q, k, v4, mask):\n        qk = q @ k.transpose(-2, -1) / math.sqrt(q.size(-1))\n        qk = qk + mask\n        attn_weight = torch.softmax(qk, dim=-1)\n        output = attn_weight @ v4\n        return output\n# Inputs to the model\nQ = torch.randn(1, 64, 56, 56)\nK = torch.randn(1, 64, 56, 56)\nV = torch.randn(1, 64, 56, 56)\nmask = (torch.rand(1, 56, 56) > 0.7).fill_(-1000000000.0)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, q, k, v6, mask):\n        qk = q @ k.transpose(-2, -1) / math.sqrt(q.size(-1))\n        qk = qk + mask\n        attn_weight = torch.softmax(qk, dim=-1)\n        output = attn_weight @ v6\n        return output\n# Inputs to the model\nQ = torch.randn(1, 64, 56, 56)\nK = torch.randn(1, 64, 56, 56)\nV = torch.randn(1, 64, 56, 56)\nmask = (torch.rand(1, 56, 56) > 0.7).fill_(-1000000000.0)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, q, k, v4, mask):\n        qk = q @ k.transpose(-2, -1) / math.sqrt(q.size(-1))\n        qk = qk + mask\n        attn_weight = torch.softmax(qk, dim=-1)\n        output = attn_weight @ v4\n        return output\n# Inputs to the model\nQ = torch.randn(1, 64, 56, 56)\nK = torch.randn(1, 64, 56, 56)\nV = torch.randn(1, 64, 56, 56)\nmask = (torch.rand(1, 56, 56) > 0.7).fill_(-1000000000.0)\n",
                "\nclass ModelWithCustomMethod(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layer1 = torch.nn.Linear(64, 56)\n    def my_custom_linear_op(self, x):\n        y = self.layer1(x)\n        return y + 10\n    def forward(self, q, k, v4, mask):\n        q = self.my_custom_linear_op(q)\n        k = self.my_custom_linear_op(k)\n        v = self.my_custom_linear_op(v4)\n        qk = q @ k.transpose(-2, -1) / math.sqrt(q.size(-1))\n        qk = qk + mask\n        attn_weight = torch.softmax(qk, dim=-1)\n        output = attn_weight @ v4\n        return output\n# Inputs to the model\nQ = torch.randn(1, 64)\nK = torch.randn(1, 64)\nV = torch.randn(1, 64)\nmask = (torch.rand(1, 56, 56) > 0.7).fill_(-1000000000.0)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, q3, k, v3, mask):\n        qk = q3 @ k.transpose(-2, -1) / math.sqrt(q3.size(-1))\n        qk = qk + mask\n        attn_weight = torch.softmax(qk, dim=-1)\n        output = attn_weight @ v3\n        return output\n# Inputs to the model\nQ = torch.randn(1, 64, 56, 56)\nK = torch.randn(1, 64, 56, 56)\nV = torch.randn(1, 64, 56, 56)\nmask = (torch.rand(1, 56, 56) > 0.7).fill_(-1000000000.0)\n"
            ],
            "g_time": 10.070098161697388
        }
    }
}
{
    "opt": "splitwithsizes_cat_replace",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.features = torch.nn.ModuleList([torch.nn.Conv2d(3, 32, 3, 1, 1), torch.nn.BatchNorm2d(32), torch.nn.Conv2d(32, 32, 3, 1, 1), torch.nn.Upsample(scale_factor=2.0, mode='nearest')])\n    def forward(self, v1):\n        split_tensors = torch.split(v1, [1, 1, 1], dim=1)\n        concatenated_tensor = torch.cat(split_tensors, dim=1)\n        return (concatenated_tensor,)\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.features = torch.nn.ModuleList([torch.nn.Conv2d(3, 32, 3, 1, 1, bias=True), torch.nn.BatchNorm2d(32), torch.nn.Linear(1, 1), torch.nn.ReLU(), torch.nn.Conv2d(32, 32, 3, 1, 1, bias=True)])\n    def forward(self, v1):\n        split_tensors = torch.split(v1, [1, 1, 1], dim=1)\n        concatenated_tensor = torch.cat(split_tensors, dim=1)\n        return (concatenated_tensor, torch.split(v1, [1, 1, 1], dim=1))\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.features = torch.nn.ModuleList([torch.nn.Conv2d(3, 32, 3, 1, 1), torch.nn.Conv2d(32, 32, 3, 1, 1), torch.nn.Conv2d(32, 32, 3, 2, 3), torch.nn.Conv2d(32, 32, 3, 1, 1), torch.nn.Conv2d(32, 64, 3, 2, 0)])\n    def forward(self, x27):\n        x23, x24, x25, x26 = torch.split(x27, [1, 1, 1], dim=0)\n        x37, x38 = torch.split(x33, [1, 1, 1], dim=0)\n        x42, x43, x44 = torch.split(x41, [1, 1, 1], dim=0)\n        x45, x46 = torch.split(x33, [1, 1, 1], dim=0)\n        return x37, x38, x39, x40\n# Inputs to the model\nx1 = torch.randn(3, 1, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.features = torch.nn.ModuleList([torch.nn.Conv2d(3, 32, 3, 1, 1), torch.nn.AvgPool2d(kernel_size=3, stride=2, padding=(1, 3), ), torch.nn.ReLU(), torch.nn.Conv2d(32, 64, 5, 1, 3), torch.nn.AdaptiveMaxPool2d(output_size=(7, 7)), torch.nn.Sigmoid(), torch.nn.Conv2d(64, 12, 1, 1, 0), torch.nn.Conv2d(12, 128, 3, 1, 1), torch.nn.GroupNorm(num_groups=5, num_channels=24, affine=True)])\n    def forward(self, x, y):\n        z1 = self.features[0](x)\n        z1 = self.features[1](z1)\n        z1 = self.features[2](z1)\n        z1 = self.features[3](z1)\n        z1 = self.features[4](z1)\n        z1 = self.features[5](z1)\n        z1 = self.features[6](z1)\n        z1 = self.features[7](z1)\n        z1 = self.features[8](z1)\n        z2 = self.features[9](x)\n        z2 = self.features[10](z2)\n        o1 = y - z2\n        o2 = o1 - z2[:, 3, ::].transpose(0, 1)\n        return o2[:, :, :, :4]\n# Inputs to the model\nx = torch.randn(1, 3, 20, 20)\ny = torch.randn(1, 20, 20, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.features = torch.nn.ModuleList([torch.nn.Conv2d(3, 32, 3, 1, 1), torch.nn.Conv2d(32, 32, 3, 1, 1), torch.nn.Conv2d(32, 32, 3, 2, 3), torch.nn.Conv2d(32, 32, 3, 2, 1), torch.nn.Conv2d(32, 32, 3, 1, 1), torch.nn.Conv2d(32, 32, 3, 1, 1), torch.nn.Conv2d(32, 64, 3, 2, 0)])\n    def forward(self, v1):\n        split_tensors = torch.split(v1, [1, 1, 1], dim=1)\n        concatenated_tensor = torch.cat(split_tensors, dim=1)\n        return (concatenated_tensor, torch.split(v1, [1, 1, 1], dim=1))\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.features = torch.nn.ModuleList([torch.nn.Conv2d(3, 32, 3, 1, 1), torch.nn.Conv2d(32,32,3, 1, 1), torch.nn.Conv2d(32,32,3, 1, 1)])\n    def forward(self, v1):\n        split_tensors1 = torch.split(v1, [1, 1, 1], dim=1)\n        concatenated_tensor1 = torch.cat(split_tensors1, dim=1)\n        split_tensors2 = torch.split(concatenated_tensor1, [1, 1, 1], dim=1)\n        concatenated_tensor2 = torch.cat(split_tensors2, dim=1)\n        return (concatenated_tensor2, split_tensors1, split_tensors2)\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.features = torch.nn.ModuleList([torch.nn.Conv2d(3, 32, 3, 1, 1), torch.nn.Conv2d(32, 32, 3, 1, 1), torch.nn.Conv2d(32, 32, 3, 2, 3), torch.nn.Conv2d(32, 32, 3, 1, 1), torch.nn.Conv2d(32, 64, 3, 1, 1)])\n    def forward(self, v1):\n        split_tensors = torch.split(v1, [1, 1, 1], dim=1)\n        concatenated_tensor = torch.cat(split_tensors, dim=1)\n        return (concatenated_tensor, torch.split(v1, [1, 1, 1], dim=1))\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.features = torch.nn.ModuleList([torch.nn.Conv2d(3, 32, 3, 1, 1), torch.nn.Linear(3, 30), torch.nn.ELU(), torch.nn.Conv2d(32, 32, 3)])\n    def forward(self, v1):\n        split_tensors = torch.split(v1, [1, 1, 1], dim=1)\n        concatenated_tensor = torch.cat(split_tensors, dim=1)\n        return (concatenated_tensor, torch.split(v1, [1, 1, 1], dim=1))\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.features = torch.nn.ModuleList([torch.nn.Conv2d(3, 32, 3, 1, 1), torch.nn.Conv2d(32, 32, 3, 1, 1), torch.nn.Conv2d(32, 32, 3, 1, 3), torch.nn.Conv2d(32, 32, 3, 1, 1), torch.nn.Conv2d(32, 32, 3, 1, 3), torch.nn.Conv2d(32, 32, 3, 1, 1), torch.nn.Conv2d(32, 32, 3, 1, 0)])\n    def forward(self, v1):\n        split_tensors = torch.split(v1, [1, 1, 1], dim=1)\n        concatenated_tensor = torch.cat(split_tensors, dim=1)\n        return (concatenated_tensor, torch.split(v1, [1, 1, 1], dim=1))\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.features = torch.nn.ModuleList([torch.nn.Conv2d(3, 32, 3, 1, 1), torch.nn.Conv2d(32, 32, 3, 1, 1), torch.nn.Conv2d(32, 32, 3, 2, 3)])\n        self.dropout = torch.nn.Dropout2d(0.5)\n    def forward(self, v1):\n        split_tensors = torch.split(v1, [1, 1, 1], dim=1)\n        concatenated_tensor = torch.cat(split_tensors, dim=1)\n        return (concatenated_tensor, torch.split(v1, [1, 1, 1], dim=1))\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.features = torch.nn.ModuleList([torch.nn.Conv2d(3, 32, 3, 1, 1), torch.nn.BatchNorm2d(32), torch.nn.Conv2d(32, 32, 3, 1, 1), torch.nn.Upsample(scale_factor=2.0, mode='nearest')])\n    def forward(self, v1):\n        split_tensors = torch.split(v1, [1, 1, 1], dim=1)\n        concatenated_tensor = torch.cat(split_tensors, dim=1)\n        return (concatenated_tensor,)\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.features = torch.nn.ModuleList([torch.nn.Conv2d(3, 32, 3, 1, 1, bias=True), torch.nn.BatchNorm2d(32), torch.nn.Linear(1, 1), torch.nn.ReLU(), torch.nn.Conv2d(32, 32, 3, 1, 1, bias=True)])\n    def forward(self, v1):\n        split_tensors = torch.split(v1, [1, 1, 1], dim=1)\n        concatenated_tensor = torch.cat(split_tensors, dim=1)\n        return (concatenated_tensor, torch.split(v1, [1, 1, 1], dim=1))\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.features = torch.nn.ModuleList([torch.nn.Conv2d(3, 32, 3, 1, 1), torch.nn.Conv2d(32, 32, 3, 1, 1), torch.nn.Conv2d(32, 32, 3, 2, 3), torch.nn.Conv2d(32, 32, 3, 1, 1), torch.nn.Conv2d(32, 64, 3, 2, 0)])\n    def forward(self, x27):\n        x23, x24, x25, x26 = torch.split(x27, [1, 1, 1], dim=0)\n        x37, x38 = torch.split(x33, [1, 1, 1], dim=0)\n        x42, x43, x44 = torch.split(x41, [1, 1, 1], dim=0)\n        x45, x46 = torch.split(x33, [1, 1, 1], dim=0)\n        return x37, x38, x39, x40\n# Inputs to the model\nx1 = torch.randn(3, 1, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.features = torch.nn.ModuleList([torch.nn.Conv2d(3, 32, 3, 1, 1), torch.nn.AvgPool2d(kernel_size=3, stride=2, padding=(1, 3), ), torch.nn.ReLU(), torch.nn.Conv2d(32, 64, 5, 1, 3), torch.nn.AdaptiveMaxPool2d(output_size=(7, 7)), torch.nn.Sigmoid(), torch.nn.Conv2d(64, 12, 1, 1, 0), torch.nn.Conv2d(12, 128, 3, 1, 1), torch.nn.GroupNorm(num_groups=5, num_channels=24, affine=True)])\n    def forward(self, x, y):\n        z1 = self.features[0](x)\n        z1 = self.features[1](z1)\n        z1 = self.features[2](z1)\n        z1 = self.features[3](z1)\n        z1 = self.features[4](z1)\n        z1 = self.features[5](z1)\n        z1 = self.features[6](z1)\n        z1 = self.features[7](z1)\n        z1 = self.features[8](z1)\n        z2 = self.features[9](x)\n        z2 = self.features[10](z2)\n        o1 = y - z2\n        o2 = o1 - z2[:, 3, ::].transpose(0, 1)\n        return o2[:, :, :, :4]\n# Inputs to the model\nx = torch.randn(1, 3, 20, 20)\ny = torch.randn(1, 20, 20, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.features = torch.nn.ModuleList([torch.nn.Conv2d(3, 32, 3, 1, 1), torch.nn.Conv2d(32, 32, 3, 1, 1), torch.nn.Conv2d(32, 32, 3, 2, 3), torch.nn.Conv2d(32, 32, 3, 2, 1), torch.nn.Conv2d(32, 32, 3, 1, 1), torch.nn.Conv2d(32, 32, 3, 1, 1), torch.nn.Conv2d(32, 64, 3, 2, 0)])\n    def forward(self, v1):\n        split_tensors = torch.split(v1, [1, 1, 1], dim=1)\n        concatenated_tensor = torch.cat(split_tensors, dim=1)\n        return (concatenated_tensor, torch.split(v1, [1, 1, 1], dim=1))\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.features = torch.nn.ModuleList([torch.nn.Conv2d(3, 32, 3, 1, 1), torch.nn.Conv2d(32,32,3, 1, 1), torch.nn.Conv2d(32,32,3, 1, 1)])\n    def forward(self, v1):\n        split_tensors1 = torch.split(v1, [1, 1, 1], dim=1)\n        concatenated_tensor1 = torch.cat(split_tensors1, dim=1)\n        split_tensors2 = torch.split(concatenated_tensor1, [1, 1, 1], dim=1)\n        concatenated_tensor2 = torch.cat(split_tensors2, dim=1)\n        return (concatenated_tensor2, split_tensors1, split_tensors2)\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.features = torch.nn.ModuleList([torch.nn.Conv2d(3, 32, 3, 1, 1), torch.nn.Conv2d(32, 32, 3, 1, 1), torch.nn.Conv2d(32, 32, 3, 2, 3), torch.nn.Conv2d(32, 32, 3, 1, 1), torch.nn.Conv2d(32, 64, 3, 1, 1)])\n    def forward(self, v1):\n        split_tensors = torch.split(v1, [1, 1, 1], dim=1)\n        concatenated_tensor = torch.cat(split_tensors, dim=1)\n        return (concatenated_tensor, torch.split(v1, [1, 1, 1], dim=1))\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.features = torch.nn.ModuleList([torch.nn.Conv2d(3, 32, 3, 1, 1), torch.nn.Linear(3, 30), torch.nn.ELU(), torch.nn.Conv2d(32, 32, 3)])\n    def forward(self, v1):\n        split_tensors = torch.split(v1, [1, 1, 1], dim=1)\n        concatenated_tensor = torch.cat(split_tensors, dim=1)\n        return (concatenated_tensor, torch.split(v1, [1, 1, 1], dim=1))\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.features = torch.nn.ModuleList([torch.nn.Conv2d(3, 32, 3, 1, 1), torch.nn.Conv2d(32, 32, 3, 1, 1), torch.nn.Conv2d(32, 32, 3, 1, 3), torch.nn.Conv2d(32, 32, 3, 1, 1), torch.nn.Conv2d(32, 32, 3, 1, 3), torch.nn.Conv2d(32, 32, 3, 1, 1), torch.nn.Conv2d(32, 32, 3, 1, 0)])\n    def forward(self, v1):\n        split_tensors = torch.split(v1, [1, 1, 1], dim=1)\n        concatenated_tensor = torch.cat(split_tensors, dim=1)\n        return (concatenated_tensor, torch.split(v1, [1, 1, 1], dim=1))\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.features = torch.nn.ModuleList([torch.nn.Conv2d(3, 32, 3, 1, 1), torch.nn.Conv2d(32, 32, 3, 1, 1), torch.nn.Conv2d(32, 32, 3, 2, 3)])\n        self.dropout = torch.nn.Dropout2d(0.5)\n    def forward(self, v1):\n        split_tensors = torch.split(v1, [1, 1, 1], dim=1)\n        concatenated_tensor = torch.cat(split_tensors, dim=1)\n        return (concatenated_tensor, torch.split(v1, [1, 1, 1], dim=1))\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n"
            ],
            "g_time": 16.11494016647339
        }
    }
}
{
    "opt": "binary_unary=6",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(12, 4)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 - x1.sum() * 0.5\n        v3 = torch.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(2, 12)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 1)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 - other\n        v3 = torch.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3 * 256 * 256, 1)\n \n    def forward(self, x1):\n        v3 = x1.flatten(1)\n        v3 = self.linear(v3)\n        v2 = v3 - 1\n        v1 = torch.relu(v2)\n        return v1\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 256, x2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(4, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 - 2.5\n        v3 = torch.nn.functional.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 4)\n",
                "\nother = torch.randn(2,2)\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(4, 2)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 - other\n        v3 = torch.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(2, 4)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, other):\n        super().__init__()\n        self.linear = torch.nn.Linear(1, 1)\n        self.other = other\n    \n    def forward(self, x1):\n        x2 = self.linear(x1)\n        x3 = x2 - self.other\n        x4 = torch.relu(x3)\n        return x4\n\n# Initializing the model\nm = Model(10.0)\n\n# Inputs to the model\nx1 = torch.randn(32, 1)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(64*64, 1024, bias=False)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 - 0.01\n        v3 = torch.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(30, 10)\n \n    def forward(self, x1, other):\n        v1 = self.linear(x1)\n        v2 = v1 - other\n        v3 = torch.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n# Inputs to the model\nx1 = torch.randn(16, 30)\nother = torch.randn(1)\n",
                "\nclass Model2(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n \n    def forward(self, x2):\n        v1 = self.linear(x2)\n        v2 = v1 - 1\n        v3 = F.relu(v2)\n        return v3\n\n# Initializing the model\nm2 = Model2()\n\n# Inputs to the model\nx2 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(64, 32, bias=True)\n \n    def forward(self, x1, x2):\n        # x1.shape = [x1]\n        d1 = torch.add(x1, x1, alpha=1)\n        d2 = torch.add(x1, x2, alpha=1)\n        d3 = torch.add(x2, x1, alpha=1)\n        # d4.shape = [d4]\n        v1 = self.conv(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\nx2 = torch.randn(1, 3, 64, 64)\nx1, x2, d4 = m(x1, x2)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(12, 4)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 - x1.sum() * 0.5\n        v3 = torch.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(2, 12)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 1)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 - other\n        v3 = torch.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3 * 256 * 256, 1)\n \n    def forward(self, x1):\n        v3 = x1.flatten(1)\n        v3 = self.linear(v3)\n        v2 = v3 - 1\n        v1 = torch.relu(v2)\n        return v1\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 256, x2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(4, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 - 2.5\n        v3 = torch.nn.functional.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 4)\n",
                "\nother = torch.randn(2,2)\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(4, 2)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 - other\n        v3 = torch.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(2, 4)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, other):\n        super().__init__()\n        self.linear = torch.nn.Linear(1, 1)\n        self.other = other\n    \n    def forward(self, x1):\n        x2 = self.linear(x1)\n        x3 = x2 - self.other\n        x4 = torch.relu(x3)\n        return x4\n\n# Initializing the model\nm = Model(10.0)\n\n# Inputs to the model\nx1 = torch.randn(32, 1)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(64*64, 1024, bias=False)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 - 0.01\n        v3 = torch.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(30, 10)\n \n    def forward(self, x1, other):\n        v1 = self.linear(x1)\n        v2 = v1 - other\n        v3 = torch.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n# Inputs to the model\nx1 = torch.randn(16, 30)\nother = torch.randn(1)\n",
                "\nclass Model2(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n \n    def forward(self, x2):\n        v1 = self.linear(x2)\n        v2 = v1 - 1\n        v3 = F.relu(v2)\n        return v3\n\n# Initializing the model\nm2 = Model2()\n\n# Inputs to the model\nx2 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(64, 32, bias=True)\n \n    def forward(self, x1, x2):\n        # x1.shape = [x1]\n        d1 = torch.add(x1, x1, alpha=1)\n        d2 = torch.add(x1, x2, alpha=1)\n        d3 = torch.add(x2, x1, alpha=1)\n        # d4.shape = [d4]\n        v1 = self.conv(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\nx2 = torch.randn(1, 3, 64, 64)\nx1, x2, d4 = m(x1, x2)\n"
            ],
            "g_time": 9.71400237083435
        }
    }
}
{
    "opt": "pointless_cumsum_replacement",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1):\n        b = {}\n        a = {}\n        b['dtype'] = torch.int8\n        b['layout'] = torch.strided\n        b['device'] = torch.device('cuda:0')\n        a['dtype'] = torch.float16\n        a['layout'] = torch.strided\n        a['device'] = torch.device('cuda:0')\n        a['dtype_to'] = torch.float32\n        a['dtype_from'] = torch.float16\n        b['dtype_to'] = torch.float32\n        b['dtype_from'] = torch.float16\n        t1 = torch.full([256, 256], 1, dtype=b['dtype'], layout=b['layout'], device=b['device'], pin_memory=False)\n        t2 = t1.to(dtype=a['dtype'])\n        t3 = torch.cumsum(t2, 1)\n        return t3\n# Inputs to the model\nx1 = torch.randn(256, 256, device='cuda:0')\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1):\n        b = {}\n        a = {}\n        b['dtype'] = torch.int8\n        b['layout'] = torch.strided\n        b['device'] = torch.device('cuda:0')\n        a['dtype'] = torch.int8\n        a['layout'] = torch.strided\n        a['device'] = torch.device('cuda:0')\n        a['dtype_to'] = torch.int8\n        a['dtype_from'] = torch.int8\n        b['dtype_to'] = torch.int8\n        b['dtype_from'] = torch.int8\n        t1 = torch.full([1024, 1024], 1, dtype=b['dtype'], layout=b['layout'], device=b['device'], pin_memory=False)\n        t2 = t1.to(dtype=a['dtype'])\n        t3 = torch.cumsum(t2, 1)\n        return t3\n# Inputs to the model\nx1 = torch.randn(1024, 1024, device='cuda:0')\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1):\n        b = {}\n        a = {}\n        b['dtype'] = torch.uint8\n        b['layout'] = torch.strided\n        b['device'] = torch.device('cuda:0')\n        a['dtype'] = torch.float16\n        a['layout'] = torch.strided\n        a['device'] = torch.device('cuda:0')\n        a['dtype_to'] = torch.float16\n        a['dtype_from'] = torch.uint8\n        b['dtype_to'] = torch.uint8\n        b['dtype_from'] = torch.float16\n        t1 = torch.full([1, 3], 1, dtype=b['dtype'], layout=b['layout'], device=b['device'], pin_memory=False)\n        t2 = t1.to(dtype=a['dtype'])\n        t3 = torch.cumsum(t2, 1)\n        return t3\n# Inputs to the model\nx1 = torch.randn(1, 3, device='cuda:0')\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1):\n        b = {}\n        a = {}\n        b['dtype'] = torch.float32\n        b['layout'] = torch.strided\n        b['device'] = torch.device('cuda:0')\n        a['dtype'] = torch.float32\n        a['layout'] = torch.strided\n        a['device'] = torch.device('cuda:0')\n        a['dtype_to'] = torch.float32\n        a['dtype_from'] = torch.float32\n        b['dtype_to'] = torch.float32\n        b['dtype_from'] = torch.float32\n        t1 = torch.full([1, 1024], 1.13797082e-010, dtype=b['dtype'], layout=b['layout'], device=b['device'], pin_memory=True)\n        t2 = t1.to(dtype=a['dtype'])\n        t3 = torch.cumsum(t2, 1)\n        return t3\n# Inputs to the model\nx1 = torch.randn(1, 1024, device='cuda:0')\n",
                "\n\n\n\n# Inputs to the model\n\n\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1):\n        b = {}\n        a = {}\n        b['dtype'] = torch.bool\n        b['layout'] = torch.strided\n        b['device'] = torch.device('cpu')\n        a['dtype'] = torch.float64\n        a['layout'] = torch.strided\n        a['device'] = torch.device('cpu')\n        a['dtype_to'] = torch.bool\n        a['dtype_from'] = torch.float64\n        b['dtype_to'] = torch.bool\n        b['dtype_from'] = torch.float64\n        t1 = torch.full([2048, 2048], 1, dtype=b['dtype'], layout=b['layout'], device=b['device'], pin_memory=False)\n        t2 = t1.to(dtype=a['dtype'])\n        t3 = torch.cumsum(t2, 1)\n        return t3\n# Inputs to the model\nx1 = torch.randn(2048, 2048, device='cpu')\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1):\n        b = {}\n        a = {}\n        b['dtype'] = torch.int8\n        b['layout'] = torch.strided\n        b['device'] = torch.device('cuda:0')\n        a['dtype'] = torch.float16\n        a['layout'] = torch.strided\n        a['device'] = torch.device('cuda:0')\n        a['dtype_to'] = torch.float16\n        a['dtype_from'] = torch.int8\n        b['dtype_to'] = torch.float16\n        b['dtype_from'] = torch.int8\n        t1 = torch.full([3, 1], 1, dtype=b['dtype'], layout=b['layout'], device=b['device'], pin_memory=False)\n        t2 = t1.to(dtype=a['dtype'])\n        t3 = torch.cumsum(t2, 1)\n        return t3\n# Inputs to the model\nx1 = torch.randn(3, 1, device='cuda:0')\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1):\n        b = {}\n        a = {}\n        b['dtype'] = torch.bool\n        b['layout'] = torch.sparse_coo\n        b['device'] = torch.device('cpu')\n        a['dtype'] = torch.uint8\n        a['layout'] = torch.strided\n        a['device'] = torch.device('cpu')\n        a['dtype_to'] = torch.uint8\n        a['dtype_from'] = torch.bool\n        b['dtype_to'] = torch.bool\n        b['dtype_from'] = torch.uint8\n        t1 = torch.full([128, 128], 1, dtype=b['dtype'], layout=b['layout'], device=b['device'], pin_memory=False)\n        t2 = t1.to(dtype=a['dtype'])\n        t3 = torch.cumsum(t2, 1)\n        return t3\n# Inputs to the model\nx1 = torch.randn(128, 128, device='cpu')\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1):\n        b = {}\n        a = {}\n        b['dtype'] = torch.int8\n        b['layout'] = torch.strided\n        b['device'] = torch.device('cuda:0')\n        a['dtype'] = torch.int16\n        a['layout'] = torch.strided\n        a['device'] = torch.device('cuda:0')\n        a['dtype_to'] = torch.float16\n        a['dtype_from'] = torch.int16\n        b['dtype_to'] = torch.int16\n        b['dtype_from'] = torch.float16\n        t1 = torch.full([128, 1], 1, dtype=b['dtype'], layout=b['layout'], device=b['device'], pin_memory=False)\n        t2 = t1.to(dtype=a['dtype'])\n        t3 = torch.cumsum(t2, 1)\n        return t3\n# Inputs to the model\nx1 = torch.randn(128, 1, device='cuda:0')\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1):\n        b = {}\n        a = {}\n        b['dtype'] = torch.float32\n        b['layout'] = torch.strided\n        b['device'] = torch.device('cpu')\n        a['dtype'] = torch.int64\n        a['layout'] = torch.strided\n        a['device'] = torch.device('cpu')\n        a['dtype_to'] = torch.float32\n        a['dtype_from'] = torch.int64\n        b['dtype_to'] = torch.int16\n        b['dtype_from'] = torch.float32\n        t1 = torch.full([8192, 128], 1, dtype=b['dtype'], layout=b['layout'], device=b['device'], pin_memory=False)\n        t2 = t1.to(dtype=a['dtype'])\n        t3 = t2.to(dtype=torch.float32)\n        t4 = torch.cumsum(t3, 1)\n        return t4\n# Inputs to the model\nx1 = torch.randn(8192, 128, device='cpu')\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1):\n        b = {}\n        a = {}\n        b['dtype'] = torch.int8\n        b['layout'] = torch.strided\n        b['device'] = torch.device('cuda:0')\n        a['dtype'] = torch.float16\n        a['layout'] = torch.strided\n        a['device'] = torch.device('cuda:0')\n        a['dtype_to'] = torch.float32\n        a['dtype_from'] = torch.float16\n        b['dtype_to'] = torch.float32\n        b['dtype_from'] = torch.float16\n        t1 = torch.full([256, 256], 1, dtype=b['dtype'], layout=b['layout'], device=b['device'], pin_memory=False)\n        t2 = t1.to(dtype=a['dtype'])\n        t3 = torch.cumsum(t2, 1)\n        return t3\n# Inputs to the model\nx1 = torch.randn(256, 256, device='cuda:0')\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1):\n        b = {}\n        a = {}\n        b['dtype'] = torch.int8\n        b['layout'] = torch.strided\n        b['device'] = torch.device('cuda:0')\n        a['dtype'] = torch.int8\n        a['layout'] = torch.strided\n        a['device'] = torch.device('cuda:0')\n        a['dtype_to'] = torch.int8\n        a['dtype_from'] = torch.int8\n        b['dtype_to'] = torch.int8\n        b['dtype_from'] = torch.int8\n        t1 = torch.full([1024, 1024], 1, dtype=b['dtype'], layout=b['layout'], device=b['device'], pin_memory=False)\n        t2 = t1.to(dtype=a['dtype'])\n        t3 = torch.cumsum(t2, 1)\n        return t3\n# Inputs to the model\nx1 = torch.randn(1024, 1024, device='cuda:0')\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1):\n        b = {}\n        a = {}\n        b['dtype'] = torch.uint8\n        b['layout'] = torch.strided\n        b['device'] = torch.device('cuda:0')\n        a['dtype'] = torch.float16\n        a['layout'] = torch.strided\n        a['device'] = torch.device('cuda:0')\n        a['dtype_to'] = torch.float16\n        a['dtype_from'] = torch.uint8\n        b['dtype_to'] = torch.uint8\n        b['dtype_from'] = torch.float16\n        t1 = torch.full([1, 3], 1, dtype=b['dtype'], layout=b['layout'], device=b['device'], pin_memory=False)\n        t2 = t1.to(dtype=a['dtype'])\n        t3 = torch.cumsum(t2, 1)\n        return t3\n# Inputs to the model\nx1 = torch.randn(1, 3, device='cuda:0')\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1):\n        b = {}\n        a = {}\n        b['dtype'] = torch.float32\n        b['layout'] = torch.strided\n        b['device'] = torch.device('cuda:0')\n        a['dtype'] = torch.float32\n        a['layout'] = torch.strided\n        a['device'] = torch.device('cuda:0')\n        a['dtype_to'] = torch.float32\n        a['dtype_from'] = torch.float32\n        b['dtype_to'] = torch.float32\n        b['dtype_from'] = torch.float32\n        t1 = torch.full([1, 1024], 1.13797082e-010, dtype=b['dtype'], layout=b['layout'], device=b['device'], pin_memory=True)\n        t2 = t1.to(dtype=a['dtype'])\n        t3 = torch.cumsum(t2, 1)\n        return t3\n# Inputs to the model\nx1 = torch.randn(1, 1024, device='cuda:0')\n",
                "\n\n\n\n# Inputs to the model\n\n\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1):\n        b = {}\n        a = {}\n        b['dtype'] = torch.bool\n        b['layout'] = torch.strided\n        b['device'] = torch.device('cpu')\n        a['dtype'] = torch.float64\n        a['layout'] = torch.strided\n        a['device'] = torch.device('cpu')\n        a['dtype_to'] = torch.bool\n        a['dtype_from'] = torch.float64\n        b['dtype_to'] = torch.bool\n        b['dtype_from'] = torch.float64\n        t1 = torch.full([2048, 2048], 1, dtype=b['dtype'], layout=b['layout'], device=b['device'], pin_memory=False)\n        t2 = t1.to(dtype=a['dtype'])\n        t3 = torch.cumsum(t2, 1)\n        return t3\n# Inputs to the model\nx1 = torch.randn(2048, 2048, device='cpu')\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1):\n        b = {}\n        a = {}\n        b['dtype'] = torch.int8\n        b['layout'] = torch.strided\n        b['device'] = torch.device('cuda:0')\n        a['dtype'] = torch.float16\n        a['layout'] = torch.strided\n        a['device'] = torch.device('cuda:0')\n        a['dtype_to'] = torch.float16\n        a['dtype_from'] = torch.int8\n        b['dtype_to'] = torch.float16\n        b['dtype_from'] = torch.int8\n        t1 = torch.full([3, 1], 1, dtype=b['dtype'], layout=b['layout'], device=b['device'], pin_memory=False)\n        t2 = t1.to(dtype=a['dtype'])\n        t3 = torch.cumsum(t2, 1)\n        return t3\n# Inputs to the model\nx1 = torch.randn(3, 1, device='cuda:0')\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1):\n        b = {}\n        a = {}\n        b['dtype'] = torch.bool\n        b['layout'] = torch.sparse_coo\n        b['device'] = torch.device('cpu')\n        a['dtype'] = torch.uint8\n        a['layout'] = torch.strided\n        a['device'] = torch.device('cpu')\n        a['dtype_to'] = torch.uint8\n        a['dtype_from'] = torch.bool\n        b['dtype_to'] = torch.bool\n        b['dtype_from'] = torch.uint8\n        t1 = torch.full([128, 128], 1, dtype=b['dtype'], layout=b['layout'], device=b['device'], pin_memory=False)\n        t2 = t1.to(dtype=a['dtype'])\n        t3 = torch.cumsum(t2, 1)\n        return t3\n# Inputs to the model\nx1 = torch.randn(128, 128, device='cpu')\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1):\n        b = {}\n        a = {}\n        b['dtype'] = torch.int8\n        b['layout'] = torch.strided\n        b['device'] = torch.device('cuda:0')\n        a['dtype'] = torch.int16\n        a['layout'] = torch.strided\n        a['device'] = torch.device('cuda:0')\n        a['dtype_to'] = torch.float16\n        a['dtype_from'] = torch.int16\n        b['dtype_to'] = torch.int16\n        b['dtype_from'] = torch.float16\n        t1 = torch.full([128, 1], 1, dtype=b['dtype'], layout=b['layout'], device=b['device'], pin_memory=False)\n        t2 = t1.to(dtype=a['dtype'])\n        t3 = torch.cumsum(t2, 1)\n        return t3\n# Inputs to the model\nx1 = torch.randn(128, 1, device='cuda:0')\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1):\n        b = {}\n        a = {}\n        b['dtype'] = torch.float32\n        b['layout'] = torch.strided\n        b['device'] = torch.device('cpu')\n        a['dtype'] = torch.int64\n        a['layout'] = torch.strided\n        a['device'] = torch.device('cpu')\n        a['dtype_to'] = torch.float32\n        a['dtype_from'] = torch.int64\n        b['dtype_to'] = torch.int16\n        b['dtype_from'] = torch.float32\n        t1 = torch.full([8192, 128], 1, dtype=b['dtype'], layout=b['layout'], device=b['device'], pin_memory=False)\n        t2 = t1.to(dtype=a['dtype'])\n        t3 = t2.to(dtype=torch.float32)\n        t4 = torch.cumsum(t3, 1)\n        return t4\n# Inputs to the model\nx1 = torch.randn(8192, 128, device='cpu')\n"
            ],
            "g_time": 10.42045259475708
        }
    }
}
{
    "opt": "unary=22",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    \n        self.linear = torch.nn.Linear(64, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.tanh(v1)\n        return v2",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Conv2d(64, 8, 1, stride=1, padding=1)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.tanh(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 64, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(16, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.tanh(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(768, 768)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.tanh(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 768)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(64, 64)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.tanh(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(8, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.tanh(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(128, 64)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.tanh(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(4, 128)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(8, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.tanh(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(10, 20, 1, stride=1, padding=1)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.tanh(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 10)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(32, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.tanh(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 32)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    \n        self.linear = torch.nn.Linear(64, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.tanh(v1)\n        return v2",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Conv2d(64, 8, 1, stride=1, padding=1)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.tanh(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 64, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(16, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.tanh(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(768, 768)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.tanh(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 768)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(64, 64)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.tanh(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(8, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.tanh(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(128, 64)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.tanh(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(4, 128)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(8, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.tanh(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(10, 20, 1, stride=1, padding=1)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.tanh(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 10)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(32, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.tanh(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 32)\n"
            ],
            "g_time": 4.6325342655181885
        }
    }
}
{
    "opt": "binary=0",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n    def forward(self, x1, other=1):\n        v1 = self.conv(x1)\n        v2 = v1 + other\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n    def forward(self, x1, other=1, padding1=None, padding2=None, padding3=None):\n        v1 = self.conv(x1)\n        v2 = v1 + other\n        return v2\n# Inputs to the model\nx1 = torch.randn(7, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n    def forward(self, x1, other=1, padding1=1):\n        v1 = self.conv(x1)\n        v2 = v1 + other\n        v2 = v2 - padding1\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 3, 1, stride=1, padding=1)\n    def forward(self, x1, other):\n        v1 = self.conv(x1)\n        v2 = v1 + other\n        return v2\n# Inputs to the model\nx1 = tensor([[[[-0.3437,  0.0370, -0.2266],\n               [-0.2343,  0.4359,  0.0107],\n               [-0.1531,  0.2089,  1.3906]],\n\n              [[ 0.4258, -0.0726, -0.2852],\n               [ 0.0086, -0.3048,  0.1763],\n               [ 0.0561, -0.1537,  2.2871]],\n\n              [[ 1.6945,  0.0889,  0.0160],\n               [ 0.6236,  0.5533, -0.6163],\n               [-1.4789, -1.2319, -0.4052]]]])\nother = tensor([[[[-0.5458,  0.0136,  0.0286],\n                 [-0.0652,  0.0015, -0.4068],\n                 [-0.1764,  0.4147,  0.6264]],\n\n                [[ 0.8327, -0.5841,  0.1549],\n                 [-0.1427, -0.4024,  0.2850],\n                 [-0.5063, -0.3230, -0.0020]],\n\n                [[ 0.2381, -0.4404,  0.2855],\n                 [ 0.7881,  1.4808,  0.0637],\n                 [-0.7895, -1.4691, -1.3427]]]])\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1):\n        v1 = x1.permute(0, 2, 3, 1)\n        v3 = v1.contiguous()\n        return v1, v3\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64).float()\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 2, 1, stride=1, padding=1)\n        self.conv2 = torch.nn.Conv2d(2, 4, 1, stride=1, padding=1)\n    def forward(self, x1, f1=1, f2=2):\n        v1 = self.conv1(x1) + f1\n        v2 = self.conv2(v1) + f2\n        return v2\n# Inputs to the model\nx1 = torch.randn(3, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n    def forward(self, x1, other):\n        v1 = self.conv(x1)\n        v2 = v1 + other\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 1, 1, stride=1, padding=1)\n    def forward(self, x):\n        v1 = self.conv(x)\n        return torch.mean(v1, (2,3))\n# Inputs to the model\nx = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n    def forward(self, x1, other=1, padding1=None, padding2=None):\n        v1 = self.conv(x1)\n        if padding1 == None:\n            padding1 = torch.randn(v1.shape)\n        v2 = v1 + other\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(6, 4, 1, stride=1, padding=1)\n    def forward(self, x1, other):\n        v1 = self.conv(x1)\n        v2 = v1 + other\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 6, 3, 3)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n    def forward(self, x1, other=1):\n        v1 = self.conv(x1)\n        v2 = v1 + other\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n    def forward(self, x1, other=1, padding1=None, padding2=None, padding3=None):\n        v1 = self.conv(x1)\n        v2 = v1 + other\n        return v2\n# Inputs to the model\nx1 = torch.randn(7, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n    def forward(self, x1, other=1, padding1=1):\n        v1 = self.conv(x1)\n        v2 = v1 + other\n        v2 = v2 - padding1\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 3, 1, stride=1, padding=1)\n    def forward(self, x1, other):\n        v1 = self.conv(x1)\n        v2 = v1 + other\n        return v2\n# Inputs to the model\nx1 = tensor([[[[-0.3437,  0.0370, -0.2266],\n               [-0.2343,  0.4359,  0.0107],\n               [-0.1531,  0.2089,  1.3906]],\n\n              [[ 0.4258, -0.0726, -0.2852],\n               [ 0.0086, -0.3048,  0.1763],\n               [ 0.0561, -0.1537,  2.2871]],\n\n              [[ 1.6945,  0.0889,  0.0160],\n               [ 0.6236,  0.5533, -0.6163],\n               [-1.4789, -1.2319, -0.4052]]]])\nother = tensor([[[[-0.5458,  0.0136,  0.0286],\n                 [-0.0652,  0.0015, -0.4068],\n                 [-0.1764,  0.4147,  0.6264]],\n\n                [[ 0.8327, -0.5841,  0.1549],\n                 [-0.1427, -0.4024,  0.2850],\n                 [-0.5063, -0.3230, -0.0020]],\n\n                [[ 0.2381, -0.4404,  0.2855],\n                 [ 0.7881,  1.4808,  0.0637],\n                 [-0.7895, -1.4691, -1.3427]]]])\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1):\n        v1 = x1.permute(0, 2, 3, 1)\n        v3 = v1.contiguous()\n        return v1, v3\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64).float()\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 2, 1, stride=1, padding=1)\n        self.conv2 = torch.nn.Conv2d(2, 4, 1, stride=1, padding=1)\n    def forward(self, x1, f1=1, f2=2):\n        v1 = self.conv1(x1) + f1\n        v2 = self.conv2(v1) + f2\n        return v2\n# Inputs to the model\nx1 = torch.randn(3, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n    def forward(self, x1, other):\n        v1 = self.conv(x1)\n        v2 = v1 + other\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 1, 1, stride=1, padding=1)\n    def forward(self, x):\n        v1 = self.conv(x)\n        return torch.mean(v1, (2,3))\n# Inputs to the model\nx = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n    def forward(self, x1, other=1, padding1=None, padding2=None):\n        v1 = self.conv(x1)\n        if padding1 == None:\n            padding1 = torch.randn(v1.shape)\n        v2 = v1 + other\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(6, 4, 1, stride=1, padding=1)\n    def forward(self, x1, other):\n        v1 = self.conv(x1)\n        v2 = v1 + other\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 6, 3, 3)\n"
            ],
            "g_time": 17.07674741744995
        }
    }
}

### Please generate a valid PyTorch model example with public PyTorch APIs meets the specified requirements. Plus, please also generate the input tensor for the newly generated model.

# Description of requirements
The model should contain a pattern of `torch.nn.functional.linear(tensor.permute(...), ...)`. This pattern characterizes scenarios where the torch.nn.functional.linear function is invoked with a tensor method, 'permute',  which rearranges the given tensor's dimensions, as its first argument. 
The permute method is invoked on an input tensor, and it swaps the last two dimensions of this tensor. This modified tensor is then used as the main input for the linear function.

# Model
class Model(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.linear = torch.nn.Linear(2, 2)

    def forward(self, x1):
        v1 = x1.permute(0, 2, 1)
        v2 = torch.nn.functional.linear(v1, self.linear.weight, self.linear.bias)
        return v1

# Initializing the model
m = Model()

# Inputs to the model
x1 = torch.randn(1, 2, 2)
__output__ = m(x1)

### Please generate a valid PyTorch model example with public PyTorch APIs meets the specified requirements. Plus, please also generate the input tensor for the newly generated model. The model should be different from the previous one.

# Description of requirements
The model should contain a pattern of `torch.nn.ConvXd -> torch.nn.BatchNormXd` or `torch.nn.ConvXd -> F.batch_norm`. This pattern characterizes scenarios where a convolution layer (Conv1d, Conv2d, or Conv3d) is followed by a batch normalization layer (BatchNorm1d, BatchNorm2d, or BatchNorm3d) or a functional batch normalization (F.batch_norm).

In the first scenario, the output of the convolution layer should not be used by other nodes. Both the convolution and batch normalization layers should be in evaluation mode (not training), and the batch normalization layer should track running statistics. If these conditions are met, the convolution and batch normalization layers are fused into a single layer, and the batch normalization node is removed from the graph.

In the second scenario, the convolution layer should be in evaluation mode, and the batch normalization function should have exactly eight arguments. The batch normalization's training flag should be False, and its epsilon value should be a float. The arguments related to the batch normalization's running mean, running variance, weight, and bias should be constant attributes. If these conditions are met, the convolution layer's weights and bias are updated to incorporate the batch normalization parameters, and the batch normalization node is removed from the graph.

# Model
{
    "opt": "sfdp=2",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n \n    def forward(self, query, key, value):\n        qk = torch.matmul(query, key.transpose(-2, -1))\n        inv_scale_factor = float(query.shape[-1]) ** -0.5\n        scaled_qk = qk.div(inv_scale_factor)\n        softmax_qk = scaled_qk.softmax(dim=-1)\n        dropout_qk = torch.nn.functional.dropout(softmax_qk, p=0.5)\n        output = dropout_qk.matmul(value)\n        return output\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nquery = torch.randn(32, 8, 10)\nkey = torch.randn(32, 12, 10)\nvalue = torch.randn(32, 12, 20)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        \n    def forward(self, query, key, value, dropout_p=0.6, inv_scale_factor=1/(10**6)):\n        qk = torch.matmul(query, key.transpose(-2, -1))\n        scaled_qk = qk.div(inv_scale_factor)\n        softmax_qk = scaled_qk.softmax(dim=-1, dtype=torch.float32)\n        dropout_qk = torch.nn.functional.dropout(softmax_qk, p=dropout_p, training=True)\n        output = dropout_qk.matmul(value)\n        return output\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, query_tensor, key_tensor):\n        super().__init__()\n        self.query_dim = query_tensor.shape[-2]\n        self.key_dim = key_tensor.shape[-2]\n\n        self.matmul_query_key = torch.matmul(query_tensor, key_tensor.transpose(-2, -1))\n        self.softmax_query_key = torch.nn.functional.softmax(self.matmul_query_key/math.sqrt(self.query_dim))\n        self.dropout_query_key = torch.nn.functional.dropout(self.softmax_query_key)\n\n    def forward(self, value_tensor):\n        return self.dropout_query_key.matmul(value_tensor)\n\n# Initializing the model\nq = torch.randn(1, 3, 64)\nk = torch.randn(2, 3, 64)\nv = torch.randn(2, 3, 64)\nm = Model(q, k)\n\n# Inputs to the model\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n \n    def forward(self, query, key, value, inv_scale_factor, dropout_p):\n        qk = torch.matmul(query, key.transpose(-2, -1))\n        scaled_qk = qk.div(inv_scale_factor)\n        softmax_qk = scaled_qk.softmax(dim=-1)\n        dropout_qk = torch.nn.functional.dropout(softmax_qk, p=dropout_p)\n        output = dropout_qk.matmul(value)\n        return output\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nquery = torch.randn(4, 2, 4)\nkey = torch.randn(4, 2, 64)\nvalue = torch.randn(4, 2, 64)\ninv_scale_factor = 2\ndropout_p = 0.1\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, query, key, value, inv_scale_factor, dropout_p):\n        qk = torch.matmul(query, key.transpose(-2, -1))\n        scaled_qk = qk.div(inv_scale_factor)\n        softmax_qk = scaled_qk.softmax(dim=-1)\n        dropout_qk = torch.nn.functional.dropout(softmax_qk, p=dropout_p)\n        output = dropout_qk.matmul(value)\n        return output\n\n# Initializing the model using random inputs\nm = Model()\nquery = torch.randn(1, 10, 4096)\nkey = torch.randn(1, 10, 4096)\nvalue = torch.randn(1, 10, 4096)\ninv_scale_factor = 0.1\ndropout_p = 0.2\n",
                "\nq = torch.randn(8, 32, 128)\nk = torch.randn(8, 20, 128)\nv = torch.randn(8, 20, 512)\nm = torch.randn(8, 32, 512)\ninv_scale_factor = torch.randn(8, 1, 1)\ndropout_p = 0.5\n\ndef compute_attention(query, key, value, dropout_p=0.5):\n    qk = torch.matmul(query, key.transpose(-2, -1))\n    scaled_qk = qk.div(inv_scale_factor)\n    softmax_qk = scaled_qk.softmax(dim=-1)\n    dropout_qk = torch.nn.functional.dropout(softmax_qk, p=dropout_p)\n    output = dropout_qk.matmul(value)\n    return output\n\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, nb_head, nb_qkvi_dim, nb_v_dim, dropout_p=0.0):\n        super().__init__()\n        self.nb_head = nb_head\n        self.nb_qkvi_dim = nb_qkvi_dim\n        self.nb_v_dim = nb_v_dim\n        self.dropout_p = dropout_p\n        self.qkV_scale = 1 / math.sqrt(nb_qkvi_dim)\n        self.q_proj = torch.nn.Linear(nb_qkvi_dim, nb_qkvi_dim, bias=False)\n        self.k_proj = torch.nn.Linear(nb_qkvi_dim, nb_qkvi_dim, bias=False)\n        self.v_proj = torch.nn.Linear(nb_qkvi_dim, nb_v_dim, bias=False)\n        self.drop = torch.nn.Dropout(dropout_p)\n \n    def forward(self, x2, x3, x4):\n        qk = torch.matmul(self.q_proj(x2), self.k_proj(x3).transpose(-2, -1))\n        scaled_qk = qk.div(self.qkV_scale)\n        softmax_qk = scaled_qk.softmax(dim=-1)\n        dropout_qk = self.drop(softmax_qk)\n        output = dropout_qk.matmul(self.v_proj(x4))\n        return output\n\n# Initializing the model\nm = Model(8, 1024, 1024, 0.0)\n\n# Inputs to the model\nx2 = torch.randn(64, 8, 1024, 1)\nx3 = torch.randn(64, 16, 1024, 1)\nx4 = torch.randn(64, 1024, 1024, 1)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.dropout = torch.nn.Dropout(0.2)\n \n    def forward(self, query, key, value, inv_scale_factor, dropout_p=0.2):\n        qk = torch.matmul(query, key.transpose(-2, -1))\n        scaled_qk = qk.div(inv_scale_factor)\n        softmax_qk = scaled_qk.softmax(dim=-1)\n        dropout_qk = self.dropout(softmax_qk)\n        output = dropout_qk.matmul(value)\n        return output\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nquery = torch.randn(1, 2, 256, 64)\nkey = torch.randn(1, 2, 64, 256)\nvalue = torch.randn(1, 2, 256, 256)\ninv_scale_factor = 1. / torch.sqrt(torch.tensor(query.shape[-1], dtype=torch.float))\nx5 = m(query, key, value, inv_scale_factor)\n\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, query, key, value, scale_factor, dropout_p):\n        super().__init__()\n        self.query = nn.Parameter(query)\n        self.key = nn.Parameter(key)\n        self.value = nn.Parameter(value)\n        self.scale_factor = nn.Parameter(scale_factor)\n        self.dropout_p = dropout_p\n \n    def forward(self):\n        qk = torch.matmul(self.query, self.key.transpose(-2, -1))\n        scaled_qk = qk.div(self.scale_factor)\n        softmax_qk = scaled_qk.softmax(dim=-1)\n        dropout_qk = torch.nn.functional.dropout(softmax_qk, p=self.dropout_p)\n        output = dropout_qk.matmul(self.value)\n        return output\n\n# Initializing the model\nbatch_size = 12\nq_dim = 12\nk_dim = 16\nv_dim = 24\ndim = 4\nN = 10\nquery = torch.randn(batch_size, q_dim, dim, dim).to(\"cuda\")\nkey = torch.randn(batch_size, N, k_dim, dim, dim).to(\"cuda\")\nvalue = torch.randn(batch_size, N, v_dim, dim, dim).to(\"cuda\")\nscale_factor = torch.tensor([1.0 / math.sqrt(dim)]).to(\"cuda\")\ndropout_p = 0.5\nm = Model(query, key, value, scale_factor, dropout_p)\n\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.softmax = torch.nn.Softmax(dim=-1)\n \n    def forward(self, query, key, value, dropout_p, inv_scale_factor):\n        qk = torch.matmul(query, key.transpose(-2, -1))\n        scaled_qk = qk.div(inv_scale_factor)\n        softmax_qk = self.softmax(scaled_qk)\n        dropout_qk = torch.nn.functional.dropout(softmax_qk, p=dropout_p)\n        output = dropout_qk.matmul(value)\n        return output\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nquery = torch.randn(1, 8, 33, 16)\nkey = torch.randn(1, 8, 16, 33)\nvalue = torch.randn(1, 8, 16, 33)\ndropout_p = 0.15\ninv_scale_factor = 10\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n \n    def forward(self, query, key, value):\n        qk = torch.matmul(query, key.transpose(-2, -1))\n        inv_scale_factor = float(query.shape[-1]) ** -0.5\n        scaled_qk = qk.div(inv_scale_factor)\n        softmax_qk = scaled_qk.softmax(dim=-1)\n        dropout_qk = torch.nn.functional.dropout(softmax_qk, p=0.5)\n        output = dropout_qk.matmul(value)\n        return output\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nquery = torch.randn(32, 8, 10)\nkey = torch.randn(32, 12, 10)\nvalue = torch.randn(32, 12, 20)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        \n    def forward(self, query, key, value, dropout_p=0.6, inv_scale_factor=1/(10**6)):\n        qk = torch.matmul(query, key.transpose(-2, -1))\n        scaled_qk = qk.div(inv_scale_factor)\n        softmax_qk = scaled_qk.softmax(dim=-1, dtype=torch.float32)\n        dropout_qk = torch.nn.functional.dropout(softmax_qk, p=dropout_p, training=True)\n        output = dropout_qk.matmul(value)\n        return output\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, query_tensor, key_tensor):\n        super().__init__()\n        self.query_dim = query_tensor.shape[-2]\n        self.key_dim = key_tensor.shape[-2]\n\n        self.matmul_query_key = torch.matmul(query_tensor, key_tensor.transpose(-2, -1))\n        self.softmax_query_key = torch.nn.functional.softmax(self.matmul_query_key/math.sqrt(self.query_dim))\n        self.dropout_query_key = torch.nn.functional.dropout(self.softmax_query_key)\n\n    def forward(self, value_tensor):\n        return self.dropout_query_key.matmul(value_tensor)\n\n# Initializing the model\nq = torch.randn(1, 3, 64)\nk = torch.randn(2, 3, 64)\nv = torch.randn(2, 3, 64)\nm = Model(q, k)\n\n# Inputs to the model\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n \n    def forward(self, query, key, value, inv_scale_factor, dropout_p):\n        qk = torch.matmul(query, key.transpose(-2, -1))\n        scaled_qk = qk.div(inv_scale_factor)\n        softmax_qk = scaled_qk.softmax(dim=-1)\n        dropout_qk = torch.nn.functional.dropout(softmax_qk, p=dropout_p)\n        output = dropout_qk.matmul(value)\n        return output\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nquery = torch.randn(4, 2, 4)\nkey = torch.randn(4, 2, 64)\nvalue = torch.randn(4, 2, 64)\ninv_scale_factor = 2\ndropout_p = 0.1\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, query, key, value, inv_scale_factor, dropout_p):\n        qk = torch.matmul(query, key.transpose(-2, -1))\n        scaled_qk = qk.div(inv_scale_factor)\n        softmax_qk = scaled_qk.softmax(dim=-1)\n        dropout_qk = torch.nn.functional.dropout(softmax_qk, p=dropout_p)\n        output = dropout_qk.matmul(value)\n        return output\n\n# Initializing the model using random inputs\nm = Model()\nquery = torch.randn(1, 10, 4096)\nkey = torch.randn(1, 10, 4096)\nvalue = torch.randn(1, 10, 4096)\ninv_scale_factor = 0.1\ndropout_p = 0.2\n",
                "\nq = torch.randn(8, 32, 128)\nk = torch.randn(8, 20, 128)\nv = torch.randn(8, 20, 512)\nm = torch.randn(8, 32, 512)\ninv_scale_factor = torch.randn(8, 1, 1)\ndropout_p = 0.5\n\ndef compute_attention(query, key, value, dropout_p=0.5):\n    qk = torch.matmul(query, key.transpose(-2, -1))\n    scaled_qk = qk.div(inv_scale_factor)\n    softmax_qk = scaled_qk.softmax(dim=-1)\n    dropout_qk = torch.nn.functional.dropout(softmax_qk, p=dropout_p)\n    output = dropout_qk.matmul(value)\n    return output\n\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, nb_head, nb_qkvi_dim, nb_v_dim, dropout_p=0.0):\n        super().__init__()\n        self.nb_head = nb_head\n        self.nb_qkvi_dim = nb_qkvi_dim\n        self.nb_v_dim = nb_v_dim\n        self.dropout_p = dropout_p\n        self.qkV_scale = 1 / math.sqrt(nb_qkvi_dim)\n        self.q_proj = torch.nn.Linear(nb_qkvi_dim, nb_qkvi_dim, bias=False)\n        self.k_proj = torch.nn.Linear(nb_qkvi_dim, nb_qkvi_dim, bias=False)\n        self.v_proj = torch.nn.Linear(nb_qkvi_dim, nb_v_dim, bias=False)\n        self.drop = torch.nn.Dropout(dropout_p)\n \n    def forward(self, x2, x3, x4):\n        qk = torch.matmul(self.q_proj(x2), self.k_proj(x3).transpose(-2, -1))\n        scaled_qk = qk.div(self.qkV_scale)\n        softmax_qk = scaled_qk.softmax(dim=-1)\n        dropout_qk = self.drop(softmax_qk)\n        output = dropout_qk.matmul(self.v_proj(x4))\n        return output\n\n# Initializing the model\nm = Model(8, 1024, 1024, 0.0)\n\n# Inputs to the model\nx2 = torch.randn(64, 8, 1024, 1)\nx3 = torch.randn(64, 16, 1024, 1)\nx4 = torch.randn(64, 1024, 1024, 1)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.dropout = torch.nn.Dropout(0.2)\n \n    def forward(self, query, key, value, inv_scale_factor, dropout_p=0.2):\n        qk = torch.matmul(query, key.transpose(-2, -1))\n        scaled_qk = qk.div(inv_scale_factor)\n        softmax_qk = scaled_qk.softmax(dim=-1)\n        dropout_qk = self.dropout(softmax_qk)\n        output = dropout_qk.matmul(value)\n        return output\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nquery = torch.randn(1, 2, 256, 64)\nkey = torch.randn(1, 2, 64, 256)\nvalue = torch.randn(1, 2, 256, 256)\ninv_scale_factor = 1. / torch.sqrt(torch.tensor(query.shape[-1], dtype=torch.float))\nx5 = m(query, key, value, inv_scale_factor)\n\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, query, key, value, scale_factor, dropout_p):\n        super().__init__()\n        self.query = nn.Parameter(query)\n        self.key = nn.Parameter(key)\n        self.value = nn.Parameter(value)\n        self.scale_factor = nn.Parameter(scale_factor)\n        self.dropout_p = dropout_p\n \n    def forward(self):\n        qk = torch.matmul(self.query, self.key.transpose(-2, -1))\n        scaled_qk = qk.div(self.scale_factor)\n        softmax_qk = scaled_qk.softmax(dim=-1)\n        dropout_qk = torch.nn.functional.dropout(softmax_qk, p=self.dropout_p)\n        output = dropout_qk.matmul(self.value)\n        return output\n\n# Initializing the model\nbatch_size = 12\nq_dim = 12\nk_dim = 16\nv_dim = 24\ndim = 4\nN = 10\nquery = torch.randn(batch_size, q_dim, dim, dim).to(\"cuda\")\nkey = torch.randn(batch_size, N, k_dim, dim, dim).to(\"cuda\")\nvalue = torch.randn(batch_size, N, v_dim, dim, dim).to(\"cuda\")\nscale_factor = torch.tensor([1.0 / math.sqrt(dim)]).to(\"cuda\")\ndropout_p = 0.5\nm = Model(query, key, value, scale_factor, dropout_p)\n\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.softmax = torch.nn.Softmax(dim=-1)\n \n    def forward(self, query, key, value, dropout_p, inv_scale_factor):\n        qk = torch.matmul(query, key.transpose(-2, -1))\n        scaled_qk = qk.div(inv_scale_factor)\n        softmax_qk = self.softmax(scaled_qk)\n        dropout_qk = torch.nn.functional.dropout(softmax_qk, p=dropout_p)\n        output = dropout_qk.matmul(value)\n        return output\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nquery = torch.randn(1, 8, 33, 16)\nkey = torch.randn(1, 8, 16, 33)\nvalue = torch.randn(1, 8, 16, 33)\ndropout_p = 0.15\ninv_scale_factor = 10\n"
            ],
            "g_time": 14.448067665100098
        }
    }
}
{
    "opt": "binary_unary=2",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(16, 16, 5, stride=2, padding=2, dilation=2, groups=1)\n        self.convt = torch.nn.ConvTranspose2d(16, 16, 5, stride=2, padding=2, output_padding=1, groups=1, bias=True)\n        self.relu = torch.nn.ReLU(inplace=False)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = self.convt(v1)\n        v3 = self.relu(v2)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 16, 28, 28)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(16, 16, 5, stride=2, padding=0, dilation=1, groups=1)\n        self.conv2 = torch.nn.Conv2d(16, 64, 5, stride=1, padding=2, dilation=2, groups=1)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = F.relu(v1)\n        v3 = self.conv2(v2)\n        v4 = v3 - 64\n        v5 = F.relu(v4)\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 16, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(64, 64, 3, stride=2, padding=1)\n        self.conv2 = torch.nn.Conv2d(64, 64, 3, stride=2, padding=1, groups=2)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = self.conv2(v1)\n        v3 = v2 - 0.7\n        v4 = F.relu(v3)\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 64, 32, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(16, 16, 3, padding=1, stride=1, groups=1)\n        self.conv2 = torch.nn.Conv2d(16, 32, 3, padding=1, stride=2, groups=1)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = self.conv2(v1)\n        v3 = v2 - 0.5\n        v4 = F.relu(v3)\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 16, 28, 28)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 64, 1, padding=0)\n        self.conv2 = torch.nn.Conv2d(1, 12, 1, padding=0)\n    def forward(self, x1):\n        v1 = torch.squeeze(self.conv1(x1), 1)\n        v2 = torch.squeeze(self.conv2(x1), 1)\n        v3 = torch.add(v1, v2)\n        v4 = F.relu(v3)\n\n        return v4\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(64, 64, 1)\n        self.conv2 = torch.nn.Conv2d(64, 64, 3, padding=1)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = v1 - 0.5\n        v3 = F.relu(v2)\n        v4 = self.conv2(v3)\n        v5 = v4 - 0.5\n        v6 = F.relu(v5)\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 64, 32, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 32, 5, stride=1, padding=2)\n    def forward(self, x1):\n        v1 = F.relu(x1)\n        v2 = F.sigmoid(x1)\n        v3 = self.conv1(v1)\n        v4 = v3 - v2\n        v5 = F.tanh(v4)\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 3, 128, 128)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(2, 16, 5, stride=2, padding=2, dilation=2, groups=2)\n        self.conv2 = torch.nn.Conv2d(16, 32, 3, stride=2, padding=1, dilation=1, groups=1)\n        self.conv3 = torch.nn.Conv2d(32, 64, 1, stride=2, padding=0, dilation=1, groups=1)\n    def forward(self, x1):\n        v1 = F.leaky_relu(self.conv1(x1), negative_slope=0.2)\n        v2 = F.leaky_relu(self.conv2(v1), negative_slope=0.2)\n        v3 = v2 - 0.5\n        v4 = F.leaky_relu(v3, negative_slope=0.2)\n        v5 = F.leaky_relu(self.conv3(v4), negative_slope=0.2)\n        v6 = v5 - 0.5\n        v7 = F.leaky_relu(v6, negative_slope=0.2)\n        return v7\n# Inputs to the model\nx1 = torch.randn(1, 2, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(64, 64, 5, stride=2, padding=2, dilation=2, groups=4)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 - 0.5\n        v3 = F.relu(v2)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 64, 28, 28)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(16, 16, 3, stride=1, padding=1, bias=False, dilation=1, groups=1)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 - 0.5 # Add a model specific tensor or scalar\n        v3 = F.relu(v2)\n        return v3\n# Inputs to the model\nx1 = torch.randn(2, 16, 128, 128)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(16, 16, 5, stride=2, padding=2, dilation=2, groups=1)\n        self.convt = torch.nn.ConvTranspose2d(16, 16, 5, stride=2, padding=2, output_padding=1, groups=1, bias=True)\n        self.relu = torch.nn.ReLU(inplace=False)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = self.convt(v1)\n        v3 = self.relu(v2)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 16, 28, 28)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(16, 16, 5, stride=2, padding=0, dilation=1, groups=1)\n        self.conv2 = torch.nn.Conv2d(16, 64, 5, stride=1, padding=2, dilation=2, groups=1)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = F.relu(v1)\n        v3 = self.conv2(v2)\n        v4 = v3 - 64\n        v5 = F.relu(v4)\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 16, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(64, 64, 3, stride=2, padding=1)\n        self.conv2 = torch.nn.Conv2d(64, 64, 3, stride=2, padding=1, groups=2)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = self.conv2(v1)\n        v3 = v2 - 0.7\n        v4 = F.relu(v3)\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 64, 32, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(16, 16, 3, padding=1, stride=1, groups=1)\n        self.conv2 = torch.nn.Conv2d(16, 32, 3, padding=1, stride=2, groups=1)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = self.conv2(v1)\n        v3 = v2 - 0.5\n        v4 = F.relu(v3)\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 16, 28, 28)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 64, 1, padding=0)\n        self.conv2 = torch.nn.Conv2d(1, 12, 1, padding=0)\n    def forward(self, x1):\n        v1 = torch.squeeze(self.conv1(x1), 1)\n        v2 = torch.squeeze(self.conv2(x1), 1)\n        v3 = torch.add(v1, v2)\n        v4 = F.relu(v3)\n\n        return v4\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(64, 64, 1)\n        self.conv2 = torch.nn.Conv2d(64, 64, 3, padding=1)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = v1 - 0.5\n        v3 = F.relu(v2)\n        v4 = self.conv2(v3)\n        v5 = v4 - 0.5\n        v6 = F.relu(v5)\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 64, 32, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 32, 5, stride=1, padding=2)\n    def forward(self, x1):\n        v1 = F.relu(x1)\n        v2 = F.sigmoid(x1)\n        v3 = self.conv1(v1)\n        v4 = v3 - v2\n        v5 = F.tanh(v4)\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 3, 128, 128)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(2, 16, 5, stride=2, padding=2, dilation=2, groups=2)\n        self.conv2 = torch.nn.Conv2d(16, 32, 3, stride=2, padding=1, dilation=1, groups=1)\n        self.conv3 = torch.nn.Conv2d(32, 64, 1, stride=2, padding=0, dilation=1, groups=1)\n    def forward(self, x1):\n        v1 = F.leaky_relu(self.conv1(x1), negative_slope=0.2)\n        v2 = F.leaky_relu(self.conv2(v1), negative_slope=0.2)\n        v3 = v2 - 0.5\n        v4 = F.leaky_relu(v3, negative_slope=0.2)\n        v5 = F.leaky_relu(self.conv3(v4), negative_slope=0.2)\n        v6 = v5 - 0.5\n        v7 = F.leaky_relu(v6, negative_slope=0.2)\n        return v7\n# Inputs to the model\nx1 = torch.randn(1, 2, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(64, 64, 5, stride=2, padding=2, dilation=2, groups=4)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 - 0.5\n        v3 = F.relu(v2)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 64, 28, 28)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(16, 16, 3, stride=1, padding=1, bias=False, dilation=1, groups=1)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 - 0.5 # Add a model specific tensor or scalar\n        v3 = F.relu(v2)\n        return v3\n# Inputs to the model\nx1 = torch.randn(2, 16, 128, 128)\n"
            ],
            "g_time": 10.85466480255127
        }
    }
}
{
    "opt": "unary=17",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(2, 2, 2, stride=1)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        return v1\n# Inputs to the model\nx1 = torch.randn(1, 2, 32, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(4, 12, 3, padding=1, stride=1)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = torch.nn.functional.relu(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 4, 24, 24)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(16, 16, 3, stride=1)\n        self.conv = torch.nn.Conv2d(16, 32, 3, stride=1)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = torch.nn.functional.relu(v1)\n        v3 = self.conv(v2)\n        v5 = torch.sigmoid(v3)\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 16, 24, 24)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.convtranspose2d_1 = torch.nn.ConvTranspose2d(4, 4, (2, 2), stride=(2, 2), bias=False)\n    def forward(self, input):\n        v1 = self.convtranspose2d_1(input)\n        return v1\n# Inputs to the model\ninput = torch.randn(1, 4, 5, 5)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.ConvTranspose2d(1, 32, 3, stride=2, padding=1)\n        self.conv2 = torch.nn.ConvTranspose2d(32, 48, 3, stride=2, padding=2)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v1_1 = v1.permute(0, 2, 3, 1)\n        v1_2 = torch.softmax(v1_1, dim=-1)\n        v1_3 = v1_2.permute(0, 3, 1, 2)\n        v2 = self.conv2(v1_3)\n        v3 = v2.view(-1)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 1, 56, 56)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.utils.weight_norm(torch.nn.Conv2d(3, 3, 3, stride=2, padding=1))\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = torch.relu(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 3, 16, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.ConvTranspose2d(1, 1, 3, stride=1)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = torch.nn.functional.relu(v1)\n        v3 = v2.transpose(3, 2)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 1, 24, 24)\n",
                "\nclass Model(torch.nn.Sequential):\n    def __init__(self):\n        super().__init__()\n        self.add_module('conv', torch.nn.ConvTranspose2d(1, 8, 3, stride=1))\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = torch.nn.ReLU(v1)\n        v3 = v1 + v2\n        return x1\n# Inputs to the model\nx1 = torch.randn(1, 1, 28, 28)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.convtranpose = torch.nn.ConvTranspose2d(1, 1, kernel_size=3, stride=3, padding=1, output_padding=1)\n        self.relu = torch.nn.ReLU()\n    def forward(self, x1):\n        v1 = self.convtranpose(x1)\n        v2 = self.relu(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 1, 16, 16)\n",
                "\n\n\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.convtranspos = torch.nn.ConvTranspose2d(16, 16, 3, padding=1)\n        self.convtranspos1 = torch.nn.ConvTranspose2d(16, 16, 3, padding=1)\n    def forward(self, x1):\n        v1 = torch.nn.functional.relu(x1)\n        v2 = self.convtranspos(v1)\n        v3 = torch.nn.functional.relu(v2)\n        v4 = v3.transpose(2, 1)\n        v5 = self.convtranspos1(v4)\n        v6 = torch.nn.functional.sigmoid(v5)\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 16, 224, 224)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(2, 2, 2, stride=1)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        return v1\n# Inputs to the model\nx1 = torch.randn(1, 2, 32, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(4, 12, 3, padding=1, stride=1)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = torch.nn.functional.relu(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 4, 24, 24)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(16, 16, 3, stride=1)\n        self.conv = torch.nn.Conv2d(16, 32, 3, stride=1)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = torch.nn.functional.relu(v1)\n        v3 = self.conv(v2)\n        v5 = torch.sigmoid(v3)\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 16, 24, 24)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.convtranspose2d_1 = torch.nn.ConvTranspose2d(4, 4, (2, 2), stride=(2, 2), bias=False)\n    def forward(self, input):\n        v1 = self.convtranspose2d_1(input)\n        return v1\n# Inputs to the model\ninput = torch.randn(1, 4, 5, 5)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.ConvTranspose2d(1, 32, 3, stride=2, padding=1)\n        self.conv2 = torch.nn.ConvTranspose2d(32, 48, 3, stride=2, padding=2)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v1_1 = v1.permute(0, 2, 3, 1)\n        v1_2 = torch.softmax(v1_1, dim=-1)\n        v1_3 = v1_2.permute(0, 3, 1, 2)\n        v2 = self.conv2(v1_3)\n        v3 = v2.view(-1)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 1, 56, 56)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.utils.weight_norm(torch.nn.Conv2d(3, 3, 3, stride=2, padding=1))\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = torch.relu(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 3, 16, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.ConvTranspose2d(1, 1, 3, stride=1)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = torch.nn.functional.relu(v1)\n        v3 = v2.transpose(3, 2)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 1, 24, 24)\n",
                "\nclass Model(torch.nn.Sequential):\n    def __init__(self):\n        super().__init__()\n        self.add_module('conv', torch.nn.ConvTranspose2d(1, 8, 3, stride=1))\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = torch.nn.ReLU(v1)\n        v3 = v1 + v2\n        return x1\n# Inputs to the model\nx1 = torch.randn(1, 1, 28, 28)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.convtranpose = torch.nn.ConvTranspose2d(1, 1, kernel_size=3, stride=3, padding=1, output_padding=1)\n        self.relu = torch.nn.ReLU()\n    def forward(self, x1):\n        v1 = self.convtranpose(x1)\n        v2 = self.relu(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 1, 16, 16)\n",
                "\n\n\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.convtranspos = torch.nn.ConvTranspose2d(16, 16, 3, padding=1)\n        self.convtranspos1 = torch.nn.ConvTranspose2d(16, 16, 3, padding=1)\n    def forward(self, x1):\n        v1 = torch.nn.functional.relu(x1)\n        v2 = self.convtranspos(v1)\n        v3 = torch.nn.functional.relu(v2)\n        v4 = v3.transpose(2, 1)\n        v5 = self.convtranspos1(v4)\n        v6 = torch.nn.functional.sigmoid(v5)\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 16, 224, 224)\n"
            ],
            "g_time": 8.150640964508057
        }
    }
}
{
    "opt": "unary=21",
    "response": {
        "0": {
            "raw": [
                "\nclass ModelTanh(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(2, 1, kernel_size=(1, 1), stride=(1, 1))\n    def forward(self, x4):\n        v1 = self.conv(x4)\n        v2 = torch.tanh(v1)\n        return v1 + v2\n# Inputs to the model\ntensor = torch.randn(1, 2, 3, 3)\n",
                "\nclass ModelTanh(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 7, 5, stride=5, padding=1)\n        self.conv2 = torch.nn.Conv2d(3, 7, 5, stride=5, padding=1)\n    def forward(self, x2):\n        v1 = self.conv1(x2)\n        v2 = self.conv2(x2)\n        v3 = torch.tanh(v1)\n        return v3 * v2\n# Inputs to the model\nx2 = torch.randn(1, 3, 32, 32)\n",
                "\nclass ModelTanh(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.tanh = torch.nn.Tanh()\n    def forward(self, x3):\n        v1 = self.tanh(x3)\n        v2 = self.tanh(v1)\n        return v2 + x3\n# Inputs to the model\ntensor = torch.randn(1, 16, 2, 2)\n",
                "\nclass ModelTanh(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 7, 5, stride=5, padding=1)\n        self.conv2 = torch.nn.Conv2d(54, 17, 2, stride=1, padding=40)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = torch.tanh(v1)\n        v3 = self.conv2(v2)\n        v4 = torch.tanh(v3)\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 3, 32, 32)\n",
                "\nclass ModelTanh(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 5, 5, stride=1, padding=10)\n    def forward(self, x3):\n        v1 = self.conv(x3)\n        v2 = torch.tanh(v1)\n        return v2\n# Inputs to the model\nx3 = torch.randn(1, 3, 32, 32)\n",
                "\nclass ModelTanh(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, input1):\n        v1 = torch.tanh(input1)\n        v2 = torch.neg(v1)\n        return v2\n# Inputs to the model\ninput1 = torch.randn(55296)\n",
                "\nclass ModelTanh(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(32, 16, kernel_size=(1, 3), stride=(1, 1), padding=(1, 1))\n    def forward(self, x):\n        v1 = self.conv(x)\n        v2 = torch.tanh(v1)\n        return v2\n# Inputs to the model\nx = torch.randn(32, 32, 3, 3)\n",
                "\nclass ModelTanh(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(1, 1, 3, stride=1, padding=1)\n    def forward(self, x28):\n        v1 = self.conv(x28)\n        v3 = v1 + 20.6958\n        return torch.tanh(v1)\n# Inputs to the model\nx28 = torch.randn(1, 1, 32, 224)\n",
                "\nclass ModelTanh(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 14, 11, stride=9, padding=5)\n        self.conv2 = torch.nn.Conv2d(14, 3, 9, stride=8, padding=3)\n    def forward(self, x):\n        v1 = self.conv1(x)\n        v2 = torch.tanh(v1)\n        v3 = self.conv2(v2)\n        return v3\n# Inputs to the model\nx = torch.randn(1, 3, 297, 348)\n",
                "\nclass ModelTanh(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 7, 100, stride=1, padding=0, dilation=2)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = torch.tanh(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 3, 32, 32)\n"
            ],
            "code": [
                "\nclass ModelTanh(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(2, 1, kernel_size=(1, 1), stride=(1, 1))\n    def forward(self, x4):\n        v1 = self.conv(x4)\n        v2 = torch.tanh(v1)\n        return v1 + v2\n# Inputs to the model\ntensor = torch.randn(1, 2, 3, 3)\n",
                "\nclass ModelTanh(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 7, 5, stride=5, padding=1)\n        self.conv2 = torch.nn.Conv2d(3, 7, 5, stride=5, padding=1)\n    def forward(self, x2):\n        v1 = self.conv1(x2)\n        v2 = self.conv2(x2)\n        v3 = torch.tanh(v1)\n        return v3 * v2\n# Inputs to the model\nx2 = torch.randn(1, 3, 32, 32)\n",
                "\nclass ModelTanh(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.tanh = torch.nn.Tanh()\n    def forward(self, x3):\n        v1 = self.tanh(x3)\n        v2 = self.tanh(v1)\n        return v2 + x3\n# Inputs to the model\ntensor = torch.randn(1, 16, 2, 2)\n",
                "\nclass ModelTanh(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 7, 5, stride=5, padding=1)\n        self.conv2 = torch.nn.Conv2d(54, 17, 2, stride=1, padding=40)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = torch.tanh(v1)\n        v3 = self.conv2(v2)\n        v4 = torch.tanh(v3)\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 3, 32, 32)\n",
                "\nclass ModelTanh(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 5, 5, stride=1, padding=10)\n    def forward(self, x3):\n        v1 = self.conv(x3)\n        v2 = torch.tanh(v1)\n        return v2\n# Inputs to the model\nx3 = torch.randn(1, 3, 32, 32)\n",
                "\nclass ModelTanh(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, input1):\n        v1 = torch.tanh(input1)\n        v2 = torch.neg(v1)\n        return v2\n# Inputs to the model\ninput1 = torch.randn(55296)\n",
                "\nclass ModelTanh(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(32, 16, kernel_size=(1, 3), stride=(1, 1), padding=(1, 1))\n    def forward(self, x):\n        v1 = self.conv(x)\n        v2 = torch.tanh(v1)\n        return v2\n# Inputs to the model\nx = torch.randn(32, 32, 3, 3)\n",
                "\nclass ModelTanh(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(1, 1, 3, stride=1, padding=1)\n    def forward(self, x28):\n        v1 = self.conv(x28)\n        v3 = v1 + 20.6958\n        return torch.tanh(v1)\n# Inputs to the model\nx28 = torch.randn(1, 1, 32, 224)\n",
                "\nclass ModelTanh(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 14, 11, stride=9, padding=5)\n        self.conv2 = torch.nn.Conv2d(14, 3, 9, stride=8, padding=3)\n    def forward(self, x):\n        v1 = self.conv1(x)\n        v2 = torch.tanh(v1)\n        v3 = self.conv2(v2)\n        return v3\n# Inputs to the model\nx = torch.randn(1, 3, 297, 348)\n",
                "\nclass ModelTanh(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 7, 100, stride=1, padding=0, dilation=2)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = torch.tanh(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 3, 32, 32)\n"
            ],
            "g_time": 5.57067346572876
        }
    }
}
{
    "opt": "sfdp=5",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.heads = 12\n        self.seq_len = 128\n        self.dim = 32 // self.heads\n    def forward(self, query, key, value, attn_mask):\n        qk = query @ key.transpose(-2, -1) / math.sqrt(query.size(-1))\n        qk = qk + attn_mask\n        attn_weight = torch.softmax(qk, dim=-1)\n        attn_weight = torch.dropout(attn_weight, 0.15, True)\n        output = attn_weight @ value\n        return output\n# Inputs to the model\nquery = torch.randn(1, 8, 128, 32)\nkey = torch.randn(1, 8, 128, 32)\nvalue = torch.randn(1, 8, 128, 32)\nattn_mask = torch.randn(1, 1, 128, 128)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.heads = 8\n        self.seq_len = 10\n        self.dim = 512\n    def forward(self, query, key, value, attn_mask):\n        qk = query @ key.transpose(-2, -1) / math.sqrt(query.size(-1))\n        qk = qk + attn_mask\n        attn_weight = torch.softmax(qk, dim=-1)\n        attn_weight = torch.dropout(attn_weight, 0.8, True)\n        output = attn_weight @ value\n        return output\n# Inputs to the model\nquery = torch.randn(1, 4, 10, 512)\nkey = torch.randn(1, 4, 10, 512)\nvalue = torch.randn(1, 4, 10, 512)\nattn_mask = torch.randn(1, 1, 10, 10)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.heads = 65\n        self.seq_len = 568\n        self.dim = 75 // self.heads\n    def forward(self, query, key, value, attn_mask):\n        qk = query @ key.transpose(-2, -1) / math.sqrt(query.size(-1))\n        qk = qk + attn_mask\n        attn_weight = torch.softmax(qk, dim=-1)\n        attn_weight = torch.dropout(attn_weight, 0.6, True)\n        output = attn_weight @ value\n        return output\n# Inputs to the model\nquery = torch.randn(1, 553, 568, 75)\nkey = torch.randn(1, 553, 568, 75)\nvalue = torch.randn(1, 553, 568, 75)\nattn_mask = torch.randn(1, 1, 568, 568)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.heads = 16\n        self.seq_len = 256\n        self.dim = 128 // self.heads\n    def forward(self, query, key, value, attn_mask):\n        qk = query @ key.transpose(-2, -1) / math.sqrt(query.size(-1))\n        qk = qk + attn_mask\n        attn_weight = torch.softmax(qk, dim=-1)\n        attn_weight = torch.dropout(attn_weight, 0, True)\n        output = attn_weight @ value\n        return output\n# Inputs to the model\nquery = torch.randn(1, 256, 256, 128)\nkey = torch.randn(1, 256, 256, 128)\nvalue = torch.randn(1, 256, 256, 128)\nattn_mask = torch.randn(1, 1, 256, 256)\n",
                "\nclass Model(nn.Module):\n    def __init__(self, input_dim=256, hidden_dim=512, num_classes=10, num_heads=12, max_len=1000):\n        super(Model, self).__init__()\n        self.num_heads = num_heads\n        self.hidden_dim = hidden_dim\n        self.num_classes = num_classes\n        self.max_len = max_len\n        self.attn = nn.MultiheadAttention(input_dim, num_heads)\n        self.fc = nn.Linear(self.hidden_dim * self.num_heads, self.num_classes)\n\n    def forward(self, x):\n        attn_output, _ = self.attn(x, x, x)\n        attn_output = attn_output.flatten(1)\n        return self.fc(attn_output)\n# Inputs to the model\nx = torch.randn(1, self.max_len, self.hidden_dim)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.heads = 18\n        self.seq_len = 4096\n        self.dim = 64 // self.heads\n    def forward(self, query, key, value, attn_mask):\n        qk = query @ key.transpose(-2, -1) / math.sqrt(query.size(-1))\n        qk = qk + attn_mask\n        attn_weight = torch.softmax(qk, dim=-1)\n        attn_weight = torch.dropout(attn_weight, 0.8, True)\n        output = attn_weight @ value\n        return output\n# Inputs to the model\nquery = torch.randn(1, 512, 4096, 64)\nkey = torch.randn(1, 512, 4096, 64)\nvalue = torch.randn(1, 512, 4096, 64)\nattn_mask = torch.randn(1, 1, 4096, 4096)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.heads = 97\n        self.seq_len = 128\n        self.dim = 64 // self.heads\n    def forward(self, query, key, value, attn_mask):\n        qk = query @ key.transpose(-2, -1) / math.sqrt(query.size(-1))\n        qk = qk + attn_mask\n        attn_weight = torch.softmax(qk, dim=-1)\n        attn_weight = torch.dropout(attn_weight, 0.1, True)\n        output = attn_weight @ value\n        return output\n# Inputs to the model\nquery = torch.randn(1, 64, 128, 64)\nkey = torch.randn(1, 64, 128, 64)\nvalue = torch.randn(1, 64, 128, 64)\nattn_mask = torch.randn(1, 1, 128, 128)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.heads = 16\n        self.seq_len = 768\n        self.dim = 512 // self.heads\n    def forward(self, query, key, value, attn_mask):\n        qk = query @ key.transpose(-2, -1) / math.sqrt(query.size(-1))\n        qk = qk + attn_mask\n        attn_weight = torch.softmax(qk, dim=-1)\n        attn_weight = torch.dropout(attn_weight, 0.2, True)\n        output = attn_weight @ value\n        return output\n# Inputs to the model\nquery = torch.randn(1, 64, 768, 512)\nkey = torch.randn(1, 64, 768, 512)\nvalue = torch.randn(1, 64, 768, 512)\nattn_mask = torch.randn(1, 1, 768, 768)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.heads = 12\n        self.seq_len = 64\n        self.dim = 29 // self.heads\n    def forward(self, query, key, value, attn_mask):\n        qk = query @ key.transpose(-2, -1) / math.sqrt(query.size(-1))\n        qk = qk + attn_mask\n        attn_weight = torch.softmax(qk, dim=-1)\n        attn_weight = torch.dropout(attn_weight, 0.8, True)\n        output = attn_weight @ value\n        return output\n# Inputs to the model\nquery = torch.randn(1, 24, 64, 29)\nkey = torch.randn(1, 24, 64, 29)\nvalue = torch.randn(1, 24, 64, 29)\nattn_mask = torch.randn(1, 1, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.heads = 62\n        self.seq_len = 266\n        self.dim = 111\n    def forward(self, query, key, value, attn_mask):\n        qk = query @ key.transpose(-2, -1) / math.sqrt(query.size(-1))\n        qk = qk + attn_mask\n        attn_weight = torch.softmax(qk, dim=-1)\n        attn_weight = torch.dropout(attn_weight, 0.3, False)\n        output = attn_weight @ value\n        return output\n# Inputs to the model\nquery = torch.randn(1, 84, 266, 111)\nkey = torch.randn(1, 84, 266, 111)\nvalue = torch.randn(1, 84, 266, 111)\nattn_mask = torch.randn(1, 1, 266, 266)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.heads = 12\n        self.seq_len = 128\n        self.dim = 32 // self.heads\n    def forward(self, query, key, value, attn_mask):\n        qk = query @ key.transpose(-2, -1) / math.sqrt(query.size(-1))\n        qk = qk + attn_mask\n        attn_weight = torch.softmax(qk, dim=-1)\n        attn_weight = torch.dropout(attn_weight, 0.15, True)\n        output = attn_weight @ value\n        return output\n# Inputs to the model\nquery = torch.randn(1, 8, 128, 32)\nkey = torch.randn(1, 8, 128, 32)\nvalue = torch.randn(1, 8, 128, 32)\nattn_mask = torch.randn(1, 1, 128, 128)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.heads = 8\n        self.seq_len = 10\n        self.dim = 512\n    def forward(self, query, key, value, attn_mask):\n        qk = query @ key.transpose(-2, -1) / math.sqrt(query.size(-1))\n        qk = qk + attn_mask\n        attn_weight = torch.softmax(qk, dim=-1)\n        attn_weight = torch.dropout(attn_weight, 0.8, True)\n        output = attn_weight @ value\n        return output\n# Inputs to the model\nquery = torch.randn(1, 4, 10, 512)\nkey = torch.randn(1, 4, 10, 512)\nvalue = torch.randn(1, 4, 10, 512)\nattn_mask = torch.randn(1, 1, 10, 10)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.heads = 65\n        self.seq_len = 568\n        self.dim = 75 // self.heads\n    def forward(self, query, key, value, attn_mask):\n        qk = query @ key.transpose(-2, -1) / math.sqrt(query.size(-1))\n        qk = qk + attn_mask\n        attn_weight = torch.softmax(qk, dim=-1)\n        attn_weight = torch.dropout(attn_weight, 0.6, True)\n        output = attn_weight @ value\n        return output\n# Inputs to the model\nquery = torch.randn(1, 553, 568, 75)\nkey = torch.randn(1, 553, 568, 75)\nvalue = torch.randn(1, 553, 568, 75)\nattn_mask = torch.randn(1, 1, 568, 568)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.heads = 16\n        self.seq_len = 256\n        self.dim = 128 // self.heads\n    def forward(self, query, key, value, attn_mask):\n        qk = query @ key.transpose(-2, -1) / math.sqrt(query.size(-1))\n        qk = qk + attn_mask\n        attn_weight = torch.softmax(qk, dim=-1)\n        attn_weight = torch.dropout(attn_weight, 0, True)\n        output = attn_weight @ value\n        return output\n# Inputs to the model\nquery = torch.randn(1, 256, 256, 128)\nkey = torch.randn(1, 256, 256, 128)\nvalue = torch.randn(1, 256, 256, 128)\nattn_mask = torch.randn(1, 1, 256, 256)\n",
                "\nclass Model(nn.Module):\n    def __init__(self, input_dim=256, hidden_dim=512, num_classes=10, num_heads=12, max_len=1000):\n        super(Model, self).__init__()\n        self.num_heads = num_heads\n        self.hidden_dim = hidden_dim\n        self.num_classes = num_classes\n        self.max_len = max_len\n        self.attn = nn.MultiheadAttention(input_dim, num_heads)\n        self.fc = nn.Linear(self.hidden_dim * self.num_heads, self.num_classes)\n\n    def forward(self, x):\n        attn_output, _ = self.attn(x, x, x)\n        attn_output = attn_output.flatten(1)\n        return self.fc(attn_output)\n# Inputs to the model\nx = torch.randn(1, self.max_len, self.hidden_dim)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.heads = 18\n        self.seq_len = 4096\n        self.dim = 64 // self.heads\n    def forward(self, query, key, value, attn_mask):\n        qk = query @ key.transpose(-2, -1) / math.sqrt(query.size(-1))\n        qk = qk + attn_mask\n        attn_weight = torch.softmax(qk, dim=-1)\n        attn_weight = torch.dropout(attn_weight, 0.8, True)\n        output = attn_weight @ value\n        return output\n# Inputs to the model\nquery = torch.randn(1, 512, 4096, 64)\nkey = torch.randn(1, 512, 4096, 64)\nvalue = torch.randn(1, 512, 4096, 64)\nattn_mask = torch.randn(1, 1, 4096, 4096)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.heads = 97\n        self.seq_len = 128\n        self.dim = 64 // self.heads\n    def forward(self, query, key, value, attn_mask):\n        qk = query @ key.transpose(-2, -1) / math.sqrt(query.size(-1))\n        qk = qk + attn_mask\n        attn_weight = torch.softmax(qk, dim=-1)\n        attn_weight = torch.dropout(attn_weight, 0.1, True)\n        output = attn_weight @ value\n        return output\n# Inputs to the model\nquery = torch.randn(1, 64, 128, 64)\nkey = torch.randn(1, 64, 128, 64)\nvalue = torch.randn(1, 64, 128, 64)\nattn_mask = torch.randn(1, 1, 128, 128)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.heads = 16\n        self.seq_len = 768\n        self.dim = 512 // self.heads\n    def forward(self, query, key, value, attn_mask):\n        qk = query @ key.transpose(-2, -1) / math.sqrt(query.size(-1))\n        qk = qk + attn_mask\n        attn_weight = torch.softmax(qk, dim=-1)\n        attn_weight = torch.dropout(attn_weight, 0.2, True)\n        output = attn_weight @ value\n        return output\n# Inputs to the model\nquery = torch.randn(1, 64, 768, 512)\nkey = torch.randn(1, 64, 768, 512)\nvalue = torch.randn(1, 64, 768, 512)\nattn_mask = torch.randn(1, 1, 768, 768)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.heads = 12\n        self.seq_len = 64\n        self.dim = 29 // self.heads\n    def forward(self, query, key, value, attn_mask):\n        qk = query @ key.transpose(-2, -1) / math.sqrt(query.size(-1))\n        qk = qk + attn_mask\n        attn_weight = torch.softmax(qk, dim=-1)\n        attn_weight = torch.dropout(attn_weight, 0.8, True)\n        output = attn_weight @ value\n        return output\n# Inputs to the model\nquery = torch.randn(1, 24, 64, 29)\nkey = torch.randn(1, 24, 64, 29)\nvalue = torch.randn(1, 24, 64, 29)\nattn_mask = torch.randn(1, 1, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.heads = 62\n        self.seq_len = 266\n        self.dim = 111\n    def forward(self, query, key, value, attn_mask):\n        qk = query @ key.transpose(-2, -1) / math.sqrt(query.size(-1))\n        qk = qk + attn_mask\n        attn_weight = torch.softmax(qk, dim=-1)\n        attn_weight = torch.dropout(attn_weight, 0.3, False)\n        output = attn_weight @ value\n        return output\n# Inputs to the model\nquery = torch.randn(1, 84, 266, 111)\nkey = torch.randn(1, 84, 266, 111)\nvalue = torch.randn(1, 84, 266, 111)\nattn_mask = torch.randn(1, 1, 266, 266)\n"
            ],
            "g_time": 9.6546630859375
        }
    }
}
{
    "opt": "unary=24",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(9, 2, 6, stride=1, padding=7)\n    def forward(self, x):\n        negative_slope = 0.7378717\n        v1 = self.conv(x)\n        v2 = v1 > 0\n        v3 = v1 * negative_slope\n        v4 = torch.where(v2, v1, v3)\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 9, 9, 10)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(2, 2, (63, 42), stride=(63, 42), padding=0, groups=3, dilation=1)\n    def forward(self, x):\n        negative_slope = 229.202003\n        v1 = self.conv(x)\n        v2 = v1 > 0\n        v3 = v1 * negative_slope\n        v4 = torch.where(v2, v1, v3)\n        return v4\n# Inputs to the model\nx1 = torch.randn(2, 2, 63, 42)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(1, 7, 7, stride=1, padding=3)\n    def forward(self, x):\n        negative_slope = 0.6669809\n        v1 = self.conv(x)\n        v2 = v1 > 0\n        v3 = v1 * negative_slope\n        v4 = torch.where(v2, v1, v3)\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 1, 14, 14)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(5, 1, 3, stride=2, padding=7)\n    def forward(self, x):\n        negative_slope = 2.781156\n        v1 = self.conv(x)\n        v2 = v1 > 0\n        v3 = v1 * negative_slope\n        v4 = torch.where(v2, v1, v3)\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 5, 5, 5)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(1, 65, (1, 1), stride=(1, 1))\n    def forward(self, x):\n        negative_slope = -1.759189\n        v1 = self.conv(x)\n        v2 = v1 > 0\n        v3 = v1 * negative_slope\n        v4 = torch.where(v2, v1, v3)\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 1, 14, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(1, 9, 1, stride=1, padding=7)\n    def forward(self, x, negative_slope=19.768183):\n        v1 = self.conv(x)\n        v2 = v1 > 0\n        v3 = v1 * negative_slope\n        v4 = torch.where(v2, v1, v3)\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 1, 9, 9)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(1, 29, 5, stride=1, padding=5)\n    def forward(self, x):\n        negative_slope = 2.399605\n        v1 = self.conv(x)\n        v2 = v1 > 0\n        v3 = v1 * negative_slope\n        v4 = torch.where(v2, v1, v3)\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 1, 18, 18)\n",
                "\nclass MyModule4_Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.features = torch.nn.ModuleDict({\n        '0': torch.nn.Conv2d(3, 5, 2, stride=(2, 1), padding=(2, 1)),\n        '1': torch.nn.Conv2d(5, 5, 2, stride=(2, 2), padding=(1, 1), bias=False),\n        })\n        self.features.update({'2': torch.nn.Conv2d(5, 5, 2, stride=(2, 1), padding=(4, 1))})\n        self.features.update({'3': torch.nn.Conv2d(5, 7, 2, stride=(2, 2), padding=(3, 1))})\n        self.regressor = torch.nn.Linear(5, 15)\n    def forward(self, x):\n        negative_slope = 1.8414381\n        v2 = self.features['0'](x)\n        v6 = v2 > 0\n        v7 = v2 * negative_slope\n        v9 = torch.where(v6, v2, v7)\n        v10 = self.features['1'](v9)\n        v3 = v10 > 0\n        v4 = v10 * negative_slope\n        v105 = torch.where(v3, v10, v4)\n        v12 = self.features['2'](v105)\n        v5 = v12 > 0\n        v6 = v12 * negative_slope\n        v96 = torch.where(v5, v12, v6)\n        v14 = self.features['3'](v96)\n        v50 = v14 > 0\n        v51 = v14 * negative_slope\n        v53 = v14 > 0\n        v54 = v14 * negative_slope\n        v43 = torch.where(v50, v14, v51)\n        v44 = torch.where(v53, v14, v54)\n        return (v43 + v44, self.regressor(v2))\n    def trainable_named_children(self):\n        return [(\"features\", self.features), (\"regressor\", self.regressor)]\n# Inputs to the model\nx1 = torch.randn(2, 3, 16, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(1, 9, 9, stride=1, padding=0)\n    def forward(self, x):\n        negative_slope = 0.004190892\n        v1 = self.conv(x)\n        v2 = v1 > 0\n        v3 = v1 * negative_slope\n        v4 = torch.where(v2, v1, v3)\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 1, 3, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(1, 7, (8, 3), stride=1, padding=7)\n    def forward(self, x):\n        negative_slope = 0.008062\n        v1 = self.conv(x)\n        v2 = v1 > 0\n        v3 = v1 * negative_slope\n        v4 = torch.where(v2, v1, v3)\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 1, 12, 16)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(9, 2, 6, stride=1, padding=7)\n    def forward(self, x):\n        negative_slope = 0.7378717\n        v1 = self.conv(x)\n        v2 = v1 > 0\n        v3 = v1 * negative_slope\n        v4 = torch.where(v2, v1, v3)\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 9, 9, 10)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(2, 2, (63, 42), stride=(63, 42), padding=0, groups=3, dilation=1)\n    def forward(self, x):\n        negative_slope = 229.202003\n        v1 = self.conv(x)\n        v2 = v1 > 0\n        v3 = v1 * negative_slope\n        v4 = torch.where(v2, v1, v3)\n        return v4\n# Inputs to the model\nx1 = torch.randn(2, 2, 63, 42)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(1, 7, 7, stride=1, padding=3)\n    def forward(self, x):\n        negative_slope = 0.6669809\n        v1 = self.conv(x)\n        v2 = v1 > 0\n        v3 = v1 * negative_slope\n        v4 = torch.where(v2, v1, v3)\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 1, 14, 14)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(5, 1, 3, stride=2, padding=7)\n    def forward(self, x):\n        negative_slope = 2.781156\n        v1 = self.conv(x)\n        v2 = v1 > 0\n        v3 = v1 * negative_slope\n        v4 = torch.where(v2, v1, v3)\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 5, 5, 5)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(1, 65, (1, 1), stride=(1, 1))\n    def forward(self, x):\n        negative_slope = -1.759189\n        v1 = self.conv(x)\n        v2 = v1 > 0\n        v3 = v1 * negative_slope\n        v4 = torch.where(v2, v1, v3)\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 1, 14, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(1, 9, 1, stride=1, padding=7)\n    def forward(self, x, negative_slope=19.768183):\n        v1 = self.conv(x)\n        v2 = v1 > 0\n        v3 = v1 * negative_slope\n        v4 = torch.where(v2, v1, v3)\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 1, 9, 9)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(1, 29, 5, stride=1, padding=5)\n    def forward(self, x):\n        negative_slope = 2.399605\n        v1 = self.conv(x)\n        v2 = v1 > 0\n        v3 = v1 * negative_slope\n        v4 = torch.where(v2, v1, v3)\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 1, 18, 18)\n",
                "\nclass MyModule4_Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.features = torch.nn.ModuleDict({\n        '0': torch.nn.Conv2d(3, 5, 2, stride=(2, 1), padding=(2, 1)),\n        '1': torch.nn.Conv2d(5, 5, 2, stride=(2, 2), padding=(1, 1), bias=False),\n        })\n        self.features.update({'2': torch.nn.Conv2d(5, 5, 2, stride=(2, 1), padding=(4, 1))})\n        self.features.update({'3': torch.nn.Conv2d(5, 7, 2, stride=(2, 2), padding=(3, 1))})\n        self.regressor = torch.nn.Linear(5, 15)\n    def forward(self, x):\n        negative_slope = 1.8414381\n        v2 = self.features['0'](x)\n        v6 = v2 > 0\n        v7 = v2 * negative_slope\n        v9 = torch.where(v6, v2, v7)\n        v10 = self.features['1'](v9)\n        v3 = v10 > 0\n        v4 = v10 * negative_slope\n        v105 = torch.where(v3, v10, v4)\n        v12 = self.features['2'](v105)\n        v5 = v12 > 0\n        v6 = v12 * negative_slope\n        v96 = torch.where(v5, v12, v6)\n        v14 = self.features['3'](v96)\n        v50 = v14 > 0\n        v51 = v14 * negative_slope\n        v53 = v14 > 0\n        v54 = v14 * negative_slope\n        v43 = torch.where(v50, v14, v51)\n        v44 = torch.where(v53, v14, v54)\n        return (v43 + v44, self.regressor(v2))\n    def trainable_named_children(self):\n        return [(\"features\", self.features), (\"regressor\", self.regressor)]\n# Inputs to the model\nx1 = torch.randn(2, 3, 16, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(1, 9, 9, stride=1, padding=0)\n    def forward(self, x):\n        negative_slope = 0.004190892\n        v1 = self.conv(x)\n        v2 = v1 > 0\n        v3 = v1 * negative_slope\n        v4 = torch.where(v2, v1, v3)\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 1, 3, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(1, 7, (8, 3), stride=1, padding=7)\n    def forward(self, x):\n        negative_slope = 0.008062\n        v1 = self.conv(x)\n        v2 = v1 > 0\n        v3 = v1 * negative_slope\n        v4 = torch.where(v2, v1, v3)\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 1, 12, 16)\n"
            ],
            "g_time": 17.7669038772583
        }
    }
}
{
    "opt": "unary=16",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(64, 32)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1.relu()\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 1)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.relu(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, input_dim, output_dim):\n        super().__init__()\n        self.linear = torch.nn.Linear(input_dim, output_dim)\n \n    def forward(self, x1):\n        return torch.relu(self.linear(x1))\n\n# Initializing the model\nm = Model(10, 4)\n\n# Inputs to the model\nx1 = torch.randn(1, 10)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 4)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.relu(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(2, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(128, 64)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.nn.functional.relu(v1)\n        return v2\n\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 128)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.linear1 = torch.nn.Linear(2, 4)\n        self.linear2 = torch.nn.Linear(4, 2)\n \n    def forward(self, x):\n        v1 = self.linear1(x)\n        v2 = F.relu(v1)\n        v3 = self.linear2(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx = torch.randn(2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 12)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.nn.functional.relu(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(2, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(16, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.nn.functional.relu(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(2, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(16, 16)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.relu(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc = torch.nn.Linear(65 * 1024, 2048)\n \n    def forward(self, x1):\n        v1 = self.fc(x1)\n        v2 = torch.relu(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 65 * 1024)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(64, 32)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1.relu()\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 1)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.relu(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, input_dim, output_dim):\n        super().__init__()\n        self.linear = torch.nn.Linear(input_dim, output_dim)\n \n    def forward(self, x1):\n        return torch.relu(self.linear(x1))\n\n# Initializing the model\nm = Model(10, 4)\n\n# Inputs to the model\nx1 = torch.randn(1, 10)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 4)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.relu(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(2, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(128, 64)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.nn.functional.relu(v1)\n        return v2\n\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 128)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.linear1 = torch.nn.Linear(2, 4)\n        self.linear2 = torch.nn.Linear(4, 2)\n \n    def forward(self, x):\n        v1 = self.linear1(x)\n        v2 = F.relu(v1)\n        v3 = self.linear2(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx = torch.randn(2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 12)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.nn.functional.relu(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(2, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(16, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.nn.functional.relu(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(2, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(16, 16)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.relu(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc = torch.nn.Linear(65 * 1024, 2048)\n \n    def forward(self, x1):\n        v1 = self.fc(x1)\n        v2 = torch.relu(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 65 * 1024)\n"
            ],
            "g_time": 5.108187437057495
        }
    }
}
{
    "opt": "unary=29",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min_value=3.07202, max_value=0.0):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(3, 3, 2, stride=2, padding=1)\n        self.min_value = min_value\n        self.max_value = max_value\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = torch.clamp_min(v1, self.min_value)\n        v3 = torch.clamp_max(v2, self.max_value)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 3, 81, 81)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min_value=0.26788444244146534, max_value=-5.072126746641376):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(56, 4, (3, 2), stride=(4, 2), padding=(0, 0))\n        self.min_value = min_value\n        self.max_value = max_value\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = torch.clamp_min(v1, self.min_value)\n        v3 = torch.clamp_max(v2, self.max_value)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 56, 4, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min_value=1.25, max_value=7.225):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(16, 16, (3, 3), stride=(1, 1), padding=(1, 1))\n        self.min_value = min_value\n        self.max_value = max_value\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = torch.clamp_min(v1, self.min_value)\n        v3 = torch.clamp_max(v2, self.max_value)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 16, 10, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min_value=-1.0689e+99, max_value=4.4123e+11):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose1d(32, 2, 2, stride=2, padding=0, bias=True)\n        self.min_value = min_value\n        self.max_value = max_value\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = torch.clamp_min(v1, self.min_value)\n        v3 = torch.clamp_max(v2, self.max_value)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 32, 6, 5)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min_value=-30.5614, max_value=3.71393):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose3d(1, 25, 6, stride=2, padding=0)\n        self.min_value = min_value\n        self.max_value = max_value\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = torch.clamp_min(v1, self.min_value)\n        v3 = torch.clamp_max(v2, self.max_value)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 1, 3, 13, 15)\n",
                "\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self._21 = nn.ConvTranspose2d(5, 4, (3, 3), stride=1, padding=(1, 1), bias=True)\n\n    def forward(self, x1):\n        v1 = self._21(x1)\n        v2 = torch.clamp_min(v1, -0.662077819824219)\n        v3 = torch.clamp_min(v2, -0.0572269584655761)\n        v4 = torch.clamp_max(v3, 0.2742073059082031)\n        v5 = v4.detach()\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 5, 3, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min_value=-0.127523, max_value=2.2004):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(3, 4, 3, stride=3, padding=1)\n        self.min_value = min_value\n        self.max_value = max_value\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = torch.clamp_min(v1, self.min_value)\n        v3 = torch.clamp_max(v2, self.max_value)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 3, 14, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min_value=8.0, max_value=-15):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(256, 128, 8, padding=3)\n        self.min_value = min_value\n        self.max_value = max_value\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = torch.clamp_min(v1, self.min_value)\n        v3 = torch.clamp_max(v2, self.max_value)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 256, 302, 126)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min_value=2.22045e-16, max_value=299.0):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(1, 3, (3, 3), stride=(2, 2), padding=(0, 1))\n        self.min_value = min_value\n        self.max_value = max_value\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = torch.clamp_min(v1, self.min_value)\n        v3 = torch.clamp_max(v2, self.max_value)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 1, 2, 9)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min_value=0.3, max_value=0.4):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(32, 32, (1, 1))\n        self.min_value = min_value\n        self.max_value = max_value\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = torch.clamp_min(v1, self.min_value)\n        v3 = torch.clamp_max(v2, self.max_value)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 32, 2, 2)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min_value=3.07202, max_value=0.0):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(3, 3, 2, stride=2, padding=1)\n        self.min_value = min_value\n        self.max_value = max_value\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = torch.clamp_min(v1, self.min_value)\n        v3 = torch.clamp_max(v2, self.max_value)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 3, 81, 81)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min_value=0.26788444244146534, max_value=-5.072126746641376):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(56, 4, (3, 2), stride=(4, 2), padding=(0, 0))\n        self.min_value = min_value\n        self.max_value = max_value\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = torch.clamp_min(v1, self.min_value)\n        v3 = torch.clamp_max(v2, self.max_value)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 56, 4, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min_value=1.25, max_value=7.225):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(16, 16, (3, 3), stride=(1, 1), padding=(1, 1))\n        self.min_value = min_value\n        self.max_value = max_value\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = torch.clamp_min(v1, self.min_value)\n        v3 = torch.clamp_max(v2, self.max_value)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 16, 10, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min_value=-1.0689e+99, max_value=4.4123e+11):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose1d(32, 2, 2, stride=2, padding=0, bias=True)\n        self.min_value = min_value\n        self.max_value = max_value\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = torch.clamp_min(v1, self.min_value)\n        v3 = torch.clamp_max(v2, self.max_value)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 32, 6, 5)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min_value=-30.5614, max_value=3.71393):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose3d(1, 25, 6, stride=2, padding=0)\n        self.min_value = min_value\n        self.max_value = max_value\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = torch.clamp_min(v1, self.min_value)\n        v3 = torch.clamp_max(v2, self.max_value)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 1, 3, 13, 15)\n",
                "\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self._21 = nn.ConvTranspose2d(5, 4, (3, 3), stride=1, padding=(1, 1), bias=True)\n\n    def forward(self, x1):\n        v1 = self._21(x1)\n        v2 = torch.clamp_min(v1, -0.662077819824219)\n        v3 = torch.clamp_min(v2, -0.0572269584655761)\n        v4 = torch.clamp_max(v3, 0.2742073059082031)\n        v5 = v4.detach()\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 5, 3, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min_value=-0.127523, max_value=2.2004):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(3, 4, 3, stride=3, padding=1)\n        self.min_value = min_value\n        self.max_value = max_value\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = torch.clamp_min(v1, self.min_value)\n        v3 = torch.clamp_max(v2, self.max_value)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 3, 14, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min_value=8.0, max_value=-15):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(256, 128, 8, padding=3)\n        self.min_value = min_value\n        self.max_value = max_value\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = torch.clamp_min(v1, self.min_value)\n        v3 = torch.clamp_max(v2, self.max_value)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 256, 302, 126)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min_value=2.22045e-16, max_value=299.0):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(1, 3, (3, 3), stride=(2, 2), padding=(0, 1))\n        self.min_value = min_value\n        self.max_value = max_value\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = torch.clamp_min(v1, self.min_value)\n        v3 = torch.clamp_max(v2, self.max_value)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 1, 2, 9)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min_value=0.3, max_value=0.4):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(32, 32, (1, 1))\n        self.min_value = min_value\n        self.max_value = max_value\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = torch.clamp_min(v1, self.min_value)\n        v3 = torch.clamp_max(v2, self.max_value)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 32, 2, 2)\n"
            ],
            "g_time": 8.117400884628296
        }
    }
}
{
    "opt": "unary=14",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose_16 = torch.nn.ConvTranspose2d(2, 87, 1, stride=1, padding=0)\n    def forward(self, x1):\n        v1 = self.conv_transpose_16(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 2, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose_9 = torch.nn.ConvTranspose2d(2, 5, 1, stride=1, padding=0)\n    def forward(self, x1):\n        v1 = self.conv_transpose_9(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 2, 1, 1)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose_1 = torch.nn.ConvTranspose2d(20, 20, 1, stride=1, padding=0)\n    def forward(self, x1):\n        v1 = self.conv_transpose_1(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 20, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose_58 = torch.nn.ConvTranspose2d(24, 58, 2, stride=2, padding=0)\n    def forward(self, x1):\n        v1 = self.conv_transpose_58(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 24, 30, 30)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose_2 = torch.nn.ConvTranspose2d(1, 1, 1, stride=1, padding=0)\n    def forward(self, x1):\n        v1 = self.conv_transpose_2(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 1, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose_1 = torch.nn.ConvTranspose2d(126, 63, 1, stride=1, padding=0)\n    def forward(self, x1):\n        v1 = self.conv_transpose_1(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 126, 32, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose_44 = torch.nn.ConvTranspose2d(1, 2, 1, stride=1, padding=0)\n    def forward(self, x1):\n        v1 = self.conv_transpose_44(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 1, 128, 128)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose_1 = torch.nn.ConvTranspose2d(91, 91, 1, stride=1, padding=0)\n    def forward(self, x1):\n        v1 = self.conv_transpose_1(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 91, 18, 18)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv2d_3 = torch.nn.Conv2d(3, 3, 1, stride=1, padding=0)\n        self.conv_transpose_1 = torch.nn.ConvTranspose2d(3, 3, 1, stride=1, padding=0)\n    def forward(self, x1):\n        v1 = self.conv2d_3(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = self.conv_transpose_1(v2)\n        v4 = torch.nn.functional.interpolate(v3, scale_factor=3.0, mode='nearest')\n        v5 = v4 * v1\n        v6 = v1 + v5\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 3, 28, 28)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose_1 = torch.nn.ConvTranspose1d(90, 90, 1, stride=1, padding=0)\n    def forward(self, x1):\n        v1 = self.conv_transpose_1(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 90, 18)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose_16 = torch.nn.ConvTranspose2d(2, 87, 1, stride=1, padding=0)\n    def forward(self, x1):\n        v1 = self.conv_transpose_16(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 2, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose_9 = torch.nn.ConvTranspose2d(2, 5, 1, stride=1, padding=0)\n    def forward(self, x1):\n        v1 = self.conv_transpose_9(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 2, 1, 1)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose_1 = torch.nn.ConvTranspose2d(20, 20, 1, stride=1, padding=0)\n    def forward(self, x1):\n        v1 = self.conv_transpose_1(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 20, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose_58 = torch.nn.ConvTranspose2d(24, 58, 2, stride=2, padding=0)\n    def forward(self, x1):\n        v1 = self.conv_transpose_58(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 24, 30, 30)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose_2 = torch.nn.ConvTranspose2d(1, 1, 1, stride=1, padding=0)\n    def forward(self, x1):\n        v1 = self.conv_transpose_2(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 1, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose_1 = torch.nn.ConvTranspose2d(126, 63, 1, stride=1, padding=0)\n    def forward(self, x1):\n        v1 = self.conv_transpose_1(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 126, 32, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose_44 = torch.nn.ConvTranspose2d(1, 2, 1, stride=1, padding=0)\n    def forward(self, x1):\n        v1 = self.conv_transpose_44(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 1, 128, 128)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose_1 = torch.nn.ConvTranspose2d(91, 91, 1, stride=1, padding=0)\n    def forward(self, x1):\n        v1 = self.conv_transpose_1(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 91, 18, 18)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv2d_3 = torch.nn.Conv2d(3, 3, 1, stride=1, padding=0)\n        self.conv_transpose_1 = torch.nn.ConvTranspose2d(3, 3, 1, stride=1, padding=0)\n    def forward(self, x1):\n        v1 = self.conv2d_3(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = self.conv_transpose_1(v2)\n        v4 = torch.nn.functional.interpolate(v3, scale_factor=3.0, mode='nearest')\n        v5 = v4 * v1\n        v6 = v1 + v5\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 3, 28, 28)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose_1 = torch.nn.ConvTranspose1d(90, 90, 1, stride=1, padding=0)\n    def forward(self, x1):\n        v1 = self.conv_transpose_1(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 90, 18)\n"
            ],
            "g_time": 7.391427516937256
        }
    }
}
{
    "opt": "sfdp=3",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self, dropout_p=0.42):\n        super().__init__()\n        self.dropout_p = dropout_p\n\n    def _generate_bias(self, query):\n        batch_size = query.shape[0]\n        num_heads = query.shape[1]\n        n = int(query.shape[2])\n        return torch.randn(num_heads, batch_size, n, n)\n\n    def forward(self, query, key, value, scale_factor):\n        bias = self._generate_bias(query)\n        qk = torch.matmul(query, key.transpose(-2, -1))\n        scaled_qk = qk.mul(scale_factor)\n        softmax_qk = scaled_qk.softmax(dim=-1)\n        d_q = torch.nn.functional.dropout(softmax_qk, p=self.dropout_p)\n        output = d_q.matmul(value)\n        return output\n\n# Initializing the model\nquery = torch.randn(16, 8, 512, 64)\nkey = torch.randn(16, 8, 64, 512)\nvalue = torch.randn(16, 8, 512, 64)\nscale_factor = torch.randn(16, 8, 1, 1)\n\nm = Model()\n",
                "\n\nclass Model(torch.nn.Module):\n  ...\n    def scaled_dot_product_attention(self, query, key, value, mask=None, scale_factor=1/sqrt(query.shape[-1])):\n        ",
                "\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, n_head, dim_q, dim_k, dim_v, dropout_p):\n        super().__init__()\n        self.n_head = n_head\n        self.dim_q = dim_q\n        self.dim_k = dim_k\n        self.dim_v = dim_v\n        if dim_k!= dim_v:\n            raise ValueError('the dimension of kernal of key and value tensor must be equal!')\n        self.dim_model = n_head * dim_v\n        self.w_q = nn.Linear(dim_q, self.dim_model)\n        self.w_k = nn.Linear(dim_k, self.dim_model)\n        self.w_v = nn.Linear(dim_v, self.dim_model)\n        self.dropout_layer = nn.Dropout(p=dropout_p)\n\n        self.layer_norm = nn.LayerNorm(self.dim_q)\n \n    def forward(self, query, key, value, attn_mask=None):\n        residual = query\n        batch_size = query.size(0)\n        query = self.layer_norm(query)\n\n        query = self.w_q(query)\n        key = self.w_k(key)\n        value = self.w_v(value)\n\n        query_parts = torch.split(query, split_size_or_sections=self.dim_v, dim=-1)\n        key_parts = torch.split(key, split_size_or_sections=self.dim_v, dim=-1)\n        value_parts = torch.split(value, split_size_or_sections=self.dim_v, dim=-1)\n\n        output_parts = []\n        scale_factor = 1 / math.sqrt(self.dim_k)\n        for i in range(self.n_head): \n            q = query_parts[i]\n            k = key_parts[i]\n            v = value_parts[i]\n            qk = torch.matmul(q, k.transpose(-2, -1))\n            scaled_qk = qk.mul(scale_factor)\n            softmax_qk = scaled_qk.softmax(dim=-1)\n            if attn_mask is not None:\n                softmax_qk = softmax_qk * attn_mask\n            dropout_qk = torch.nn.functional.dropout(softmax_qk, p=self.dropout_layer.p)\n            output_part = dropout_qk.matmul(v)\n            output_parts.append(output_part)\n\n        output = torch.cat(output_parts, dim=-1).view(batch_size, -1, self.dim_model)\n        output = output + residual\n        return output\n\n# Initializing the model\ndim_query = dim_key = dim_value = 128\nn_head = 4\ndropout = 0.0\nm = MultiHeadAttention(n_head, self.dim_query, self.dim_key, self.dim_value, dropout_p=0)\n\n# Inputs to the model\nx1 = torch.randn(1, 16, 128)\nx2 = torch.randn(1, 16, 128)\nx3 = torch.randn(1, 16, 128)\n",
                "\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, d_model, num_heads):\n        super().__init__()\n        self.d_model = d_model\n        self.d_k = d_model // num_heads\n        self.h = num_heads\n        self.linears = nn.ModuleList([nn.Linear(d_model, d_model) for _ in range(3)])\n        self. ScaledDotProductAttention()\n \n    def ScaledDotProductAttention(self, query, key, value, scale_factor=1, dropout_p=0.0):\n        q_s = query.shape[:-1]\n        k_s = key.shape[:-1]\n        v_s = value.shape[:-1]\n        assert (q_s[:-1] == k_s[:-1])\n        assert (v_s[:-1] == k_s[:-1])\n        head_dim = q_s[-1]\n        head = torch.matmul(query.reshape(-1, head_dim), key.transpose(-2, -1).reshape(-1, head_dim))\n        scaled_head = head.mul(scale_factor)\n        softmax_head = scaled_head.softmax(dim=-1).reshape(*q_s, *v_s, head_dim)\n        doutput = torch.nn.functional.dropout(softmax_head, p=dropout_p)\n        output = doutput.matmul(value)\n        return output\n \n    def forward(self, x):\n        nbatch, seq_max, embed_dim = x.shape\n        qkv = [l(x).view(nbatch, seq_max, self.h, self.d_k).transpose(1, 2) for l in self.linears[:-1]]\n        q, k, v = qkv\n        scale_factor = self.d_k ** -0.5\n        o = self.ScaledDotProductAttention(q, k, v, scale_factor)\n        o = o.transpose(1, 2).contiguous().view(nbatch, seq_max, self.h * self.d_k)\n        o = self.linears[-1](o)\n        return o\n\n# Initializing the model\nm = MultiHeadAttention(2, 2)\n\n# Inputs to the model\nx1 = torch.randn(1, 4, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.embedding_dim = config['embedding_dim']\n        self.nb_heads = config['nb_heads']\n        self.nb_blocks = config['nb_blocks']\n\n        self.scale_factor = 1.0 / (self.embedding_dim ** 0.5)\n        self.dropout_p = 0.1\n\n        self.query = torch.nn.Linear(self.embedding_dim, self.embedding_dim)\n        self.key = torch.nn.Linear(self.embedding_dim, self.embedding_dim)\n        self.value = torch.nn.Linear(self.embedding_dim, self.embedding_dim)\n\n    def forward(self, inputs):\n        query = self.query(inputs)\n        key = self.key(inputs)\n        value = self.value(inputs)\n        \n        res = []\n        res += query.unsqueeze(1).split([self.nb_heads[i] for i in range(self.nb_blocks)], dim=-1)\n        res += key.split([self.nb_heads[i] for i in range(self.nb_blocks)], dim=-1)\n        res += value.split([self.nb_heads[i] for i in range(self.nb_blocks)], dim=-1)\n                \n        out, attn = res[0], res[1:]\n        for i in range(self.nb_blocks):\n            temp, attn = out, attn[i]\n            out, attn = self.attention(out, attn)\n            out = out.add(temp)\n \n        outputs = out.split([self.nb_heads[i] for i in range(self.nb_blocks)], dim=-1)\n \n        res = outputs[0]\n        for i in range(1, len(outputs)):\n            res += out[i]\n\n        return res\n\n    def attention(self, query, key):\n        qk = torch.matmul(query, key.transpose(-2, -1))\n        scaled_qk = qk.mul(self.scale_factor)\n        softmax_qk = scaled_qk.softmax(dim=-1)\n        dropout_qk = torch.nn.functional.dropout(softmax_qk, p=dropout_p)\n        return dropout_qk.matmul(value), dropout_qk\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, hidden_size):\n        super().__init__()\n        self.hidden_size = hidden_size\n        self.scale_factor = 16384\n        self.dropout_p = 0.5\n\n    def forward(self, query, key, value):\n        qk = torch.matmul(query, key.transpose(-2, -1))\n        scaled_qk = qk.mul(self.scale_factor)\n        softmax_qk = scaled_qk.softmax(dim=-1)\n        dropout_qk = torch.nn.functional.dropout(softmax_qk, p=self.dropout_p)\n        output = dropout_qk.matmul(value)\n        return output\n\n# Initializing the model\nm = Model(hidden_size=128)\n\n# Inputs to the model\nquery = torch.randn(4, 8, 128)\nkey = torch.randn(4, 16, 128)\nvalue = torch.randn(4, 16, 128)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, batch, dimension, hidden_dimension, dropout_rate):\n        super().__init__()\n        self.w_hidden = torch.nn.Linear(dimension, hidden_dimension)\n        self.w_out = torch.nn.Linear(hidden_dimension, dimension)\n        self.dropout = torch.nn.Dropout(p=dropout_rate)\n\n    def forward(self, q, k, v, padding_mask=None):\n        x = torch.matmul(q, k.transpose(-2, -1))\n        x = self.dropout(x)\n        x = self.w_hidden(x)\n        x = torch.tanh(x)\n        x = self.dropout(x)\n        x = torch.matmul(x, v)\n        return x\n\n# Initializing the model\nb = 4\nd = 16\nh = 8\ndropout_rate = 0.25\nm = Model(b, d, h, dropout_rate)\n\n# Inputs to the model\nq = torch.randn(b, d)\nk = torch.randn(b, d)\nv = torch.randn(b, d)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, attention_n_head=5, attention_head_size=16, attention_dropout_rate=0.1):\n        super().__init__()\n        self.attention_n_head = attention_n_head\n        self.attention_head_size = attention_head_size\n        self.attention_dropout_rate = attention_dropout_rate\n        self.dense = torch.nn.Linear(256, attention_n_head * self.attention_head_size)\n        self.query_layer = torch.nn.Linear(256, attention_n_head * self.attention_head_size)\n        self.key_layer = torch.nn.Linear(256, attention_n_head * self.attention_head_size)\n        self.value_layer = torch.nn.Linear(256, attention_n_head * self.attention_head_size)\n        self.dropout = torch.nn.Dropout(attention_dropout_rate)\n        self.out = torch.nn.Linear(attention_n_head * self.attention_head_size, 256)\n \n    def forward(self, query, key, value, mask):\n        batch_size = value.shape[0]\n        hidden_size = value.shape[-1]\n        scaled_dot = torch.matmul(query, key.transpose(-2, -1)) \\\n             / math.pow(self.attention_head_size, 0.5)\n        output = torch.softmax(scaled_dot, dim=-1) + mask[:, None, None, :]\n        output = self.dropout(output)\n        output = torch.matmul(output, value)\n        concatenated = output.view(batch_size, -1, self.n_head * self.attention_head_size)\n        return self.out(concatenated), output\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nquery = torch.randn(1, 8, 256)\nkey = torch.randn(1, 8, 256)\nvalue = torch.randn(1, 8, 256)\nmask = torch.tensor([[0, 1, 1, 1, 1, 1, 1, 1]])\n__output__, __attention__ = m(query, key, value, mask)\n",
                "\n    class Model(torch.nn.Module):\n        def __init__(self):\n            super().__init__()\n            self.softmax_qk = torch.nn.Softmax(dim=-1)\n            self.dropout_qk = torch.nn.Dropout(dropout_p)\n            self.output_proj = torch.nn.Linear(hidden_size, output_size)\n \n        def forward(self, q, k, v, mask):\n            qk = torch.matmul(q, k.transpose(-2, -1))\n            scaled_qk = qk.mul(scale_factor)\n            softmax_qk = self.softmax_qk(scaled_qk)\n            dropout_qk = self.dropout_qk(softmax_qk)\n            output = torch.matmul(dropout_qk, v)\n            return output\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nq = torch.randn(1, 16, 16, HIDDEN_SIZE)\nk = torch.randn(1, 16, 16, HIDDEN_SIZE)\nv = torch.randn(1, 16, 16, HIDDEN_SIZE)\nmask = torch.ones(16, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, num_heads=12):\n        super().__init__()\n        self.num_heads = num_heads\n\n    def forward(self, q, k, v, scale_factor, dropout_p):\n        qk = torch.matmul(q, k.transpose(-2, -1))\n        scaled_qk = qk.mul(scale_factor)\n        softmax_qk = scaled_qk.softmax(dim=-1)\n        dropout_qk = torch.nn.functional.dropout(softmax_qk, p=dropout_p)\n        output = dropout_qk.matmul(v)\n        return output\n\n# Initializing the model\nm = Model(num_heads=12)\n\n# Inputs to the model\nq = torch.randn(1, 12, 64, 64)\nk = torch.randn(1, 12, 256, 64)\nv = torch.randn(1, 12, 256, 64)\nscale_factor = torch.randn(12)\ndropout_p = 0.2\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self, dropout_p=0.42):\n        super().__init__()\n        self.dropout_p = dropout_p\n\n    def _generate_bias(self, query):\n        batch_size = query.shape[0]\n        num_heads = query.shape[1]\n        n = int(query.shape[2])\n        return torch.randn(num_heads, batch_size, n, n)\n\n    def forward(self, query, key, value, scale_factor):\n        bias = self._generate_bias(query)\n        qk = torch.matmul(query, key.transpose(-2, -1))\n        scaled_qk = qk.mul(scale_factor)\n        softmax_qk = scaled_qk.softmax(dim=-1)\n        d_q = torch.nn.functional.dropout(softmax_qk, p=self.dropout_p)\n        output = d_q.matmul(value)\n        return output\n\n# Initializing the model\nquery = torch.randn(16, 8, 512, 64)\nkey = torch.randn(16, 8, 64, 512)\nvalue = torch.randn(16, 8, 512, 64)\nscale_factor = torch.randn(16, 8, 1, 1)\n\nm = Model()\n",
                "\n\nclass Model(torch.nn.Module):\n  ...\n    def scaled_dot_product_attention(self, query, key, value, mask=None, scale_factor=1/sqrt(query.shape[-1])):\n        ",
                "\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, n_head, dim_q, dim_k, dim_v, dropout_p):\n        super().__init__()\n        self.n_head = n_head\n        self.dim_q = dim_q\n        self.dim_k = dim_k\n        self.dim_v = dim_v\n        if dim_k!= dim_v:\n            raise ValueError('the dimension of kernal of key and value tensor must be equal!')\n        self.dim_model = n_head * dim_v\n        self.w_q = nn.Linear(dim_q, self.dim_model)\n        self.w_k = nn.Linear(dim_k, self.dim_model)\n        self.w_v = nn.Linear(dim_v, self.dim_model)\n        self.dropout_layer = nn.Dropout(p=dropout_p)\n\n        self.layer_norm = nn.LayerNorm(self.dim_q)\n \n    def forward(self, query, key, value, attn_mask=None):\n        residual = query\n        batch_size = query.size(0)\n        query = self.layer_norm(query)\n\n        query = self.w_q(query)\n        key = self.w_k(key)\n        value = self.w_v(value)\n\n        query_parts = torch.split(query, split_size_or_sections=self.dim_v, dim=-1)\n        key_parts = torch.split(key, split_size_or_sections=self.dim_v, dim=-1)\n        value_parts = torch.split(value, split_size_or_sections=self.dim_v, dim=-1)\n\n        output_parts = []\n        scale_factor = 1 / math.sqrt(self.dim_k)\n        for i in range(self.n_head): \n            q = query_parts[i]\n            k = key_parts[i]\n            v = value_parts[i]\n            qk = torch.matmul(q, k.transpose(-2, -1))\n            scaled_qk = qk.mul(scale_factor)\n            softmax_qk = scaled_qk.softmax(dim=-1)\n            if attn_mask is not None:\n                softmax_qk = softmax_qk * attn_mask\n            dropout_qk = torch.nn.functional.dropout(softmax_qk, p=self.dropout_layer.p)\n            output_part = dropout_qk.matmul(v)\n            output_parts.append(output_part)\n\n        output = torch.cat(output_parts, dim=-1).view(batch_size, -1, self.dim_model)\n        output = output + residual\n        return output\n\n# Initializing the model\ndim_query = dim_key = dim_value = 128\nn_head = 4\ndropout = 0.0\nm = MultiHeadAttention(n_head, self.dim_query, self.dim_key, self.dim_value, dropout_p=0)\n\n# Inputs to the model\nx1 = torch.randn(1, 16, 128)\nx2 = torch.randn(1, 16, 128)\nx3 = torch.randn(1, 16, 128)\n",
                "\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, d_model, num_heads):\n        super().__init__()\n        self.d_model = d_model\n        self.d_k = d_model // num_heads\n        self.h = num_heads\n        self.linears = nn.ModuleList([nn.Linear(d_model, d_model) for _ in range(3)])\n        self. ScaledDotProductAttention()\n \n    def ScaledDotProductAttention(self, query, key, value, scale_factor=1, dropout_p=0.0):\n        q_s = query.shape[:-1]\n        k_s = key.shape[:-1]\n        v_s = value.shape[:-1]\n        assert (q_s[:-1] == k_s[:-1])\n        assert (v_s[:-1] == k_s[:-1])\n        head_dim = q_s[-1]\n        head = torch.matmul(query.reshape(-1, head_dim), key.transpose(-2, -1).reshape(-1, head_dim))\n        scaled_head = head.mul(scale_factor)\n        softmax_head = scaled_head.softmax(dim=-1).reshape(*q_s, *v_s, head_dim)\n        doutput = torch.nn.functional.dropout(softmax_head, p=dropout_p)\n        output = doutput.matmul(value)\n        return output\n \n    def forward(self, x):\n        nbatch, seq_max, embed_dim = x.shape\n        qkv = [l(x).view(nbatch, seq_max, self.h, self.d_k).transpose(1, 2) for l in self.linears[:-1]]\n        q, k, v = qkv\n        scale_factor = self.d_k ** -0.5\n        o = self.ScaledDotProductAttention(q, k, v, scale_factor)\n        o = o.transpose(1, 2).contiguous().view(nbatch, seq_max, self.h * self.d_k)\n        o = self.linears[-1](o)\n        return o\n\n# Initializing the model\nm = MultiHeadAttention(2, 2)\n\n# Inputs to the model\nx1 = torch.randn(1, 4, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.embedding_dim = config['embedding_dim']\n        self.nb_heads = config['nb_heads']\n        self.nb_blocks = config['nb_blocks']\n\n        self.scale_factor = 1.0 / (self.embedding_dim ** 0.5)\n        self.dropout_p = 0.1\n\n        self.query = torch.nn.Linear(self.embedding_dim, self.embedding_dim)\n        self.key = torch.nn.Linear(self.embedding_dim, self.embedding_dim)\n        self.value = torch.nn.Linear(self.embedding_dim, self.embedding_dim)\n\n    def forward(self, inputs):\n        query = self.query(inputs)\n        key = self.key(inputs)\n        value = self.value(inputs)\n        \n        res = []\n        res += query.unsqueeze(1).split([self.nb_heads[i] for i in range(self.nb_blocks)], dim=-1)\n        res += key.split([self.nb_heads[i] for i in range(self.nb_blocks)], dim=-1)\n        res += value.split([self.nb_heads[i] for i in range(self.nb_blocks)], dim=-1)\n                \n        out, attn = res[0], res[1:]\n        for i in range(self.nb_blocks):\n            temp, attn = out, attn[i]\n            out, attn = self.attention(out, attn)\n            out = out.add(temp)\n \n        outputs = out.split([self.nb_heads[i] for i in range(self.nb_blocks)], dim=-1)\n \n        res = outputs[0]\n        for i in range(1, len(outputs)):\n            res += out[i]\n\n        return res\n\n    def attention(self, query, key):\n        qk = torch.matmul(query, key.transpose(-2, -1))\n        scaled_qk = qk.mul(self.scale_factor)\n        softmax_qk = scaled_qk.softmax(dim=-1)\n        dropout_qk = torch.nn.functional.dropout(softmax_qk, p=dropout_p)\n        return dropout_qk.matmul(value), dropout_qk\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, hidden_size):\n        super().__init__()\n        self.hidden_size = hidden_size\n        self.scale_factor = 16384\n        self.dropout_p = 0.5\n\n    def forward(self, query, key, value):\n        qk = torch.matmul(query, key.transpose(-2, -1))\n        scaled_qk = qk.mul(self.scale_factor)\n        softmax_qk = scaled_qk.softmax(dim=-1)\n        dropout_qk = torch.nn.functional.dropout(softmax_qk, p=self.dropout_p)\n        output = dropout_qk.matmul(value)\n        return output\n\n# Initializing the model\nm = Model(hidden_size=128)\n\n# Inputs to the model\nquery = torch.randn(4, 8, 128)\nkey = torch.randn(4, 16, 128)\nvalue = torch.randn(4, 16, 128)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, batch, dimension, hidden_dimension, dropout_rate):\n        super().__init__()\n        self.w_hidden = torch.nn.Linear(dimension, hidden_dimension)\n        self.w_out = torch.nn.Linear(hidden_dimension, dimension)\n        self.dropout = torch.nn.Dropout(p=dropout_rate)\n\n    def forward(self, q, k, v, padding_mask=None):\n        x = torch.matmul(q, k.transpose(-2, -1))\n        x = self.dropout(x)\n        x = self.w_hidden(x)\n        x = torch.tanh(x)\n        x = self.dropout(x)\n        x = torch.matmul(x, v)\n        return x\n\n# Initializing the model\nb = 4\nd = 16\nh = 8\ndropout_rate = 0.25\nm = Model(b, d, h, dropout_rate)\n\n# Inputs to the model\nq = torch.randn(b, d)\nk = torch.randn(b, d)\nv = torch.randn(b, d)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, attention_n_head=5, attention_head_size=16, attention_dropout_rate=0.1):\n        super().__init__()\n        self.attention_n_head = attention_n_head\n        self.attention_head_size = attention_head_size\n        self.attention_dropout_rate = attention_dropout_rate\n        self.dense = torch.nn.Linear(256, attention_n_head * self.attention_head_size)\n        self.query_layer = torch.nn.Linear(256, attention_n_head * self.attention_head_size)\n        self.key_layer = torch.nn.Linear(256, attention_n_head * self.attention_head_size)\n        self.value_layer = torch.nn.Linear(256, attention_n_head * self.attention_head_size)\n        self.dropout = torch.nn.Dropout(attention_dropout_rate)\n        self.out = torch.nn.Linear(attention_n_head * self.attention_head_size, 256)\n \n    def forward(self, query, key, value, mask):\n        batch_size = value.shape[0]\n        hidden_size = value.shape[-1]\n        scaled_dot = torch.matmul(query, key.transpose(-2, -1)) \\\n             / math.pow(self.attention_head_size, 0.5)\n        output = torch.softmax(scaled_dot, dim=-1) + mask[:, None, None, :]\n        output = self.dropout(output)\n        output = torch.matmul(output, value)\n        concatenated = output.view(batch_size, -1, self.n_head * self.attention_head_size)\n        return self.out(concatenated), output\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nquery = torch.randn(1, 8, 256)\nkey = torch.randn(1, 8, 256)\nvalue = torch.randn(1, 8, 256)\nmask = torch.tensor([[0, 1, 1, 1, 1, 1, 1, 1]])\n__output__, __attention__ = m(query, key, value, mask)\n",
                "\n    class Model(torch.nn.Module):\n        def __init__(self):\n            super().__init__()\n            self.softmax_qk = torch.nn.Softmax(dim=-1)\n            self.dropout_qk = torch.nn.Dropout(dropout_p)\n            self.output_proj = torch.nn.Linear(hidden_size, output_size)\n \n        def forward(self, q, k, v, mask):\n            qk = torch.matmul(q, k.transpose(-2, -1))\n            scaled_qk = qk.mul(scale_factor)\n            softmax_qk = self.softmax_qk(scaled_qk)\n            dropout_qk = self.dropout_qk(softmax_qk)\n            output = torch.matmul(dropout_qk, v)\n            return output\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nq = torch.randn(1, 16, 16, HIDDEN_SIZE)\nk = torch.randn(1, 16, 16, HIDDEN_SIZE)\nv = torch.randn(1, 16, 16, HIDDEN_SIZE)\nmask = torch.ones(16, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, num_heads=12):\n        super().__init__()\n        self.num_heads = num_heads\n\n    def forward(self, q, k, v, scale_factor, dropout_p):\n        qk = torch.matmul(q, k.transpose(-2, -1))\n        scaled_qk = qk.mul(scale_factor)\n        softmax_qk = scaled_qk.softmax(dim=-1)\n        dropout_qk = torch.nn.functional.dropout(softmax_qk, p=dropout_p)\n        output = dropout_qk.matmul(v)\n        return output\n\n# Initializing the model\nm = Model(num_heads=12)\n\n# Inputs to the model\nq = torch.randn(1, 12, 64, 64)\nk = torch.randn(1, 12, 256, 64)\nv = torch.randn(1, 12, 256, 64)\nscale_factor = torch.randn(12)\ndropout_p = 0.2\n"
            ],
            "g_time": 23.413921356201172
        }
    }
}
{
    "opt": "unary=27",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min, max):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(64, 3, 1, stride=1, padding=0)\n        self.min = min\n        self.max = max\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = torch.clamp_min(v1, self.min)\n        v3 = torch.clamp_max(v2, self.max)\n        return v3\nmin = -60\nmax = 35\n# Inputs to the model\nx1 = torch.randn(1, 64, 80, 400)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min, max):\n        super().__init__()\n        self.add = torch.nn.ModuleList([torch.nn.Conv2d(1, 1, 1), torch.nn.Conv2d(1, 1, 1)])\n        self.min = min\n        self.max = max\n    def forward(self, x1):\n        v1 = self.add[0](x1)\n        v2 = torch.clamp_min(v1, self.min)\n        v3 = torch.clamp_max(v2, self.max)\n        v4 = v3 + v3\n        v5 = self.add[1](v4)\n        v6 = torch.clamp_min(v5, self.min)\n        v7 = torch.clamp_max(v6, self.max)\n        return v7\nmin = 0.5\nmax = 1.0\n# Inputs to the model\nx1 = torch.randn(1, 1, 768, 1)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min, max):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(1, 128, 1, stride=1, padding=0)\n        self.min = min\n        self.max = max\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = torch.clamp_min(v1, self.min)\n        v3 = torch.clamp_max(v2, self.max)\n        return v3\nmin = 1\nmax = 127\n# Inputs to the model\nx1 = torch.randn(1, 1, 1, 128)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min, max):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 1, 4, stride=1, padding=0)\n        self.min = min\n        self.max = max\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = torch.clamp_min(v1, self.min)\n        v3 = torch.clamp_max(v2, self.max)\n        return v3\nmin = 0.3\nmax = 3\n# Inputs to the model\nx1 = torch.randn(1, 3, 16, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min, max):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(5, 1, 5, stride=1, padding=2)\n        self.min = min\n        self.max = max\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = torch.clamp_min(v1, self.min)\n        v3 = torch.clamp_max(v2, self.max)\n        return v3\nmin = 2.0\nmax = 3.14159\n# Inputs to the model\nx1 = torch.randn(1, 5, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min, max):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(1, 7, 1, stride=1, padding=0)\n        self.min = min\n        self.max = max\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = torch.clamp_min(v1, self.min)\n        v3 = torch.clamp_max(v2, self.max)\n        return v3\nmin = 10\nmax = 20\n# Inputs to the model\nx1 = torch.randn(1000, 1, 2, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min, max):\n        super().__init__()\n        self.conv = torch.nn.Conv1d(3, 1, 1, stride=1, padding=0)\n        self.batchnorm = torch.nn.BatchNorm1d(3)\n        self.conv1 = torch.nn.Conv1d(3, 1, 1, stride=1, padding=0)\n        self.avgpool = torch.nn.AvgPool1d(1, stride=1, padding=0)\n        self.conv2 = torch.nn.Conv1d(3, 1, 1, stride=1, padding=0)\n        self.min = min\n        self.max = max\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v1 = self.batchnorm(v1)\n        v1 = v1.type(torch.int32)\n        v2 = self.conv1(v1)\n        v3 = self.avgpool(v2)\n        v4 = self.conv2(v3)\n        v5 = torch.clamp_min(v4, self.min)\n        v6 = torch.clamp_max(v5, self.max)\n        return v6\nmin = 100\nmax = 10000\n# Inputs to the model\nx1 = torch.randn(32, 3, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min, max):\n        super().__init__()\n        self.conv = torch.nn.MaxPool2d(kernel_size=(14, 2), stride=(3, 1))\n        self.min = min\n        self.max = max\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = torch.clamp_min(v1, self.min)\n        v3 = torch.clamp_max(v2, self.max)\n        return v3\nmin = 0.2\nmax = 4\n# Inputs to the model\nx1 = torch.randn(1, 1, 64, 115)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min, max):\n        super().__init__()\n        self.relu = torch.nn.ReLU()\n        self.conv = torch.nn.Conv2d(32, 4, 5, stride=1, padding=2)\n        self.conv1 = torch.nn.Conv2d(32, 4, 5, stride=1, padding=2)\n        self.min = min\n        self.max = max\n    def forward(self, x1):\n        v0 = x1\n        v1 = self.relu(v0)\n        v2 = self.conv(v1)\n        v3 = self.conv1(v1)\n        v4 = v2 + v3\n        v5 = torch.clamp_min(v4, self.min)\n        v6 = torch.clamp_max(v5, self.max)\n        return v6\nmin = 0.5\nmax = 1.0\n# Inputs to the model\nx1 = torch.randn(1, 32, 28, 64)\n",
                "2\nclass Model(torch.nn.Module):\n    def __init__(self, min, max):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(10, 3, (1, 100), stride=(1, 1), padding=(0, 1))\n        self.min = min\n        self.max = max\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = torch.clamp_min(v1, self.min)\n        v3 = torch.clamp_max(v2, self.max)\n        return v3\nmin = 1\nmax = 1\n# Inputs to the model\nx1 = torch.randn(1, 10, 1, 416)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min, max):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(64, 3, 1, stride=1, padding=0)\n        self.min = min\n        self.max = max\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = torch.clamp_min(v1, self.min)\n        v3 = torch.clamp_max(v2, self.max)\n        return v3\nmin = -60\nmax = 35\n# Inputs to the model\nx1 = torch.randn(1, 64, 80, 400)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min, max):\n        super().__init__()\n        self.add = torch.nn.ModuleList([torch.nn.Conv2d(1, 1, 1), torch.nn.Conv2d(1, 1, 1)])\n        self.min = min\n        self.max = max\n    def forward(self, x1):\n        v1 = self.add[0](x1)\n        v2 = torch.clamp_min(v1, self.min)\n        v3 = torch.clamp_max(v2, self.max)\n        v4 = v3 + v3\n        v5 = self.add[1](v4)\n        v6 = torch.clamp_min(v5, self.min)\n        v7 = torch.clamp_max(v6, self.max)\n        return v7\nmin = 0.5\nmax = 1.0\n# Inputs to the model\nx1 = torch.randn(1, 1, 768, 1)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min, max):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(1, 128, 1, stride=1, padding=0)\n        self.min = min\n        self.max = max\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = torch.clamp_min(v1, self.min)\n        v3 = torch.clamp_max(v2, self.max)\n        return v3\nmin = 1\nmax = 127\n# Inputs to the model\nx1 = torch.randn(1, 1, 1, 128)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min, max):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 1, 4, stride=1, padding=0)\n        self.min = min\n        self.max = max\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = torch.clamp_min(v1, self.min)\n        v3 = torch.clamp_max(v2, self.max)\n        return v3\nmin = 0.3\nmax = 3\n# Inputs to the model\nx1 = torch.randn(1, 3, 16, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min, max):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(5, 1, 5, stride=1, padding=2)\n        self.min = min\n        self.max = max\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = torch.clamp_min(v1, self.min)\n        v3 = torch.clamp_max(v2, self.max)\n        return v3\nmin = 2.0\nmax = 3.14159\n# Inputs to the model\nx1 = torch.randn(1, 5, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min, max):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(1, 7, 1, stride=1, padding=0)\n        self.min = min\n        self.max = max\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = torch.clamp_min(v1, self.min)\n        v3 = torch.clamp_max(v2, self.max)\n        return v3\nmin = 10\nmax = 20\n# Inputs to the model\nx1 = torch.randn(1000, 1, 2, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min, max):\n        super().__init__()\n        self.conv = torch.nn.Conv1d(3, 1, 1, stride=1, padding=0)\n        self.batchnorm = torch.nn.BatchNorm1d(3)\n        self.conv1 = torch.nn.Conv1d(3, 1, 1, stride=1, padding=0)\n        self.avgpool = torch.nn.AvgPool1d(1, stride=1, padding=0)\n        self.conv2 = torch.nn.Conv1d(3, 1, 1, stride=1, padding=0)\n        self.min = min\n        self.max = max\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v1 = self.batchnorm(v1)\n        v1 = v1.type(torch.int32)\n        v2 = self.conv1(v1)\n        v3 = self.avgpool(v2)\n        v4 = self.conv2(v3)\n        v5 = torch.clamp_min(v4, self.min)\n        v6 = torch.clamp_max(v5, self.max)\n        return v6\nmin = 100\nmax = 10000\n# Inputs to the model\nx1 = torch.randn(32, 3, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min, max):\n        super().__init__()\n        self.conv = torch.nn.MaxPool2d(kernel_size=(14, 2), stride=(3, 1))\n        self.min = min\n        self.max = max\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = torch.clamp_min(v1, self.min)\n        v3 = torch.clamp_max(v2, self.max)\n        return v3\nmin = 0.2\nmax = 4\n# Inputs to the model\nx1 = torch.randn(1, 1, 64, 115)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min, max):\n        super().__init__()\n        self.relu = torch.nn.ReLU()\n        self.conv = torch.nn.Conv2d(32, 4, 5, stride=1, padding=2)\n        self.conv1 = torch.nn.Conv2d(32, 4, 5, stride=1, padding=2)\n        self.min = min\n        self.max = max\n    def forward(self, x1):\n        v0 = x1\n        v1 = self.relu(v0)\n        v2 = self.conv(v1)\n        v3 = self.conv1(v1)\n        v4 = v2 + v3\n        v5 = torch.clamp_min(v4, self.min)\n        v6 = torch.clamp_max(v5, self.max)\n        return v6\nmin = 0.5\nmax = 1.0\n# Inputs to the model\nx1 = torch.randn(1, 32, 28, 64)\n",
                "2\nclass Model(torch.nn.Module):\n    def __init__(self, min, max):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(10, 3, (1, 100), stride=(1, 1), padding=(0, 1))\n        self.min = min\n        self.max = max\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = torch.clamp_min(v1, self.min)\n        v3 = torch.clamp_max(v2, self.max)\n        return v3\nmin = 1\nmax = 1\n# Inputs to the model\nx1 = torch.randn(1, 10, 1, 416)\n"
            ],
            "g_time": 10.887184619903564
        }
    }
}
{
    "opt": "unary=11",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(100, 100, 3, stride=(2, 2), padding=(1, 0))\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v4 / 6\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 100, 64, 128)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose3d(3, 64, 7, stride=(2, 3, 4), dilation=1, padding=(0, 1, 4), output_padding=1, groups=2, bias=False, padding_mode='zeros')\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v4 / 6\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose3d(3, 64, (2, 3, 5), (1, 6, 3), (3, 3, 1), 0)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v4 / 6\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 3, 32, 32, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(1, 1, 150, padding=29)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v4 / 6\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 1, 96, 81)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(1, 1, 200, stride=1, padding=50)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v4 / 6\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 1, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(1, 1, 19, stride=3, padding=11, groups=1, bias=True)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v4 / 6\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 1, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(3, 1, 200, strides=(2, 3), padding=0, output_padding=15, dilation=4)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v4 / 6\n        return v5\n# Inputs to the model\nx1 = torch.randn(2, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(3, 16, (7, 5), (1, 2), None)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v4 / 6\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 3, 100, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(3, 64, 3, (2, 3), 3, (2, 1), 0, 3)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v4 / 6\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 3, 128, 128)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(1, 1, 5, stride=1, groups=2, bias=True, padding=3)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v4 / 6\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 1, 32, 32)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(100, 100, 3, stride=(2, 2), padding=(1, 0))\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v4 / 6\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 100, 64, 128)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose3d(3, 64, 7, stride=(2, 3, 4), dilation=1, padding=(0, 1, 4), output_padding=1, groups=2, bias=False, padding_mode='zeros')\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v4 / 6\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose3d(3, 64, (2, 3, 5), (1, 6, 3), (3, 3, 1), 0)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v4 / 6\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 3, 32, 32, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(1, 1, 150, padding=29)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v4 / 6\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 1, 96, 81)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(1, 1, 200, stride=1, padding=50)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v4 / 6\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 1, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(1, 1, 19, stride=3, padding=11, groups=1, bias=True)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v4 / 6\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 1, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(3, 1, 200, strides=(2, 3), padding=0, output_padding=15, dilation=4)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v4 / 6\n        return v5\n# Inputs to the model\nx1 = torch.randn(2, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(3, 16, (7, 5), (1, 2), None)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v4 / 6\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 3, 100, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(3, 64, 3, (2, 3), 3, (2, 1), 0, 3)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v4 / 6\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 3, 128, 128)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(1, 1, 5, stride=1, groups=2, bias=True, padding=3)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v4 / 6\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 1, 32, 32)\n"
            ],
            "g_time": 7.240551233291626
        }
    }
}
{
    "opt": "unary=6",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 3, 9, stride=4, padding=4)\n        self.bn = torch.nn.BatchNorm2d(3)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = 3 + v1\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v1 * v4\n        v6 = v5 / 6\n        v7 = self.bn(v6)\n        return v7\n# Inputs to the model\nx1 = torch.randn(2, 3, 256, 256)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n    def forward(self, x1):\n        t1 = self.conv(x1)\n        t2 = t1 + 3\n        t3 = torch.clamp_min(t2, 9)\n        t4 = torch.clamp_max(t3, 18)\n        t5 = t1 * t4\n        t6 = t5 / 18\n        t7 = t6 + t6\n        t8 = torch.clamp_min(t7, 0)\n        t9 = torch.clamp_max(t8, 6)\n        t10 = t1 + t3\n        t11 = torch.clamp_min(t10, 0)\n        t12 = torch.clamp_max(t11, 6)\n        t13 = t4 + t7\n        t14 = torch.clamp_min(t13, 0)\n        t15 = torch.clamp_max(t14, 6)\n        return t12 + t15\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n        self.bn = torch.nn.BatchNorm2d(8)\n        self.relu6 = torch.nn.ReLU6(inplace=True)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v1 * v4\n        v6 = v5 / 6\n        v7 = self.relu6(v6)\n        v8 = self.bn(v7)\n        return v8\n# Inputs to the model\nx1 = torch.randn(1, 3, 32, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 4, 1, stride=1, padding=1)\n        self.bn = torch.nn.BatchNorm2d(4)\n    def forward(self, x1, x2):\n        v1 = self.conv(x1)\n        v2 = v1 + x2\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v1 * v4\n        v6 = v5 / 6\n        v7 = self.bn(v6)\n        return v7\n# Inputs to the model\nx1 = torch.randn(1, 3, 32, 32)\nx2 = torch.randn(1, 3, 32, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 64, 7, stride=1, padding=0)\n        self.bn = torch.nn.BatchNorm2d(64)\n    def forward(self, x2, x3):\n        v1 = self.conv(x2)\n        v1 += x3\n        v2 = torch.clamp_min(v1, 0)\n        v3 = torch.clamp_max(v2, 6)\n        v4 = v3 / 6\n        v5 = self.bn(v4)\n        return v5\n# Inputs to the model\nx1 = torch.randn(3, 3, 64, 64)\nx2 = torch.randn(3, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 3, 1, stride=1, padding=1)\n        self.bn = torch.nn.BatchNorm2d(3)\n        self.relu6 = torch.nn.ReLU6(inplace=True)\n        self.conv1 = torch.nn.Conv2d(3, 3, 1, stride=1, padding=1)\n        self.bn1 = torch.nn.BatchNorm2d(3)\n        self.relu61 = torch.nn.ReLU6(inplace=True)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = 3 + v1\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v1 * v4\n        v6 = v5 / 6\n        v7 = self.bn(v6)\n\n        v8 = self.relu6(v7)\n        v9 = self.conv1(v8)\n        v10 = v9 + 3\n        v11 = torch.clamp_min(v10, 0)\n        v12 = torch.clamp_max(v11, 6)\n        v13 = v9 * v12\n        v14 = v13 / 6\n        v15 = self.bn(v14)\n        v16 = self.relu61(v15)\n\n        return v16\n# Inputs to the model\nx1 = torch.randn(2, 3, 32, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n        self.bn = torch.nn.BatchNorm2d(8)\n        self.relu6 = torch.nn.ReLU6(inplace=True)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v1 * v4\n        v6 = v5 / 6\n        v7 = self.relu6(v6)\n        v8 = self.bn(v7)\n        return v8\n# Inputs to the model\nx1 = torch.randn(2, 3, 32, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n        self.bn = torch.nn.BatchNorm2d(8)\n        self.relu6 = torch.nn.ReLU6(inplace=True)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v1 * v4\n        v6 = v5 / 6\n        v7 = self.relu6(v6)\n        v8 = self.bn(v7)\n        return v8\n# Inputs to the model\nx1 = torch.randn(1, 3, 42, 42)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 3, 1, stride=1, padding=1)\n        self.bn = torch.nn.BatchNorm2d(3)\n    def forward(self, x1):\n        # import torchvision\n        # import torch.autograd.profiler as profiler\n        # device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n        # x1 = x1.to(device)\n        # model = torchvision.models.inception_v3(pretrained=True).to(device)\n        # y = model(x1)\n        # with profiler.profile(record_shapes=True, use_cuda=True) as prof:\n        #     with profiler.record_function(\"model_inference\"):\n        #         y = model(x1)\n        # print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))\n        v1 = self.conv(x1)\n        v2 = 3 + v1\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v1 * v4\n        v6 = v5 / 6\n        v7 = self.bn(v6)\n        return v7\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 4, 1, stride=1, padding=1)\n        self.padd = torch.nn.ZeroPad2d((1, 0, 1, 2))\n        self.bn = torch.nn.BatchNorm2d(4)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = 3 + v1\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v1 * v4\n        v6 = v5 / 6\n        v7 = self.padd(v6)\n        v8 = self.bn(v7)\n        return v8\n# Inputs to the model\nx1 = torch.randn(2, 3, 32, 32)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 3, 9, stride=4, padding=4)\n        self.bn = torch.nn.BatchNorm2d(3)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = 3 + v1\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v1 * v4\n        v6 = v5 / 6\n        v7 = self.bn(v6)\n        return v7\n# Inputs to the model\nx1 = torch.randn(2, 3, 256, 256)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n    def forward(self, x1):\n        t1 = self.conv(x1)\n        t2 = t1 + 3\n        t3 = torch.clamp_min(t2, 9)\n        t4 = torch.clamp_max(t3, 18)\n        t5 = t1 * t4\n        t6 = t5 / 18\n        t7 = t6 + t6\n        t8 = torch.clamp_min(t7, 0)\n        t9 = torch.clamp_max(t8, 6)\n        t10 = t1 + t3\n        t11 = torch.clamp_min(t10, 0)\n        t12 = torch.clamp_max(t11, 6)\n        t13 = t4 + t7\n        t14 = torch.clamp_min(t13, 0)\n        t15 = torch.clamp_max(t14, 6)\n        return t12 + t15\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n        self.bn = torch.nn.BatchNorm2d(8)\n        self.relu6 = torch.nn.ReLU6(inplace=True)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v1 * v4\n        v6 = v5 / 6\n        v7 = self.relu6(v6)\n        v8 = self.bn(v7)\n        return v8\n# Inputs to the model\nx1 = torch.randn(1, 3, 32, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 4, 1, stride=1, padding=1)\n        self.bn = torch.nn.BatchNorm2d(4)\n    def forward(self, x1, x2):\n        v1 = self.conv(x1)\n        v2 = v1 + x2\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v1 * v4\n        v6 = v5 / 6\n        v7 = self.bn(v6)\n        return v7\n# Inputs to the model\nx1 = torch.randn(1, 3, 32, 32)\nx2 = torch.randn(1, 3, 32, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 64, 7, stride=1, padding=0)\n        self.bn = torch.nn.BatchNorm2d(64)\n    def forward(self, x2, x3):\n        v1 = self.conv(x2)\n        v1 += x3\n        v2 = torch.clamp_min(v1, 0)\n        v3 = torch.clamp_max(v2, 6)\n        v4 = v3 / 6\n        v5 = self.bn(v4)\n        return v5\n# Inputs to the model\nx1 = torch.randn(3, 3, 64, 64)\nx2 = torch.randn(3, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 3, 1, stride=1, padding=1)\n        self.bn = torch.nn.BatchNorm2d(3)\n        self.relu6 = torch.nn.ReLU6(inplace=True)\n        self.conv1 = torch.nn.Conv2d(3, 3, 1, stride=1, padding=1)\n        self.bn1 = torch.nn.BatchNorm2d(3)\n        self.relu61 = torch.nn.ReLU6(inplace=True)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = 3 + v1\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v1 * v4\n        v6 = v5 / 6\n        v7 = self.bn(v6)\n\n        v8 = self.relu6(v7)\n        v9 = self.conv1(v8)\n        v10 = v9 + 3\n        v11 = torch.clamp_min(v10, 0)\n        v12 = torch.clamp_max(v11, 6)\n        v13 = v9 * v12\n        v14 = v13 / 6\n        v15 = self.bn(v14)\n        v16 = self.relu61(v15)\n\n        return v16\n# Inputs to the model\nx1 = torch.randn(2, 3, 32, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n        self.bn = torch.nn.BatchNorm2d(8)\n        self.relu6 = torch.nn.ReLU6(inplace=True)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v1 * v4\n        v6 = v5 / 6\n        v7 = self.relu6(v6)\n        v8 = self.bn(v7)\n        return v8\n# Inputs to the model\nx1 = torch.randn(2, 3, 32, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n        self.bn = torch.nn.BatchNorm2d(8)\n        self.relu6 = torch.nn.ReLU6(inplace=True)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v1 * v4\n        v6 = v5 / 6\n        v7 = self.relu6(v6)\n        v8 = self.bn(v7)\n        return v8\n# Inputs to the model\nx1 = torch.randn(1, 3, 42, 42)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 3, 1, stride=1, padding=1)\n        self.bn = torch.nn.BatchNorm2d(3)\n    def forward(self, x1):\n        # import torchvision\n        # import torch.autograd.profiler as profiler\n        # device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n        # x1 = x1.to(device)\n        # model = torchvision.models.inception_v3(pretrained=True).to(device)\n        # y = model(x1)\n        # with profiler.profile(record_shapes=True, use_cuda=True) as prof:\n        #     with profiler.record_function(\"model_inference\"):\n        #         y = model(x1)\n        # print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))\n        v1 = self.conv(x1)\n        v2 = 3 + v1\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v1 * v4\n        v6 = v5 / 6\n        v7 = self.bn(v6)\n        return v7\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 4, 1, stride=1, padding=1)\n        self.padd = torch.nn.ZeroPad2d((1, 0, 1, 2))\n        self.bn = torch.nn.BatchNorm2d(4)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = 3 + v1\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v1 * v4\n        v6 = v5 / 6\n        v7 = self.padd(v6)\n        v8 = self.bn(v7)\n        return v8\n# Inputs to the model\nx1 = torch.randn(2, 3, 32, 32)\n"
            ],
            "g_time": 13.55680537223816
        }
    }
}
{
    "opt": "replace_fx",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.dropout1 = torch.nn.ModuleList([torch.nn.Dropout(p=0.5) for _ in range(10)])\n        self.dropout2 = torch.nn.ModuleList([torch.nn.Dropout(p=0.5) for _ in range(10)])\n    def forward(self, x1):\n        x3 = torch.nn.functional.dropout(x1, p=0.5)\n        x4 = F.dropout(x1, p=0.5)\n        x5 = torch.nn.functional.dropout(x1)\n        x6 = torch.nn.functional.dropout(x1, p=0.5, inplace=True)\n        return x3 + x4 + x5 + x6\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x):\n        t1 = torch.nn.functional.dropout(x, p=0.3)\n        t1 = torch.nn.functional.dropout(x, p=0.3)\n        t2 = torch.nn.functional.dropout(x, p=0.3)\n        t3 = torch.nn.functional.dropout(x, p=0.3)\n        t4 = torch.nn.functional.gumbel_softmax(t1, tau=1.0)\n        t5 = torch.nn.functional.gumbel_softmax(t2, tau=1.0)\n        t6 = torch.nn.functional.gumbel_softmax(t3, tau=1.0)\n        t7 = torch.nn.functional.gumbel_softmax(t4, tau=1.0)\n        t8 = torch.nn.functional.gumbel_softmax(t5, tau=1.0)\n        t9 = torch.nn.functional.gumbel_softmax(t6, tau=1.0)\n        return t1, t3, t7, t7, t9\n# Inputs to the model\nx = torch.Tensor([[0.25, 0.25, 0.25, 0.25]])\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1):\n        t1 = torch.nn.functional.dropout(x1, p=0.3)\n        t2 = torch.rand_like(x1, requires_grad=True) + t1\n        return t2\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1):\n        x1 = F.dropout(x1, p=0.3)\n        t1 = torch.nn.functional.dropout(x1, p=0.3)\n        return t1\n# Inputs to the model\nx1 = torch.Tensor([[0.25, 0.25, 0.25, 0.25]])\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x):\n        t1 = torch.rand_like(x, distribution=\"normal\")\n        t2 = torch.rand_like(x, distribution=\"normal\")\n        return x + t1\n# Inputs to the model\nx = torch.Tensor([[0.25, 0.25, 0.25, 0.25]])\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x):\n        t1 = dropout(x, p=0.3)\n        t2 = torch.nn.functional.gumbel_softmax(t1, tau=1.0)\n        return t2\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x):\n        t1 = dropout(x, p=0.3)\n        return t1 + 1.0\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1):\n        x2 = dropout(x1, p=0.3)\n        x3 = dropout(x1, p=0.3) + 1.0\n        x4 = dropout(x1, p=0.3) + 1.0\n        x5 = dropout(x1, p=0.3) + 1.0\n        x6 = dropout(x1, p=0.3) + 1.0\n        x7 = dropout(x1, p=0.3) + 1.0\n        x8 = dropout(x1, p=0.3) + 1.0\n        x9 = dropout(x1, p=0.3) + 1.0\n        x0 = dropout(x1, p=0.3) + 1.0\n        x1 = dropout(x1, p=0.3) + 1.0\n        x10 = torch.rand_like(x1)\n        return x2 + x3 + x0 + x9 + x4 + x5\n# Inputs to the model\nx1 = torch.randn(2, 1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x):\n        t1 = torch.nn.functional.dropout(x, p=0.3)\n        return x + x + x\n# Inputs to the model\nx = torch.Tensor([[0.25, 0.25, 0.25, 0.25]])\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.dropout1 = torch.nn.Dropout(p=0.5)\n    def forward(self, x1):\n        t1 = torch.nn.functional.dropout(x1, p=0.5)\n        t2 = torch.rand_like(t1)\n        t3 = F.avg_pool2d(t2, 3)\n        return t3\n# Inputs to the model\nx1 = torch.randn(1, 3, 3, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def forward(self, x1):\n        t1 = torch.nn.functional.dropout(x1, p=0.3)\n        t2 = torch.nn.functional.softmax(t1, dim=-1)\n        return t2\n# Inputs to the model\nx1 = torch.randn(1, 64, 768)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x):\n        F.dropout(x, training=True, inplace=True)\n        F.dropout(x, training=False, inplace=True)\n# Inputs to the model\nx = torch.Tensor([[0.25, 0.25, 0.25, 0.25]])\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.dropout1 = torch.nn.ModuleList([torch.nn.Dropout(p=0.5) for _ in range(10)])\n        self.dropout2 = torch.nn.ModuleList([torch.nn.Dropout(p=0.5) for _ in range(10)])\n    def forward(self, x1):\n        x3 = torch.nn.functional.dropout(x1, p=0.5)\n        x4 = F.dropout(x1, p=0.5)\n        x5 = torch.nn.functional.dropout(x1)\n        x6 = torch.nn.functional.dropout(x1, p=0.5, inplace=True)\n        return x3 + x4 + x5 + x6\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x):\n        t1 = torch.nn.functional.dropout(x, p=0.3)\n        t1 = torch.nn.functional.dropout(x, p=0.3)\n        t2 = torch.nn.functional.dropout(x, p=0.3)\n        t3 = torch.nn.functional.dropout(x, p=0.3)\n        t4 = torch.nn.functional.gumbel_softmax(t1, tau=1.0)\n        t5 = torch.nn.functional.gumbel_softmax(t2, tau=1.0)\n        t6 = torch.nn.functional.gumbel_softmax(t3, tau=1.0)\n        t7 = torch.nn.functional.gumbel_softmax(t4, tau=1.0)\n        t8 = torch.nn.functional.gumbel_softmax(t5, tau=1.0)\n        t9 = torch.nn.functional.gumbel_softmax(t6, tau=1.0)\n        return t1, t3, t7, t7, t9\n# Inputs to the model\nx = torch.Tensor([[0.25, 0.25, 0.25, 0.25]])\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1):\n        t1 = torch.nn.functional.dropout(x1, p=0.3)\n        t2 = torch.rand_like(x1, requires_grad=True) + t1\n        return t2\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1):\n        x1 = F.dropout(x1, p=0.3)\n        t1 = torch.nn.functional.dropout(x1, p=0.3)\n        return t1\n# Inputs to the model\nx1 = torch.Tensor([[0.25, 0.25, 0.25, 0.25]])\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x):\n        t1 = torch.rand_like(x, distribution=\"normal\")\n        t2 = torch.rand_like(x, distribution=\"normal\")\n        return x + t1\n# Inputs to the model\nx = torch.Tensor([[0.25, 0.25, 0.25, 0.25]])\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x):\n        t1 = dropout(x, p=0.3)\n        t2 = torch.nn.functional.gumbel_softmax(t1, tau=1.0)\n        return t2\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x):\n        t1 = dropout(x, p=0.3)\n        return t1 + 1.0\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1):\n        x2 = dropout(x1, p=0.3)\n        x3 = dropout(x1, p=0.3) + 1.0\n        x4 = dropout(x1, p=0.3) + 1.0\n        x5 = dropout(x1, p=0.3) + 1.0\n        x6 = dropout(x1, p=0.3) + 1.0\n        x7 = dropout(x1, p=0.3) + 1.0\n        x8 = dropout(x1, p=0.3) + 1.0\n        x9 = dropout(x1, p=0.3) + 1.0\n        x0 = dropout(x1, p=0.3) + 1.0\n        x1 = dropout(x1, p=0.3) + 1.0\n        x10 = torch.rand_like(x1)\n        return x2 + x3 + x0 + x9 + x4 + x5\n# Inputs to the model\nx1 = torch.randn(2, 1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x):\n        t1 = torch.nn.functional.dropout(x, p=0.3)\n        return x + x + x\n# Inputs to the model\nx = torch.Tensor([[0.25, 0.25, 0.25, 0.25]])\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.dropout1 = torch.nn.Dropout(p=0.5)\n    def forward(self, x1):\n        t1 = torch.nn.functional.dropout(x1, p=0.5)\n        t2 = torch.rand_like(t1)\n        t3 = F.avg_pool2d(t2, 3)\n        return t3\n# Inputs to the model\nx1 = torch.randn(1, 3, 3, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def forward(self, x1):\n        t1 = torch.nn.functional.dropout(x1, p=0.3)\n        t2 = torch.nn.functional.softmax(t1, dim=-1)\n        return t2\n# Inputs to the model\nx1 = torch.randn(1, 64, 768)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x):\n        F.dropout(x, training=True, inplace=True)\n        F.dropout(x, training=False, inplace=True)\n# Inputs to the model\nx = torch.Tensor([[0.25, 0.25, 0.25, 0.25]])\n"
            ],
            "g_time": 12.979005336761475
        }
    }
}
{
    "opt": "unary=19",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self,):\n        super().__init__()\n        self.linear = torch.nn.Linear(784, 84)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = torch.cat([v2, v2], axis=1)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 784)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(6, 1)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.sigmoid(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 6)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(16, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.sigmoid(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(8, 1)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.sigmoid(v1)\n        return v2\n\n# Initializing the model \nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(350, 10)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.sigmoid(v1)\n        return v2\n \n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 350)\n",
                "\nclass Model(torch.nn.Linear):\n    def __init__(self, input_size, output_size):\n        super().__init__(input_size, output_size)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.sigmoid(v1)\n        return v2\n\n# Initializing the model\nm = Model(3, 8)\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.lin = torch.nn.Linear(8, 1, bias=True)\n \n    def forward(self, x1):\n        v1 = self.lin(x1)\n        v2 = torch.sigmoid(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(64, 1)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.sigmoid(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.sigmoid(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 1, bias=False)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.sigmoid(v1)\n        return v2\n \n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self,):\n        super().__init__()\n        self.linear = torch.nn.Linear(784, 84)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = torch.cat([v2, v2], axis=1)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 784)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(6, 1)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.sigmoid(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 6)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(16, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.sigmoid(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(8, 1)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.sigmoid(v1)\n        return v2\n\n# Initializing the model \nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(350, 10)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.sigmoid(v1)\n        return v2\n \n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 350)\n",
                "\nclass Model(torch.nn.Linear):\n    def __init__(self, input_size, output_size):\n        super().__init__(input_size, output_size)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.sigmoid(v1)\n        return v2\n\n# Initializing the model\nm = Model(3, 8)\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.lin = torch.nn.Linear(8, 1, bias=True)\n \n    def forward(self, x1):\n        v1 = self.lin(x1)\n        v2 = torch.sigmoid(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(64, 1)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.sigmoid(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.sigmoid(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 1, bias=False)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.sigmoid(v1)\n        return v2\n \n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n"
            ],
            "g_time": 5.026895046234131
        }
    }
}
{
    "opt": "unary=20",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_t = torch.nn.ConvTranspose2d(24, 47, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n    def forward(self, x1):\n        v1 = self.conv_t(x1)\n        v2 = torch.sigmoid(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 24, 25, 19)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_t = torch.nn.ConvTranspose2d(4, 5, kernel_size=(1, 1), stride=(1, 1), padding=(0, 0))\n    def forward(self, x1):\n        v1 = self.conv_t(x1)\n        v2 = torch.sigmoid(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 4, 65, 49)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_t = torch.nn.ConvTranspose2d(21, 21, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    def forward(self, x1):\n        v1 = self.conv_t(x1)\n        v2 = torch.sigmoid(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 21, 42, 26)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_t = torch.nn.ConvTranspose2d(52, 64, kernel_size=(1, 1), stride=(2, 2), padding=(0, 0))\n    def forward(self, x1):\n        v1 = self.conv_t(x1)\n        v2 = torch.sigmoid(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 52, 80, 80)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_t = torch.nn.ConvTranspose2d(170, 200, kernel_size=(19, 19), stride=(1, 1), padding=(5, 5))\n    def forward(self, x1):\n        v1 = self.conv_t(x1)\n        v2 = torch.sigmoid(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(2, 170, 15, 15)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_t = torch.nn.ConvTranspose2d(5, 6, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n    def forward(self, x1):\n        v1 = self.conv_t(x1)\n        v2 = torch.sigmoid(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(3, 5, 32, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_t = torch.nn.ConvTranspose2d(16, 16, kernel_size=(1, 1), stride=(1, 1), padding=(1, 1))\n    def forward(self, x1):\n        v1 = self.conv_t(x1)\n        v2 = torch.sigmoid(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 16, 16, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_t = torch.nn.ConvTranspose2d(37, 41, kernel_size=(3, 3), stride=(2, 2), padding=(2, 2), dilation=(2, 2))\n    def forward(self, x1):\n        v1 = self.conv_t(x1)\n        v2 = torch.sigmoid(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 37, 57, 59)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_t = torch.nn.ConvTranspose2d(3, 6, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n    def forward(self, x1):\n        v1 = self.conv_t(x1)\n        v2 = torch.sigmoid(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 3, 512, 512)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_t = torch.nn.ConvTranspose2d(31, 31, kernel_size=(5, 5), stride=(2, 2), padding=(0, 0), groups=16)\n    def forward(self, x1):\n        v1 = self.conv_t(x1)\n        v2 = torch.sigmoid(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 31, 10, 14)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_t = torch.nn.ConvTranspose2d(24, 47, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n    def forward(self, x1):\n        v1 = self.conv_t(x1)\n        v2 = torch.sigmoid(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 24, 25, 19)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_t = torch.nn.ConvTranspose2d(4, 5, kernel_size=(1, 1), stride=(1, 1), padding=(0, 0))\n    def forward(self, x1):\n        v1 = self.conv_t(x1)\n        v2 = torch.sigmoid(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 4, 65, 49)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_t = torch.nn.ConvTranspose2d(21, 21, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    def forward(self, x1):\n        v1 = self.conv_t(x1)\n        v2 = torch.sigmoid(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 21, 42, 26)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_t = torch.nn.ConvTranspose2d(52, 64, kernel_size=(1, 1), stride=(2, 2), padding=(0, 0))\n    def forward(self, x1):\n        v1 = self.conv_t(x1)\n        v2 = torch.sigmoid(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 52, 80, 80)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_t = torch.nn.ConvTranspose2d(170, 200, kernel_size=(19, 19), stride=(1, 1), padding=(5, 5))\n    def forward(self, x1):\n        v1 = self.conv_t(x1)\n        v2 = torch.sigmoid(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(2, 170, 15, 15)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_t = torch.nn.ConvTranspose2d(5, 6, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n    def forward(self, x1):\n        v1 = self.conv_t(x1)\n        v2 = torch.sigmoid(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(3, 5, 32, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_t = torch.nn.ConvTranspose2d(16, 16, kernel_size=(1, 1), stride=(1, 1), padding=(1, 1))\n    def forward(self, x1):\n        v1 = self.conv_t(x1)\n        v2 = torch.sigmoid(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 16, 16, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_t = torch.nn.ConvTranspose2d(37, 41, kernel_size=(3, 3), stride=(2, 2), padding=(2, 2), dilation=(2, 2))\n    def forward(self, x1):\n        v1 = self.conv_t(x1)\n        v2 = torch.sigmoid(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 37, 57, 59)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_t = torch.nn.ConvTranspose2d(3, 6, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n    def forward(self, x1):\n        v1 = self.conv_t(x1)\n        v2 = torch.sigmoid(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 3, 512, 512)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_t = torch.nn.ConvTranspose2d(31, 31, kernel_size=(5, 5), stride=(2, 2), padding=(0, 0), groups=16)\n    def forward(self, x1):\n        v1 = self.conv_t(x1)\n        v2 = torch.sigmoid(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 31, 10, 14)\n"
            ],
            "g_time": 5.297715902328491
        }
    }
}
{
    "opt": "linear_permute_fusion",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear_1 = torch.nn.Linear(2, 2)\n        self.linear_2 = torch.nn.Linear(2, 4)\n        self.linear_3 = torch.nn.Linear(2, 1)\n    def forward(self, x6):\n        v0 = x6\n        v1 = torch.nn.functional.linear(v0, self.linear_1.weight, self.linear_1.bias)\n        v2 = v1.permute(0, 2, 1)\n        v3 = v2.contiguous()\n        v4 = torch.nn.functional.linear(v3, self.linear_2.weight, self.linear_2.bias)\n        v5 = v4.reshape(1, 4, 1).mean(2)\n        v6 = torch.nn.functional.linear(v5, self.linear_3.weight, self.linear_3.bias)\n        v7 = v6.reshape(1, 1, 1)\n        return v7\n# Inputs to the model\nx6 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.max_pooling = torch.nn.MaxPool2d(4)\n    def forward(self, x1):\n        v1 = F.adaptive_avg_pool2d(x1, self.max_pooling.output_size)\n        v2 = v1.transpose(0, 3)\n        v3 = v2.transpose(0, 1)\n        return v3\n# Inputs to the model\nx3 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(2, 2)\n    def forward(self, x1):\n        v1 = F.linear(x1, self.linear[0].weight, self.linear[0].bias)\n        v2 = v1.permute(0, 2, 1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(2, 4)\n        self.sigmoid = torch.nn.Sigmoid()\n        self.conv2d = torch.nn.Conv2d(4, 4, 3)\n    def forward(self, x1):\n        v1 = F.linear(x1, self.linear.weight, self.linear.bias)\n        v2 = v1.transpose(2, 1)\n        #v2 = v1.permute(0, 2, 1)\n        v3 = v2.contiguous()\n        v4 = self.sigmoid(v3)\n        v5 = v4.unsqueeze(1)\n        v6 = v5.contiguous()\n        v7 = self.conv2d(v6)\n        return v7\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear_1 = torch.nn.Linear(2, 3)\n        self.relu = torch.nn.ReLU()\n        self.linear_2 = torch.nn.Linear(3, 4)\n        self.tanh = torch.nn.Tanh()\n        self.linear_3 = torch.nn.Linear(4, 2)\n    def forward(self, x1):\n        v0 = x1\n        v1 = torch.nn.functional.linear(v0, self.linear_1.weight, self.linear_1.bias)\n        v2 = self.relu(v1)\n        v3 = torch.nn.functional.linear(v2, self.linear_2.weight, self.linear_2.bias)\n        v4 = self.tanh(v3)\n        v5 = torch.nn.functional.linear(v4, self.linear_3.weight, self.linear_3.bias)\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear_1 = torch.nn.Linear(2, 4)\n        self.linear_2 = torch.nn.Linear(2, 2)\n    def forward(self, x3):\n        v0 = x3\n        v1 = torch.nn.functional.linear(v0, self.linear_1.weight, self.linear_1.bias)\n        v2 = v1.permute(0, 2, 1)\n        v3 = v2.contiguous()\n        v4 = torch.nn.functional.linear(v3, self.linear_2.weight, self.linear_2.bias)\n        v5 = torch.sigmoid(v4)\n        return v5\n# Inputs to the model\nx3 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(2, 2)\n    def forward(self, x1):\n        v1 = torch.nn.functional.linear(x1, self.linear.weight, self.linear.bias)\n        v2 = F.max_pool2d(v1, kernel_size=2, stride=2)\n        v3 = v2.permute(0, 2, 1)\n        return v3\n# Inputs to the model\nx1 = torch.randn(2, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(2, 2)\n    def forward(self, x2):\n        v0 = x2\n        v1 = v0.reshape(2)\n        v2 = torch.nn.functional.linear(v1, self.linear.weight, self.linear.bias)\n        v3 = v2.permute(0, 2, 1)\n        return v3\n# Inputs to the model\nx2 = torch.randn(1, 4)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(2, 2)\n    def forward(self, x1):\n        v1 = F.linear(x1, self.linear.weight, self.linear.bias)\n        v2 = v1.permute(0, 2, 1)\n        v3 = v2[:1]\n        return v3\n# Inputs to the model\nx1 = torch.randn(3, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear_1 = torch.nn.Linear(2, 8)\n        self.linear_2 = torch.nn.Linear(2, 2)\n        self.linear_3 = torch.nn.Linear(2, 3)\n    def forward(self, x5):\n        v0 = x5\n        v1 = torch.nn.functional.linear(v0, self.linear_1.weight, self.linear_1.bias)\n        v2 = v1.permute(0, 2, 1)\n        v3 = v2.permute(0, 1, 3)\n        v4 = v3.contiguous()\n        v5 = torch.nn.functional.linear(v3, self.linear_2.weight, self.linear_2.bias)\n        v6 = v5.permute(0, 2, 1)\n        v7 = v6.contiguous()\n        v8 = v7.view(1, 3, 6)\n        v9 = torch.nn.functional.linear(v8, self.linear_3.weight, self.linear_3.bias)\n        return v9\n# Inputs to the model\nx5 = torch.randn(1, 2, 2)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear_1 = torch.nn.Linear(2, 2)\n        self.linear_2 = torch.nn.Linear(2, 4)\n        self.linear_3 = torch.nn.Linear(2, 1)\n    def forward(self, x6):\n        v0 = x6\n        v1 = torch.nn.functional.linear(v0, self.linear_1.weight, self.linear_1.bias)\n        v2 = v1.permute(0, 2, 1)\n        v3 = v2.contiguous()\n        v4 = torch.nn.functional.linear(v3, self.linear_2.weight, self.linear_2.bias)\n        v5 = v4.reshape(1, 4, 1).mean(2)\n        v6 = torch.nn.functional.linear(v5, self.linear_3.weight, self.linear_3.bias)\n        v7 = v6.reshape(1, 1, 1)\n        return v7\n# Inputs to the model\nx6 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.max_pooling = torch.nn.MaxPool2d(4)\n    def forward(self, x1):\n        v1 = F.adaptive_avg_pool2d(x1, self.max_pooling.output_size)\n        v2 = v1.transpose(0, 3)\n        v3 = v2.transpose(0, 1)\n        return v3\n# Inputs to the model\nx3 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(2, 2)\n    def forward(self, x1):\n        v1 = F.linear(x1, self.linear[0].weight, self.linear[0].bias)\n        v2 = v1.permute(0, 2, 1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(2, 4)\n        self.sigmoid = torch.nn.Sigmoid()\n        self.conv2d = torch.nn.Conv2d(4, 4, 3)\n    def forward(self, x1):\n        v1 = F.linear(x1, self.linear.weight, self.linear.bias)\n        v2 = v1.transpose(2, 1)\n        #v2 = v1.permute(0, 2, 1)\n        v3 = v2.contiguous()\n        v4 = self.sigmoid(v3)\n        v5 = v4.unsqueeze(1)\n        v6 = v5.contiguous()\n        v7 = self.conv2d(v6)\n        return v7\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear_1 = torch.nn.Linear(2, 3)\n        self.relu = torch.nn.ReLU()\n        self.linear_2 = torch.nn.Linear(3, 4)\n        self.tanh = torch.nn.Tanh()\n        self.linear_3 = torch.nn.Linear(4, 2)\n    def forward(self, x1):\n        v0 = x1\n        v1 = torch.nn.functional.linear(v0, self.linear_1.weight, self.linear_1.bias)\n        v2 = self.relu(v1)\n        v3 = torch.nn.functional.linear(v2, self.linear_2.weight, self.linear_2.bias)\n        v4 = self.tanh(v3)\n        v5 = torch.nn.functional.linear(v4, self.linear_3.weight, self.linear_3.bias)\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear_1 = torch.nn.Linear(2, 4)\n        self.linear_2 = torch.nn.Linear(2, 2)\n    def forward(self, x3):\n        v0 = x3\n        v1 = torch.nn.functional.linear(v0, self.linear_1.weight, self.linear_1.bias)\n        v2 = v1.permute(0, 2, 1)\n        v3 = v2.contiguous()\n        v4 = torch.nn.functional.linear(v3, self.linear_2.weight, self.linear_2.bias)\n        v5 = torch.sigmoid(v4)\n        return v5\n# Inputs to the model\nx3 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(2, 2)\n    def forward(self, x1):\n        v1 = torch.nn.functional.linear(x1, self.linear.weight, self.linear.bias)\n        v2 = F.max_pool2d(v1, kernel_size=2, stride=2)\n        v3 = v2.permute(0, 2, 1)\n        return v3\n# Inputs to the model\nx1 = torch.randn(2, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(2, 2)\n    def forward(self, x2):\n        v0 = x2\n        v1 = v0.reshape(2)\n        v2 = torch.nn.functional.linear(v1, self.linear.weight, self.linear.bias)\n        v3 = v2.permute(0, 2, 1)\n        return v3\n# Inputs to the model\nx2 = torch.randn(1, 4)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(2, 2)\n    def forward(self, x1):\n        v1 = F.linear(x1, self.linear.weight, self.linear.bias)\n        v2 = v1.permute(0, 2, 1)\n        v3 = v2[:1]\n        return v3\n# Inputs to the model\nx1 = torch.randn(3, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear_1 = torch.nn.Linear(2, 8)\n        self.linear_2 = torch.nn.Linear(2, 2)\n        self.linear_3 = torch.nn.Linear(2, 3)\n    def forward(self, x5):\n        v0 = x5\n        v1 = torch.nn.functional.linear(v0, self.linear_1.weight, self.linear_1.bias)\n        v2 = v1.permute(0, 2, 1)\n        v3 = v2.permute(0, 1, 3)\n        v4 = v3.contiguous()\n        v5 = torch.nn.functional.linear(v3, self.linear_2.weight, self.linear_2.bias)\n        v6 = v5.permute(0, 2, 1)\n        v7 = v6.contiguous()\n        v8 = v7.view(1, 3, 6)\n        v9 = torch.nn.functional.linear(v8, self.linear_3.weight, self.linear_3.bias)\n        return v9\n# Inputs to the model\nx5 = torch.randn(1, 2, 2)\n"
            ],
            "g_time": 9.734827995300293
        }
    }
}
{
    "opt": "unary=26",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_t = torch.nn.ConvTranspose1d(333, 5, kernel_size=3, stride=1, padding=2, bias=True)\n    def forward(self, x349):\n        v1 = self.conv_t(x349)\n        v2 = v1 >= 0\n        v3 = v1 * torch.max(torch.FloatTensor(x349.shape[0]), torch.FloatTensor([torch.mean(torch.abs(x349))]))\n        v4 = torch.where(v2, v1, v3)\n        return v4 + torch.nn.functional.adaptive_avg_pool1d(v4, (1))\n# Inputs to the model\nx349 = torch.randn(19, 333, 39)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.conv_t = torch.nn.ConvTranspose2d(10, 6, 3, stride=1, padding=1, bias=False)\n    def forward(self, x4):\n        v1 = self.conv_t(x4)\n        v2 = v1 > 0\n        v3 = v1 * -0.5521483\n        v4 = torch.where(v2, v1, v3)\n        return v2\n# Inputs to the model\nx4 = torch.randn(23, 10, 8, 54)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.conv_t = torch.nn.ConvTranspose3d(99, 31, kernel_size=(3, 3, 6), stride=(3, 1, 6), padding=(1, 0, 1), output_padding=(0, 1, 5), groups=27, bias=False)\n    def forward(self, x1):\n        v1 = self.conv_t(x1)\n        v2 = v1 > 0\n        v3 = v1 * -0.2277873\n        v4 = torch.where(v2, v1, v3)\n        return v4\n# Inputs to the model\nx1 = torch.randn(28, 99, 28, 14, 39)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.conv_t = torch.nn.ConvTranspose2d(130, 50, 1, stride=1, padding=0, bias=False)[0]\n    def forward(self, x36):\n        v1 = self.conv_t(x36)\n        v2 = v1 > -5.659916\n        v3 = v1 * -5.659916\n        v4 = torch.where(v2, v1, v3)\n        return v4\n# Inputs to the model\nx36 = torch.randn(2, 130, 14, 13)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.conv_t = torch.nn.Sequential(torch.nn.ConvTranspose2d(139, 417, 11, stride=2, padding=5, output_padding=1, bias=True),torch.nn.ReLU(),torch.nn.ConvTranspose2d(417, 305, 11, stride=1, padding=4, bias=True),torch.nn.ReLU(),torch.nn.ConvTranspose2d(305, 146, 10, stride=1, padding=2, bias=True))\n    def forward(self, x27):\n        t1 = self.conv_t(x27)\n        t2 = t1 > 0\n        t3 = t1 * 0.248297\n        t4 = torch.where(t2, t1, t3)\n        return t4 + torch.nn.functional.adaptive_avg_pool2d(t4, (1, 1))\n# Inputs to the model\nx27 = torch.randn(18, 139, 50, 25)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_t = torch.nn.ConvTranspose2d(83, 24, 6, stride=4, padding=3, bias=False)\n    def forward(self, x1):\n        o1 = self.conv_t(x1)\n        o2 = o1 > 0\n        o3 = o1 * 0.065540657\n        o4 = torch.where(o2, o1, o3)\n        return o4\n# Inputs to the model\nx1 = torch.randn(437, 83, 5, 68)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.conv_t = torch.nn.ConvTranspose2d(198, 74, 3, stride=2, padding=0, output_padding=0, bias=False)\n    def forward(self, x18):\n        v1 = self.conv_t(x18)\n        v2 = v1 > 1\n        v3 = v1 * -0.866664\n        v4 = torch.where(v2, v1, v3)\n        return v4\n# Inputs to the model\nx18 = torch.randn(15, 198, 56, 68)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_t = torch.nn.ConvTranspose2d(167, 95, 8, stride=5, padding=2, bias=False)\n    def forward(self, x107):\n        v1 = self.conv_t(x107)\n        v2 = v1 > 0\n        v3 = v1 * 0.9135688\n        v4 = torch.where(v2, v1, v3)\n        return v4\n# Inputs to the model\nx107 = torch.randn(65535, 167, 13, 43)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_t = torch.nn.ConvTranspose2d(21, 13, 15, stride=5, padding=0, bias=False)\n    def forward(self, x10):\n        j1 = self.conv_t(x10)\n        j2 = j1 > 0\n        j3 = j1 * 0.278\n        j4 = torch.where(j2, j1, j3)\n        return j4\n# Inputs to the model\nx10 = torch.randn(4, 21, 13, 12)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_t = torch.nn.ConvTranspose2d(288, 215, 6, stride=4, padding=3, output_padding=1, bias=False)\n    def forward(self, x22):\n        v1 = self.conv_t(x22)\n        v2 = v1 > 0\n        v3 = v1 * -0.016042025\n        v4 = torch.where(v2, v1, v3)\n        return torch.nn.functional.interpolate(v4, size=[196, 55])\n# Inputs to the model\nx22 = torch.randn(10, 288, 30, 34)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_t = torch.nn.ConvTranspose1d(333, 5, kernel_size=3, stride=1, padding=2, bias=True)\n    def forward(self, x349):\n        v1 = self.conv_t(x349)\n        v2 = v1 >= 0\n        v3 = v1 * torch.max(torch.FloatTensor(x349.shape[0]), torch.FloatTensor([torch.mean(torch.abs(x349))]))\n        v4 = torch.where(v2, v1, v3)\n        return v4 + torch.nn.functional.adaptive_avg_pool1d(v4, (1))\n# Inputs to the model\nx349 = torch.randn(19, 333, 39)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.conv_t = torch.nn.ConvTranspose2d(10, 6, 3, stride=1, padding=1, bias=False)\n    def forward(self, x4):\n        v1 = self.conv_t(x4)\n        v2 = v1 > 0\n        v3 = v1 * -0.5521483\n        v4 = torch.where(v2, v1, v3)\n        return v2\n# Inputs to the model\nx4 = torch.randn(23, 10, 8, 54)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.conv_t = torch.nn.ConvTranspose3d(99, 31, kernel_size=(3, 3, 6), stride=(3, 1, 6), padding=(1, 0, 1), output_padding=(0, 1, 5), groups=27, bias=False)\n    def forward(self, x1):\n        v1 = self.conv_t(x1)\n        v2 = v1 > 0\n        v3 = v1 * -0.2277873\n        v4 = torch.where(v2, v1, v3)\n        return v4\n# Inputs to the model\nx1 = torch.randn(28, 99, 28, 14, 39)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.conv_t = torch.nn.ConvTranspose2d(130, 50, 1, stride=1, padding=0, bias=False)[0]\n    def forward(self, x36):\n        v1 = self.conv_t(x36)\n        v2 = v1 > -5.659916\n        v3 = v1 * -5.659916\n        v4 = torch.where(v2, v1, v3)\n        return v4\n# Inputs to the model\nx36 = torch.randn(2, 130, 14, 13)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.conv_t = torch.nn.Sequential(torch.nn.ConvTranspose2d(139, 417, 11, stride=2, padding=5, output_padding=1, bias=True),torch.nn.ReLU(),torch.nn.ConvTranspose2d(417, 305, 11, stride=1, padding=4, bias=True),torch.nn.ReLU(),torch.nn.ConvTranspose2d(305, 146, 10, stride=1, padding=2, bias=True))\n    def forward(self, x27):\n        t1 = self.conv_t(x27)\n        t2 = t1 > 0\n        t3 = t1 * 0.248297\n        t4 = torch.where(t2, t1, t3)\n        return t4 + torch.nn.functional.adaptive_avg_pool2d(t4, (1, 1))\n# Inputs to the model\nx27 = torch.randn(18, 139, 50, 25)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_t = torch.nn.ConvTranspose2d(83, 24, 6, stride=4, padding=3, bias=False)\n    def forward(self, x1):\n        o1 = self.conv_t(x1)\n        o2 = o1 > 0\n        o3 = o1 * 0.065540657\n        o4 = torch.where(o2, o1, o3)\n        return o4\n# Inputs to the model\nx1 = torch.randn(437, 83, 5, 68)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.conv_t = torch.nn.ConvTranspose2d(198, 74, 3, stride=2, padding=0, output_padding=0, bias=False)\n    def forward(self, x18):\n        v1 = self.conv_t(x18)\n        v2 = v1 > 1\n        v3 = v1 * -0.866664\n        v4 = torch.where(v2, v1, v3)\n        return v4\n# Inputs to the model\nx18 = torch.randn(15, 198, 56, 68)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_t = torch.nn.ConvTranspose2d(167, 95, 8, stride=5, padding=2, bias=False)\n    def forward(self, x107):\n        v1 = self.conv_t(x107)\n        v2 = v1 > 0\n        v3 = v1 * 0.9135688\n        v4 = torch.where(v2, v1, v3)\n        return v4\n# Inputs to the model\nx107 = torch.randn(65535, 167, 13, 43)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_t = torch.nn.ConvTranspose2d(21, 13, 15, stride=5, padding=0, bias=False)\n    def forward(self, x10):\n        j1 = self.conv_t(x10)\n        j2 = j1 > 0\n        j3 = j1 * 0.278\n        j4 = torch.where(j2, j1, j3)\n        return j4\n# Inputs to the model\nx10 = torch.randn(4, 21, 13, 12)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_t = torch.nn.ConvTranspose2d(288, 215, 6, stride=4, padding=3, output_padding=1, bias=False)\n    def forward(self, x22):\n        v1 = self.conv_t(x22)\n        v2 = v1 > 0\n        v3 = v1 * -0.016042025\n        v4 = torch.where(v2, v1, v3)\n        return torch.nn.functional.interpolate(v4, size=[196, 55])\n# Inputs to the model\nx22 = torch.randn(10, 288, 30, 34)\n"
            ],
            "g_time": 10.007792949676514
        }
    }
}
{
    "opt": "permute_linear_fusion",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(2, 2)\n        self.flatten = torch.nn.Flatten(0, 1)\n        self.sigmoid = torch.nn.Sigmoid()\n    def forward(self, x1):\n        v1 = x1.permute(0, 2, 1)\n        v2 = torch.nn.functional.linear(v1, self.linear.weight, self.linear.bias)\n        v4 = self.sigmoid(v3)\n        x2 = v4.expand_as(v1)\n        v3 = v3.unsqueeze(1)\n        v3 = v3.to(v1.dtype)\n        v4 = (v1 * v3) / x2\n        v4 = v4.narrow(dim=1, start=0, length=1)\n        v5 = v4.flatten(2)\n        v5 = torch.stack(v5, dim=1)\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(2, 2)\n        self.linear_2 = torch.nn.Linear(2, 2)\n    def forward(self, x1):\n        v1 = x1.permute(0, 2, 1)\n        v2 = torch.nn.functional.linear(v1, self.linear.weight, self.linear.bias)\n        v3 = torch.nn.functional.linear(v1, self.linear_2.weight, self.linear_2.bias)\n        v4 = v2 * x1 + v3\n        v5 = v4.permute(0, 2, 1)\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Sequential()\n        self.linear.add_module(\"flatten\", torch.nn.Flatten(0, 1))\n        self.linear.add_module(\"linear\", torch.nn.Linear(2, 2))\n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1.squeeze(-1)\n        v3 = v2.transpose(1, 2)\n        v4 = torch.sum(v3, dim=1, keepdim=True)\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(2, 2)\n        self.sigmoid = torch.nn.Identity()\n    def forward(self, x1):\n        v1 = x1.permute(0, 2, 1)\n        v2 = torch.nn.functional.linear(v1, self.linear.weight, self.linear.bias)\n        v3 = self.sigmoid(v2)\n        v4 = v3.squeeze(-1)\n        v5 = v4.transpose(1, 2)\n        v6 = torch.sum(v5, dim=1, keepdim=True)\n        v7 = torch.nn.functional.linear(v6, self.linear.weight, self.linear.bias)\n        v8 = self.sigmoid(v7)\n        v9 = v8.squeeze(-1)\n        v10 = v9.transpose(1, 2)\n        v11 = torch.sum(v10, dim=1, keepdim=True)\n        return v11\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(2, 2)\n    def forward(self, x1):\n        v1 = x1.permute(0, 2, 1)\n        v2 = torch.nn.functional.linear(v1, self.linear.weight, self.linear.bias)\n        v3 = v2.abs()\n        v4 = torch.all((v3 > 0).to(torch.int64), dim=-1)\n        v5 = v4.permute(0, 2, 1)\n        v5 = (v5 == 1).to(v5.dtype)\n        x2 = v5.reshape(1, 2, 2)\n        v6 = v5 * x2\n        v7 = v5.permute(0, 2, 1)\n        x3 = v6.sum(dim=1) > x3.max()\n        return x3\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(2, 2)\n        self.flatten = torch.nn.Flatten(0, 1)\n        self.relu = torch.nn.ReLU()\n        self.sigmoid = torch.nn.Sigmoid()\n    def forward(self, x1):\n        v1 = x1.permute(0, 2, 1)\n        v2 = self.relu(v1)\n        v3 = torch.nn.functional.linear(v2, self.linear.weight, self.linear.bias)\n        v4 = self.sigmoid(v3)\n        v5 = v4.transpose(1, 2)\n        v6 = self.flatten(v5)\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(2, 2)\n    def forward(self, x1):\n        v1 = x1.permute(0, 2, 1)\n        v2 = torch.nn.functional.linear(v1, self.linear.weight, self.linear.bias)\n        v3 = v2.unsqueeze(-1)\n        v4 = v3.squeeze(-1)\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(2, 2)\n    def forward(self, x1):\n        i1 = torch.sum(x1, dim=-1)\n        v1 = x1.permute(0, 2, 1)\n        i2 = torch.sum(v1, dim=-1)\n        v2 = torch.nn.functional.linear(v1, self.linear.weight, self.linear.bias)\n        i3 = torch.sum(v2, dim=-1)\n        v3 = i1 + i2 + i3\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(2, 2, bias=False)\n    def forward(self, x1):\n        v1 = x1.permute(0, 2, 1)\n        v2 = torch.nn.functional.linear(v1, self.linear.weight)\n        return x1 + v2\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear1 = torch.nn.Linear(6, 2)\n        self.linear2 = torch.nn.Linear(2, 2)\n    def forward(self, x1):\n        v1 = x1.transpose(1, 2)\n        v2 = torch.nn.functional.linear(v1, self.linear1.weight, self.linear1.bias)\n        v3 = torch.nn.functional.linear(self.linear2.weight.to(v1.dtype) * v2, self.linear2.weight, self.linear2.bias)\n        v4 = v3.squeeze(-1)\n        v5 = v4.transpose(1, 2)\n        return torch.sum(v5, dim=1, keepdim=True)\n# Inputs to the model\nx1 = torch.randn(1, 2, 6)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(2, 2)\n        self.flatten = torch.nn.Flatten(0, 1)\n        self.sigmoid = torch.nn.Sigmoid()\n    def forward(self, x1):\n        v1 = x1.permute(0, 2, 1)\n        v2 = torch.nn.functional.linear(v1, self.linear.weight, self.linear.bias)\n        v4 = self.sigmoid(v3)\n        x2 = v4.expand_as(v1)\n        v3 = v3.unsqueeze(1)\n        v3 = v3.to(v1.dtype)\n        v4 = (v1 * v3) / x2\n        v4 = v4.narrow(dim=1, start=0, length=1)\n        v5 = v4.flatten(2)\n        v5 = torch.stack(v5, dim=1)\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(2, 2)\n        self.linear_2 = torch.nn.Linear(2, 2)\n    def forward(self, x1):\n        v1 = x1.permute(0, 2, 1)\n        v2 = torch.nn.functional.linear(v1, self.linear.weight, self.linear.bias)\n        v3 = torch.nn.functional.linear(v1, self.linear_2.weight, self.linear_2.bias)\n        v4 = v2 * x1 + v3\n        v5 = v4.permute(0, 2, 1)\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Sequential()\n        self.linear.add_module(\"flatten\", torch.nn.Flatten(0, 1))\n        self.linear.add_module(\"linear\", torch.nn.Linear(2, 2))\n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1.squeeze(-1)\n        v3 = v2.transpose(1, 2)\n        v4 = torch.sum(v3, dim=1, keepdim=True)\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(2, 2)\n        self.sigmoid = torch.nn.Identity()\n    def forward(self, x1):\n        v1 = x1.permute(0, 2, 1)\n        v2 = torch.nn.functional.linear(v1, self.linear.weight, self.linear.bias)\n        v3 = self.sigmoid(v2)\n        v4 = v3.squeeze(-1)\n        v5 = v4.transpose(1, 2)\n        v6 = torch.sum(v5, dim=1, keepdim=True)\n        v7 = torch.nn.functional.linear(v6, self.linear.weight, self.linear.bias)\n        v8 = self.sigmoid(v7)\n        v9 = v8.squeeze(-1)\n        v10 = v9.transpose(1, 2)\n        v11 = torch.sum(v10, dim=1, keepdim=True)\n        return v11\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(2, 2)\n    def forward(self, x1):\n        v1 = x1.permute(0, 2, 1)\n        v2 = torch.nn.functional.linear(v1, self.linear.weight, self.linear.bias)\n        v3 = v2.abs()\n        v4 = torch.all((v3 > 0).to(torch.int64), dim=-1)\n        v5 = v4.permute(0, 2, 1)\n        v5 = (v5 == 1).to(v5.dtype)\n        x2 = v5.reshape(1, 2, 2)\n        v6 = v5 * x2\n        v7 = v5.permute(0, 2, 1)\n        x3 = v6.sum(dim=1) > x3.max()\n        return x3\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(2, 2)\n        self.flatten = torch.nn.Flatten(0, 1)\n        self.relu = torch.nn.ReLU()\n        self.sigmoid = torch.nn.Sigmoid()\n    def forward(self, x1):\n        v1 = x1.permute(0, 2, 1)\n        v2 = self.relu(v1)\n        v3 = torch.nn.functional.linear(v2, self.linear.weight, self.linear.bias)\n        v4 = self.sigmoid(v3)\n        v5 = v4.transpose(1, 2)\n        v6 = self.flatten(v5)\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(2, 2)\n    def forward(self, x1):\n        v1 = x1.permute(0, 2, 1)\n        v2 = torch.nn.functional.linear(v1, self.linear.weight, self.linear.bias)\n        v3 = v2.unsqueeze(-1)\n        v4 = v3.squeeze(-1)\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(2, 2)\n    def forward(self, x1):\n        i1 = torch.sum(x1, dim=-1)\n        v1 = x1.permute(0, 2, 1)\n        i2 = torch.sum(v1, dim=-1)\n        v2 = torch.nn.functional.linear(v1, self.linear.weight, self.linear.bias)\n        i3 = torch.sum(v2, dim=-1)\n        v3 = i1 + i2 + i3\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(2, 2, bias=False)\n    def forward(self, x1):\n        v1 = x1.permute(0, 2, 1)\n        v2 = torch.nn.functional.linear(v1, self.linear.weight)\n        return x1 + v2\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear1 = torch.nn.Linear(6, 2)\n        self.linear2 = torch.nn.Linear(2, 2)\n    def forward(self, x1):\n        v1 = x1.transpose(1, 2)\n        v2 = torch.nn.functional.linear(v1, self.linear1.weight, self.linear1.bias)\n        v3 = torch.nn.functional.linear(self.linear2.weight.to(v1.dtype) * v2, self.linear2.weight, self.linear2.bias)\n        v4 = v3.squeeze(-1)\n        v5 = v4.transpose(1, 2)\n        return torch.sum(v5, dim=1, keepdim=True)\n# Inputs to the model\nx1 = torch.randn(1, 2, 6)\n"
            ],
            "g_time": 9.457656145095825
        }
    }
}
{
    "opt": "binary=4",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(in_features=128, out_features=128, bias=True)\n \n    def forward(self, x1, x2, scale, shift):\n        v1 = self.sigmoid(max(-15, min(x1, 15)))\n        v2 = self.sigmoid(max(-15, min(x2, 15)))\n        v3 = self.addmm(v1, v2, self.linear.weight, beta=1, alpha=scale)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(2, 128)\nx2 = torch.randn(2, 128)\nscale = torch.randn(128)\nshift = torch.randn(128)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, input_size, output_size):\n        super().__init__()\n        self.linear = torch.nn.Linear(input_size, output_size)\n        self.other = torch.randn(output_size, )\n \n    def forward(self, x2):\n        v0 = self.linear(x2)\n        v1 = v0 + self.other\n        return v1\n\n# Initializing the model\nm = Model(10, 10)\n\n# Inputs to the model\nx2 = torch.randn(1, 10)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 + other\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model. The value of other can be any tensor with the same shape as the output of the linear transformation.\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(5, 7)\n \n    def forward(self, __input0, other):\n        v1 = self.linear(__input0)\n        v2 = v1 + other\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\ninput0 = torch.randn(1, 5, 8, 8)\nother = torch.randn(1, 7, 4, 4)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 4)\n \n    def forward(self, x1, x2):\n        v1 = self.linear(x1)\n        v2 = v1 + x2\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(8, 3)\nx2 = torch.randn(8, 4)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(8, 16)\n        self.other = torch.ones(8, 16)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 + self.other\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(8, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 + other\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(128, 10)\n \n    def forward(self, x1, x2):\n        v1 = self.linear(x1)\n        v2 = v1 + x2\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 128)\nx2 = torch.randn(1, 10)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(100, 200)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 + other\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 100)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc = torch.nn.Linear(20, 10)\n \n    def forward(self, x):\n        v1 = self.fc(x)\n        v2 = v1 + other\n        return v2\n\n# Input to the model\nx = torch.randn(1, 20)\nother = torch.randn(1, 10)\n# Initializing the model\nm = Model()\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(in_features=128, out_features=128, bias=True)\n \n    def forward(self, x1, x2, scale, shift):\n        v1 = self.sigmoid(max(-15, min(x1, 15)))\n        v2 = self.sigmoid(max(-15, min(x2, 15)))\n        v3 = self.addmm(v1, v2, self.linear.weight, beta=1, alpha=scale)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(2, 128)\nx2 = torch.randn(2, 128)\nscale = torch.randn(128)\nshift = torch.randn(128)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, input_size, output_size):\n        super().__init__()\n        self.linear = torch.nn.Linear(input_size, output_size)\n        self.other = torch.randn(output_size, )\n \n    def forward(self, x2):\n        v0 = self.linear(x2)\n        v1 = v0 + self.other\n        return v1\n\n# Initializing the model\nm = Model(10, 10)\n\n# Inputs to the model\nx2 = torch.randn(1, 10)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 + other\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model. The value of other can be any tensor with the same shape as the output of the linear transformation.\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(5, 7)\n \n    def forward(self, __input0, other):\n        v1 = self.linear(__input0)\n        v2 = v1 + other\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\ninput0 = torch.randn(1, 5, 8, 8)\nother = torch.randn(1, 7, 4, 4)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 4)\n \n    def forward(self, x1, x2):\n        v1 = self.linear(x1)\n        v2 = v1 + x2\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(8, 3)\nx2 = torch.randn(8, 4)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(8, 16)\n        self.other = torch.ones(8, 16)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 + self.other\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(8, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 + other\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(128, 10)\n \n    def forward(self, x1, x2):\n        v1 = self.linear(x1)\n        v2 = v1 + x2\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 128)\nx2 = torch.randn(1, 10)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(100, 200)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 + other\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 100)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc = torch.nn.Linear(20, 10)\n \n    def forward(self, x):\n        v1 = self.fc(x)\n        v2 = v1 + other\n        return v2\n\n# Input to the model\nx = torch.randn(1, 20)\nother = torch.randn(1, 10)\n# Initializing the model\nm = Model()\n"
            ],
            "g_time": 7.409649610519409
        }
    }
}
{
    "opt": "unary=10",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v4 / 6\n        return v5\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(8, 16, bias=False)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 + 3\n        v3 = F.relu6(v2, inplace=False)\n        v4 = v3 / 6\n        return v4\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(20, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(2, 1)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v4 / 6\n        return v5\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(10, 1, bias=False)\n \n    def forward(self, x1):\n        y1 = self.linear(x1)\n        y2 = y1 + 3\n        y3 = torch.clamp_min(y2, 0)\n        y4 = torch.clamp_max(y3, 6)\n        y5 = y4 / 6\n        return y5\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 10)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v4 / 6\n        return v5\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(256, 256)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v4 / 6\n        return v5\n \n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 256)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp(v2, min=0, max=6)\n        v4 = v3 / 6\n        return v4\n\n# Initializing the model\nm = Model()\n\n# Input to the model\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 3)\n\n    def forward(self, x1):\n        l1 = self.linear(x1)\n        l2 = l1 + 3\n        l3 = torch.clamp_min(l2, 0)\n        l4 = torch.clamp_max(l3, 6)\n        l5 = l4 / 6\n        return l5\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(2, 2)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v4 / 6\n        return v5\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.l1 = torch.nn.Linear(16, 32)\n        \n    def forward(self, x1):\n        l1 = self.l1(x1)\n        l2 = l1 + 3\n        l3 = torch.clamp_min(l2, 0)\n        l4 = torch.clamp_max(l3, 6)\n        l5 = l4 / 6\n        return l5\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 16)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v4 / 6\n        return v5\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(8, 16, bias=False)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 + 3\n        v3 = F.relu6(v2, inplace=False)\n        v4 = v3 / 6\n        return v4\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(20, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(2, 1)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v4 / 6\n        return v5\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(10, 1, bias=False)\n \n    def forward(self, x1):\n        y1 = self.linear(x1)\n        y2 = y1 + 3\n        y3 = torch.clamp_min(y2, 0)\n        y4 = torch.clamp_max(y3, 6)\n        y5 = y4 / 6\n        return y5\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 10)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v4 / 6\n        return v5\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(256, 256)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v4 / 6\n        return v5\n \n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 256)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp(v2, min=0, max=6)\n        v4 = v3 / 6\n        return v4\n\n# Initializing the model\nm = Model()\n\n# Input to the model\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 3)\n\n    def forward(self, x1):\n        l1 = self.linear(x1)\n        l2 = l1 + 3\n        l3 = torch.clamp_min(l2, 0)\n        l4 = torch.clamp_max(l3, 6)\n        l5 = l4 / 6\n        return l5\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(2, 2)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp_min(v2, 0)\n        v4 = torch.clamp_max(v3, 6)\n        v5 = v4 / 6\n        return v5\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.l1 = torch.nn.Linear(16, 32)\n        \n    def forward(self, x1):\n        l1 = self.l1(x1)\n        l2 = l1 + 3\n        l3 = torch.clamp_min(l2, 0)\n        l4 = torch.clamp_max(l3, 6)\n        l5 = l4 / 6\n        return l5\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 16)\n"
            ],
            "g_time": 6.01233172416687
        }
    }
}
{
    "opt": "unary=28",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min_value=-1., max_value=1.):\n        super().__init__()\n        self.linear = torch.nn.Linear(512, 512)\n \n    def forward(self, x):\n        v1 = self.linear(x)\n        v2 = torch.clamp_min(v1, min_value)\n        v3 = torch.clamp_max(v2, max_value)\n        return v3\n\n# Initializing the model\nmin_value = -1.\nmax_value = 1.\nm = Model(min_value=min_value, max_value=max_value)\n\n# Inputs to the model\nx1 = torch.randn(1, 512)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n        self.min_value = 1.5\n        self.max_value = 4.8\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.clip_min(v1, self.min_value)\n        v3 = torch.clip_max(v2, self.max_value)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(20, 80)\n \n    def forward(self, x1, min_value=-1.758, max_value=0.5241):\n        v1 = self.linear(x1)\n        v2 = torch.clamp_min(v1, min_value)\n        v3 = torch.clamp_max(v2, max_value)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 20)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min_value, max_value):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n        self.min_value = min_value\n        self.max_value = max_value\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.clamp_min(v1, self.min_value)\n        v3 = torch.clamp_max(v2, self.max_value)\n        return v3\n\n# Initializing the model\nm = Model(min_value=torch.zeros(8))\n\n# Inputs to the model\nx1 = torch.randn(5, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 1, bias=True)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.clamp_min(v1, min_value=-2)\n        v3 = torch.clamp_max(v2, max_value=2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.linear = torch.nn.Linear(3, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.clamp_min(v1, min=0.5)\n        v3 = torch.clamp_max(v2, max=0.5)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min_value, max_value):\n        super().__init__()\n        self.linear = torch.nn.Linear(128, 32)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.clamp_min(v1, min_value=min_value)\n        v3 = torch.clamp_max(v2, max_value=max_value)\n        return v3\n\n# Initializing the model\nm = Model(min_value=0.11, max_value=3.02)\n\n# Inputs to the model\nx1 = torch.randn(1, 128)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min_value=0.7, max_value=1.2):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 7)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.clamp_min(v1, min_value=self.min_value)\n        v3 = torch.clamp_max(v2, max_value)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min_value=-20, max_value=20):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 2)\n        self.min_value = min_value\n        self.max_value = max_value\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.clamp_min(v1, self.min_value)\n        v3 = torch.clamp_max(v2, self.max_value)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 2)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.clamp_min(v1, 0.5)\n        v3 = torch.clamp_max(v2, 1.5)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min_value=-1., max_value=1.):\n        super().__init__()\n        self.linear = torch.nn.Linear(512, 512)\n \n    def forward(self, x):\n        v1 = self.linear(x)\n        v2 = torch.clamp_min(v1, min_value)\n        v3 = torch.clamp_max(v2, max_value)\n        return v3\n\n# Initializing the model\nmin_value = -1.\nmax_value = 1.\nm = Model(min_value=min_value, max_value=max_value)\n\n# Inputs to the model\nx1 = torch.randn(1, 512)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n        self.min_value = 1.5\n        self.max_value = 4.8\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.clip_min(v1, self.min_value)\n        v3 = torch.clip_max(v2, self.max_value)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(20, 80)\n \n    def forward(self, x1, min_value=-1.758, max_value=0.5241):\n        v1 = self.linear(x1)\n        v2 = torch.clamp_min(v1, min_value)\n        v3 = torch.clamp_max(v2, max_value)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 20)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min_value, max_value):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n        self.min_value = min_value\n        self.max_value = max_value\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.clamp_min(v1, self.min_value)\n        v3 = torch.clamp_max(v2, self.max_value)\n        return v3\n\n# Initializing the model\nm = Model(min_value=torch.zeros(8))\n\n# Inputs to the model\nx1 = torch.randn(5, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 1, bias=True)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.clamp_min(v1, min_value=-2)\n        v3 = torch.clamp_max(v2, max_value=2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.linear = torch.nn.Linear(3, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.clamp_min(v1, min=0.5)\n        v3 = torch.clamp_max(v2, max=0.5)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min_value, max_value):\n        super().__init__()\n        self.linear = torch.nn.Linear(128, 32)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.clamp_min(v1, min_value=min_value)\n        v3 = torch.clamp_max(v2, max_value=max_value)\n        return v3\n\n# Initializing the model\nm = Model(min_value=0.11, max_value=3.02)\n\n# Inputs to the model\nx1 = torch.randn(1, 128)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min_value=0.7, max_value=1.2):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 7)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.clamp_min(v1, min_value=self.min_value)\n        v3 = torch.clamp_max(v2, max_value)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, min_value=-20, max_value=20):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 2)\n        self.min_value = min_value\n        self.max_value = max_value\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.clamp_min(v1, self.min_value)\n        v3 = torch.clamp_max(v2, self.max_value)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 2)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.clamp_min(v1, 0.5)\n        v3 = torch.clamp_max(v2, 1.5)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n"
            ],
            "g_time": 6.568156957626343
        }
    }
}
{
    "opt": "binary=10",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(13, 9)\n \n    def forward(self, x1, other):\n        v1 = self.linear(x1)\n        v2 = v1 + other\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 13)\nother = torch.randn(1, 9)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(8, 8, bias=True)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 + other\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 8)\n",
                "\nimport torch\n\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(16, 16)\n \n    def forward(self, x1, x2=None):\n        if x2 is not None:\n            v1 = self.linear(x1)\n            v2 = v1 + x2\n            return v2\n        v1 = self.linear(x1)\n        return v1\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n \n    def forward(self, x1, other):\n        v1 = self.linear(x1)\n        v2 = v1 + other\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(2, 3, 4)\nx2 = torch.randn(2, 8, 4)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n \n    def forward(self, x1, other):\n        v1 = self.linear(x1)\n        v2 = v1 + other\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\nother = torch.randn(1, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(10, 11)\n \n    def forward(self, v):\n        v1 = self.linear(v)\n        v2 = v1 + 100\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx = torch.randn(5, 10)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(64, 8)\n \n    def forward(self, x1, other):\n        v1 = self.linear(x1)\n        v2 = v1 + other\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 64)\nother = torch.randn(1, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(10, 20)\n \n    def forward(self, x1, x2):\n        v1 = self.linear(x1)\n        v2 = v1 + x2\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(10)\nx2 = torch.randn(1, 20)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n \n    def forward(self, x1, other):\n        v1 = self.linear(x1)\n        v2 = v1 + other\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\nother = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(10, 10)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 + other\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 10)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(13, 9)\n \n    def forward(self, x1, other):\n        v1 = self.linear(x1)\n        v2 = v1 + other\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 13)\nother = torch.randn(1, 9)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(8, 8, bias=True)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 + other\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 8)\n",
                "\nimport torch\n\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(16, 16)\n \n    def forward(self, x1, x2=None):\n        if x2 is not None:\n            v1 = self.linear(x1)\n            v2 = v1 + x2\n            return v2\n        v1 = self.linear(x1)\n        return v1\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n \n    def forward(self, x1, other):\n        v1 = self.linear(x1)\n        v2 = v1 + other\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(2, 3, 4)\nx2 = torch.randn(2, 8, 4)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n \n    def forward(self, x1, other):\n        v1 = self.linear(x1)\n        v2 = v1 + other\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\nother = torch.randn(1, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(10, 11)\n \n    def forward(self, v):\n        v1 = self.linear(v)\n        v2 = v1 + 100\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx = torch.randn(5, 10)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(64, 8)\n \n    def forward(self, x1, other):\n        v1 = self.linear(x1)\n        v2 = v1 + other\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 64)\nother = torch.randn(1, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(10, 20)\n \n    def forward(self, x1, x2):\n        v1 = self.linear(x1)\n        v2 = v1 + x2\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(10)\nx2 = torch.randn(1, 20)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n \n    def forward(self, x1, other):\n        v1 = self.linear(x1)\n        v2 = v1 + other\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\nother = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(10, 10)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 + other\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 10)\n"
            ],
            "g_time": 5.384732961654663
        }
    }
}
{
    "opt": "unary=3",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(32, 24, (1, 1), stride=(1, 1), padding=(0, 0))\n        self.conv2 = torch.nn.Conv2d(24, 30, (1, 1), stride=(1, 1), padding=(0, 0))\n        self.conv3 = torch.nn.Conv2d(30, 16, (1, 1), stride=(1, 1), padding=(0, 0))\n        self.conv4 = torch.nn.Conv2d(16, 23, (1, 1), stride=(1, 1), padding=(0, 0))\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        v7 = self.conv2(v6)\n        v8 = v7 * 0.5\n        v9 = v7 * 0.7071067811865476\n        v10 = torch.erf(v9)\n        v11 = v10 + 1\n        v12 = v8 * v11\n        v13 = self.conv3(v12)\n        v14 = v13 * 0.5\n        v15 = v13 * 0.7071067811865476\n        v16 = torch.erf(v15)\n        v17 = v16 + 1\n        v18 = v14 * v17\n        v19 = self.conv4(v18)\n        return v19\n# Inputs to the model\nx1 = torch.randn(1, 32, 17, 17)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(32, 96, (3, 3), stride=(1, 1), padding=(1, 1))\n        self.conv2 = torch.nn.Conv2d(96, 136, (1, 1), stride=(1, 1), padding=(0, 0))\n        self.conv3 = torch.nn.Conv2d(136, 64, (3, 3), stride=(1, 1), padding=(1, 1))\n        self.conv4 = torch.nn.Conv2d(136, 295, (1, 1), stride=(1, 1), padding=(0, 0))\n        self.conv5 = torch.nn.Conv2d(295, 56, (3, 3), stride=(1, 1), padding=(1, 1))\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        v7 = self.conv2(v6)\n        v8 = v7 * 0.5\n        v9 = v7 * 0.7071067811865476\n        v10 = torch.erf(v9)\n        v11 = v10 + 1\n        v12 = v8 * v11\n        v13 = self.conv3(v12)\n        v14 = v13 * 0.5\n        v15 = v13 * 0.7071067811865476\n        v16 = torch.erf(v15)\n        v17 = v16 + 1\n        v18 = v14 * v17\n        v19 = self.conv4(v18)\n        v20 = v19 * 0.5\n        v21 = v19 * 0.7071067811865476\n        v22 = torch.erf(v21)\n        v23 = v22 + 1\n        v24 = v20 * v23\n        v25 = self.conv5(v24)\n        return v25\n# Inputs to the model\nx1 = torch.randn(1, 32, 17, 17)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv0_conv = torch.nn.Conv2d(3, 10, (8, 3), stride=(4, 1), padding=(0, 0))\n        self.conv1_conv = torch.nn.Conv2d(10, 8, 3, stride=1, padding=0)\n    def forward(self, x1):\n        v1 = self.conv0_conv(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        v7 = self.conv1_conv(v6)\n        return v7\n# Inputs to the model\nx1 = torch.randn(1, 3, 224, 736)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(16, 5, 3, stride=2, padding=1)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 16, 100, 120)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(24, 16, (1, 1), stride=(1, 1), padding=(0, 0))\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 24, 17, 17)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 128, 1, stride=2, padding=0)\n        self.conv2 = torch.nn.Conv2d(128, 128, 3, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        v7 = self.conv2(v6)\n        return v7\n# Inputs to the model\nx1 = torch.randn(1, 3, 224, 224)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(1, 1, 1, stride=1, padding=0)\n        self.conv2 = torch.nn.Conv2d(1, 1, 3, stride=1, padding=0)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        v7 = self.conv2(v6)\n        return v7\n# Inputs to the model\nx1 = torch.randn(1, 1, 4, 4)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(1, 6, 1, stride=1, padding=0)\n        self.conv2 = torch.nn.Conv2d(6, 7, 1, stride=1, padding=0)\n        self.conv3 = torch.nn.Conv2d(6, 2, 2, stride=1, padding=1)\n        self.conv4 = torch.nn.Conv2d(2, 3, 2, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        v7 = self.conv2(v6)\n        v8 = self.conv3(v6)\n        v9 = self.conv4(v8)\n        return v9\n# Inputs to the model\nx1 = torch.randn(1, 1, 28, 12)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(13, 22, 1, stride=1, padding=0)\n        self.conv2 = torch.nn.Conv2d(22, 31, 1, stride=1, padding=0)\n        self.conv3 = torch.nn.Conv2d(31, 5, 3, stride=1, padding=0)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        v7 = self.conv2(v6)\n        v8 = v7 * 0.5\n        v9 = v7 * 0.7071067811865476\n        v10 = torch.erf(v9)\n        v11 = v10 + 1\n        v12 = v8 * v11\n        v13 = self.conv3(v12)\n        return v13\n# Inputs to the model\nx1 = torch.randn(1, 13, 7, 7)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(21, 49, 1, stride=1, padding=0)\n        self.conv2 = torch.nn.Conv2d(49, 29, 1, stride=1, padding=0)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        v7 = self.conv2(v6)\n        return v7\n# Inputs to the model\nx1 = torch.randn(1, 21, 8, 12)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(32, 24, (1, 1), stride=(1, 1), padding=(0, 0))\n        self.conv2 = torch.nn.Conv2d(24, 30, (1, 1), stride=(1, 1), padding=(0, 0))\n        self.conv3 = torch.nn.Conv2d(30, 16, (1, 1), stride=(1, 1), padding=(0, 0))\n        self.conv4 = torch.nn.Conv2d(16, 23, (1, 1), stride=(1, 1), padding=(0, 0))\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        v7 = self.conv2(v6)\n        v8 = v7 * 0.5\n        v9 = v7 * 0.7071067811865476\n        v10 = torch.erf(v9)\n        v11 = v10 + 1\n        v12 = v8 * v11\n        v13 = self.conv3(v12)\n        v14 = v13 * 0.5\n        v15 = v13 * 0.7071067811865476\n        v16 = torch.erf(v15)\n        v17 = v16 + 1\n        v18 = v14 * v17\n        v19 = self.conv4(v18)\n        return v19\n# Inputs to the model\nx1 = torch.randn(1, 32, 17, 17)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(32, 96, (3, 3), stride=(1, 1), padding=(1, 1))\n        self.conv2 = torch.nn.Conv2d(96, 136, (1, 1), stride=(1, 1), padding=(0, 0))\n        self.conv3 = torch.nn.Conv2d(136, 64, (3, 3), stride=(1, 1), padding=(1, 1))\n        self.conv4 = torch.nn.Conv2d(136, 295, (1, 1), stride=(1, 1), padding=(0, 0))\n        self.conv5 = torch.nn.Conv2d(295, 56, (3, 3), stride=(1, 1), padding=(1, 1))\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        v7 = self.conv2(v6)\n        v8 = v7 * 0.5\n        v9 = v7 * 0.7071067811865476\n        v10 = torch.erf(v9)\n        v11 = v10 + 1\n        v12 = v8 * v11\n        v13 = self.conv3(v12)\n        v14 = v13 * 0.5\n        v15 = v13 * 0.7071067811865476\n        v16 = torch.erf(v15)\n        v17 = v16 + 1\n        v18 = v14 * v17\n        v19 = self.conv4(v18)\n        v20 = v19 * 0.5\n        v21 = v19 * 0.7071067811865476\n        v22 = torch.erf(v21)\n        v23 = v22 + 1\n        v24 = v20 * v23\n        v25 = self.conv5(v24)\n        return v25\n# Inputs to the model\nx1 = torch.randn(1, 32, 17, 17)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv0_conv = torch.nn.Conv2d(3, 10, (8, 3), stride=(4, 1), padding=(0, 0))\n        self.conv1_conv = torch.nn.Conv2d(10, 8, 3, stride=1, padding=0)\n    def forward(self, x1):\n        v1 = self.conv0_conv(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        v7 = self.conv1_conv(v6)\n        return v7\n# Inputs to the model\nx1 = torch.randn(1, 3, 224, 736)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(16, 5, 3, stride=2, padding=1)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 16, 100, 120)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(24, 16, (1, 1), stride=(1, 1), padding=(0, 0))\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 24, 17, 17)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 128, 1, stride=2, padding=0)\n        self.conv2 = torch.nn.Conv2d(128, 128, 3, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        v7 = self.conv2(v6)\n        return v7\n# Inputs to the model\nx1 = torch.randn(1, 3, 224, 224)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(1, 1, 1, stride=1, padding=0)\n        self.conv2 = torch.nn.Conv2d(1, 1, 3, stride=1, padding=0)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        v7 = self.conv2(v6)\n        return v7\n# Inputs to the model\nx1 = torch.randn(1, 1, 4, 4)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(1, 6, 1, stride=1, padding=0)\n        self.conv2 = torch.nn.Conv2d(6, 7, 1, stride=1, padding=0)\n        self.conv3 = torch.nn.Conv2d(6, 2, 2, stride=1, padding=1)\n        self.conv4 = torch.nn.Conv2d(2, 3, 2, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        v7 = self.conv2(v6)\n        v8 = self.conv3(v6)\n        v9 = self.conv4(v8)\n        return v9\n# Inputs to the model\nx1 = torch.randn(1, 1, 28, 12)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(13, 22, 1, stride=1, padding=0)\n        self.conv2 = torch.nn.Conv2d(22, 31, 1, stride=1, padding=0)\n        self.conv3 = torch.nn.Conv2d(31, 5, 3, stride=1, padding=0)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        v7 = self.conv2(v6)\n        v8 = v7 * 0.5\n        v9 = v7 * 0.7071067811865476\n        v10 = torch.erf(v9)\n        v11 = v10 + 1\n        v12 = v8 * v11\n        v13 = self.conv3(v12)\n        return v13\n# Inputs to the model\nx1 = torch.randn(1, 13, 7, 7)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(21, 49, 1, stride=1, padding=0)\n        self.conv2 = torch.nn.Conv2d(49, 29, 1, stride=1, padding=0)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        v7 = self.conv2(v6)\n        return v7\n# Inputs to the model\nx1 = torch.randn(1, 21, 8, 12)\n"
            ],
            "g_time": 20.874313354492188
        }
    }
}
{
    "opt": "mm_plus_mm",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def forward(self, x1, x2):\n        t1 = torch.mm(x1, x2)\n        t2 = torch.mm(x1, x2)\n        t3 = torch.mm(x1, x2)\n        return t1 + t2 + t3\n# Inputs to the model\nx1 = torch.randn(4, 4)\nx2 = torch.randn(4, 4)\n",
                "\nclass Model(torch.nn.Module):\n    def forward(self, x):\n        t0 = torch.mm(x, x)\n        t1 = torch.mm(x, x)\n        t2 = t1[:1,:]\n        t3 = t2*torch.tensor(3.0)\n        t4 = torch.mm(t0, t0)\n        out = torch.mm(t0, x)\n        out = t2 + t0 + out\n        return out\n# Inputs to the model\nx = torch.randn(1, 10)\n",
                "\nclass Model(torch.nn.Module):\n    def forward(self, x1, x2, x3, x4):\n        h1 = torch.mm(x1, x2)\n        h2 = torch.mm(x1, x2)\n        h3 = torch.mm(x3, x4)\n        h4 = torch.mm(x3, x4)\n        h3 = torch.mm(x1, x2)\n        return h1 + h2 + h3 + h4\n# Inputs to trigger different graph structures for the matrix multiplication nodes.\nx1 = torch.randn(4, 4)\nx2 = torch.randn(4, 4)\nx3 = torch.randn(4, 4)\nx4 = torch.randn(4, 4)\n",
                "\nclass Model(torch.nn.Module):\n    def forward(self, input1, input2):\n        t1 = torch.mm(input1, input1)\n        t2 = torch.mm(input2, input2)\n        return t1 + t2\n# Inputs to the model\ninput1 = torch.randn(5, 5)\ninput2 = torch.randn(5, 5)\n",
                "\nclass Model(torch.nn.Module):\n    def forward(self, x, y):\n        x = x - y\n        y = x ** 2\n        z = y - x\n        w = z * y\n        z = y * x\n        z = z + w\n        return z\n# Inputs to the model\nx = torch.randn(4, 4)\ny = torch.randn(4, 4)\n",
                "\nclass Model(torch.nn.Module):\n    def forward(self, x1, x2):\n        h1 = torch.mm(x1, x2)\n        return h1 + torch.mm(x2.t(), x1.t())\n# Inputs to the model\nx1 = torch.randn(5, 5)\nx2 = torch.randn(5, 5)\n",
                "\nclass Model(torch.nn.Module):\n    def forward(self, x, y, z):\n        t1 = torch.mm(y, z)\n        t2 = torch.mm(x, x)\n        out = t1 + t2 + y\n        return out\n# Inputs to the model\nx = torch.randn(100, 100)\ny = torch.randn(100, 100)\nz = torch.randn(100, 100)\n",
                "\nclass Model(torch.nn.Module):\n    def forward(self, input1):\n        t1 = torch.mm(input1, input1)\n        t2 = torch.mm(input1, input1)\n        return t1 + t2\n# Inputs to the model\ninput1 = torch.randn(3, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def forward(self, input1, input2):\n        t1 = torch.mm(input1, input2)\n        t2 = torch.mm(input1, input1)\n        return t2 + t1\n# Inputs to the model\ninput1 = torch.randn(5, 5)\ninput2 = torch.randn(5, 5)\n",
                "\nclass Model(torch.nn.Module):\n    def forward(self, x1, x2, x3):\n        h1 = torch.mm(x1, x2)\n        h2 = torch.mm(x3, x3)\n        h3 = torch.mm(x3, x2)\n        return h1 + h2 + h3\n# Inputs to the model\nx1 = torch.randn(5, 5)\nx2 = torch.randn(5, 5)\nx3 = torch.randn(5, 5)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def forward(self, x1, x2):\n        t1 = torch.mm(x1, x2)\n        t2 = torch.mm(x1, x2)\n        t3 = torch.mm(x1, x2)\n        return t1 + t2 + t3\n# Inputs to the model\nx1 = torch.randn(4, 4)\nx2 = torch.randn(4, 4)\n",
                "\nclass Model(torch.nn.Module):\n    def forward(self, x):\n        t0 = torch.mm(x, x)\n        t1 = torch.mm(x, x)\n        t2 = t1[:1,:]\n        t3 = t2*torch.tensor(3.0)\n        t4 = torch.mm(t0, t0)\n        out = torch.mm(t0, x)\n        out = t2 + t0 + out\n        return out\n# Inputs to the model\nx = torch.randn(1, 10)\n",
                "\nclass Model(torch.nn.Module):\n    def forward(self, x1, x2, x3, x4):\n        h1 = torch.mm(x1, x2)\n        h2 = torch.mm(x1, x2)\n        h3 = torch.mm(x3, x4)\n        h4 = torch.mm(x3, x4)\n        h3 = torch.mm(x1, x2)\n        return h1 + h2 + h3 + h4\n# Inputs to trigger different graph structures for the matrix multiplication nodes.\nx1 = torch.randn(4, 4)\nx2 = torch.randn(4, 4)\nx3 = torch.randn(4, 4)\nx4 = torch.randn(4, 4)\n",
                "\nclass Model(torch.nn.Module):\n    def forward(self, input1, input2):\n        t1 = torch.mm(input1, input1)\n        t2 = torch.mm(input2, input2)\n        return t1 + t2\n# Inputs to the model\ninput1 = torch.randn(5, 5)\ninput2 = torch.randn(5, 5)\n",
                "\nclass Model(torch.nn.Module):\n    def forward(self, x, y):\n        x = x - y\n        y = x ** 2\n        z = y - x\n        w = z * y\n        z = y * x\n        z = z + w\n        return z\n# Inputs to the model\nx = torch.randn(4, 4)\ny = torch.randn(4, 4)\n",
                "\nclass Model(torch.nn.Module):\n    def forward(self, x1, x2):\n        h1 = torch.mm(x1, x2)\n        return h1 + torch.mm(x2.t(), x1.t())\n# Inputs to the model\nx1 = torch.randn(5, 5)\nx2 = torch.randn(5, 5)\n",
                "\nclass Model(torch.nn.Module):\n    def forward(self, x, y, z):\n        t1 = torch.mm(y, z)\n        t2 = torch.mm(x, x)\n        out = t1 + t2 + y\n        return out\n# Inputs to the model\nx = torch.randn(100, 100)\ny = torch.randn(100, 100)\nz = torch.randn(100, 100)\n",
                "\nclass Model(torch.nn.Module):\n    def forward(self, input1):\n        t1 = torch.mm(input1, input1)\n        t2 = torch.mm(input1, input1)\n        return t1 + t2\n# Inputs to the model\ninput1 = torch.randn(3, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def forward(self, input1, input2):\n        t1 = torch.mm(input1, input2)\n        t2 = torch.mm(input1, input1)\n        return t2 + t1\n# Inputs to the model\ninput1 = torch.randn(5, 5)\ninput2 = torch.randn(5, 5)\n",
                "\nclass Model(torch.nn.Module):\n    def forward(self, x1, x2, x3):\n        h1 = torch.mm(x1, x2)\n        h2 = torch.mm(x3, x3)\n        h3 = torch.mm(x3, x2)\n        return h1 + h2 + h3\n# Inputs to the model\nx1 = torch.randn(5, 5)\nx2 = torch.randn(5, 5)\nx3 = torch.randn(5, 5)\n"
            ],
            "g_time": 6.033157825469971
        }
    }
}
{
    "opt": "addmm",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x0, x1, x2, x3, x4, x5, x6, x7, x8, x9, x10, x11):\n        v1 = torch.mm(x1, x2)\n        v2 = torch.mm(x3, x4)\n        v3, v4 = torch.max(torch.Tensor([v1, v2]), dim=0)\n        return x1 + x2\n# Inputs to the model\nx0 = torch.randn(3, 3, requires_grad=True)\nx1 = torch.randn(3, 3, requires_grad=True)\nx2 = torch.randn(3, 3, requires_grad=True)\nx3 = torch.randn(3, 3, requires_grad=True)\nx4 = torch.randn(3, 3, requires_grad=True)\nx5 = torch.randn(3, 3, requires_grad=True)\nx6 = torch.randn(3, 3, requires_grad=True)\nx7 = torch.randn(3, 3, requires_grad=True)\nx8 = torch.randn(3, 3, requires_grad=True)\nx9 = torch.randn(3, 3, requires_grad=True)\nx10 = torch.randn(3, 3, requires_grad=True)\nx11 = torch.randn(3, 3, requires_grad=True)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x, x2, x3, x4, x5):\n        v1 = torch.mm(x, x2)\n        v2 = torch.mm(x3, x)\n        v3 = torch.mm(x4, x5)\n        v4 = v1 - v2 + v3\n        v5 = torch.mm(torch.mm(x2, x2), v4)\n        return torch.mm(v5, x3)\n# Inputs to the model\nx = torch.randn(3, 3)\nx2 = torch.randn(3, 3)\nx3 = torch.randn(3, 3)\nx4 = torch.randn(3, 3)\nx5 = torch.randn(3, 3)\n# model ends",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, input1, input2):\n        t1 = torch.mm(input1, input2)\n        t2 = t1 + input1\n        return t2\n# Inputs to the model\ninput1 = torch.randn(3, 3)\ninput2 = torch.randn(3, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, x2, x3, x4, inp):\n        v1 = torch.mm(x1, inp)\n        v2 = torch.mm(x2, inp)\n        v3 = torch.mm(x3, inp)\n        v4 = torch.mm(x4, inp)\n        return v1 + v2 + v3 + v4\n# Inputs to the model\nx1 = torch.randn(3, 3)\nx2 = torch.randn(3, 3)\nx3 = torch.randn(3, 3)\nx4 = torch.randn(3, 3)\ninp = torch.randn(3, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.model = torch.nn.Linear(16, 16, bias=True)\n        self.w = torch.norm(self.model.weight, dim=1) + torch.norm(self.model.bias)\n    def forward(self, inp):\n        self.model.weight = 10*torch.eye(16)\n        self.model.bias = torch.zeros(16)\n        temp = self.model(inp)\n        return torch.mm(temp, inp) / self.w\n# Inputs to the model\ninp = torch.randn(16, 16, requires_grad=True)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, x2, x3, x4, x5, x6, x7, x8, x9, x10, x11, x12, x13, x14, inp):\n        v1 = torch.mm(inp, x1)\n        v2 = torch.mm(inp, x2)\n        v3 = torch.mm(inp, x3)\n        v4 = torch.mm(inp, x4)\n        v5 = torch.mm(inp, x5)\n        v6 = torch.mm(inp, x6)\n        v7 = torch.mm(inp, x7)\n        v8 = torch.mm(inp, x8)\n        v9 = torch.mm(inp, x9)\n        v10 = torch.mm(x2, x10)\n        v11 = torch.mm(x12, x11)\n        v12 = torch.mm(x13, x14)\n        return v9 + v10 + v11 + v12\n# Inputs to the model\nx1 = torch.randn(3, 3)\nx2 = torch.randn(3, 3)\nx3 = torch.randn(3, 3)\nx4 = torch.randn(3, 3)\nx5 = torch.randn(3, 3)\nx6 = torch.randn(3, 3)\nx7 = torch.randn(3, 3)\nx8 = torch.randn(3, 3)\nx9 = torch.randn(3, 3)\nx10 = torch.randn(3, 3)\nx11 = torch.randn(3, 3)\nx12 = torch.randn(3, 3)\nx13 = torch.randn(3, 3)\nx14 = torch.randn(3, 3)\ninp = torch.randn(3, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, w1, inp):\n        v1 = w1 * inp\n        v2 = v1 * w1\n        return v1 + v2\n# Inputs to the model\nw1 = torch.randn(3, 3, requires_grad=True)\ninp = torch.randn(3, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv0 = torch.nn.Conv2d(3, 32, kernel_size=1)\n        self.conv1 = torch.nn.Conv2d(32, 32, kernel_size=1)\n        self.conv2 = torch.nn.Conv2d(32, 3, kernel_size=1)\n    def forward(self, x):\n        v1 = self.conv0(x)\n        v2 = self.conv1(v1)\n        return self.conv2(v2)\n# Inputs to the model\nx = torch.randn(1, 3, 10, 10)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, x2, t1, t2):\n        t1 = t1 + x1\n        return (t1 + t2) * 2\n# Inputs to the model\nx1 = torch.randn(3, 3, requires_grad=True)\nx2 = torch.randn(3, 3, requires_grad=True)\nt1 = torch.randn(3, 3, requires_grad=True)\nt2 = torch.randn(3, 3, requires_grad=True)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.mm1 = torch.nn.Linear(4, 4)\n    def forward(self, x, y, z):\n        self.mm1.weight = torch.Tensor([[1.0, 1.0, 1.0, 1.0], [2.0, 2.0, 2.0, 2.0], [3.0, 3.0, 3.0, 3.0], [4.0, 4.0, 4.0, 4.0]])\n        v1 = self.mm1(x)\n        v2 = torch.mm(y, v1)\n        v3 = v2 + z\n        return v3\n# Inputs to the model\nx = torch.randn(3, 4)\ny = torch.randn(3, 4)\nz = torch.randn(3, 4)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x0, x1, x2, x3, x4, x5, x6, x7, x8, x9, x10, x11):\n        v1 = torch.mm(x1, x2)\n        v2 = torch.mm(x3, x4)\n        v3, v4 = torch.max(torch.Tensor([v1, v2]), dim=0)\n        return x1 + x2\n# Inputs to the model\nx0 = torch.randn(3, 3, requires_grad=True)\nx1 = torch.randn(3, 3, requires_grad=True)\nx2 = torch.randn(3, 3, requires_grad=True)\nx3 = torch.randn(3, 3, requires_grad=True)\nx4 = torch.randn(3, 3, requires_grad=True)\nx5 = torch.randn(3, 3, requires_grad=True)\nx6 = torch.randn(3, 3, requires_grad=True)\nx7 = torch.randn(3, 3, requires_grad=True)\nx8 = torch.randn(3, 3, requires_grad=True)\nx9 = torch.randn(3, 3, requires_grad=True)\nx10 = torch.randn(3, 3, requires_grad=True)\nx11 = torch.randn(3, 3, requires_grad=True)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x, x2, x3, x4, x5):\n        v1 = torch.mm(x, x2)\n        v2 = torch.mm(x3, x)\n        v3 = torch.mm(x4, x5)\n        v4 = v1 - v2 + v3\n        v5 = torch.mm(torch.mm(x2, x2), v4)\n        return torch.mm(v5, x3)\n# Inputs to the model\nx = torch.randn(3, 3)\nx2 = torch.randn(3, 3)\nx3 = torch.randn(3, 3)\nx4 = torch.randn(3, 3)\nx5 = torch.randn(3, 3)\n# model ends",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, input1, input2):\n        t1 = torch.mm(input1, input2)\n        t2 = t1 + input1\n        return t2\n# Inputs to the model\ninput1 = torch.randn(3, 3)\ninput2 = torch.randn(3, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, x2, x3, x4, inp):\n        v1 = torch.mm(x1, inp)\n        v2 = torch.mm(x2, inp)\n        v3 = torch.mm(x3, inp)\n        v4 = torch.mm(x4, inp)\n        return v1 + v2 + v3 + v4\n# Inputs to the model\nx1 = torch.randn(3, 3)\nx2 = torch.randn(3, 3)\nx3 = torch.randn(3, 3)\nx4 = torch.randn(3, 3)\ninp = torch.randn(3, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.model = torch.nn.Linear(16, 16, bias=True)\n        self.w = torch.norm(self.model.weight, dim=1) + torch.norm(self.model.bias)\n    def forward(self, inp):\n        self.model.weight = 10*torch.eye(16)\n        self.model.bias = torch.zeros(16)\n        temp = self.model(inp)\n        return torch.mm(temp, inp) / self.w\n# Inputs to the model\ninp = torch.randn(16, 16, requires_grad=True)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, x2, x3, x4, x5, x6, x7, x8, x9, x10, x11, x12, x13, x14, inp):\n        v1 = torch.mm(inp, x1)\n        v2 = torch.mm(inp, x2)\n        v3 = torch.mm(inp, x3)\n        v4 = torch.mm(inp, x4)\n        v5 = torch.mm(inp, x5)\n        v6 = torch.mm(inp, x6)\n        v7 = torch.mm(inp, x7)\n        v8 = torch.mm(inp, x8)\n        v9 = torch.mm(inp, x9)\n        v10 = torch.mm(x2, x10)\n        v11 = torch.mm(x12, x11)\n        v12 = torch.mm(x13, x14)\n        return v9 + v10 + v11 + v12\n# Inputs to the model\nx1 = torch.randn(3, 3)\nx2 = torch.randn(3, 3)\nx3 = torch.randn(3, 3)\nx4 = torch.randn(3, 3)\nx5 = torch.randn(3, 3)\nx6 = torch.randn(3, 3)\nx7 = torch.randn(3, 3)\nx8 = torch.randn(3, 3)\nx9 = torch.randn(3, 3)\nx10 = torch.randn(3, 3)\nx11 = torch.randn(3, 3)\nx12 = torch.randn(3, 3)\nx13 = torch.randn(3, 3)\nx14 = torch.randn(3, 3)\ninp = torch.randn(3, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, w1, inp):\n        v1 = w1 * inp\n        v2 = v1 * w1\n        return v1 + v2\n# Inputs to the model\nw1 = torch.randn(3, 3, requires_grad=True)\ninp = torch.randn(3, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv0 = torch.nn.Conv2d(3, 32, kernel_size=1)\n        self.conv1 = torch.nn.Conv2d(32, 32, kernel_size=1)\n        self.conv2 = torch.nn.Conv2d(32, 3, kernel_size=1)\n    def forward(self, x):\n        v1 = self.conv0(x)\n        v2 = self.conv1(v1)\n        return self.conv2(v2)\n# Inputs to the model\nx = torch.randn(1, 3, 10, 10)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, x2, t1, t2):\n        t1 = t1 + x1\n        return (t1 + t2) * 2\n# Inputs to the model\nx1 = torch.randn(3, 3, requires_grad=True)\nx2 = torch.randn(3, 3, requires_grad=True)\nt1 = torch.randn(3, 3, requires_grad=True)\nt2 = torch.randn(3, 3, requires_grad=True)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.mm1 = torch.nn.Linear(4, 4)\n    def forward(self, x, y, z):\n        self.mm1.weight = torch.Tensor([[1.0, 1.0, 1.0, 1.0], [2.0, 2.0, 2.0, 2.0], [3.0, 3.0, 3.0, 3.0], [4.0, 4.0, 4.0, 4.0]])\n        v1 = self.mm1(x)\n        v2 = torch.mm(y, v1)\n        v3 = v2 + z\n        return v3\n# Inputs to the model\nx = torch.randn(3, 4)\ny = torch.randn(3, 4)\nz = torch.randn(3, 4)\n"
            ],
            "g_time": 14.346944093704224
        }
    }
}
{
    "opt": "unary=12",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 3, 2, stride=1, padding=0, dilation=1)\n        self.conv1 = torch.nn.Conv2d(3, 3, 2, stride=1, padding=0, dilation=1)\n        self.conv2 = torch.nn.Conv2d(3, 3, 3, stride=1, padding=0, dilation=2)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = self.conv1(v1)\n        v3 = self.conv2(v1)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(64, 32, 3, stride=1, padding=1, bias=0)\n        self.sigmoid = torch.nn.Sigmoid()\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = self.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 64, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(8, 2, 31, stride=1, padding=5, dilation=1)\n        self.conv2 = torch.nn.Conv2d(8, 2, 31, stride=1, padding=5, dilation=2)\n        self.sigmoid = torch.nn.Sigmoid()\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = self.conv2(x1)\n        v3 = torch.mul(v1, v2)\n        v4 = self.sigmoid(v3)\n        v5 = v1 + v4\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 8, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 3, 21, stride=3, padding=0, dilation=1)\n        self.sigmoid = torch.nn.Sigmoid()\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = self.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 3, 31, 31)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(39, 19, 45, stride=10, padding=17, dilation=10)\n        self.sigmoid = torch.nn.Sigmoid()\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = self.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 39, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.ConvTranspose2d(48, 50, 2, stride=1, padding=0)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = torch.mul(v1, v2)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 48, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(64, 64, 1, stride=1, padding=0)\n        self.flatten = torch.nn.Flatten()\n        self.linear_ = torch.nn.Linear(64, 128)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = self.flatten(v1)\n        v3 = self.linear_(v2)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 64, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(121, 64, 1, stride=1, padding=0)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v2 * v1\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 121, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(224, 24, 1, stride=1, padding=0)\n        self.conv1 = torch.nn.Conv2d(24, 24, 1, stride=1, padding=0)\n        self.conv2 = torch.nn.Conv2d(24, 24, 1, stride=1, padding=0)\n        self.conv3 = torch.nn.Conv2d(24, 24, 1, stride=1, padding=0)\n        self.conv4 = torch.nn.Conv2d(24, 24, 1, stride=1, padding=0)\n        self.sigmoid = torch.nn.Sigmoid()\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = self.conv1(v1)\n        v3 = self.conv2(v2)\n        v4 = self.conv3(v3)\n        v5 = self.conv4(v4)\n        v6 = self.sigmoid(v5)\n        v7 = v5 * v6\n        return v7\n# Inputs to the model\nx1 = torch.randn(1, 224, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(64, 150, 1, stride=1, padding=0, dilation=1)\n        self.sigmoid = torch.nn.Sigmoid()\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = self.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n# Input to the model\nx1 = torch.randn(1, 64, 64, 64)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 3, 2, stride=1, padding=0, dilation=1)\n        self.conv1 = torch.nn.Conv2d(3, 3, 2, stride=1, padding=0, dilation=1)\n        self.conv2 = torch.nn.Conv2d(3, 3, 3, stride=1, padding=0, dilation=2)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = self.conv1(v1)\n        v3 = self.conv2(v1)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(64, 32, 3, stride=1, padding=1, bias=0)\n        self.sigmoid = torch.nn.Sigmoid()\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = self.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 64, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(8, 2, 31, stride=1, padding=5, dilation=1)\n        self.conv2 = torch.nn.Conv2d(8, 2, 31, stride=1, padding=5, dilation=2)\n        self.sigmoid = torch.nn.Sigmoid()\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = self.conv2(x1)\n        v3 = torch.mul(v1, v2)\n        v4 = self.sigmoid(v3)\n        v5 = v1 + v4\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 8, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 3, 21, stride=3, padding=0, dilation=1)\n        self.sigmoid = torch.nn.Sigmoid()\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = self.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 3, 31, 31)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(39, 19, 45, stride=10, padding=17, dilation=10)\n        self.sigmoid = torch.nn.Sigmoid()\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = self.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 39, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.ConvTranspose2d(48, 50, 2, stride=1, padding=0)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = torch.mul(v1, v2)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 48, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(64, 64, 1, stride=1, padding=0)\n        self.flatten = torch.nn.Flatten()\n        self.linear_ = torch.nn.Linear(64, 128)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = self.flatten(v1)\n        v3 = self.linear_(v2)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 64, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(121, 64, 1, stride=1, padding=0)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v2 * v1\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 121, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(224, 24, 1, stride=1, padding=0)\n        self.conv1 = torch.nn.Conv2d(24, 24, 1, stride=1, padding=0)\n        self.conv2 = torch.nn.Conv2d(24, 24, 1, stride=1, padding=0)\n        self.conv3 = torch.nn.Conv2d(24, 24, 1, stride=1, padding=0)\n        self.conv4 = torch.nn.Conv2d(24, 24, 1, stride=1, padding=0)\n        self.sigmoid = torch.nn.Sigmoid()\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = self.conv1(v1)\n        v3 = self.conv2(v2)\n        v4 = self.conv3(v3)\n        v5 = self.conv4(v4)\n        v6 = self.sigmoid(v5)\n        v7 = v5 * v6\n        return v7\n# Inputs to the model\nx1 = torch.randn(1, 224, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(64, 150, 1, stride=1, padding=0, dilation=1)\n        self.sigmoid = torch.nn.Sigmoid()\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = self.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n# Input to the model\nx1 = torch.randn(1, 64, 64, 64)\n"
            ],
            "g_time": 9.997373104095459
        }
    }
}
{
    "opt": "unary=2",
    "response": {
        "0": {
            "raw": [
                "\nimport torch\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(1, 2, 2, bias=False, dilation=3)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        temp1 = v1.transpose(1, 2).transpose(2, 1)\n        v2 = temp1 * 0.5\n        v3 = temp1 * temp1 * temp1\n        v4 = v3 * 0.044715\n        temp2 = v1 + v4\n        v5 = temp2 * 0.7978845608028654\n        v6 = torch.tanh(v5)\n        v7 = v6 + 1\n        v8 = v2 * v7\n        v9 = v1.sum(axis=2, keepdim=True).sum(axis=1, keepdim=True)\n        return v9\n# Inputs to the model\nx1 = torch.randn(3, 1, 6, 6)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose1d(32, 32, 3, stride=1, padding=1)\n        self.conv = torch.nn.Conv1d(1, 4096, 1, groups=1)\n        self.add = torch.nn.quantized.FloatFunctional()\n        self.mul = torch.nn.quantized.FloatFunctional()\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v10 = self.conv(v1)\n        v2 = self.mul.mul(v10, torch.tensor(0.5, dtype=torch.float32))\n        v3 = v2 * v10 * v10\n        v4 = v3 * torch.tensor(0.044715, dtype=torch.float32)\n        v11 = self.add.add(v10, v4)\n        v5 = v11 * torch.tensor(0.7978845608028654, dtype=torch.float32)\n        v12 = torch.tanh(v5)\n        v8 = v12 * torch.tensor(2, dtype=torch.float32)\n        v13 = self.add.mul(v1, v8)\n        return v13\n# Inputs to the model\nx1 = torch.randn(1, 32, 8, requires_grad=True)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1):\n        v1 = x1\n        v2 = v1 * 0.5\n        v3 = v1 * v1 * v1\n        v4 = v3 * 0.044715\n        v5 = v1 + v4\n        v6 = v5 * 0.7978845608028654\n        v7 = torch.tanh(v6)\n        v8 = v7 + 1\n        v9 = v2 * v8\n        return v9\n# Inputs to the model\nx1 = torch.randn(8, 8, 1, 1, 2, 1024, 25)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose0 = torch.nn.ConvTranspose2d(1, 1, 2, stride=2, padding=0, groups=2)\n        self.conv_transpose1 = torch.nn.ConvTranspose2d(1, 1, 2, stride=2, padding=0, dilation=2, groups=2)\n    def forward(self, x1):\n        v1 = self.conv_transpose0(x1)\n        v2 = self.conv_transpose1(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 1, 7, 7)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose1 = torch.nn.ConvTranspose2d(1, 1, 1, stride=[1, 1])\n        self.conv2 = torch.nn.Conv2d(1, 1, 1, stride=[1, 1], groups=1)\n    def forward(self, x1):\n        v1 = self.conv_transpose1(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * v1 * v1\n        v4 = v3 * 0.044715\n        v5 = v1 + v4\n        v6 = v5 * 0.7978845608028654\n        v7 = torch.tanh(v6)\n        v8 = v7 + 1\n        v9 = v2 * v8\n        v10 = self.conv2(v9)\n        return v10\n# Inputs to the model\nx1 = torch.randn(1, 1, 6, 5)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(1, 9, [3, 2], stride=2, dilation=[2, 1], padding=[0, 2])\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * v1 * v1\n        v4 = v3 * 0.044715\n        v5 = v1 + v4\n        v6 = v5 * 0.7978845608028654\n        v7 = torch.tanh(v6)\n        v8 = v7 + 1\n        v9 = v2 * v8\n        return v9\n# Inputs to the model\nx1 = torch.randn(1, 1, 6, 7)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(16, 16, (1, 1), stride=(1, 1))\n        self.conv = torch.nn.Conv2d(16, 16, (3, 3), groups=1, dilation=1, bias=False)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * v1 * v1\n        v4 = v3 * 0.044715\n        v5 = v1 + v4\n        v6 = v5 * 0.7978845608028654\n        v7 = torch.tanh(v6)\n        v8 = v7 + 1\n        v9 = v2 * v8\n        v10 = self.conv(v9)\n        return v10\n# Inputs to the model\nx1 = torch.randn(1, 16, 8, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(1, 8, 7, stride=3, padding=7)\n        self.gelu = torch.nn.GELU()\n        self.conv_transpose = torch.nn.ConvTranspose2d(1, 1, (1, 1), stride=(1, 1))\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = self.gelu(v1)\n        v3 = v2 * 0.5\n        v4 = v2 * v2 * v2\n        v5 = v4 * 0.044715\n        v6 = v2 + v5\n        v7 = v6 * 0.7978845608028654\n        v8 = torch.tanh(v7)\n        v9 = v8 + 1\n        v10 = v3 * v9\n        v11 = self.conv_transpose(v10)\n        return v11\n# Inputs to the model\nx1 = torch.randn(1, 1, 35, 35)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(7, 2, 8, stride=[2, 1], padding=[4, 2], dilation=[1, 2])\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * v1 * v1\n        v4 = v3 * 0.044715\n        v5 = v1 + v4\n        v6 = v5 * 0.7978845608028654\n        v7 = torch.tanh(v6)\n        v8 = v7 + 1\n        v9 = v2 * v8\n        return v9\n# Inputs to the model\nx1 = torch.randn(1, 7, 7, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose3d = torch.nn.ConvTranspose3d(2, 16, (1, 4, 5), stride=(1, 2, 5))\n    def forward(self, x1):\n        v1 = self.conv_transpose3d(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * v1 * v1\n        v4 = v3 * 0.044715\n        v5 = v1 + v4\n        v6 = v5 * 0.7978845608028654\n        v7 = torch.tanh(v6)\n        v8 = v7 + 1\n        v9 = v2 * v8\n        return v9\n# Inputs to the model\nx1 = torch.randn(1, 2, 4, 7, 12)\n"
            ],
            "code": [
                "\nimport torch\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(1, 2, 2, bias=False, dilation=3)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        temp1 = v1.transpose(1, 2).transpose(2, 1)\n        v2 = temp1 * 0.5\n        v3 = temp1 * temp1 * temp1\n        v4 = v3 * 0.044715\n        temp2 = v1 + v4\n        v5 = temp2 * 0.7978845608028654\n        v6 = torch.tanh(v5)\n        v7 = v6 + 1\n        v8 = v2 * v7\n        v9 = v1.sum(axis=2, keepdim=True).sum(axis=1, keepdim=True)\n        return v9\n# Inputs to the model\nx1 = torch.randn(3, 1, 6, 6)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose1d(32, 32, 3, stride=1, padding=1)\n        self.conv = torch.nn.Conv1d(1, 4096, 1, groups=1)\n        self.add = torch.nn.quantized.FloatFunctional()\n        self.mul = torch.nn.quantized.FloatFunctional()\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v10 = self.conv(v1)\n        v2 = self.mul.mul(v10, torch.tensor(0.5, dtype=torch.float32))\n        v3 = v2 * v10 * v10\n        v4 = v3 * torch.tensor(0.044715, dtype=torch.float32)\n        v11 = self.add.add(v10, v4)\n        v5 = v11 * torch.tensor(0.7978845608028654, dtype=torch.float32)\n        v12 = torch.tanh(v5)\n        v8 = v12 * torch.tensor(2, dtype=torch.float32)\n        v13 = self.add.mul(v1, v8)\n        return v13\n# Inputs to the model\nx1 = torch.randn(1, 32, 8, requires_grad=True)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1):\n        v1 = x1\n        v2 = v1 * 0.5\n        v3 = v1 * v1 * v1\n        v4 = v3 * 0.044715\n        v5 = v1 + v4\n        v6 = v5 * 0.7978845608028654\n        v7 = torch.tanh(v6)\n        v8 = v7 + 1\n        v9 = v2 * v8\n        return v9\n# Inputs to the model\nx1 = torch.randn(8, 8, 1, 1, 2, 1024, 25)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose0 = torch.nn.ConvTranspose2d(1, 1, 2, stride=2, padding=0, groups=2)\n        self.conv_transpose1 = torch.nn.ConvTranspose2d(1, 1, 2, stride=2, padding=0, dilation=2, groups=2)\n    def forward(self, x1):\n        v1 = self.conv_transpose0(x1)\n        v2 = self.conv_transpose1(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 1, 7, 7)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose1 = torch.nn.ConvTranspose2d(1, 1, 1, stride=[1, 1])\n        self.conv2 = torch.nn.Conv2d(1, 1, 1, stride=[1, 1], groups=1)\n    def forward(self, x1):\n        v1 = self.conv_transpose1(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * v1 * v1\n        v4 = v3 * 0.044715\n        v5 = v1 + v4\n        v6 = v5 * 0.7978845608028654\n        v7 = torch.tanh(v6)\n        v8 = v7 + 1\n        v9 = v2 * v8\n        v10 = self.conv2(v9)\n        return v10\n# Inputs to the model\nx1 = torch.randn(1, 1, 6, 5)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(1, 9, [3, 2], stride=2, dilation=[2, 1], padding=[0, 2])\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * v1 * v1\n        v4 = v3 * 0.044715\n        v5 = v1 + v4\n        v6 = v5 * 0.7978845608028654\n        v7 = torch.tanh(v6)\n        v8 = v7 + 1\n        v9 = v2 * v8\n        return v9\n# Inputs to the model\nx1 = torch.randn(1, 1, 6, 7)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(16, 16, (1, 1), stride=(1, 1))\n        self.conv = torch.nn.Conv2d(16, 16, (3, 3), groups=1, dilation=1, bias=False)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * v1 * v1\n        v4 = v3 * 0.044715\n        v5 = v1 + v4\n        v6 = v5 * 0.7978845608028654\n        v7 = torch.tanh(v6)\n        v8 = v7 + 1\n        v9 = v2 * v8\n        v10 = self.conv(v9)\n        return v10\n# Inputs to the model\nx1 = torch.randn(1, 16, 8, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(1, 8, 7, stride=3, padding=7)\n        self.gelu = torch.nn.GELU()\n        self.conv_transpose = torch.nn.ConvTranspose2d(1, 1, (1, 1), stride=(1, 1))\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = self.gelu(v1)\n        v3 = v2 * 0.5\n        v4 = v2 * v2 * v2\n        v5 = v4 * 0.044715\n        v6 = v2 + v5\n        v7 = v6 * 0.7978845608028654\n        v8 = torch.tanh(v7)\n        v9 = v8 + 1\n        v10 = v3 * v9\n        v11 = self.conv_transpose(v10)\n        return v11\n# Inputs to the model\nx1 = torch.randn(1, 1, 35, 35)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(7, 2, 8, stride=[2, 1], padding=[4, 2], dilation=[1, 2])\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * v1 * v1\n        v4 = v3 * 0.044715\n        v5 = v1 + v4\n        v6 = v5 * 0.7978845608028654\n        v7 = torch.tanh(v6)\n        v8 = v7 + 1\n        v9 = v2 * v8\n        return v9\n# Inputs to the model\nx1 = torch.randn(1, 7, 7, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose3d = torch.nn.ConvTranspose3d(2, 16, (1, 4, 5), stride=(1, 2, 5))\n    def forward(self, x1):\n        v1 = self.conv_transpose3d(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * v1 * v1\n        v4 = v3 * 0.044715\n        v5 = v1 + v4\n        v6 = v5 * 0.7978845608028654\n        v7 = torch.tanh(v6)\n        v8 = v7 + 1\n        v9 = v2 * v8\n        return v9\n# Inputs to the model\nx1 = torch.randn(1, 2, 4, 7, 12)\n"
            ],
            "g_time": 12.496778726577759
        }
    }
}
{
    "opt": "unary=25",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 > 0\n        v3 = v1 * 0.01\n        v4 = torch.where(v2, v1, v3)\n        return v4\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(32, 32)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 > 0\n        v3 = v1 * 0.01\n        v4 = torch.where(v2, v1, v3)\n        return v4\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(16, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, negative_slope=0.1):\n        super().__init__()\n        self.negative_slope = negative_slope\n \n    def forward(self, x1):\n        v1 = torch.nn.Linear(6, 12, bias=True)(x1)\n        v2 = v1 > 0\n        v3 = v1 * self.negative_slope\n        v4 = torch.where(v2, v1, v3)\n        return v4\n\n# Initializing the model\nm = Model(negative_slope=0.1)\n\n# Inputs to the model\nx1 = torch.randn(1, 6)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(10, 10)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 > 0\n        v3 = v1 * 0.01\n        v4 = torch.where(v2, v1, v3)\n        return v4\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 10)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n \n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, negative_slope=0.1):\n        super().__init__()\n        self.linear = torch.nn.Linear(5, 5)\n        self.negative_slope = negative_slope\n \n    def forward(self, x):\n        v1 = self.linear(x)\n        v2 = v1 > 0\n        v3 = v1 * self.negative_slope\n        v4 = torch.where(v2, v1, v3)\n        return v4\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx = torch.randn(3, 5)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, negative_slope=0.1):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 16)\n        self.negative_slope = negative_slope\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 > 0\n        v3 = v1 * self.negative_slope\n        v4 = torch.where(v2, v1, v3)\n        return v4\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(20, 40)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 > 0\n        v3 = v1 * 0.01\n        v4 = torch.where(v2, v1, v3)\n        return v4\n\n# Initializing the model; setting the negative slope to 0 for the first layer of the network\nm = Model()\nm.linear.bias.data.fill_(0)\n\n# Inputs to the model\nx1 = torch.randn(1, 20, 10, 10)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(64, 1024)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 > 0\n        v3 = v1 * negative_slope\n        v4 = torch.where(v2, v1, v3)\n        return v4\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, negative_slope):\n        super().__init__()\n        self.linear = torch.nn.Linear(8, 8)\n        self.negative_slope = negative_slope\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 > 0\n        v3 = v1 * self.negative_slope\n        v4 = torch.where(v2, v1, v3)\n        return v4\n        \nfor negative in [negative for negative in np.arange(0.001, 0.1, 0.001)]:\n    m = Model(negative)\n    # Inputs to the model\n    x1 = torch.randn(1, 8)\n    __output_diff__ = m(x1)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 > 0\n        v3 = v1 * 0.01\n        v4 = torch.where(v2, v1, v3)\n        return v4\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(32, 32)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 > 0\n        v3 = v1 * 0.01\n        v4 = torch.where(v2, v1, v3)\n        return v4\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(16, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, negative_slope=0.1):\n        super().__init__()\n        self.negative_slope = negative_slope\n \n    def forward(self, x1):\n        v1 = torch.nn.Linear(6, 12, bias=True)(x1)\n        v2 = v1 > 0\n        v3 = v1 * self.negative_slope\n        v4 = torch.where(v2, v1, v3)\n        return v4\n\n# Initializing the model\nm = Model(negative_slope=0.1)\n\n# Inputs to the model\nx1 = torch.randn(1, 6)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(10, 10)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 > 0\n        v3 = v1 * 0.01\n        v4 = torch.where(v2, v1, v3)\n        return v4\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 10)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n \n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, negative_slope=0.1):\n        super().__init__()\n        self.linear = torch.nn.Linear(5, 5)\n        self.negative_slope = negative_slope\n \n    def forward(self, x):\n        v1 = self.linear(x)\n        v2 = v1 > 0\n        v3 = v1 * self.negative_slope\n        v4 = torch.where(v2, v1, v3)\n        return v4\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx = torch.randn(3, 5)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, negative_slope=0.1):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 16)\n        self.negative_slope = negative_slope\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 > 0\n        v3 = v1 * self.negative_slope\n        v4 = torch.where(v2, v1, v3)\n        return v4\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(20, 40)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 > 0\n        v3 = v1 * 0.01\n        v4 = torch.where(v2, v1, v3)\n        return v4\n\n# Initializing the model; setting the negative slope to 0 for the first layer of the network\nm = Model()\nm.linear.bias.data.fill_(0)\n\n# Inputs to the model\nx1 = torch.randn(1, 20, 10, 10)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(64, 1024)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 > 0\n        v3 = v1 * negative_slope\n        v4 = torch.where(v2, v1, v3)\n        return v4\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, negative_slope):\n        super().__init__()\n        self.linear = torch.nn.Linear(8, 8)\n        self.negative_slope = negative_slope\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 > 0\n        v3 = v1 * self.negative_slope\n        v4 = torch.where(v2, v1, v3)\n        return v4\n        \nfor negative in [negative for negative in np.arange(0.001, 0.1, 0.001)]:\n    m = Model(negative)\n    # Inputs to the model\n    x1 = torch.randn(1, 8)\n    __output_diff__ = m(x1)\n"
            ],
            "g_time": 7.141325235366821
        }
    }
}
{
    "opt": "sfdp=1",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n \n    def forward(self, query, key, value, dropout_p):\n        qk = torch.matmul(query, key.transpose(-2, -1))\n        inv_scale_factor = (1.0 / (query.size(-2) ** 0.5))\n        scaled_qk = qk.div(inv_scale_factor)\n        softmax_qk = scaled_qk.softmax(dim=-1)\n        dropout_qk = torch.nn.functional.dropout(softmax_qk, p=dropout_p)\n        return dropout_qk.matmul(value)\n\ndropout_p = 0.0\n\n# Initializing the model\nm = Model()\n\n# Tensors to the model\nquery = torch.randn(10, 100, 64, 32)\nkey = torch.randn(10, 200, 32, 16)\nvalue = torch.randn(10, 200, 32, 16)\n",
                "\nclass BertAttModel(BertPreTrainedModel):\n    def __init__(self, config):\n        super().__init__(config)\n        self.num_attention_heads = config.attention_heads\n        self.hidden_size = config.hidden_size\n\n        self.matmul1 = torch.nn.Linear(self.hidden_size, self.num_attention_heads * self.hidden_size)\n        self.matmul2 = torch.nn.Linear(self.hidden_size, self.num_attention_heads * self.hidden_size, bias=False)\n        self.matmul3 = torch.nn.Linear(self.hidden_size, self.num_attention_heads * self.hidden_size, bias=False)\n    \n    def forward(self, query, key, value, dropout_p):\n        inv_scale_factor = torch.rsqrt(torch.Tensor([self.num_attention_heads*self.hidden_size*128])).to(query.device)\n\n        qk = self.matmul1(query)\n        qk = torch.matmul(qk, key.transpose(-2,-1)/inv_scale_factor.unsqueeze(0).unsqueeze(0))\n        \n        dropout_qk = torch.nn.functional.dropout(qk, p=dropout_p)\n        softmax_qk = torch.nn.Softmax(dim=-1)(dropout_qk)\n        output = torch.matmul(softmax_qk, value)\n\n        return output\n\n# Initializing the query/key/value tensors\nquery = torch.randn(1, 32, 4)\nkey = torch.randn(1, 32, 4)\n__value = value = torch.randn(1, 32, 4)\n\n# Dropout probability\ndropout_p = 0.1\n\n# Initializing the model\nmodel = BertAttModel.from_pase(model_name, dropout_p=dropout_p)\n\nprint(__value)\n# Inputs to the model\noutput = model(query, key, value, dropout_p)\n\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, query, key, value, dropout_p):\n        inv_scale_factor = torch.pow((1.0 - dropout_p), 0.5)\n        qk = torch.matmul(query, key.transpose(-2, -1))\n        scaled_qk = qk.div(inv_scale_factor)\n        return scaled_qk.softmax(dim=-1).matmul(value)\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nquery = torch.randn(37, 16, 8)\nkey = torch.randn(37, 6, 8)\nvalue = torch.randn(37, 6, 8)\ndropout_p = torch.nn.functional.dropout(torch.rand(1), p=0.5)\n",
                "\ndef scaled_dot_product_attention(query, key, value, inv_scale_factor, dropout_p):\n    qk = torch.matmul(query, key.transpose(-2, -1))\n    scaled_qk = qk.div(inv_scale_factor)\n    softmax_qk = scaled_qk.softmax(dim=-1)\n    dropout_qk = torch.nn.functional.dropout(softmax_qk, p=dropout_p)\n    output = dropout_qk.matmul(value)\n    return output\n\nclass CustomAttentionLayer(torch.nn.Module):\n    def __init__(self, dim, heads=1, dropout=0.0, bias=True):\n        super().__init__()\n        self.proj1 = torch.nn.Linear(dim, dim)\n        self.proj2 = torch.nn.Linear(dim, dim, bias=False)\n        self.proj3 = torch.nn.Linear(dim, dim, bias=bias)\n        self.projection = torch.nn.Linear(dim, dim)\n        self.scale_factor = dim ** -0.5\n        assert(dim % heads == 0)\n        self.heads = heads\n        self.dropout = torch.nn.Dropout(dropout)\n \n    def forward(self, input, mask=None, return_att_weights=False):\n        n = input.size(1)\n        a, b, c = input.size(-3), input.size(-2), input.size(-1)\n        input = input.view(n, a * b, c).transpose(-2, -1)\n        q = self.proj1(input)\n        k = self.proj2(input)\n        v = self.proj3(input)\n        q = q.view(n, a, b, self.heads * c // self.heads)\n        k = k.view(n, a, b, self.heads * c // self.heads)\n        v = v.view(n, a, b, self.heads * c // self.heads)\n        v = v.transpose(-3, -2)\n        dropout_qk = scaled_dot_product_attention(q, k, v, self.scale_factor, self.dropout.p)\n        dropout_qk = dropout_qk.transpose(-3, -2)\n        output = dropout_qk.contiguous().view(n, a * b, self.heads * c // self.heads)\n        output = output + input\n        output = self.projection(output)\n        if return_att_weights:\n            att_weights = torch.matmul(q.transpose(2, 3), k.transpose(2, 3)).transpose(1, 2)\n            return output, att_weights[:, :a * b // self.heads, :a * b // self.heads]\n        return output\n \nclass Model(torch.nn.Module):\n    def __init__(self, dim, depth=2, heads=1, dim_head=None, scale_head=1, dropout=0.0, activation=None, num_classes=100):\n        super().__init__()\n        dim_head = dim_head or dim\n        assert dim_head % heads == 0\n        self.heads = heads\n        inner_dim = heads * dim_head\n        self.scale_factor = scale_head ** -0.5\n        padding = (depth % 2 == 0) * (depth // 2) * (dim_head // 2)\n        layers = [torch.nn.ModuleList([\n            CustomAttentionLayer(dim=dim, heads=heads, dropout=dropout, bias=True),\n            torch.nn.Linear(num_classes, num_classes),\n        ])]\n        for i in range(1, depth):\n            layers[-1].append(torch.nn.ModuleList([\n                CustomAttentionLayer(dim=dim, heads=heads, dropout=dropout, bias=False),\n                torch.nn.Linear(num_classes, num_classes, bias=False),\n            ]))\n        self.layers = torch.nn.ModuleList([\n            torch.nn.Sequential(\n                torch.nn.Linear(num_classes, dim, bias=False),\n                torch.nn.Conv1d(in_channels=dim, out_channels=dim, kernel_size=1, stride=1, padding=padding, bias=False),\n            ) for _ in range(depth - 1)\n        ]) + layers\n        self.activation = activation or torch.nn.ReLU()\n        self.output = torch.nn.Linear(num_classes, num_classes)\n \n    def forward(self, x1, mask=None, return_att_weights=False):\n        n = x1.size(1)\n        x = x1\n        for i in range(len(self.layers)):\n            layer_input = self.layers[i][0](x)\n            for j in range(i):\n                if j % 2 == 0:\n                    layer_input = self.layers[j][0](layer_input, return_att_weights=return_att_weights)\n                else:\n                    layer_input = self.layers[j][1](layer_input) + layer_input\n            if i < len(self.layers) - 1:\n                layer_output = self.activation(layer_input)\n                for k in range(i - len(self.layers) + 2, 0, -2):\n                    if k - 1 < 0:\n                        layer_output = self.layers[k - 1][0](layer_output)\n                    else:\n                        layer_output = self.layers[k - 1][0](layer_output, return_att_weights=return_att_weights)\n                    layer_output = layer_output + self.layers[k - 1][1](layer_output)\n                    layer_output = self.activation(layer_output)\n                x = self.layers[i][1](layer_output)\n        output = self.output(x)\n        if return_att_weights:\n            att_weights = torch.tensor([\n                torch.sum(\n                    self.scale_factor * torch.sigmoid(t.narrow(-2, 0, self.heads * n // self.heads).contiguous().view(n, self.heads, n // self.heads, n // self.heads)), dim=[-1, -2]\n                ).t()\n                for t in x1  # torch.stack([torch.sum(self.scale_factor * torch.sigmoid(t.narrow(-2, 0, self.heads * n // self.heads).contiguous().view(n, self.heads, n // self.heads, n // self.heads)), dim=[-1, -2]).t() for t in x1])\n            ])\n            return output, att_weights[:, :n // self.heads, :n // self.heads]\n        return output\n\n# Initializing the model\nm = Model(2)\n\n# Inputs to the model\nx1 = torch.randn(1, 100, 2)\nx2 = torch.randn(100, 100)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        pass\n \n    def forward(self, query, key, value, scale_factor):\n        ",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.q = torch.nn.Linear(in_features, out_features)\n        self.k = torch.nn.Linear(in_features, out_features)\n        self.v = torch.nn.Linear(in_features, out_features)\n \n    def forward(self, q, k, v):\n        q = self.q(q)\n        k = self.k(k)\n        v = self.v(v)\n        qk = torch.matmul(qk, k.transpose(-2, -1))\n        scaled_qk = qk.div(1/sqrt(out_features))\n        softmax_qk = scaled_qk.softmax(dim=-1)\n        output = softmax_qk.matmul(v)\n        return output\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(batch_size, in_features)\nx2 = torch.randn(batch_size, in_features)\nx3 = torch.randn(batch_size, in_features)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.attn_dropout = torch.nn.Dropout(dropout_p)\n \n    def forward(self, query, key, value):\n        qk = torch.matmul(query, key.transpose(-2, -1))\n        scaled_qk = qk.div(inv_scale_factor)\n        softmax_qk = scaled_qk.softmax(dim=-1)\n        dropout_qk = self.attn_dropout(softmax_qk)\n        output = dropout_qk.matmul(value)\n        return output\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nquery = torch.randn(1, 1, 3, 2)\nkey = torch.randn(1, 20, 3, 2)\nvalue = torch.randn(1, 20, 3, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, num_heads=2):\n        super().__init__()\n        self.num_heads = num_heads\n \n    def forward(self, query, key, value, inv_scale_factor=1.0, dropout_p=0.5):\n        qk = torch.matmul(query, key.transpose(-2, -1))\n        scaled_qk = qk.div(inv_scale_factor)\n        softmax_qk = scaled_qk.softmax(dim=-1)\n        dropout_qk = torch.nn.functional.dropout(softmax_qk, p=dropout_p)\n        output = dropout_qk.matmul(value)\n        return output\n\n# Inputs to the model\nquery = torch.randn(1, 2, 3, 4)\nkey = torch.randn(1, 2, 4, 3)\nvalue = torch.randn(1, 2, 4, 5)\ninv_scale_factor = 1.0\ndropout_p = 0.5\n\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n \n    def forward(self, query, key, value, inv_scale_factor, dropout_p):\n        qk = torch.matmul(query, key.transpose(-2, -1))\n        scaled_qk = qk.div(inv_scale_factor)\n        softmax_qk = scaled_qk.softmax(dim=-1)\n        dropout_qk = torch.nn.functional.dropout(softmax_qk, p=dropout_p)\n        output = dropout_qk.matmul(value)\n        return output\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nquery = torch.randn(1, 4, 8)\nkey = torch.randn(1, 4, 8)\nvalue = torch.randn(1, 4, 8)\ninv_scale_factor = torch.tensor(1.)\ndropout_p = torch.tensor(0.)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.qk = torch.nn.Linear(30, 40)\n        self.value = torch.nn.Linear(40, 20)\n \n    def forward(self, input__1):\n        qk = self.qk(input__1)\n        scaled_qk = qk.div(inv_scale_factor)\n        softmax_qk = scaled_qk.softmax(dim=-1)\n        dropout_qk = torch.nn.functional.dropout(softmax_qk, p=dropout_p)\n        output = dropout_qk.matmul(self.value)\n        return output\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 768)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n \n    def forward(self, query, key, value, dropout_p):\n        qk = torch.matmul(query, key.transpose(-2, -1))\n        inv_scale_factor = (1.0 / (query.size(-2) ** 0.5))\n        scaled_qk = qk.div(inv_scale_factor)\n        softmax_qk = scaled_qk.softmax(dim=-1)\n        dropout_qk = torch.nn.functional.dropout(softmax_qk, p=dropout_p)\n        return dropout_qk.matmul(value)\n\ndropout_p = 0.0\n\n# Initializing the model\nm = Model()\n\n# Tensors to the model\nquery = torch.randn(10, 100, 64, 32)\nkey = torch.randn(10, 200, 32, 16)\nvalue = torch.randn(10, 200, 32, 16)\n",
                "\nclass BertAttModel(BertPreTrainedModel):\n    def __init__(self, config):\n        super().__init__(config)\n        self.num_attention_heads = config.attention_heads\n        self.hidden_size = config.hidden_size\n\n        self.matmul1 = torch.nn.Linear(self.hidden_size, self.num_attention_heads * self.hidden_size)\n        self.matmul2 = torch.nn.Linear(self.hidden_size, self.num_attention_heads * self.hidden_size, bias=False)\n        self.matmul3 = torch.nn.Linear(self.hidden_size, self.num_attention_heads * self.hidden_size, bias=False)\n    \n    def forward(self, query, key, value, dropout_p):\n        inv_scale_factor = torch.rsqrt(torch.Tensor([self.num_attention_heads*self.hidden_size*128])).to(query.device)\n\n        qk = self.matmul1(query)\n        qk = torch.matmul(qk, key.transpose(-2,-1)/inv_scale_factor.unsqueeze(0).unsqueeze(0))\n        \n        dropout_qk = torch.nn.functional.dropout(qk, p=dropout_p)\n        softmax_qk = torch.nn.Softmax(dim=-1)(dropout_qk)\n        output = torch.matmul(softmax_qk, value)\n\n        return output\n\n# Initializing the query/key/value tensors\nquery = torch.randn(1, 32, 4)\nkey = torch.randn(1, 32, 4)\n__value = value = torch.randn(1, 32, 4)\n\n# Dropout probability\ndropout_p = 0.1\n\n# Initializing the model\nmodel = BertAttModel.from_pase(model_name, dropout_p=dropout_p)\n\nprint(__value)\n# Inputs to the model\noutput = model(query, key, value, dropout_p)\n\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, query, key, value, dropout_p):\n        inv_scale_factor = torch.pow((1.0 - dropout_p), 0.5)\n        qk = torch.matmul(query, key.transpose(-2, -1))\n        scaled_qk = qk.div(inv_scale_factor)\n        return scaled_qk.softmax(dim=-1).matmul(value)\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nquery = torch.randn(37, 16, 8)\nkey = torch.randn(37, 6, 8)\nvalue = torch.randn(37, 6, 8)\ndropout_p = torch.nn.functional.dropout(torch.rand(1), p=0.5)\n",
                "\ndef scaled_dot_product_attention(query, key, value, inv_scale_factor, dropout_p):\n    qk = torch.matmul(query, key.transpose(-2, -1))\n    scaled_qk = qk.div(inv_scale_factor)\n    softmax_qk = scaled_qk.softmax(dim=-1)\n    dropout_qk = torch.nn.functional.dropout(softmax_qk, p=dropout_p)\n    output = dropout_qk.matmul(value)\n    return output\n\nclass CustomAttentionLayer(torch.nn.Module):\n    def __init__(self, dim, heads=1, dropout=0.0, bias=True):\n        super().__init__()\n        self.proj1 = torch.nn.Linear(dim, dim)\n        self.proj2 = torch.nn.Linear(dim, dim, bias=False)\n        self.proj3 = torch.nn.Linear(dim, dim, bias=bias)\n        self.projection = torch.nn.Linear(dim, dim)\n        self.scale_factor = dim ** -0.5\n        assert(dim % heads == 0)\n        self.heads = heads\n        self.dropout = torch.nn.Dropout(dropout)\n \n    def forward(self, input, mask=None, return_att_weights=False):\n        n = input.size(1)\n        a, b, c = input.size(-3), input.size(-2), input.size(-1)\n        input = input.view(n, a * b, c).transpose(-2, -1)\n        q = self.proj1(input)\n        k = self.proj2(input)\n        v = self.proj3(input)\n        q = q.view(n, a, b, self.heads * c // self.heads)\n        k = k.view(n, a, b, self.heads * c // self.heads)\n        v = v.view(n, a, b, self.heads * c // self.heads)\n        v = v.transpose(-3, -2)\n        dropout_qk = scaled_dot_product_attention(q, k, v, self.scale_factor, self.dropout.p)\n        dropout_qk = dropout_qk.transpose(-3, -2)\n        output = dropout_qk.contiguous().view(n, a * b, self.heads * c // self.heads)\n        output = output + input\n        output = self.projection(output)\n        if return_att_weights:\n            att_weights = torch.matmul(q.transpose(2, 3), k.transpose(2, 3)).transpose(1, 2)\n            return output, att_weights[:, :a * b // self.heads, :a * b // self.heads]\n        return output\n \nclass Model(torch.nn.Module):\n    def __init__(self, dim, depth=2, heads=1, dim_head=None, scale_head=1, dropout=0.0, activation=None, num_classes=100):\n        super().__init__()\n        dim_head = dim_head or dim\n        assert dim_head % heads == 0\n        self.heads = heads\n        inner_dim = heads * dim_head\n        self.scale_factor = scale_head ** -0.5\n        padding = (depth % 2 == 0) * (depth // 2) * (dim_head // 2)\n        layers = [torch.nn.ModuleList([\n            CustomAttentionLayer(dim=dim, heads=heads, dropout=dropout, bias=True),\n            torch.nn.Linear(num_classes, num_classes),\n        ])]\n        for i in range(1, depth):\n            layers[-1].append(torch.nn.ModuleList([\n                CustomAttentionLayer(dim=dim, heads=heads, dropout=dropout, bias=False),\n                torch.nn.Linear(num_classes, num_classes, bias=False),\n            ]))\n        self.layers = torch.nn.ModuleList([\n            torch.nn.Sequential(\n                torch.nn.Linear(num_classes, dim, bias=False),\n                torch.nn.Conv1d(in_channels=dim, out_channels=dim, kernel_size=1, stride=1, padding=padding, bias=False),\n            ) for _ in range(depth - 1)\n        ]) + layers\n        self.activation = activation or torch.nn.ReLU()\n        self.output = torch.nn.Linear(num_classes, num_classes)\n \n    def forward(self, x1, mask=None, return_att_weights=False):\n        n = x1.size(1)\n        x = x1\n        for i in range(len(self.layers)):\n            layer_input = self.layers[i][0](x)\n            for j in range(i):\n                if j % 2 == 0:\n                    layer_input = self.layers[j][0](layer_input, return_att_weights=return_att_weights)\n                else:\n                    layer_input = self.layers[j][1](layer_input) + layer_input\n            if i < len(self.layers) - 1:\n                layer_output = self.activation(layer_input)\n                for k in range(i - len(self.layers) + 2, 0, -2):\n                    if k - 1 < 0:\n                        layer_output = self.layers[k - 1][0](layer_output)\n                    else:\n                        layer_output = self.layers[k - 1][0](layer_output, return_att_weights=return_att_weights)\n                    layer_output = layer_output + self.layers[k - 1][1](layer_output)\n                    layer_output = self.activation(layer_output)\n                x = self.layers[i][1](layer_output)\n        output = self.output(x)\n        if return_att_weights:\n            att_weights = torch.tensor([\n                torch.sum(\n                    self.scale_factor * torch.sigmoid(t.narrow(-2, 0, self.heads * n // self.heads).contiguous().view(n, self.heads, n // self.heads, n // self.heads)), dim=[-1, -2]\n                ).t()\n                for t in x1  # torch.stack([torch.sum(self.scale_factor * torch.sigmoid(t.narrow(-2, 0, self.heads * n // self.heads).contiguous().view(n, self.heads, n // self.heads, n // self.heads)), dim=[-1, -2]).t() for t in x1])\n            ])\n            return output, att_weights[:, :n // self.heads, :n // self.heads]\n        return output\n\n# Initializing the model\nm = Model(2)\n\n# Inputs to the model\nx1 = torch.randn(1, 100, 2)\nx2 = torch.randn(100, 100)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        pass\n \n    def forward(self, query, key, value, scale_factor):\n        ",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.q = torch.nn.Linear(in_features, out_features)\n        self.k = torch.nn.Linear(in_features, out_features)\n        self.v = torch.nn.Linear(in_features, out_features)\n \n    def forward(self, q, k, v):\n        q = self.q(q)\n        k = self.k(k)\n        v = self.v(v)\n        qk = torch.matmul(qk, k.transpose(-2, -1))\n        scaled_qk = qk.div(1/sqrt(out_features))\n        softmax_qk = scaled_qk.softmax(dim=-1)\n        output = softmax_qk.matmul(v)\n        return output\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(batch_size, in_features)\nx2 = torch.randn(batch_size, in_features)\nx3 = torch.randn(batch_size, in_features)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.attn_dropout = torch.nn.Dropout(dropout_p)\n \n    def forward(self, query, key, value):\n        qk = torch.matmul(query, key.transpose(-2, -1))\n        scaled_qk = qk.div(inv_scale_factor)\n        softmax_qk = scaled_qk.softmax(dim=-1)\n        dropout_qk = self.attn_dropout(softmax_qk)\n        output = dropout_qk.matmul(value)\n        return output\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nquery = torch.randn(1, 1, 3, 2)\nkey = torch.randn(1, 20, 3, 2)\nvalue = torch.randn(1, 20, 3, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, num_heads=2):\n        super().__init__()\n        self.num_heads = num_heads\n \n    def forward(self, query, key, value, inv_scale_factor=1.0, dropout_p=0.5):\n        qk = torch.matmul(query, key.transpose(-2, -1))\n        scaled_qk = qk.div(inv_scale_factor)\n        softmax_qk = scaled_qk.softmax(dim=-1)\n        dropout_qk = torch.nn.functional.dropout(softmax_qk, p=dropout_p)\n        output = dropout_qk.matmul(value)\n        return output\n\n# Inputs to the model\nquery = torch.randn(1, 2, 3, 4)\nkey = torch.randn(1, 2, 4, 3)\nvalue = torch.randn(1, 2, 4, 5)\ninv_scale_factor = 1.0\ndropout_p = 0.5\n\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n \n    def forward(self, query, key, value, inv_scale_factor, dropout_p):\n        qk = torch.matmul(query, key.transpose(-2, -1))\n        scaled_qk = qk.div(inv_scale_factor)\n        softmax_qk = scaled_qk.softmax(dim=-1)\n        dropout_qk = torch.nn.functional.dropout(softmax_qk, p=dropout_p)\n        output = dropout_qk.matmul(value)\n        return output\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nquery = torch.randn(1, 4, 8)\nkey = torch.randn(1, 4, 8)\nvalue = torch.randn(1, 4, 8)\ninv_scale_factor = torch.tensor(1.)\ndropout_p = torch.tensor(0.)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.qk = torch.nn.Linear(30, 40)\n        self.value = torch.nn.Linear(40, 20)\n \n    def forward(self, input__1):\n        qk = self.qk(input__1)\n        scaled_qk = qk.div(inv_scale_factor)\n        softmax_qk = scaled_qk.softmax(dim=-1)\n        dropout_qk = torch.nn.functional.dropout(softmax_qk, p=dropout_p)\n        output = dropout_qk.matmul(self.value)\n        return output\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 768)\n"
            ],
            "g_time": 50.78436732292175
        }
    }
}
{
    "opt": "unary=0",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(4, 1, 1, stride=3, padding=1)\n    def forward(self, x2):\n        v1 = self.conv(x2)\n        v2 = v1 * 0.5\n        v3 = v1 * v1\n        v4 = v3 * v1\n        v5 = v4 * 0.044715\n        v6 = v1 + v5\n        v7 = v6 * 0.7978845608028654\n        v8 = torch.tanh(v7)\n        v9 = v8 + 1\n        v10 = v2 * v9\n        return v10\n# Inputs to the model\nx2 = torch.randn(1, 4, 11, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(1, 15, 52, stride=3, padding=22)\n    def forward(self, x2):\n        v1 = self.conv(x2)\n        return v1\n# Inputs to the model\nx2 = torch.randn(1, 1, 56, 40)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 3, 2, stride=2, padding=0)\n    def forward(self, x4):\n        v1 = self.conv(x4)\n        v2 = v1 * 0.5\n        v3 = v1 * v1\n        v4 = v3 * v1\n        v5 = v4 * 0.044715\n        v6 = v1 + v5\n        v7 = v6 * 0.7978845608028654\n        v8 = torch.tanh(v7)\n        v9 = v8 + 1\n        v10 = v2 * v9\n        return v10\n# Inputs to the model\nx4 = torch.randn(1, 3, 16, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(8, 12, 2, stride=1, padding=3)\n    def forward(self, x7):\n        v1 = self.conv(x7)\n        v2 = v1 * 0.5\n        v3 = v1 * v1\n        v4 = v3 * v1\n        v5 = v4 * 0.044715\n        v6 = v1 + v5\n        v7 = v6 * 0.7978845608028654\n        v8 = torch.tanh(v7)\n        v9 = v8 + 1\n        v10 = v2 * v9\n        return v10\n# Inputs to the model\nx7 = torch.randn(1, 8, 31, 65)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(88, 55, 4, stride=6, padding=1)\n    def forward(self, x19):\n        v1 = self.conv(x19)\n        v2 = v1 * 0.5\n        v3 = v1 * v1\n        v4 = v3 * v1\n        v5 = v4 * 0.044715\n        v6 = v1 + v5\n        v7 = v6 * 0.7978845608028654\n        v8 = torch.tanh(v7)\n        v9 = v8 + 1\n        v10 = v2 * v9\n        return v10\n# Inputs to the model\nx19 = torch.randn(1, 88, 3, 10)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(63, 123, 3, stride=1, padding=2)\n    def forward(self, x8):\n        v2 = torch.randn(3, 101, 69, 87)\n        v1 = self.conv(x8)\n        v3 = v1 * 0.5\n        v4 = v1 * v1\n        v5 = v4 * 0.044715\n        v6 = v1 + v5\n        v7 = v6 * 0.7978845608028654\n        v8 = torch.tanh(v7)\n        v9 = v8 + 1\n        v10 = v3 + v9\n        v11 = v10 + torch.tanh(v7)\n        v12 = v2 * v11\n        return v12\n# Inputs to the model\nx8 = torch.randn(1, 63, 93, 45)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(1, 3, 2, stride=2, padding=1)\n        self.conv2 = torch.nn.Conv2d(3, 1, 3, stride=1, padding=5)\n    def forward(self, x43):\n        v1 = self.conv1(x43)\n        v3 = self.conv2(v1)\n        v4 = v3 * 0.5\n        v5 = v3 * v3\n        v6 = v5 * v3\n        v7 = v6 * 0.044715\n        v8 = v3 + v7\n        v9 = v8 * 0.7978845608028654\n        v10 = torch.tanh(v9)\n        v11 = v10 + 1\n        v12 = v4 * v11\n        return v12\n# Inputs to the model\nx43 = torch.randn(1, 1, 13, 7)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(103, 263, 3, stride=1, padding=2)\n    def forward(self, x47):\n        v1 = self.conv(x47)\n        v2 = v1 * 0.5\n        v3 = v1 * v1\n        v4 = v3 * v1\n        v5 = v4 * 0.044715\n        v6 = v1 + v5\n        v7 = v6 * 0.7978845608028654\n        v8 = torch.tanh(v7)\n        v9 = v8 + 1\n        v10 = v2 * v9\n        return v10\n# Inputs to the model\nx47 = torch.randn(1, 103, 45, 89)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.bn1 = torch.nn.BatchNorm2d(37)\n        self.conv1 = torch.nn.Conv2d(37, 112, 2, stride=2, padding=0)\n        self.bn2 = torch.nn.BatchNorm2d(112)\n        self.conv2 = torch.nn.Conv2d(112, 288, 2, stride=2, padding=0)\n        self.bn3 = torch.nn.BatchNorm2d(288)\n        self.conv3 = torch.nn.Conv2d(288, 624, 2, stride=2, padding=0)\n        self.bn4 = torch.nn.BatchNorm2d(624)\n        self.conv4 = torch.nn.Conv2d(624, 1296, 2, stride=2, padding=0)\n        self.bn5 = torch.nn.BatchNorm2d(1296)\n        self.conv5 = torch.nn.Conv2d(1296, 2332, 2, stride=2, padding=0)\n    def forward(self, x162):\n        v1 = self.bn1(x162)\n        v2 = self.conv1(v1)\n        v4 = self.bn2(v2)\n        v5 = self.conv2(v4)\n        v7 = self.bn3(v5)\n        v8 = self.conv3(v7)\n        v10 = self.bn4(v8)\n        v11 = self.conv4(v10)\n        v13 = self.bn5(v11)\n        v14 = self.conv5(v13)\n        v16 = v14 * 0.5\n        v17 = v14 * v14\n        v18 = v17 * v14\n        v19 = v18 * 0.044715\n        v20 = v14 + v19\n        v21 = v20 * 0.7978845608028654\n        v22 = torch.tanh(v21)\n        v23 = v22 + 1\n        v24 = v16 * v23\n        return v24\n# Inputs to the model\nx162 = torch.randn(1, 37, 38, 62)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(10, 30, 3, stride=1, padding=2)\n    def forward(self, x7756):\n        v1 = self.conv(x7756)\n        v2 = v1 * torch.sin(x7756)\n        v3 = v2 * 0.5\n        v4 = v2 * v2\n        v5 = v4 * v2\n        v6 = v5 * 0.044715\n        v7 = v2 + v6\n        v8 = v7 * 0.7978845608028654\n        v9 = torch.tanh(v8)\n        v10 = v9 + 1\n        v11 = v3 * v10\n        return v11\n# Inputs to the model\nx7756 = torch.randn(1, 10, 25, 72)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(4, 1, 1, stride=3, padding=1)\n    def forward(self, x2):\n        v1 = self.conv(x2)\n        v2 = v1 * 0.5\n        v3 = v1 * v1\n        v4 = v3 * v1\n        v5 = v4 * 0.044715\n        v6 = v1 + v5\n        v7 = v6 * 0.7978845608028654\n        v8 = torch.tanh(v7)\n        v9 = v8 + 1\n        v10 = v2 * v9\n        return v10\n# Inputs to the model\nx2 = torch.randn(1, 4, 11, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(1, 15, 52, stride=3, padding=22)\n    def forward(self, x2):\n        v1 = self.conv(x2)\n        return v1\n# Inputs to the model\nx2 = torch.randn(1, 1, 56, 40)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 3, 2, stride=2, padding=0)\n    def forward(self, x4):\n        v1 = self.conv(x4)\n        v2 = v1 * 0.5\n        v3 = v1 * v1\n        v4 = v3 * v1\n        v5 = v4 * 0.044715\n        v6 = v1 + v5\n        v7 = v6 * 0.7978845608028654\n        v8 = torch.tanh(v7)\n        v9 = v8 + 1\n        v10 = v2 * v9\n        return v10\n# Inputs to the model\nx4 = torch.randn(1, 3, 16, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(8, 12, 2, stride=1, padding=3)\n    def forward(self, x7):\n        v1 = self.conv(x7)\n        v2 = v1 * 0.5\n        v3 = v1 * v1\n        v4 = v3 * v1\n        v5 = v4 * 0.044715\n        v6 = v1 + v5\n        v7 = v6 * 0.7978845608028654\n        v8 = torch.tanh(v7)\n        v9 = v8 + 1\n        v10 = v2 * v9\n        return v10\n# Inputs to the model\nx7 = torch.randn(1, 8, 31, 65)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(88, 55, 4, stride=6, padding=1)\n    def forward(self, x19):\n        v1 = self.conv(x19)\n        v2 = v1 * 0.5\n        v3 = v1 * v1\n        v4 = v3 * v1\n        v5 = v4 * 0.044715\n        v6 = v1 + v5\n        v7 = v6 * 0.7978845608028654\n        v8 = torch.tanh(v7)\n        v9 = v8 + 1\n        v10 = v2 * v9\n        return v10\n# Inputs to the model\nx19 = torch.randn(1, 88, 3, 10)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(63, 123, 3, stride=1, padding=2)\n    def forward(self, x8):\n        v2 = torch.randn(3, 101, 69, 87)\n        v1 = self.conv(x8)\n        v3 = v1 * 0.5\n        v4 = v1 * v1\n        v5 = v4 * 0.044715\n        v6 = v1 + v5\n        v7 = v6 * 0.7978845608028654\n        v8 = torch.tanh(v7)\n        v9 = v8 + 1\n        v10 = v3 + v9\n        v11 = v10 + torch.tanh(v7)\n        v12 = v2 * v11\n        return v12\n# Inputs to the model\nx8 = torch.randn(1, 63, 93, 45)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(1, 3, 2, stride=2, padding=1)\n        self.conv2 = torch.nn.Conv2d(3, 1, 3, stride=1, padding=5)\n    def forward(self, x43):\n        v1 = self.conv1(x43)\n        v3 = self.conv2(v1)\n        v4 = v3 * 0.5\n        v5 = v3 * v3\n        v6 = v5 * v3\n        v7 = v6 * 0.044715\n        v8 = v3 + v7\n        v9 = v8 * 0.7978845608028654\n        v10 = torch.tanh(v9)\n        v11 = v10 + 1\n        v12 = v4 * v11\n        return v12\n# Inputs to the model\nx43 = torch.randn(1, 1, 13, 7)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(103, 263, 3, stride=1, padding=2)\n    def forward(self, x47):\n        v1 = self.conv(x47)\n        v2 = v1 * 0.5\n        v3 = v1 * v1\n        v4 = v3 * v1\n        v5 = v4 * 0.044715\n        v6 = v1 + v5\n        v7 = v6 * 0.7978845608028654\n        v8 = torch.tanh(v7)\n        v9 = v8 + 1\n        v10 = v2 * v9\n        return v10\n# Inputs to the model\nx47 = torch.randn(1, 103, 45, 89)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.bn1 = torch.nn.BatchNorm2d(37)\n        self.conv1 = torch.nn.Conv2d(37, 112, 2, stride=2, padding=0)\n        self.bn2 = torch.nn.BatchNorm2d(112)\n        self.conv2 = torch.nn.Conv2d(112, 288, 2, stride=2, padding=0)\n        self.bn3 = torch.nn.BatchNorm2d(288)\n        self.conv3 = torch.nn.Conv2d(288, 624, 2, stride=2, padding=0)\n        self.bn4 = torch.nn.BatchNorm2d(624)\n        self.conv4 = torch.nn.Conv2d(624, 1296, 2, stride=2, padding=0)\n        self.bn5 = torch.nn.BatchNorm2d(1296)\n        self.conv5 = torch.nn.Conv2d(1296, 2332, 2, stride=2, padding=0)\n    def forward(self, x162):\n        v1 = self.bn1(x162)\n        v2 = self.conv1(v1)\n        v4 = self.bn2(v2)\n        v5 = self.conv2(v4)\n        v7 = self.bn3(v5)\n        v8 = self.conv3(v7)\n        v10 = self.bn4(v8)\n        v11 = self.conv4(v10)\n        v13 = self.bn5(v11)\n        v14 = self.conv5(v13)\n        v16 = v14 * 0.5\n        v17 = v14 * v14\n        v18 = v17 * v14\n        v19 = v18 * 0.044715\n        v20 = v14 + v19\n        v21 = v20 * 0.7978845608028654\n        v22 = torch.tanh(v21)\n        v23 = v22 + 1\n        v24 = v16 * v23\n        return v24\n# Inputs to the model\nx162 = torch.randn(1, 37, 38, 62)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(10, 30, 3, stride=1, padding=2)\n    def forward(self, x7756):\n        v1 = self.conv(x7756)\n        v2 = v1 * torch.sin(x7756)\n        v3 = v2 * 0.5\n        v4 = v2 * v2\n        v5 = v4 * v2\n        v6 = v5 * 0.044715\n        v7 = v2 + v6\n        v8 = v7 * 0.7978845608028654\n        v9 = torch.tanh(v8)\n        v10 = v9 + 1\n        v11 = v3 * v10\n        return v11\n# Inputs to the model\nx7756 = torch.randn(1, 10, 25, 72)\n"
            ],
            "g_time": 20.277196645736694
        }
    }
}
{
    "opt": "binary=6",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(16, 32)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 - 1\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layer = torch.nn.Linear(20, 20)\n \n    def forward(self, x1):\n        v1 = self.layer(x1)\n        v2 = v1 - x1\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 20)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Conv2d(16, 8, 1, stride=1, padding=1)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 16, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(8, 1)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 - 1\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n       super().__init__()\n       self.linear = torch.nn.Linear(3, 4)\n \n    def forward(self, x1):\n       v1 = self.linear(x1)\n       v2 = v1 - other\n       return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 - 2\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Conv2d(3, 5, 1, stride=1, padding=1)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 - other\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(1, 2)\n \n    def forward(self, x1, x2):\n        v1 = self.linear(x1)\n        v2 = v1 - x2\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 1)\nx2 = torch.randn(2, 1)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 1)\n\n    def forward(self, x1, other):\n        v1 = self.linear(x1)\n        v2 = v1 - other\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\nother = torch.randn(2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc = torch.nn.Linear(10, 20)\n \n    def forward(self, x1):\n        v1 = self.fc(x1)\n        v2 = v1 - 1\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 10)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(16, 32)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 - 1\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layer = torch.nn.Linear(20, 20)\n \n    def forward(self, x1):\n        v1 = self.layer(x1)\n        v2 = v1 - x1\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 20)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Conv2d(16, 8, 1, stride=1, padding=1)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 16, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(8, 1)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 - 1\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n       super().__init__()\n       self.linear = torch.nn.Linear(3, 4)\n \n    def forward(self, x1):\n       v1 = self.linear(x1)\n       v2 = v1 - other\n       return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 - 2\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Conv2d(3, 5, 1, stride=1, padding=1)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 - other\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(1, 2)\n \n    def forward(self, x1, x2):\n        v1 = self.linear(x1)\n        v2 = v1 - x2\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 1)\nx2 = torch.randn(2, 1)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 1)\n\n    def forward(self, x1, other):\n        v1 = self.linear(x1)\n        v2 = v1 - other\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\nother = torch.randn(2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc = torch.nn.Linear(10, 20)\n \n    def forward(self, x1):\n        v1 = self.fc(x1)\n        v2 = v1 - 1\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 10)\n"
            ],
            "g_time": 6.551614046096802
        }
    }
}
{
    "opt": "unary=9",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(1, 12, 3, padding=1)\n        self.conv2 = torch.nn.Conv2d(12, 24, 3, padding=1)\n    def forward(self, x1):\n        v1 = F.pad(x1, (4, 4, 1, 1), \"constant\", 1)\n        v2 = x1.transpose(1, 3)\n        v3 = self.conv1(v1)\n        v4 = self.conv2(v3)\n        v5 = torch.flatten(v4)\n        return v5\n# Inputs to the model\nx1 = torch.randn(3, 1, 4, 4)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        with torch.no_grad():\n            v2 = v1 + 3\n            v3 = v2.clamp(min=0, max=6)\n            v4 = v3 / 6\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n        self.other_conv = torch.nn.Conv2d(8, 8, 1, stride=2, padding=1)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 * 3.\n        v3 = v2.clamp(min=0, max=6)\n        v4 = v3 / 6.\n        v5 = self.other_conv(v4)\n        v6 = torch.relu(v5)\n        v7 = v6 + 3\n        v8 = v7.clamp(min=0, max=6)\n        v9 = v8 / 6\n        return v9\n# Inputs to the model\nx1 = torch.randn(5, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(8, 16, 3, stride=1, padding=1)\n        self.bn = torch.nn.BatchNorm2d(16)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v1 = self.bn(v1)\n        return v1\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 + 3\n        v3 = v2.clamp(min=0, max=6)\n        v4 = v3.sub(1)\n        v5 = v4 * 6\n        return v5\n# Inputs to the model\nx1 = torch.randn(5, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n        self.conv2 = torch.nn.Conv2d(8, 8, 1, stride=1, padding=0)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = v1 + 3\n        v3 = v2.clamp(min=0, max=6)\n        v4 = v3 / 6\n        v5 = self.conv2(v4)\n        v6 = v5 + 3\n        v7 = v6.clamp(min=0, max=6)\n        v8 = v7 / 6\n        return v8\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n        self.other_conv = torch.nn.Conv2d(8, 8, 1, stride=2, padding=1)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 + 3\n        v3 = v2.clamp(min=0, max=6)\n        v4 = v3 / 6\n        v5 = self.other_conv(v4)\n        v6 = v5 + 3\n        v7 = v6.clamp(min=0, max=6)\n        v8 = v7 / 6\n        return v8\n# Inputs to the model\nx1 = torch.randn(5, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n        self.conv2 = torch.nn.Conv2d(8, 4, 3, stride=1, padding=2, groups=2)\n        self.conv3 = torch.nn.Conv2d(4, 4, 1, stride=1, padding=5)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = self.conv2(v1.add(10))\n        v3 = v2.sub(2)\n        v4 = self.conv3(v3 / 2)\n        return v4.abs()\n# Inputs to the model\nx1 = torch.randn(3, 3, 7, 7)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 8, 1, stride=1, padding=0, padding_mode=\"reflect\")\n        self.conv2 = torch.nn.Conv2d(8, 8, 1, stride=3, groups=2, dilation=2, padding=0, padding_mode=\"replicate\")\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = self.conv2(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        return v1 + 3\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(1, 12, 3, padding=1)\n        self.conv2 = torch.nn.Conv2d(12, 24, 3, padding=1)\n    def forward(self, x1):\n        v1 = F.pad(x1, (4, 4, 1, 1), \"constant\", 1)\n        v2 = x1.transpose(1, 3)\n        v3 = self.conv1(v1)\n        v4 = self.conv2(v3)\n        v5 = torch.flatten(v4)\n        return v5\n# Inputs to the model\nx1 = torch.randn(3, 1, 4, 4)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        with torch.no_grad():\n            v2 = v1 + 3\n            v3 = v2.clamp(min=0, max=6)\n            v4 = v3 / 6\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n        self.other_conv = torch.nn.Conv2d(8, 8, 1, stride=2, padding=1)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 * 3.\n        v3 = v2.clamp(min=0, max=6)\n        v4 = v3 / 6.\n        v5 = self.other_conv(v4)\n        v6 = torch.relu(v5)\n        v7 = v6 + 3\n        v8 = v7.clamp(min=0, max=6)\n        v9 = v8 / 6\n        return v9\n# Inputs to the model\nx1 = torch.randn(5, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(8, 16, 3, stride=1, padding=1)\n        self.bn = torch.nn.BatchNorm2d(16)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v1 = self.bn(v1)\n        return v1\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 + 3\n        v3 = v2.clamp(min=0, max=6)\n        v4 = v3.sub(1)\n        v5 = v4 * 6\n        return v5\n# Inputs to the model\nx1 = torch.randn(5, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n        self.conv2 = torch.nn.Conv2d(8, 8, 1, stride=1, padding=0)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = v1 + 3\n        v3 = v2.clamp(min=0, max=6)\n        v4 = v3 / 6\n        v5 = self.conv2(v4)\n        v6 = v5 + 3\n        v7 = v6.clamp(min=0, max=6)\n        v8 = v7 / 6\n        return v8\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n        self.other_conv = torch.nn.Conv2d(8, 8, 1, stride=2, padding=1)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 + 3\n        v3 = v2.clamp(min=0, max=6)\n        v4 = v3 / 6\n        v5 = self.other_conv(v4)\n        v6 = v5 + 3\n        v7 = v6.clamp(min=0, max=6)\n        v8 = v7 / 6\n        return v8\n# Inputs to the model\nx1 = torch.randn(5, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n        self.conv2 = torch.nn.Conv2d(8, 4, 3, stride=1, padding=2, groups=2)\n        self.conv3 = torch.nn.Conv2d(4, 4, 1, stride=1, padding=5)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = self.conv2(v1.add(10))\n        v3 = v2.sub(2)\n        v4 = self.conv3(v3 / 2)\n        return v4.abs()\n# Inputs to the model\nx1 = torch.randn(3, 3, 7, 7)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 8, 1, stride=1, padding=0, padding_mode=\"reflect\")\n        self.conv2 = torch.nn.Conv2d(8, 8, 1, stride=3, groups=2, dilation=2, padding=0, padding_mode=\"replicate\")\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = self.conv2(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        return v1 + 3\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n"
            ],
            "g_time": 8.120402812957764
        }
    }
}
{
    "opt": "unary=8",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose3d(4, 14, 4, stride=2, padding=2, dilation=3, groups=2, output_padding=1)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp(v2, min=0)\n        v4 = torch.clamp(v3, max=6)\n        v5 = v1 * v4\n        v6 = v5 / 6\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 4, 12, 16, 12)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(2, 6, 4, stride=2, padding=1, dilation=1)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp(v2, min=0)\n        v4 = torch.clamp(v3, max=6)\n        v5 = v1 * v4\n        v6 = v5 / 6\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 2, 24, 24)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose1 = torch.nn.ConvTranspose2d(32, 64, 4, output_padding=1)\n        self.conv_transpose2 = torch.nn.ConvTranspose2d(64, 32, 3, stride=2, padding=1, dilation=1, groups=1, output_padding=1)\n    def forward(self, x2):\n        v1 = self.conv_transpose1(x2)\n        v2 = self.conv_transpose2(v1)\n        v3 = v2 + 3\n        v4 = torch.clamp(v3, min=0)\n        v5 = torch.clamp(v4, max=6)\n        v6 = v2 * v5\n        v7 = v6 / 6\n        return v7\n# Inputs to the model\nx2 = torch.randn(1, 32, 24, 24)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(256, 256, 3, stride=1, padding=0, dilation=2, output_padding=1)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        return v1\n# Inputs to the model\nx1 = torch.randn(1, 256, 112, 112)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose_1 = torch.nn.ConvTranspose2d(3, 3, 3, stride=2, padding=1)\n        self.conv_transpose_2 = torch.nn.ConvTranspose2d(3, 3, 3, stride=1, padding=1)\n    def forward(self, x1):\n        d = torch.randint(0, 1, (1,)).item()\n        v1 = self.conv_transpose_1(x1) if d else self.conv_transpose_2(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp(v2, min=0)\n        v4 = torch.clamp(v3, max=6)\n        v5 = v1 * v4\n        v6 = v5 / 6\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 3, 30, 30)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose1 = torch.nn.ConvTranspose2d(48, 38, kernel_size=(2, 5), stride=(1, 2), padding=(1, 2), dilation=(1, 2), groups=4, bias=True, padding_mode='zeros')\n        self.conv_transpose2 = torch.nn.ConvTranspose2d(38, 19, kernel_size=(2, 5), stride=(2, 1), padding=(1, 2), dilation=(1, 2), groups=3, bias=True, padding_mode='zeros')\n        self.conv_transpose3 = torch.nn.ConvTranspose2d(19, 9, kernel_size=(5, 1), stride=(1, 1), padding=(2, 1), dilation=(2, 1), groups=2, bias=True, padding_mode='zeros')\n    def forward(self, x1):\n        v1 = self.conv_transpose1(x1)\n        v2 = self.conv_transpose2(v1)\n        v3 = self.conv_transpose3(v2)\n        v4 = v3 + 3\n        v5 = torch.clamp(v4, min=0)\n        v6 = torch.clamp(v5, max=6)\n        v7 = v3 * v6\n        v8 = v7 / 6\n        return v8\n# Inputs to the model\nx1 = torch.randn(1, 48, 27, 22)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(33, 5, 3, stride=2, padding=1, dilation=2)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp(v2, min=0)\n        v4 = torch.clamp(v3, max=6)\n        v5 = v1 * v4\n        v6 = v5 / 6\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 33, 26, 26)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(4, 12, (3, 2), padding=(2, 1))\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp(v2, min=0)\n        v4 = torch.clamp(v3, max=6)\n        v5 = v1 * v4\n        v6 = v5 / 6\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 4, 10, 5)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose1d(1, 1, 2, stride=2, padding=0, groups=1, output_padding=0)\n        self.conv = torch.nn.Conv1d(1, 1, 1)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = self.conv(v1)\n        v3 = v2 + 3\n        v4 = torch.clamp(v3, min=0)\n        v5 = torch.clamp(v4, max=6)\n        v6 = v2 * v5\n        v7 = v6 / 6\n        return v7\n# Inputs to the model\nx1 = torch.randn(1, 1, 24)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose1 = torch.nn.ConvTranspose2d(2, 3, 1, stride=1, padding=1, dilation=2, groups=3, output_padding=1, output_size=(28, 20))\n        self.conv_transpose2 = torch.nn.ConvTranspose2d(3, 1, 2, stride=2, padding=1, dilation=2, groups=3, output_padding=1, output_size=(70, 75))\n    def forward(self, x1, x2):\n        v1 = self.conv_transpose1(torch.cat((x1, x2), dim=1)).to(torch.float64)\n        v2 = self.conv_transpose2(v1).to(torch.float64)\n        v3 = v2 + 3\n        v4 = torch.clamp(v3, min=0)\n        v5 = torch.clamp(v4, max=6)\n        v6 = v2 * v5\n        v7 = v6 / 6\n        return v7\n# Inputs to the model\nx1 = torch.randn(1, 2, 26, 50)\nx2 = torch.randn(1, 4, 24, 24)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose3d(4, 14, 4, stride=2, padding=2, dilation=3, groups=2, output_padding=1)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp(v2, min=0)\n        v4 = torch.clamp(v3, max=6)\n        v5 = v1 * v4\n        v6 = v5 / 6\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 4, 12, 16, 12)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(2, 6, 4, stride=2, padding=1, dilation=1)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp(v2, min=0)\n        v4 = torch.clamp(v3, max=6)\n        v5 = v1 * v4\n        v6 = v5 / 6\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 2, 24, 24)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose1 = torch.nn.ConvTranspose2d(32, 64, 4, output_padding=1)\n        self.conv_transpose2 = torch.nn.ConvTranspose2d(64, 32, 3, stride=2, padding=1, dilation=1, groups=1, output_padding=1)\n    def forward(self, x2):\n        v1 = self.conv_transpose1(x2)\n        v2 = self.conv_transpose2(v1)\n        v3 = v2 + 3\n        v4 = torch.clamp(v3, min=0)\n        v5 = torch.clamp(v4, max=6)\n        v6 = v2 * v5\n        v7 = v6 / 6\n        return v7\n# Inputs to the model\nx2 = torch.randn(1, 32, 24, 24)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(256, 256, 3, stride=1, padding=0, dilation=2, output_padding=1)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        return v1\n# Inputs to the model\nx1 = torch.randn(1, 256, 112, 112)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose_1 = torch.nn.ConvTranspose2d(3, 3, 3, stride=2, padding=1)\n        self.conv_transpose_2 = torch.nn.ConvTranspose2d(3, 3, 3, stride=1, padding=1)\n    def forward(self, x1):\n        d = torch.randint(0, 1, (1,)).item()\n        v1 = self.conv_transpose_1(x1) if d else self.conv_transpose_2(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp(v2, min=0)\n        v4 = torch.clamp(v3, max=6)\n        v5 = v1 * v4\n        v6 = v5 / 6\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 3, 30, 30)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose1 = torch.nn.ConvTranspose2d(48, 38, kernel_size=(2, 5), stride=(1, 2), padding=(1, 2), dilation=(1, 2), groups=4, bias=True, padding_mode='zeros')\n        self.conv_transpose2 = torch.nn.ConvTranspose2d(38, 19, kernel_size=(2, 5), stride=(2, 1), padding=(1, 2), dilation=(1, 2), groups=3, bias=True, padding_mode='zeros')\n        self.conv_transpose3 = torch.nn.ConvTranspose2d(19, 9, kernel_size=(5, 1), stride=(1, 1), padding=(2, 1), dilation=(2, 1), groups=2, bias=True, padding_mode='zeros')\n    def forward(self, x1):\n        v1 = self.conv_transpose1(x1)\n        v2 = self.conv_transpose2(v1)\n        v3 = self.conv_transpose3(v2)\n        v4 = v3 + 3\n        v5 = torch.clamp(v4, min=0)\n        v6 = torch.clamp(v5, max=6)\n        v7 = v3 * v6\n        v8 = v7 / 6\n        return v8\n# Inputs to the model\nx1 = torch.randn(1, 48, 27, 22)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(33, 5, 3, stride=2, padding=1, dilation=2)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp(v2, min=0)\n        v4 = torch.clamp(v3, max=6)\n        v5 = v1 * v4\n        v6 = v5 / 6\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 33, 26, 26)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(4, 12, (3, 2), padding=(2, 1))\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 + 3\n        v3 = torch.clamp(v2, min=0)\n        v4 = torch.clamp(v3, max=6)\n        v5 = v1 * v4\n        v6 = v5 / 6\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 4, 10, 5)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose1d(1, 1, 2, stride=2, padding=0, groups=1, output_padding=0)\n        self.conv = torch.nn.Conv1d(1, 1, 1)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = self.conv(v1)\n        v3 = v2 + 3\n        v4 = torch.clamp(v3, min=0)\n        v5 = torch.clamp(v4, max=6)\n        v6 = v2 * v5\n        v7 = v6 / 6\n        return v7\n# Inputs to the model\nx1 = torch.randn(1, 1, 24)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose1 = torch.nn.ConvTranspose2d(2, 3, 1, stride=1, padding=1, dilation=2, groups=3, output_padding=1, output_size=(28, 20))\n        self.conv_transpose2 = torch.nn.ConvTranspose2d(3, 1, 2, stride=2, padding=1, dilation=2, groups=3, output_padding=1, output_size=(70, 75))\n    def forward(self, x1, x2):\n        v1 = self.conv_transpose1(torch.cat((x1, x2), dim=1)).to(torch.float64)\n        v2 = self.conv_transpose2(v1).to(torch.float64)\n        v3 = v2 + 3\n        v4 = torch.clamp(v3, min=0)\n        v5 = torch.clamp(v4, max=6)\n        v6 = v2 * v5\n        v7 = v6 / 6\n        return v7\n# Inputs to the model\nx1 = torch.randn(1, 2, 26, 50)\nx2 = torch.randn(1, 4, 24, 24)\n"
            ],
            "g_time": 12.593396425247192
        }
    }
}
{
    "opt": "unary=7",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n       super().__init__()\n       self.linear = torch.nn.Linear(256, 256)\n\n    def forward(self, x1):\n         v1 = self.linear(x1)\n         v2 = torch.clamp(v1 + 3, 0, 6)\n         v3 = v2 / 6\n         return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 256)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(32, 32)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.clamp(v1 + 3, min=0, max=6)\n        v3 = v2 / 6\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(128, 256)\n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * F.hardtanh(v1 + 3, min_val=0, max_val=6)\n        v3 = v2 / 6\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 128)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(10, 4)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * torch.clamp(l1 + 3, min=0, max=6)\n        return v2 / 6\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 10)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.l1 = torch.nn.Linear(64, 64)\n \n    def forward(self, x1):\n        l1 = self.l1(x1)\n        l2 = l1 * torch.clamp(min=0, max=6, l1 + 3)\n        l3 = l2 / 6\n        return l3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(8, 1)\n \n    def forward(self, x1):\n        v1 = self.dropout(x1, 0.2)\n        v2 = torch.clamp(v1, min=0, max=6)\n        v3 = v2 + 3\n        v4 = v3 * 0.16666666666666666\n        v5 = v4 / 6\n        return v5\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n        self.linear = torch.nn.Linear(8 * 64 + 1, 10)\n \n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        v7 = v6.flatten(1)\n        v8 = v7.flip(dims=[1])\n        v9 = v8.unsqueeze(1)\n        v10 = torch.cat([v7, v9], dim=1)\n        return self.linear(v10)\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n  \n    def forward(self, x1):\n        v1 = self.layer1(x1)\n        v2 = v1 * torch.clamp(torch.clamp(v1 + 3.0, 0, 6), 0, 6) / 6.0\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 22, 22)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n \n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return [v1, v2, v3, v4, v5, v6]\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc = torch.nn.Linear(4, 4)\n \n    def forward(self, x1):\n        l1 = self.fc(x1)\n        l2 = l1 * torch.clamp(l1 + 3, 0, 6)\n        l3 = l2 / 6\n        return l3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 4)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n       super().__init__()\n       self.linear = torch.nn.Linear(256, 256)\n\n    def forward(self, x1):\n         v1 = self.linear(x1)\n         v2 = torch.clamp(v1 + 3, 0, 6)\n         v3 = v2 / 6\n         return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 256)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(32, 32)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.clamp(v1 + 3, min=0, max=6)\n        v3 = v2 / 6\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(128, 256)\n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * F.hardtanh(v1 + 3, min_val=0, max_val=6)\n        v3 = v2 / 6\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 128)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(10, 4)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * torch.clamp(l1 + 3, min=0, max=6)\n        return v2 / 6\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 10)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.l1 = torch.nn.Linear(64, 64)\n \n    def forward(self, x1):\n        l1 = self.l1(x1)\n        l2 = l1 * torch.clamp(min=0, max=6, l1 + 3)\n        l3 = l2 / 6\n        return l3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(8, 1)\n \n    def forward(self, x1):\n        v1 = self.dropout(x1, 0.2)\n        v2 = torch.clamp(v1, min=0, max=6)\n        v3 = v2 + 3\n        v4 = v3 * 0.16666666666666666\n        v5 = v4 / 6\n        return v5\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n        self.linear = torch.nn.Linear(8 * 64 + 1, 10)\n \n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        v7 = v6.flatten(1)\n        v8 = v7.flip(dims=[1])\n        v9 = v8.unsqueeze(1)\n        v10 = torch.cat([v7, v9], dim=1)\n        return self.linear(v10)\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n  \n    def forward(self, x1):\n        v1 = self.layer1(x1)\n        v2 = v1 * torch.clamp(torch.clamp(v1 + 3.0, 0, 6), 0, 6) / 6.0\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 22, 22)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n \n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return [v1, v2, v3, v4, v5, v6]\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc = torch.nn.Linear(4, 4)\n \n    def forward(self, x1):\n        l1 = self.fc(x1)\n        l2 = l1 * torch.clamp(l1 + 3, 0, 6)\n        l3 = l2 / 6\n        return l3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 4)\n"
            ],
            "g_time": 8.917842626571655
        }
    }
}
{
    "opt": "binary_unary=4",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.l = torch.nn.Linear(16, 16)\n \n    def forward(self, x1, **kwargs):\n        v1 = self.l(x1)\n        v2 = v1 + kwargs[\"other\"]\n        v3 = torch.nn.functional.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 16)\nother = torch.randn(1, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(512, 256)\n \n    def forward(self, x1, other):\n        v1 = self.linear(x1)\n        v2 = v1 + other\n        v3 = torch.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 512)\n__other = torch.randn(1, 256)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(16384, 16384)\n \n    def forward(self, x1, other):\n        v1 = self.linear(x1)\n        v2 = v1 + other\n        v3 = torch.relu(v2)\n        return v3\n\n# Initializing the model\nmodel = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 16384)\nother = torch.randn(1, 16384)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(256, 256)\n \n    def forward(self, x1, other):\n        v1 = self.linear(x1)\n        v2 = v1 + other\n        v3 = torch.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 256)\nother = torch.randn(1, 256)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, other):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 4)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 + other\n        v3 = torch.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model(torch.randn(4, 3))\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(2, 3)\n \n    def forward(self, x1, other):\n        v1 = self.linear(x1)\n        v2 = v1 + other\n        v3 = F.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def forward(self, x1, other=None):\n        if other is None:\n            other = torch.tensor([0, 0, 0], dtype=x1.dtype, device=x1.device)\n        return torch.relu(x1 + other)\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n \n    def forward(self, x1):\n        return self.linear(x1)\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\nother = torch.randn(8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.input_linear = torch.nn.Linear(64, 32)\n    \n    def forward(self, x1, other):\n        v1 = self.input_linear(x1)\n        v2 = v1 + other\n        v3 = torch.nn.functional.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 64)\nother = torch.randn(1, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n \n    def forward(self, x1, other):\n        v1 = self.linear(x1)\n        v2 = v1 + other\n        v3 = torch.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n__other__ = torch.randn(1, 8)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.l = torch.nn.Linear(16, 16)\n \n    def forward(self, x1, **kwargs):\n        v1 = self.l(x1)\n        v2 = v1 + kwargs[\"other\"]\n        v3 = torch.nn.functional.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 16)\nother = torch.randn(1, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(512, 256)\n \n    def forward(self, x1, other):\n        v1 = self.linear(x1)\n        v2 = v1 + other\n        v3 = torch.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 512)\n__other = torch.randn(1, 256)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(16384, 16384)\n \n    def forward(self, x1, other):\n        v1 = self.linear(x1)\n        v2 = v1 + other\n        v3 = torch.relu(v2)\n        return v3\n\n# Initializing the model\nmodel = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 16384)\nother = torch.randn(1, 16384)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(256, 256)\n \n    def forward(self, x1, other):\n        v1 = self.linear(x1)\n        v2 = v1 + other\n        v3 = torch.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 256)\nother = torch.randn(1, 256)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, other):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 4)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 + other\n        v3 = torch.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model(torch.randn(4, 3))\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(2, 3)\n \n    def forward(self, x1, other):\n        v1 = self.linear(x1)\n        v2 = v1 + other\n        v3 = F.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def forward(self, x1, other=None):\n        if other is None:\n            other = torch.tensor([0, 0, 0], dtype=x1.dtype, device=x1.device)\n        return torch.relu(x1 + other)\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n \n    def forward(self, x1):\n        return self.linear(x1)\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\nother = torch.randn(8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.input_linear = torch.nn.Linear(64, 32)\n    \n    def forward(self, x1, other):\n        v1 = self.input_linear(x1)\n        v2 = v1 + other\n        v3 = torch.nn.functional.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 64)\nother = torch.randn(1, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n \n    def forward(self, x1, other):\n        v1 = self.linear(x1)\n        v2 = v1 + other\n        v3 = torch.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n__other__ = torch.randn(1, 8)\n"
            ],
            "g_time": 5.39231014251709
        }
    }
}
{
    "opt": "unary=1",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(1024, 512)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * 0.5\n        v3 = v1 + (v1 * v1 * v1) * 0.044715\n        v4 = v3 * 0.7978845608028654\n        v5 = torch.tanh(v4)\n        v6 = v5 + 1\n        v7 = v2 * v6\n        return v7\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 1024)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(10, 1)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * 0.5\n        v3 = v1 + (v1 * v1 * v1) * 0.044715\n        v4 = v3 * 0.7978845608028654\n        v5 = torch.tanh(v4)\n        v6 = v5 + 1\n        v7 = v2 * v6\n        return v7\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 10)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 3)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * 0.5\n        v3 = v1 + (v1 * v1 * v1) * 0.044715\n        v4 = v3 * 0.7978845608028654\n        v5 = torch.tanh(v4)\n        v6 = v5 + 1\n        v7 = v2 * v6\n        return v7\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(224, 224)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * 0.5\n        v3 = v1 + (v1 * v1 * v1) * 0.044715\n        v4 = v3 * 0.7978845608028654\n        v5 = torch.tanh(v4)\n        v6 = v5 + 1\n        v7 = v2 * v6\n        return v7\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 224)\n",
                "\nclass Model(torch.nn.Module)\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(6, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * 0.5\n        v3 = v1 + (v1 * v1 * v1) * 0.044715 \n        v4 = v3 * 0.7978845608028654\n        v5 = torch.tanh(v4)\n        v6 = v5 + 1\n        v7 = v2 * v6\n        return v7\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 6)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3136, 4096)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * 0.5\n        v3 = v1 + (v1 * v1 * v1) * 0.044715\n        v4 = v3 * 0.7978845608028654\n        v5 = torch.tanh(v4)\n        v6 = v5 + 1\n        v7 = v2 * v6\n        return v7\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3136)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(8, 8, bias=True)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * 0.5\n        v3 = v1 + (v1 * v1 * v1) * 0.044715\n        v4 = v3 * 0.7978845608028654\n        v5 = torch.tanh(v4)\n        v6 = v5 + 1\n        v7 = v2 * v6\n        return v7\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(16, 4)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * 0.5\n        v3 = v1 + (v1 * v1 * v1) * 0.044715\n        v4 = v3 * 0.7978845608028654\n        v5 = torch.tanh(v4)\n        v6 = v5 + 1\n        v7 = v2 * v6\n        return v7\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(20, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * 0.5\n        v3 = v1 + (v1 * v1 * v1) * 0.044715\n        v4 = v3 * 0.7978845608028654\n        v5 = torch.tanh(v4)\n        v6 = v5 + 1\n        v7 = v2 * v6\n        return v7\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 20)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(64, 128)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * 0.5\n        v3 = v1 + (v1 * v1 * v1) * 0.044715\n        v4 = v3 * 0.7978845608028654\n        v5 = torch.tanh(v4)\n        v6 = v5 + 1\n        v7 = v2 * v6\n        return v7\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 64)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(1024, 512)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * 0.5\n        v3 = v1 + (v1 * v1 * v1) * 0.044715\n        v4 = v3 * 0.7978845608028654\n        v5 = torch.tanh(v4)\n        v6 = v5 + 1\n        v7 = v2 * v6\n        return v7\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 1024)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(10, 1)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * 0.5\n        v3 = v1 + (v1 * v1 * v1) * 0.044715\n        v4 = v3 * 0.7978845608028654\n        v5 = torch.tanh(v4)\n        v6 = v5 + 1\n        v7 = v2 * v6\n        return v7\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 10)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 3)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * 0.5\n        v3 = v1 + (v1 * v1 * v1) * 0.044715\n        v4 = v3 * 0.7978845608028654\n        v5 = torch.tanh(v4)\n        v6 = v5 + 1\n        v7 = v2 * v6\n        return v7\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(224, 224)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * 0.5\n        v3 = v1 + (v1 * v1 * v1) * 0.044715\n        v4 = v3 * 0.7978845608028654\n        v5 = torch.tanh(v4)\n        v6 = v5 + 1\n        v7 = v2 * v6\n        return v7\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 224)\n",
                "\nclass Model(torch.nn.Module)\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(6, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * 0.5\n        v3 = v1 + (v1 * v1 * v1) * 0.044715 \n        v4 = v3 * 0.7978845608028654\n        v5 = torch.tanh(v4)\n        v6 = v5 + 1\n        v7 = v2 * v6\n        return v7\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 6)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3136, 4096)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * 0.5\n        v3 = v1 + (v1 * v1 * v1) * 0.044715\n        v4 = v3 * 0.7978845608028654\n        v5 = torch.tanh(v4)\n        v6 = v5 + 1\n        v7 = v2 * v6\n        return v7\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3136)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(8, 8, bias=True)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * 0.5\n        v3 = v1 + (v1 * v1 * v1) * 0.044715\n        v4 = v3 * 0.7978845608028654\n        v5 = torch.tanh(v4)\n        v6 = v5 + 1\n        v7 = v2 * v6\n        return v7\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(16, 4)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * 0.5\n        v3 = v1 + (v1 * v1 * v1) * 0.044715\n        v4 = v3 * 0.7978845608028654\n        v5 = torch.tanh(v4)\n        v6 = v5 + 1\n        v7 = v2 * v6\n        return v7\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(20, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * 0.5\n        v3 = v1 + (v1 * v1 * v1) * 0.044715\n        v4 = v3 * 0.7978845608028654\n        v5 = torch.tanh(v4)\n        v6 = v5 + 1\n        v7 = v2 * v6\n        return v7\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 20)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(64, 128)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * 0.5\n        v3 = v1 + (v1 * v1 * v1) * 0.044715\n        v4 = v3 * 0.7978845608028654\n        v5 = torch.tanh(v4)\n        v6 = v5 + 1\n        v7 = v2 * v6\n        return v7\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 64)\n"
            ],
            "g_time": 8.1790189743042
        }
    }
}
{
    "opt": "cat_mm",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, x2):\n        v1 = torch.mm(x1, x2)\n        v2 = torch.mm(x1, x2)\n        v3 = torch.mm(x1, x2)\n        v4 = torch.mm(x1, x2)\n        v5 = torch.mm(x1, x2)\n        v6 = torch.mm(x1, x2)\n        v7 = torch.mm(x1, x2)\n        v8 = torch.mm(x1, x2)\n        v9 = torch.mm(x1, x2)\n        v10 = torch.mm(x1, x2)\n        v11 = torch.cat([v1, v2, v3, v4, v5, v6, v7, v8, v9, v10], 1)\n        return torch.cat([v1, v1, v1, v1, v1, v1, v1, v1, v1, v1, v11], 1)\n# Inputs to the model\nx1 = torch.randn(2, 1)\nx2 = torch.randn(1, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x):\n        t1 = torch.mm(x, x)\n        t2 = torch.cat([t1, t1, t1], 1)\n        return torch.mm(t2, t2)\n# Inputs to the model\nx = torch.randn(2, 1)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, x2):\n        v1 = torch.mm(x1, x2)\n        v2 = torch.mm(x1, x2)\n        return torch.cat([v1, v2, v1, v2, v2], 1)\n# Inputs to the model\nx1 = torch.randn(2, 2)\nx2 = torch.randn(2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        for i in range(5):\n            setattr(self, 'layer' + str(i + 1), torch.nn.Linear(1, 1, bias=False))\n    def forward(self, x1, x2):\n        v1 = self.layer1(x1)\n        v2 = self.layer2(x1)\n        v3 = self.layer3(x1)\n        v4 = self.layer4(x1)\n        return torch.cat([v1, v2, v3, v4], 1)\n# Inputs to the model\nx1 = torch.randn(4, 1)\nx2 = torch.randn(4, 1)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.weight = torch.randn(5, 5)\n    def forward(self, x):\n        return torch.mm(x, self.weight)\n# Inputs to the model\nx = torch.randn(5, 5)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        for i in range(3):\n            setattr(self, 'layer' + str(i + 1), torch.nn.Linear(1, 1, bias=False))\n    def forward(self, x1, x2):\n        v1 = self.layer1(x1)\n        return torch.cat([v1, v1, v1], 0)\n# Inputs to the model\nx1 = torch.randn(3, 1)\nx2 = torch.randn(1, 1)\n",
                "\nclass Reshape_Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        for i in range(3):\n            setattr(self, 'layer' + str(i + 1), torch.nn.Linear(1, 1, bias=False))\n            setattr(self, 'layer' + str(i + 4), torch.nn.Linear(1, 1, bias=False))\n    def forward(self, x1, x2):\n        x1 = x1.view(1, 1)\n        x2 = x2.reshape(1, 1)\n        v1 = self.layer1(x1)\n        v2 = self.layer4(x2)\n        return torch.cat([v1, v1, v2], 1)\n# Inputs to the model\nx1 = torch.randn(1, 1)\nx2 = torch.randn(1, 1)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, x2):\n        v1 = torch.mm(x1, x2)\n        v2 = torch.mm(x1, x2)\n        return torch.cat([v1, v1, v2, v2, v2, v2], 1)\n# Inputs to the model\nx1 = torch.randn(5, 2)\nx2 = torch.randn(2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, x2):\n        v1 = torch.mm(x1, x2)\n        v2 = torch.mm(x1, x2)\n        return torch.cat([v1, v1, v1, v1, v1], 2)\n# Inputs to the model\nx1 = torch.randn(2, 2)\nx2 = torch.randn(2, 2)\n",
                "\nclass Model1(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        for i in range(3):\n            setattr(self, 'layer' + str(i + 1), torch.nn.Linear(1, 1, bias=False))\n    def forward(self, x1, x2):\n        v1 = self.layer1(x1)\n        v2 = self.layer2(x2)\n        v3 = self.layer3(x1)\n        return torch.cat([v1, v1, v1, v2, v2, v2, v3, v3, v3], 1)\n# Inputs to the model\nx1 = torch.randn(5, 1)\nx2 = torch.randn(5, 1)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, x2):\n        v1 = torch.mm(x1, x2)\n        v2 = torch.mm(x1, x2)\n        v3 = torch.mm(x1, x2)\n        v4 = torch.mm(x1, x2)\n        v5 = torch.mm(x1, x2)\n        v6 = torch.mm(x1, x2)\n        v7 = torch.mm(x1, x2)\n        v8 = torch.mm(x1, x2)\n        v9 = torch.mm(x1, x2)\n        v10 = torch.mm(x1, x2)\n        v11 = torch.cat([v1, v2, v3, v4, v5, v6, v7, v8, v9, v10], 1)\n        return torch.cat([v1, v1, v1, v1, v1, v1, v1, v1, v1, v1, v11], 1)\n# Inputs to the model\nx1 = torch.randn(2, 1)\nx2 = torch.randn(1, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x):\n        t1 = torch.mm(x, x)\n        t2 = torch.cat([t1, t1, t1], 1)\n        return torch.mm(t2, t2)\n# Inputs to the model\nx = torch.randn(2, 1)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, x2):\n        v1 = torch.mm(x1, x2)\n        v2 = torch.mm(x1, x2)\n        return torch.cat([v1, v2, v1, v2, v2], 1)\n# Inputs to the model\nx1 = torch.randn(2, 2)\nx2 = torch.randn(2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        for i in range(5):\n            setattr(self, 'layer' + str(i + 1), torch.nn.Linear(1, 1, bias=False))\n    def forward(self, x1, x2):\n        v1 = self.layer1(x1)\n        v2 = self.layer2(x1)\n        v3 = self.layer3(x1)\n        v4 = self.layer4(x1)\n        return torch.cat([v1, v2, v3, v4], 1)\n# Inputs to the model\nx1 = torch.randn(4, 1)\nx2 = torch.randn(4, 1)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.weight = torch.randn(5, 5)\n    def forward(self, x):\n        return torch.mm(x, self.weight)\n# Inputs to the model\nx = torch.randn(5, 5)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        for i in range(3):\n            setattr(self, 'layer' + str(i + 1), torch.nn.Linear(1, 1, bias=False))\n    def forward(self, x1, x2):\n        v1 = self.layer1(x1)\n        return torch.cat([v1, v1, v1], 0)\n# Inputs to the model\nx1 = torch.randn(3, 1)\nx2 = torch.randn(1, 1)\n",
                "\nclass Reshape_Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        for i in range(3):\n            setattr(self, 'layer' + str(i + 1), torch.nn.Linear(1, 1, bias=False))\n            setattr(self, 'layer' + str(i + 4), torch.nn.Linear(1, 1, bias=False))\n    def forward(self, x1, x2):\n        x1 = x1.view(1, 1)\n        x2 = x2.reshape(1, 1)\n        v1 = self.layer1(x1)\n        v2 = self.layer4(x2)\n        return torch.cat([v1, v1, v2], 1)\n# Inputs to the model\nx1 = torch.randn(1, 1)\nx2 = torch.randn(1, 1)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, x2):\n        v1 = torch.mm(x1, x2)\n        v2 = torch.mm(x1, x2)\n        return torch.cat([v1, v1, v2, v2, v2, v2], 1)\n# Inputs to the model\nx1 = torch.randn(5, 2)\nx2 = torch.randn(2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, x2):\n        v1 = torch.mm(x1, x2)\n        v2 = torch.mm(x1, x2)\n        return torch.cat([v1, v1, v1, v1, v1], 2)\n# Inputs to the model\nx1 = torch.randn(2, 2)\nx2 = torch.randn(2, 2)\n",
                "\nclass Model1(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        for i in range(3):\n            setattr(self, 'layer' + str(i + 1), torch.nn.Linear(1, 1, bias=False))\n    def forward(self, x1, x2):\n        v1 = self.layer1(x1)\n        v2 = self.layer2(x2)\n        v3 = self.layer3(x1)\n        return torch.cat([v1, v1, v1, v2, v2, v2, v3, v3, v3], 1)\n# Inputs to the model\nx1 = torch.randn(5, 1)\nx2 = torch.randn(5, 1)\n"
            ],
            "g_time": 10.434698820114136
        }
    }
}
{
    "opt": "sink_cat_after_pointwise",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x):\n        y = torch.cat((x, x), dim=1)\n        z = y.view(y.shape[0], -1)\n        if y.shape == (2, 6):\n            return y.relu()\n        elif y.shape!= (2, 6):\n            return z.tanh()\n        return z.relu()\n# Inputs to the model\nx = torch.randn(2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x):\n        y = torch.cat((x, x), dim=1)\n        z = y.view(y.shape[0], -1)\n        x = z.tanh() if y.shape!= (2, 12) else z.relu()\n        a, b, c = x.shape\n        for _ in range(a):\n            y = y + x\n        return y\n# Inputs to the model\nx = torch.randn(2, 3, 4)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x):\n        y = torch.tensor([0.56, 1.81], dtype=torch.float32, device=x.device)\n        z = torch.cat((y, x), dim=1)\n        w = z + 1 + 1\n        if w.shape == (2, 20):\n            z = z.t()\n        #if z.shape == (2, 20):\n        return w\n# Inputs to the model\nx = torch.randn((2, 5))\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x):\n        y = torch.cat((x, x), dim=1)\n        z = torch.tanh(y.view(y.shape[0], -1))\n        return z\n# Inputs to the model\nx = torch.randn((2, 3, 4))\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x):\n        y = torch.cat((x, x), dim=1)\n        y = y.view(y.shape[0], -1)\n        if y.shape == (2, 12):\n            y = y.tanh()\n        elif y.shape == (2, 8):\n            y = torch.relu(y)\n        return torch.tanh(y) if y.shape == (2, 4) else torch.sin(y)\n# Inputs to the model\nx = torch.randn(2, 3, 4)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x):\n        y = torch.cat((x, x), dim=1)\n        x = torch.tanh(y.view(y.shape[0], -1))\n        return x\n# Inputs to the model\nx = torch.randn(2, 3, 4)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x):\n        y = torch.ones(3, requires_grad=True)\n        z = torch.cat((x, x), dim=1)\n        if z.shape == (2, 10):\n            return z.tanh()\n        return y.tanh()\n# Inputs to the model\nx = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x):\n        y = torch.cat((x, x), dim=1)\n        z = y.view(y.shape[0], -1)\n        return y if y.shape!= (2, 12) else z.relu()\n# Inputs to the model\nx = torch.randn(2, 3, 4)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x):\n        y = torch.cat((torch.ones((1, 2), requires_grad=True), torch.ones((1, 2), requires_grad=True)), dim=1)\n        return y.relu()\n# Inputs to the model\nx = torch.randn(2, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x):\n        y = torch.cat((x, x), dim=1)\n        z = torch.relu(y.view(y.shape[0], -1))\n        if not y.shape == (3, 12):\n            x = torch.tanh(y)\n            z = z.view(z.shape[0], -1)\n        return z\n# Inputs to the model\nx = torch.randn(2, 3, 4)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x):\n        y = torch.cat((x, x), dim=1)\n        z = y.view(y.shape[0], -1)\n        if y.shape == (2, 6):\n            return y.relu()\n        elif y.shape!= (2, 6):\n            return z.tanh()\n        return z.relu()\n# Inputs to the model\nx = torch.randn(2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x):\n        y = torch.cat((x, x), dim=1)\n        z = y.view(y.shape[0], -1)\n        x = z.tanh() if y.shape!= (2, 12) else z.relu()\n        a, b, c = x.shape\n        for _ in range(a):\n            y = y + x\n        return y\n# Inputs to the model\nx = torch.randn(2, 3, 4)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x):\n        y = torch.tensor([0.56, 1.81], dtype=torch.float32, device=x.device)\n        z = torch.cat((y, x), dim=1)\n        w = z + 1 + 1\n        if w.shape == (2, 20):\n            z = z.t()\n        #if z.shape == (2, 20):\n        return w\n# Inputs to the model\nx = torch.randn((2, 5))\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x):\n        y = torch.cat((x, x), dim=1)\n        z = torch.tanh(y.view(y.shape[0], -1))\n        return z\n# Inputs to the model\nx = torch.randn((2, 3, 4))\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x):\n        y = torch.cat((x, x), dim=1)\n        y = y.view(y.shape[0], -1)\n        if y.shape == (2, 12):\n            y = y.tanh()\n        elif y.shape == (2, 8):\n            y = torch.relu(y)\n        return torch.tanh(y) if y.shape == (2, 4) else torch.sin(y)\n# Inputs to the model\nx = torch.randn(2, 3, 4)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x):\n        y = torch.cat((x, x), dim=1)\n        x = torch.tanh(y.view(y.shape[0], -1))\n        return x\n# Inputs to the model\nx = torch.randn(2, 3, 4)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x):\n        y = torch.ones(3, requires_grad=True)\n        z = torch.cat((x, x), dim=1)\n        if z.shape == (2, 10):\n            return z.tanh()\n        return y.tanh()\n# Inputs to the model\nx = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x):\n        y = torch.cat((x, x), dim=1)\n        z = y.view(y.shape[0], -1)\n        return y if y.shape!= (2, 12) else z.relu()\n# Inputs to the model\nx = torch.randn(2, 3, 4)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x):\n        y = torch.cat((torch.ones((1, 2), requires_grad=True), torch.ones((1, 2), requires_grad=True)), dim=1)\n        return y.relu()\n# Inputs to the model\nx = torch.randn(2, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x):\n        y = torch.cat((x, x), dim=1)\n        z = torch.relu(y.view(y.shape[0], -1))\n        if not y.shape == (3, 12):\n            x = torch.tanh(y)\n            z = z.view(z.shape[0], -1)\n        return z\n# Inputs to the model\nx = torch.randn(2, 3, 4)\n"
            ],
            "g_time": 6.190469264984131
        }
    }
}
{
    "opt": "binary=2",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(6, 1, 1, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 - 0\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 6, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 3, 1, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 - torch.BoolTensor([True, False, False])\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 16, 1, stride=1, padding=1)\n    def forward(self, x):\n        v1 = self.conv(x)\n        v2 = v1 - True\n        return v2\n# Inputs to the model\nx = torch.randn(2, 5, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 1, 1, stride=1, padding=1)\n    def forward(self, x1, x2):\n        v1 = self.conv(x1)\n        v2 = v1 - 1e-05\n        v3 = v1 + x2\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\nx2 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 2, 1, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 - 0.4\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 3, 8, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 - 0.2\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.conv = nn.Conv2d(3, 8, 1, stride=1, padding=1)\n        self.linear = nn.Linear(1, 1, bias=False)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 - v1\n        v3 = v2 - v1\n        # v3 has the same shape and content as v1\n        v4 = v3 - 0.0\n        # v4 has the same shape and content as v1\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 16, 1, stride=1, padding=1)\n    def forward(self, x2):\n        v1 = self.conv(x2)\n        v2 = v1 - True\n        return v2\n# Inputs to the model\nx2 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 30, 3, stride=2, padding=1)\n        self.conv2 = torch.nn.Conv2d(30, 1, 1, stride=1, padding=1)\n    def forward(self, x):\n        v1 = self.conv(x)\n        v2 = self.conv2(x)\n        v3 = v1 - v2\n        return v3\n# Inputs to the model\nx = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n    def forward(self, x2, x1):\n        v1 = self.conv(x1)\n        v2 = self.conv(x2)\n        v3 = v1 - v2\n        return v3\n# Inputs to the model\nx2 = torch.randn(1, 3, 64, 64)\nx1 = torch.randn(1, 3, 64, 64)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(6, 1, 1, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 - 0\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 6, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 3, 1, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 - torch.BoolTensor([True, False, False])\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 16, 1, stride=1, padding=1)\n    def forward(self, x):\n        v1 = self.conv(x)\n        v2 = v1 - True\n        return v2\n# Inputs to the model\nx = torch.randn(2, 5, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 1, 1, stride=1, padding=1)\n    def forward(self, x1, x2):\n        v1 = self.conv(x1)\n        v2 = v1 - 1e-05\n        v3 = v1 + x2\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\nx2 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 2, 1, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 - 0.4\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 3, 8, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 - 0.2\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.conv = nn.Conv2d(3, 8, 1, stride=1, padding=1)\n        self.linear = nn.Linear(1, 1, bias=False)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 - v1\n        v3 = v2 - v1\n        # v3 has the same shape and content as v1\n        v4 = v3 - 0.0\n        # v4 has the same shape and content as v1\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 16, 1, stride=1, padding=1)\n    def forward(self, x2):\n        v1 = self.conv(x2)\n        v2 = v1 - True\n        return v2\n# Inputs to the model\nx2 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 30, 3, stride=2, padding=1)\n        self.conv2 = torch.nn.Conv2d(30, 1, 1, stride=1, padding=1)\n    def forward(self, x):\n        v1 = self.conv(x)\n        v2 = self.conv2(x)\n        v3 = v1 - v2\n        return v3\n# Inputs to the model\nx = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n    def forward(self, x2, x1):\n        v1 = self.conv(x1)\n        v2 = self.conv(x2)\n        v3 = v1 - v2\n        return v3\n# Inputs to the model\nx2 = torch.randn(1, 3, 64, 64)\nx1 = torch.randn(1, 3, 64, 64)\n"
            ],
            "g_time": 6.941635370254517
        }
    }
}
{
    "opt": "unary=18",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(in_channels=64, out_channels=32, kernel_size=3, stride=1, padding=1)\n        self.conv2 = torch.nn.Conv2d(in_channels=32, out_channels=16, kernel_size=3, stride=1, padding=1)\n        self.conv3 = torch.nn.Conv2d(in_channels=16, out_channels=2, kernel_size=3, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = self.conv2(v2)\n        v4 = torch.sigmoid(v3)\n        v5 = self.conv3(v4)\n        v6 = torch.sigmoid(v5)\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 64, 32, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(in_channels=2, out_channels=32, kernel_size=3, stride=2, padding=1)\n        self.conv2 = torch.nn.Conv2d(in_channels=32, out_channels=16, kernel_size=1, stride=1, padding=0)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = self.conv2(v2)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 2, 32, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 6, 3, stride=1, padding=0, dilation=1)\n        self.conv2 = torch.nn.Conv2d(6, 12, 3, stride=1, padding=1, dilation=1)\n        self.conv3 = torch.nn.Conv2d(12, 24, 3, stride=1, padding=0, dilation=2)\n        self.conv4 = torch.nn.Conv2d(24, 48, 3, stride=1, padding=1, dilation=1)\n        self.conv5 = torch.nn.Conv2d(48, 96, 3, stride=1, padding=0, dilation=4)\n        self.conv6 = torch.nn.Conv2d(96, 192, 3, stride=1, padding=1, dilation=1)\n        self.conv7 = torch.nn.Conv2d(192, 384, 3, stride=1, padding=0, dilation=8)\n        self.conv8 = torch.nn.Conv2d(384, 768, 3, stride=1, padding=2, dilation=1)\n        self.conv9 = torch.nn.Conv2d(768, 1024, 3, stride=1, padding=0, dilation=16)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = self.conv2(v2)\n        v4 = torch.sigmoid(v3)\n        v5 = self.conv3(v4)\n        v6 = torch.sigmoid(v5)\n        v7 = self.conv4(v6)\n        v8 = torch.sigmoid(v7)\n        v9 = self.conv5(v8)\n        v10 = torch.sigmoid(v9)\n        v11 = self.conv6(v10)\n        v12 = torch.sigmoid(v11)\n        v13 = self.conv7(v12)\n        v14 = torch.sigmoid(v13)\n        v15 = self.conv8(v14)\n        v16 = torch.sigmoid(v15)\n        v17 = self.conv9(v16)\n        v18 = torch.sigmoid(v17)\n        return v18\n# Inputs to the model\nx1 = torch.randn(1, 3, 112, 112)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.model = torch.nn.Sequential(\n            torch.nn.Conv2d(in_channels=3, out_channels=8, kernel_size=3, stride=4, padding=0, dilation=3),\n            torch.nn.Sigmoid(),\n        )\n    def forward(self, x1):\n        v1 = self.model(x1)\n        return v1\n# Inputs to the model\nx1 = torch.randn(1, 3, 28, 28)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=2, dilation=1)\n        self.conv2 = torch.nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=2, dilation=2)\n        self.conv3 = torch.nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=4, dilation=3)\n        self.conv4 = torch.nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=8, dilation=4)\n        self.conv5 = torch.nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=12, dilation=5)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = self.conv2(v2)\n        v4 = torch.sigmoid(v3)\n        v5 = self.conv3(v4)\n        v6 = torch.sigmoid(v5)\n        v7 = self.conv4(v6)\n        v8 = torch.sigmoid(v7)\n        v9 = self.conv5(v8)\n        v10 = torch.sigmoid(v9)\n        return v10\n# Inputs to the model\nx1 = torch.randn(1, 64, 112, 112)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(in_channels=1, out_channels=200,  kernel_size=(1, 1), stride=1, padding=0)\n        self.pool1 = torch.nn.MaxPool2d(kernel_size=3, stride=2)\n        self.conv2 = torch.nn.Conv2d(in_channels=200, out_channels=255, kernel_size=(1, 1), stride=1, padding=0)\n        self.pool2 = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n        self.conv3 = torch.nn.Conv2d(in_channels=255, out_channels=64,  kernel_size=(1, 1), stride=1, padding=0)\n        self.pool3 = torch.nn.AvgPool2d(kernel_size=7, stride=7)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = self.pool1(v2)\n        v4 = self.conv2(v3)\n        v5 = torch.tanh(v4)\n        v6 = self.pool2(v5)\n        v7 = self.conv3(v6)\n        v8 = torch.relu(v7)\n        v9 = self.pool3(v8)\n        return v9\n# Inputs to the model\nx1 = torch.randn(1, 1, 119, 119)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(in_channels=1, out_channels=2, kernel_size=2, stride=1, padding=0)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = torch.sigmoid(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 1, 300, 300)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        \n        self.conv1 = torch.nn.Conv2d(in_channels=3, out_channels=16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        self.conv2 = torch.nn.Conv2d(in_channels=16, out_channels=24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n        self.conv3 = torch.nn.Conv2d(in_channels=24, out_channels=32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        \n    def forward(self, x):\n        v1 = torch.nn.functional.relu(self.conv1(x))\n        v2 = torch.nn.functional.relu(self.conv2(v1))\n        v3 = torch.nn.functional.relu(self.conv3(v2))\n        \n        return v3        \n# Inputs to the model\nx1 = torch.randn(1, 3, 28, 28)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1):\n        v1 = x1.float()\n        v2 = v1.type(torch.float64)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 3, 32, 32)\n",
                "\nclass Model1(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(in_channels=1, out_channels=3, kernel_size=3, stride=1, padding=1)\n        self.pool = torch.nn.MaxPool2d(4, stride=4, padding=0)\n        self.conv1 = torch.nn.Conv2d(in_channels=3, out_channels=128, kernel_size=1, stride=1, padding=0)\n        self.conv2 = torch.nn.Conv2d(in_channels=128, out_channels=128, kernel_size=1, stride=1, padding=0)\n    def forward(self, x):\n        v1 = self.conv(x)\n        v2 = self.pool(v1)\n        v3 = self.conv1(v2)\n        v4 = torch.relu(v3)\n        v5 = self.conv2(v4)\n        return torch.cat([v1, v5], dim=1)\n# Inputs to the model\nx = torch.randn(1, 1, 256, 256)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(in_channels=64, out_channels=32, kernel_size=3, stride=1, padding=1)\n        self.conv2 = torch.nn.Conv2d(in_channels=32, out_channels=16, kernel_size=3, stride=1, padding=1)\n        self.conv3 = torch.nn.Conv2d(in_channels=16, out_channels=2, kernel_size=3, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = self.conv2(v2)\n        v4 = torch.sigmoid(v3)\n        v5 = self.conv3(v4)\n        v6 = torch.sigmoid(v5)\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 64, 32, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(in_channels=2, out_channels=32, kernel_size=3, stride=2, padding=1)\n        self.conv2 = torch.nn.Conv2d(in_channels=32, out_channels=16, kernel_size=1, stride=1, padding=0)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = self.conv2(v2)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 2, 32, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 6, 3, stride=1, padding=0, dilation=1)\n        self.conv2 = torch.nn.Conv2d(6, 12, 3, stride=1, padding=1, dilation=1)\n        self.conv3 = torch.nn.Conv2d(12, 24, 3, stride=1, padding=0, dilation=2)\n        self.conv4 = torch.nn.Conv2d(24, 48, 3, stride=1, padding=1, dilation=1)\n        self.conv5 = torch.nn.Conv2d(48, 96, 3, stride=1, padding=0, dilation=4)\n        self.conv6 = torch.nn.Conv2d(96, 192, 3, stride=1, padding=1, dilation=1)\n        self.conv7 = torch.nn.Conv2d(192, 384, 3, stride=1, padding=0, dilation=8)\n        self.conv8 = torch.nn.Conv2d(384, 768, 3, stride=1, padding=2, dilation=1)\n        self.conv9 = torch.nn.Conv2d(768, 1024, 3, stride=1, padding=0, dilation=16)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = self.conv2(v2)\n        v4 = torch.sigmoid(v3)\n        v5 = self.conv3(v4)\n        v6 = torch.sigmoid(v5)\n        v7 = self.conv4(v6)\n        v8 = torch.sigmoid(v7)\n        v9 = self.conv5(v8)\n        v10 = torch.sigmoid(v9)\n        v11 = self.conv6(v10)\n        v12 = torch.sigmoid(v11)\n        v13 = self.conv7(v12)\n        v14 = torch.sigmoid(v13)\n        v15 = self.conv8(v14)\n        v16 = torch.sigmoid(v15)\n        v17 = self.conv9(v16)\n        v18 = torch.sigmoid(v17)\n        return v18\n# Inputs to the model\nx1 = torch.randn(1, 3, 112, 112)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.model = torch.nn.Sequential(\n            torch.nn.Conv2d(in_channels=3, out_channels=8, kernel_size=3, stride=4, padding=0, dilation=3),\n            torch.nn.Sigmoid(),\n        )\n    def forward(self, x1):\n        v1 = self.model(x1)\n        return v1\n# Inputs to the model\nx1 = torch.randn(1, 3, 28, 28)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=2, dilation=1)\n        self.conv2 = torch.nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=2, dilation=2)\n        self.conv3 = torch.nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=4, dilation=3)\n        self.conv4 = torch.nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=8, dilation=4)\n        self.conv5 = torch.nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=12, dilation=5)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = self.conv2(v2)\n        v4 = torch.sigmoid(v3)\n        v5 = self.conv3(v4)\n        v6 = torch.sigmoid(v5)\n        v7 = self.conv4(v6)\n        v8 = torch.sigmoid(v7)\n        v9 = self.conv5(v8)\n        v10 = torch.sigmoid(v9)\n        return v10\n# Inputs to the model\nx1 = torch.randn(1, 64, 112, 112)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(in_channels=1, out_channels=200,  kernel_size=(1, 1), stride=1, padding=0)\n        self.pool1 = torch.nn.MaxPool2d(kernel_size=3, stride=2)\n        self.conv2 = torch.nn.Conv2d(in_channels=200, out_channels=255, kernel_size=(1, 1), stride=1, padding=0)\n        self.pool2 = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n        self.conv3 = torch.nn.Conv2d(in_channels=255, out_channels=64,  kernel_size=(1, 1), stride=1, padding=0)\n        self.pool3 = torch.nn.AvgPool2d(kernel_size=7, stride=7)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = self.pool1(v2)\n        v4 = self.conv2(v3)\n        v5 = torch.tanh(v4)\n        v6 = self.pool2(v5)\n        v7 = self.conv3(v6)\n        v8 = torch.relu(v7)\n        v9 = self.pool3(v8)\n        return v9\n# Inputs to the model\nx1 = torch.randn(1, 1, 119, 119)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(in_channels=1, out_channels=2, kernel_size=2, stride=1, padding=0)\n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = torch.sigmoid(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 1, 300, 300)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        \n        self.conv1 = torch.nn.Conv2d(in_channels=3, out_channels=16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        self.conv2 = torch.nn.Conv2d(in_channels=16, out_channels=24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n        self.conv3 = torch.nn.Conv2d(in_channels=24, out_channels=32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        \n    def forward(self, x):\n        v1 = torch.nn.functional.relu(self.conv1(x))\n        v2 = torch.nn.functional.relu(self.conv2(v1))\n        v3 = torch.nn.functional.relu(self.conv3(v2))\n        \n        return v3        \n# Inputs to the model\nx1 = torch.randn(1, 3, 28, 28)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1):\n        v1 = x1.float()\n        v2 = v1.type(torch.float64)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 3, 32, 32)\n",
                "\nclass Model1(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(in_channels=1, out_channels=3, kernel_size=3, stride=1, padding=1)\n        self.pool = torch.nn.MaxPool2d(4, stride=4, padding=0)\n        self.conv1 = torch.nn.Conv2d(in_channels=3, out_channels=128, kernel_size=1, stride=1, padding=0)\n        self.conv2 = torch.nn.Conv2d(in_channels=128, out_channels=128, kernel_size=1, stride=1, padding=0)\n    def forward(self, x):\n        v1 = self.conv(x)\n        v2 = self.pool(v1)\n        v3 = self.conv1(v2)\n        v4 = torch.relu(v3)\n        v5 = self.conv2(v4)\n        return torch.cat([v1, v5], dim=1)\n# Inputs to the model\nx = torch.randn(1, 1, 256, 256)\n"
            ],
            "g_time": 22.76441216468811
        }
    }
}
{
    "opt": "cat_slice_cat",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n \n    def forward(self, x1, x2, x3, size):\n        t1 = torch.cat([x1, x2, x3], dim=1)\n        t2 = t1[:, 0:7]\n        t3 = t2[:, 0:size]\n        t4 = torch.cat([t1, t3], dim=1)\n        return t4\n \n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 7, 64, 64)\nx2 = torch.randn(1, 7, 64, 64)\nx3 = torch.randn(1, 7, 64, 64)\nsize = 5\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.m = torch.nn.Linear(65535, 65535, bias=False)\n        self.s = torch.nn.Linear(65535, 65535, bias=True)\n \n    def forward(self, x1, x2):\n        o1 = self.m(x1)\n        o2 = o1 + self.s(x2)[:, 0:65535]\n        return o2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 65535)\nx2 = torch.randn(1, 65535)\n",
                "\nclass Model(torch.nn.Module):\n    def forward(self, x10, x11):\n        v10 = torch.cat([x10, x11], dim=1)\n        v11 = v10[:, 0:9223372036854775807]\n        v12 = v11[:, 0:1728]\n        v13 = torch.cat([v10, v12], dim=1)\n        return v13\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx10 = torch.randn(1, 3, 1, 9223372036854775807)\nx11 = torch.randn(1, 3, 1, 1728)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n \n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v3[:, 0:9223372036854775807]\n        v4 = v2[:, 0:size]\n        v5 = torch.cat([v1, v1], dim=1)\n        return v5\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n  \n    def forward(self, x1):\n        v2 = torch.cat([x1, x1], dim=1)\n        v3 = v2[:, 0:9223372036854775807]\n        v4 = v3[:, 0:63882500]\n        v5 = torch.cat([v2, v4], dim=1)\n        return v5\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 10)\nx2 = torch.randn(1, 6, 10)\nx3 = torch.randn(1, 4, 10)\nx4 = torch.randn(1, 6, 10)\nx5 = torch.randn(1, 2, 10)\nx6 = torch.randn(1, 30, 10)\nx7 = torch.randn(1, 7, 10)\nx8 = torch.randn(1, 2, 10)\nx9 = torch.randn(1, 9, 10)\nx10 = torch.randn(1, 2, 10)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n \n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = torch.cat([v1, v1], dim=1)\n        v3 = v2[:, 0:9223372036854775807]\n        v4 = v3[:, 0:v1.size()[0]]\n        v5 = torch.cat([v2, v4], dim=1)\n        return v5\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def forward(self, x1, x2):\n        l1 = [x1, x1]\n        t1 = torch.cat(l1, dim=1)\n        t2 = t1[:, 0:9223372036854775807]\n        t3 = t2[:, 0:20]\n        l2 = [t1, t3]\n        t4 = torch.cat(l2, dim=1)\n        return t4\n \n# Initializing the model\nm = Model()\n \n# Inputs to the model\nx1 = torch.randn(1, 3, 224, 224)\nx2 = torch.randn(1, 3, 32, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n \n    def forward(self, x1, x2):\n        v1 = torch.cat([x1, x2], dim=1)\n        v2 = v1[:, 0:9223372036854775807]\n        v3 = v2[:, 0:3]\n        v4 = torch.cat([v1, v3], dim=1)\n        return v4\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 5, 20, 30)\nx2 = torch.randn(1, 5, 40, 60)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n \n    def forward(self, x1, x2, x3):\n        v1 = torch.cat(x2, x1, dim=1)\n        v2 = v1[:, 0: 2147483647]\n        v3 = v2[:, 0:2]\n        v4 = torch.cat([v1, v3], dim=1)\n        return v4\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 2)\nx2 = torch.randn(1, 1)\nx3 = torch.randn(1, 1)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n\n    def forward(self, __input__1, __input__2):\n        v1 = torch.cat([__input__1, __input__2], dim=1)\n        v2 = v1[:, 0:9223372036854775807]\n        v3 = v2[:, 0:__input__1.size(1)]\n        v4 = torch.cat([v1, v3], dim=1)\n        return v4\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 3, 2)\nx2 = torch.randn(1, 3, 3, 3)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n \n    def forward(self, x1, x2, x3, size):\n        t1 = torch.cat([x1, x2, x3], dim=1)\n        t2 = t1[:, 0:7]\n        t3 = t2[:, 0:size]\n        t4 = torch.cat([t1, t3], dim=1)\n        return t4\n \n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 7, 64, 64)\nx2 = torch.randn(1, 7, 64, 64)\nx3 = torch.randn(1, 7, 64, 64)\nsize = 5\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.m = torch.nn.Linear(65535, 65535, bias=False)\n        self.s = torch.nn.Linear(65535, 65535, bias=True)\n \n    def forward(self, x1, x2):\n        o1 = self.m(x1)\n        o2 = o1 + self.s(x2)[:, 0:65535]\n        return o2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 65535)\nx2 = torch.randn(1, 65535)\n",
                "\nclass Model(torch.nn.Module):\n    def forward(self, x10, x11):\n        v10 = torch.cat([x10, x11], dim=1)\n        v11 = v10[:, 0:9223372036854775807]\n        v12 = v11[:, 0:1728]\n        v13 = torch.cat([v10, v12], dim=1)\n        return v13\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx10 = torch.randn(1, 3, 1, 9223372036854775807)\nx11 = torch.randn(1, 3, 1, 1728)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n \n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v3[:, 0:9223372036854775807]\n        v4 = v2[:, 0:size]\n        v5 = torch.cat([v1, v1], dim=1)\n        return v5\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n  \n    def forward(self, x1):\n        v2 = torch.cat([x1, x1], dim=1)\n        v3 = v2[:, 0:9223372036854775807]\n        v4 = v3[:, 0:63882500]\n        v5 = torch.cat([v2, v4], dim=1)\n        return v5\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 10)\nx2 = torch.randn(1, 6, 10)\nx3 = torch.randn(1, 4, 10)\nx4 = torch.randn(1, 6, 10)\nx5 = torch.randn(1, 2, 10)\nx6 = torch.randn(1, 30, 10)\nx7 = torch.randn(1, 7, 10)\nx8 = torch.randn(1, 2, 10)\nx9 = torch.randn(1, 9, 10)\nx10 = torch.randn(1, 2, 10)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n \n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = torch.cat([v1, v1], dim=1)\n        v3 = v2[:, 0:9223372036854775807]\n        v4 = v3[:, 0:v1.size()[0]]\n        v5 = torch.cat([v2, v4], dim=1)\n        return v5\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def forward(self, x1, x2):\n        l1 = [x1, x1]\n        t1 = torch.cat(l1, dim=1)\n        t2 = t1[:, 0:9223372036854775807]\n        t3 = t2[:, 0:20]\n        l2 = [t1, t3]\n        t4 = torch.cat(l2, dim=1)\n        return t4\n \n# Initializing the model\nm = Model()\n \n# Inputs to the model\nx1 = torch.randn(1, 3, 224, 224)\nx2 = torch.randn(1, 3, 32, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n \n    def forward(self, x1, x2):\n        v1 = torch.cat([x1, x2], dim=1)\n        v2 = v1[:, 0:9223372036854775807]\n        v3 = v2[:, 0:3]\n        v4 = torch.cat([v1, v3], dim=1)\n        return v4\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 5, 20, 30)\nx2 = torch.randn(1, 5, 40, 60)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n \n    def forward(self, x1, x2, x3):\n        v1 = torch.cat(x2, x1, dim=1)\n        v2 = v1[:, 0: 2147483647]\n        v3 = v2[:, 0:2]\n        v4 = torch.cat([v1, v3], dim=1)\n        return v4\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 2)\nx2 = torch.randn(1, 1)\nx3 = torch.randn(1, 1)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n\n    def forward(self, __input__1, __input__2):\n        v1 = torch.cat([__input__1, __input__2], dim=1)\n        v2 = v1[:, 0:9223372036854775807]\n        v3 = v2[:, 0:__input__1.size(1)]\n        v4 = torch.cat([v1, v3], dim=1)\n        return v4\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 3, 2)\nx2 = torch.randn(1, 3, 3, 3)\n"
            ],
            "g_time": 10.956997156143188
        }
    }
}
{
    "opt": "permute_matmul_fusion",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, x2):\n        v1 = x1.permute(0, 2, 1)\n        v2 = x2.permute(0, 2, 1)\n        return torch.bmm(v1, x1)\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\nx2 = torch.randn(1, 2, 2)\n",
                "\nv1 = x1.permute(0, 1, 3, 2)\nv2 = x2.permute(0, 1, 3, 2)\nv3 = v1.permute(0, 1, 3, 2)\nv4 = v1.permute(0, 1, 3, 2)\nv5 = v2.permute(0, 1, 3, 2)\nv6 = v3.permute(0, 1, 3, 2)\nv7 = v4.permute(0, 1, 3, 2)\nv8 = v5.permute(0, 1, 3, 2)\nModel(torch.nn.Module)\n# Inputs to the model\nx1 = torch.randn(1, 2, 2, 2)\nx2 = torch.randn(1, 2, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, x2):\n        v9 = torch.bmm(x2.permute(0, 2, 1), x1)\n        return v9\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\nx2 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, x2):\n        v5 = torch.bmm(x1.permute(0, 2, 1), x2)\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\nx2 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, x2):\n        v1 = x2.permute(0, 2, 1)\n        return torch.bmm(x1, v1)\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\nx2 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, x2):\n        v8 = x1.permute(0, 2, 1)\n        v9 = x2.permute(0, 2, 1)\n        return torch.bmm(v8, v9)\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\nx2 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, x2):\n        v3 = x1.permute(0, 2, 1)\n        v6 = x2.permute(0, 2, 1)\n        v2 = torch.bmm(v3, v6)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\nx2 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, x2):\n        v2 = x2.permute(0, 2, 1)\n        return torch.bmm(x1.permute(0, 2, 1), v2)\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\nx2 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, x2):\n        v3 = torch.bmm(x1, x1.permute(0, 2, 1))\n        return x1.permute(0, 2, 1)\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\nx2 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, x2):\n        v1 = x1.permute(0, 2, 1)\n        v2 = torch.bmm(v1, x2)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\nx2 = torch.randn(1, 2, 2)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, x2):\n        v1 = x1.permute(0, 2, 1)\n        v2 = x2.permute(0, 2, 1)\n        return torch.bmm(v1, x1)\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\nx2 = torch.randn(1, 2, 2)\n",
                "\nv1 = x1.permute(0, 1, 3, 2)\nv2 = x2.permute(0, 1, 3, 2)\nv3 = v1.permute(0, 1, 3, 2)\nv4 = v1.permute(0, 1, 3, 2)\nv5 = v2.permute(0, 1, 3, 2)\nv6 = v3.permute(0, 1, 3, 2)\nv7 = v4.permute(0, 1, 3, 2)\nv8 = v5.permute(0, 1, 3, 2)\nModel(torch.nn.Module)\n# Inputs to the model\nx1 = torch.randn(1, 2, 2, 2)\nx2 = torch.randn(1, 2, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, x2):\n        v9 = torch.bmm(x2.permute(0, 2, 1), x1)\n        return v9\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\nx2 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, x2):\n        v5 = torch.bmm(x1.permute(0, 2, 1), x2)\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\nx2 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, x2):\n        v1 = x2.permute(0, 2, 1)\n        return torch.bmm(x1, v1)\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\nx2 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, x2):\n        v8 = x1.permute(0, 2, 1)\n        v9 = x2.permute(0, 2, 1)\n        return torch.bmm(v8, v9)\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\nx2 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, x2):\n        v3 = x1.permute(0, 2, 1)\n        v6 = x2.permute(0, 2, 1)\n        v2 = torch.bmm(v3, v6)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\nx2 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, x2):\n        v2 = x2.permute(0, 2, 1)\n        return torch.bmm(x1.permute(0, 2, 1), v2)\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\nx2 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, x2):\n        v3 = torch.bmm(x1, x1.permute(0, 2, 1))\n        return x1.permute(0, 2, 1)\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\nx2 = torch.randn(1, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, x2):\n        v1 = x1.permute(0, 2, 1)\n        v2 = torch.bmm(v1, x2)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 2, 2)\nx2 = torch.randn(1, 2, 2)\n"
            ],
            "g_time": 7.939807653427124
        }
    }
}
{
    "opt": "binary_unary=10",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(4, 3)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 + x1\n        v3 = torch.nn.functional.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 4)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(40, 5)\n \n    def forward(self, x1, x2):\n        v1 = self.linear(x1)\n        v2 = v1 + x2\n        v3 = torch.nn.functional.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 40)\nx2 = torch.randn(1, 5)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(64, 128)\n \n    def forward(self, x2):\n        v1 = self.linear(x2)\n        v2 = v1 + other\n        v3 = F.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx2 = torch.randn(1, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(128, 128)\n \n    def forward(self, x2):\n        v1 = self.linear(x2)\n        v2 = v1 + 1\n        v3 = F.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx2 = torch.randn(1, 128)\nother = torch.randn(1, 128)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 10)\n \n    def forward(self, x):\n        out = self.linear(x)\n        out = out + x\n        out = torch.relu(out)\n        return out\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx  = torch.randn(3)\n\n# Output of the model\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(1024, 1024)\n \n    def forward(self, x1, x2):\n        v1 = self.linear(x2)\n        v2 = v1 + x1\n        v3 = nn.ReLU()(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Input to the model\nx1 = torch.randn(512)\nx2 = torch.randn(512, 1024)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.module = torch.nn.Linear(50, 50)\n \n    def forward(self, x1):\n        v1 = self.module(x1)\n        v2 = v1 + torch.rand_like(v1)\n        v3 = torch.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 50)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(4, 8, bias=False)\n        self.other = torch.randn(8, 4)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 + self.other\n        v3 = torch.nn.functional.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 4)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 5)\n \n    def forward(self, x1, x2):\n        v1 = self.linear(x1)\n        v2 = v1 + x2\n        v3 = torch.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\nx2 = torch.randn(1, 5, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(16, 64)\n \n    def forward(self, x1, x2):\n        v1 = self.linear(x1)\n        v2 = v1 + x2\n        v3 = F.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 16)\nx2 = torch.randn(1, 16)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(4, 3)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 + x1\n        v3 = torch.nn.functional.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 4)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(40, 5)\n \n    def forward(self, x1, x2):\n        v1 = self.linear(x1)\n        v2 = v1 + x2\n        v3 = torch.nn.functional.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 40)\nx2 = torch.randn(1, 5)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(64, 128)\n \n    def forward(self, x2):\n        v1 = self.linear(x2)\n        v2 = v1 + other\n        v3 = F.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx2 = torch.randn(1, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(128, 128)\n \n    def forward(self, x2):\n        v1 = self.linear(x2)\n        v2 = v1 + 1\n        v3 = F.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx2 = torch.randn(1, 128)\nother = torch.randn(1, 128)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 10)\n \n    def forward(self, x):\n        out = self.linear(x)\n        out = out + x\n        out = torch.relu(out)\n        return out\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx  = torch.randn(3)\n\n# Output of the model\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(1024, 1024)\n \n    def forward(self, x1, x2):\n        v1 = self.linear(x2)\n        v2 = v1 + x1\n        v3 = nn.ReLU()(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Input to the model\nx1 = torch.randn(512)\nx2 = torch.randn(512, 1024)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.module = torch.nn.Linear(50, 50)\n \n    def forward(self, x1):\n        v1 = self.module(x1)\n        v2 = v1 + torch.rand_like(v1)\n        v3 = torch.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 50)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(4, 8, bias=False)\n        self.other = torch.randn(8, 4)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 + self.other\n        v3 = torch.nn.functional.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 4)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 5)\n \n    def forward(self, x1, x2):\n        v1 = self.linear(x1)\n        v2 = v1 + x2\n        v3 = torch.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\nx2 = torch.randn(1, 5, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(16, 64)\n \n    def forward(self, x1, x2):\n        v1 = self.linear(x1)\n        v2 = v1 + x2\n        v3 = F.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 16)\nx2 = torch.randn(1, 16)\n"
            ],
            "g_time": 5.59627628326416
        }
    }
}
{
    "opt": "unary=23",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(64, 32, 3, stride=2)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = torch.tanh(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 64, 14, 13, dtype=torch.float32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(39, 72, 3)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = torch.tanh(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(52, 39, 5, 19, dtype=torch.float32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(27,15,7)\n    def forward(self, x2):\n        v2 = self.conv_transpose(x2)\n        v3 = torch.tanh(v2)\n        return v3\n# Inputs to the model\nx2 = torch.randn(1, 27, 19, 127, dtype=torch.float32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        # Create random parameters\n        self.param1 = torch.nn.Parameter(torch.randn(32, 43))\n        self.param2 = torch.nn.Parameter(torch.randn(23, 12))\n    def forward(self, x1):\n        v1 = torch.pow(x1, self.param1)\n        v2 = torch.softmax(v1, self.param2)\n        v3 = torch.tanh(v2)\n        return v3\n# Inputs to the model\nx1 = torch.randn(42, 100000)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(7, 19, 1, stride=2, padding=0)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = torch.tanh(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 7, 14, 17)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(8, 11, 2, stride=1, padding=0)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = torch.tanh(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 8, 32, 63, dtype=torch.float32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(3, 8, 2, stride=2, padding=2)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = torch.tanh(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 3, 16, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(19, 32, 3)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = torch.tanh(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 19, 20, 101, dtype=torch.float32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(32, 7, 5)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = torch.tanh(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 32, 1, 25, dtype=torch.float32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(5, 9, 3, stride=2, padding=1, output_padding=(0, 2), groups=2, bias=False, dilation=3)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = torch.tanh(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 5, 100, 150)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(64, 32, 3, stride=2)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = torch.tanh(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 64, 14, 13, dtype=torch.float32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(39, 72, 3)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = torch.tanh(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(52, 39, 5, 19, dtype=torch.float32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(27,15,7)\n    def forward(self, x2):\n        v2 = self.conv_transpose(x2)\n        v3 = torch.tanh(v2)\n        return v3\n# Inputs to the model\nx2 = torch.randn(1, 27, 19, 127, dtype=torch.float32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        # Create random parameters\n        self.param1 = torch.nn.Parameter(torch.randn(32, 43))\n        self.param2 = torch.nn.Parameter(torch.randn(23, 12))\n    def forward(self, x1):\n        v1 = torch.pow(x1, self.param1)\n        v2 = torch.softmax(v1, self.param2)\n        v3 = torch.tanh(v2)\n        return v3\n# Inputs to the model\nx1 = torch.randn(42, 100000)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(7, 19, 1, stride=2, padding=0)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = torch.tanh(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 7, 14, 17)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(8, 11, 2, stride=1, padding=0)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = torch.tanh(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 8, 32, 63, dtype=torch.float32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(3, 8, 2, stride=2, padding=2)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = torch.tanh(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 3, 16, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(19, 32, 3)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = torch.tanh(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 19, 20, 101, dtype=torch.float32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(32, 7, 5)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = torch.tanh(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 32, 1, 25, dtype=torch.float32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(5, 9, 3, stride=2, padding=1, output_padding=(0, 2), groups=2, bias=False, dilation=3)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = torch.tanh(v1)\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 5, 100, 150)\n"
            ],
            "g_time": 5.48411226272583
        }
    }
}
{
    "opt": "fuse_conv_bn",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.in_planes = 64\n        self.conv1 = torch.nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n        self.bn1 = torch.nn.BatchNorm2d(64, eps=0.001)\n        self.relu = torch.nn.ReLU()\n        self.maxpool = torch.nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n        self.avgpool = torch.nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = torch.nn.Linear(512 * block.expansion, num_classes)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out')\n            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n\n    def _make_layer(self, block, planes, num_blocks, stride):\n        strides = [stride] + [1] * (num_blocks - 1)\n        layers = []\n        for stride in strides:\n            layers.append(block(self.in_planes, planes, stride))\n            self.in_planes = planes * block.expansion\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        out = self.relu(self.bn1(self.conv1(x)))\n        out = self.maxpool(out)\n        out = self.layer1(out)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = self.layer4(out)\n        out = self.avgpool(out)\n        out = torch.flatten(out, 1)\n        out = self.fc(out)\n        return out\n# Inputs to the model\nx = torch.randn(2, 3, 224, 224)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(1, 1, 2, bias=False)\n        self.bn1 = torch.nn.BatchNorm2d(1, affine=False)\n        self.relu = torch.nn.ReLU()\n        self.conv2 = torch.nn.Conv2d(1, 1, 2, bias=False)\n        self.sigmoid = torch.nn.Sigmoid()\n    def forward(self, x):\n        x = self.relu(self.bn1(self.conv1(x)))\n        x = self.sigumd(self.conv2(x))\n        return x\n# Inputs to the model\nx = torch.randn(1, 1, 4, 4)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.block_sequence = torch.nn.Sequential(torch.nn.Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), torch.nn.BatchNorm2d(256), torch.nn.ReLU(), torch.nn.Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), torch.nn.BatchNorm2d(256), torch.nn.ReLU(), torch.nn.Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False), torch.nn.BatchNorm2d(256))\n        self.conv = torch.nn.Conv2d(256, 10, kernel_size=(1, 1), stride=(1, 1), bias=False)\n    def forward(self, x, y):\n        x = self.block_sequence(x)\n        y = self.conv(y)\n        x_1x1 = x + y\n        output = x_1x1\n        return output\n# Inputs to the model\nx = torch.randn(1, 512, 2, 2)\ny = torch.randn(1, 10, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(6, 6, 6, bias=True)\n        self.bn1 = torch.nn.BatchNorm2d(6)\n        self.relu = torch.nn.ReLU()\n        self.conv2 = torch.nn.Conv2d(6, 6, 18, bias=True)\n        self.sigmoid = torch.nn.Sigmoid()\n    def forward(self, input):\n        x = self.relu(self.bn1(self.conv1(input)))\n        y = self.relu(self.conv2(x))\n        y = self.sigmoid(y)\n        return y\n# Inputs to the model\ninput = torch.randn(2, 6, 18, 18)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.norm = torch.nn.BatchNorm2d(4)\n        self.conv = torch.nn.Conv2d(4, 4, 3)\n    def forward(self, x):\n        return self.conv(self.norm(x))\n# Inputs to the model\nx = torch.randn(1, 4, 4, 4)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(2, 2, 3)\n        self.relu1 = torch.nn.ReLU()\n        self.conv2 = torch.nn.Conv2d(2, 2, 1)\n        self.relu2 = torch.nn.ReLU()\n    def forward(self, x):\n        x = self.relu1(self.conv1(x))\n        x = self.relu2(self.conv2(x))\n        return x\n# Inputs to the model\nx = torch.randn(1, 2, 4, 4)\n",
                "\nclass Model(nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.in_channels = 3\n        self.conv = nn.Conv2d(3, 64, kernel_size=9, stride=1)\n        self.bbr = torch.nn.Sequential(nn.ModuleList([MyBBR() for _ in range(0,16)]))\n        self.bn = nn.BatchNorm2d(64)\n        self.relu1 = nn.ReLU()\n        self.maxpool = nn.MaxPool2d(kernel_size=4, stride=4, padding=0) # kernel size, stride\n        self.stage1_unit1 = BasicBlock_Custom(64, 64, 1, 1, 1) # in chennel, out channels, kernel size, stride, padding\n        self.stage2_unit2 = self._make_transition_layer([256, 128], [256, 128], [True, False], [2, 2]) # transition layers\n        self.stage3_unit3 = self._make_stage([3, [256, 128, 256, 128], [256, 256, 256, 256], [False, True, True, False]], 4) # blocks args\n        self.stage4_unit4 = self._make_stage([3, [256, 64, 256, 64], [256, 256, 256, 256], [False, True, True, False]], 1)\n\n    def forward(self, x):\n        x = self.conv(x)        #[1,256,1,1]\n        x = self.bbr(x)         #[1,256,1,1]\n        x = self.relu1(self.bn(x))       #[1,256,1,1]\n        x = self.maxpool(x)     #[1,256,1,1]\n        x = self.stage1_unit1(x) #[1,256,1,1]\n        x = self.stage2_unit2(x) #[1,256,2,2]\n        x = self.stage3_unit3(x) #[1,256,4,4]\n        x = self.stage4_unit4(x) #[1,256,4,4]\n        return x\n# Inputs to the model\nx = Variable(torch.randn(1, 1, 4, 4))\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(1024, 512, 3, stride=2, padding=1)\n        self.relu = torch.nn.ReLU()\n        self.bn = torch.nn.BatchNorm2d(512)\n    def forward(self, x):\n        return self.relu(self.bn(self.conv(x)))\n# Inputs to the model\nx = torch.randn(1, 1024, 14, 14)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(64, 64, 3, padding=(4, 8), dilation=(2, 3))\n    def forward(self, x):\n\n        return self.conv1(x)\n# Inputs to the model\nx = torch.randn(1, 64, 56, 56)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.lin1 = torch.nn.Linear(2, 2)\n        self.bn1 = torch.nn.BatchNorm2d(2)\n        self.relu = torch.nn.ReLU()\n        self.lin2 = torch.nn.Linear(2, 2)\n        self.tanh = torch.nn.Tanh()\n    def forward(self, x):\n        x = self.relu(self.bn1(self.lin1(x)))\n        x = self.tanh(self.lin2(x))\n        return x\n# Inputs to the model\nx = torch.randn(2, 2)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.in_planes = 64\n        self.conv1 = torch.nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n        self.bn1 = torch.nn.BatchNorm2d(64, eps=0.001)\n        self.relu = torch.nn.ReLU()\n        self.maxpool = torch.nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n        self.avgpool = torch.nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = torch.nn.Linear(512 * block.expansion, num_classes)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out')\n            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n\n    def _make_layer(self, block, planes, num_blocks, stride):\n        strides = [stride] + [1] * (num_blocks - 1)\n        layers = []\n        for stride in strides:\n            layers.append(block(self.in_planes, planes, stride))\n            self.in_planes = planes * block.expansion\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        out = self.relu(self.bn1(self.conv1(x)))\n        out = self.maxpool(out)\n        out = self.layer1(out)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = self.layer4(out)\n        out = self.avgpool(out)\n        out = torch.flatten(out, 1)\n        out = self.fc(out)\n        return out\n# Inputs to the model\nx = torch.randn(2, 3, 224, 224)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(1, 1, 2, bias=False)\n        self.bn1 = torch.nn.BatchNorm2d(1, affine=False)\n        self.relu = torch.nn.ReLU()\n        self.conv2 = torch.nn.Conv2d(1, 1, 2, bias=False)\n        self.sigmoid = torch.nn.Sigmoid()\n    def forward(self, x):\n        x = self.relu(self.bn1(self.conv1(x)))\n        x = self.sigumd(self.conv2(x))\n        return x\n# Inputs to the model\nx = torch.randn(1, 1, 4, 4)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.block_sequence = torch.nn.Sequential(torch.nn.Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), torch.nn.BatchNorm2d(256), torch.nn.ReLU(), torch.nn.Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), torch.nn.BatchNorm2d(256), torch.nn.ReLU(), torch.nn.Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False), torch.nn.BatchNorm2d(256))\n        self.conv = torch.nn.Conv2d(256, 10, kernel_size=(1, 1), stride=(1, 1), bias=False)\n    def forward(self, x, y):\n        x = self.block_sequence(x)\n        y = self.conv(y)\n        x_1x1 = x + y\n        output = x_1x1\n        return output\n# Inputs to the model\nx = torch.randn(1, 512, 2, 2)\ny = torch.randn(1, 10, 2, 2)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(6, 6, 6, bias=True)\n        self.bn1 = torch.nn.BatchNorm2d(6)\n        self.relu = torch.nn.ReLU()\n        self.conv2 = torch.nn.Conv2d(6, 6, 18, bias=True)\n        self.sigmoid = torch.nn.Sigmoid()\n    def forward(self, input):\n        x = self.relu(self.bn1(self.conv1(input)))\n        y = self.relu(self.conv2(x))\n        y = self.sigmoid(y)\n        return y\n# Inputs to the model\ninput = torch.randn(2, 6, 18, 18)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.norm = torch.nn.BatchNorm2d(4)\n        self.conv = torch.nn.Conv2d(4, 4, 3)\n    def forward(self, x):\n        return self.conv(self.norm(x))\n# Inputs to the model\nx = torch.randn(1, 4, 4, 4)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(2, 2, 3)\n        self.relu1 = torch.nn.ReLU()\n        self.conv2 = torch.nn.Conv2d(2, 2, 1)\n        self.relu2 = torch.nn.ReLU()\n    def forward(self, x):\n        x = self.relu1(self.conv1(x))\n        x = self.relu2(self.conv2(x))\n        return x\n# Inputs to the model\nx = torch.randn(1, 2, 4, 4)\n",
                "\nclass Model(nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.in_channels = 3\n        self.conv = nn.Conv2d(3, 64, kernel_size=9, stride=1)\n        self.bbr = torch.nn.Sequential(nn.ModuleList([MyBBR() for _ in range(0,16)]))\n        self.bn = nn.BatchNorm2d(64)\n        self.relu1 = nn.ReLU()\n        self.maxpool = nn.MaxPool2d(kernel_size=4, stride=4, padding=0) # kernel size, stride\n        self.stage1_unit1 = BasicBlock_Custom(64, 64, 1, 1, 1) # in chennel, out channels, kernel size, stride, padding\n        self.stage2_unit2 = self._make_transition_layer([256, 128], [256, 128], [True, False], [2, 2]) # transition layers\n        self.stage3_unit3 = self._make_stage([3, [256, 128, 256, 128], [256, 256, 256, 256], [False, True, True, False]], 4) # blocks args\n        self.stage4_unit4 = self._make_stage([3, [256, 64, 256, 64], [256, 256, 256, 256], [False, True, True, False]], 1)\n\n    def forward(self, x):\n        x = self.conv(x)        #[1,256,1,1]\n        x = self.bbr(x)         #[1,256,1,1]\n        x = self.relu1(self.bn(x))       #[1,256,1,1]\n        x = self.maxpool(x)     #[1,256,1,1]\n        x = self.stage1_unit1(x) #[1,256,1,1]\n        x = self.stage2_unit2(x) #[1,256,2,2]\n        x = self.stage3_unit3(x) #[1,256,4,4]\n        x = self.stage4_unit4(x) #[1,256,4,4]\n        return x\n# Inputs to the model\nx = Variable(torch.randn(1, 1, 4, 4))\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(1024, 512, 3, stride=2, padding=1)\n        self.relu = torch.nn.ReLU()\n        self.bn = torch.nn.BatchNorm2d(512)\n    def forward(self, x):\n        return self.relu(self.bn(self.conv(x)))\n# Inputs to the model\nx = torch.randn(1, 1024, 14, 14)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(64, 64, 3, padding=(4, 8), dilation=(2, 3))\n    def forward(self, x):\n\n        return self.conv1(x)\n# Inputs to the model\nx = torch.randn(1, 64, 56, 56)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.lin1 = torch.nn.Linear(2, 2)\n        self.bn1 = torch.nn.BatchNorm2d(2)\n        self.relu = torch.nn.ReLU()\n        self.lin2 = torch.nn.Linear(2, 2)\n        self.tanh = torch.nn.Tanh()\n    def forward(self, x):\n        x = self.relu(self.bn1(self.lin1(x)))\n        x = self.tanh(self.lin2(x))\n        return x\n# Inputs to the model\nx = torch.randn(2, 2)\n"
            ],
            "g_time": 20.58310627937317
        }
    }
}
{
    "opt": "unary=13",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear1 = torch.nn.Linear(3, 8)\n        self.linear2 = torch.nn.Linear(8, 1)\n \n    def forward(self, x1):\n        v1 = self.linear1(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        v4 = self.linear2(v3)\n        return v4\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(256, 10)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 256)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(8, 32)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 5)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(64, 64)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear1 = torch.nn.Linear(3, 8)\n        self.linear2 = torch.nn.Linear(8, 8)\n \n    def forward(self, x1):\n        v1 = self.linear1(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        v4 = self.linear2(v3)\n        v5 = v4 * v2\n        v6 = v4 + v5\n        return v6\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(64, 128, affine=True)\n        \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the \nx1 = torch.randn(1, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 6)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(5, 1)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 5)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear1 = torch.nn.Linear(3, 8)\n        self.linear2 = torch.nn.Linear(8, 1)\n \n    def forward(self, x1):\n        v1 = self.linear1(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        v4 = self.linear2(v3)\n        return v4\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(256, 10)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 256)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(8, 32)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 5)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(64, 64)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear1 = torch.nn.Linear(3, 8)\n        self.linear2 = torch.nn.Linear(8, 8)\n \n    def forward(self, x1):\n        v1 = self.linear1(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        v4 = self.linear2(v3)\n        v5 = v4 * v2\n        v6 = v4 + v5\n        return v6\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(64, 128, affine=True)\n        \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the \nx1 = torch.randn(1, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 6)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(5, 1)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = v1 * v2\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 5)\n"
            ],
            "g_time": 6.189196825027466
        }
    }
}
{
    "opt": "binary_unary=0",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(16, 16, 7, stride=1, padding=3)\n        self.conv2 = torch.nn.Conv2d(16, 16, 7, stride=1, padding=3)\n        self.conv3 = torch.nn.Conv2d(16, 16, 7, stride=1, padding=3)\n    def forward(self, x):\n        v1 = self.conv1(x)\n        v2 = torch.relu(x)\n        v3 = v1 + v2\n        v4 = torch.relu(v3)\n        v5 = self.conv2(v4)\n        v6 = v5 + v2\n        v7 = torch.relu(v6)\n        v8 = torch.relu(v4)\n        v9 = v8 + v3\n        v10 = torch.relu(v9)\n        v11 = self.conv3(v10)\n        v12 = v11 + v3\n        v13 = torch.relu(v12)\n        return v13\n# Inputs to the model\nx = torch.randn(1, 16, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(16, 16, 1, stride=1)\n        self.maxpool1 = torch.nn.MaxPool2d(32, stride=1)\n    def forward(self, x1, x2):\n        v1 = self.conv1(x1)\n        v2 = self.conv1(x2)\n        v3 = v1 + v2\n        v4 = self.maxpool1(v3)\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 16, 64, 64)\nx2 = torch.randn(1, 16, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 8, 3, stride=1, padding=3)\n        self.conv2 = torch.nn.Conv2d(8, 64, 3, stride=1, padding=3)\n    def forward(self, x):\n        v1 = self.conv1(x)\n        v2 = v1 + v1\n        v3 = self.conv2(v2)\n        v4 = v3 + x\n        return v4\n# Inputs to the model\nx = torch.randn(1, 3, 224, 224)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(16, 16, 7, stride=1, padding=3)\n        self.conv2 = torch.nn.Conv2d(16, 16, 7, stride=1, padding=3)\n        self.conv3 = torch.nn.Conv2d(16, 16, 7, stride=1, padding=3)\n    def forward(self, x, k):\n        v1 = self.conv1(x)\n        v2 = v1 + x\n        v3 = torch.relu(v2)\n        v4 = self.conv2(v3)\n        v5 = v4 + k\n        v6 = torch.relu(v5)\n        v7 = self.conv3(v6)\n        v8 = v7 + k\n        v9 = torch.relu(v8)\n        return v9\n# Inputs to the model\nx = torch.randn(1, 16, 64, 64)\nk = torch.randn(1, 16, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(16, 16, 7, stride=1, padding=3)\n        self.conv2 = torch.nn.Conv2d(16, 16, 7, stride=1, padding=3)\n        self.conv3 = torch.nn.Conv2d(16, 16, 7, stride=1, padding=3)\n    def forward(self, x):\n        v1 = self.conv1(x)\n        v2 = x + x\n        v3 = torch.relu(v2)\n        v4 = self.conv2(v3)\n        v5 = v4 * v1\n        v6 = torch.relu(v5)\n        v7 = self.conv3(v6)\n        v8 = self.conv2(v4)\n        v9 = v8 + v1\n        v10 = torch.relu(v9)\n        return v9\n# Inputs to the model\nx = torch.randn(1, 16, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, a):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(16, 16, 7, stride=1, padding=3)\n        self.conv2 = torch.nn.Conv2d(16, 16, 7, stride=1, padding=3)\n        self.conv3 = torch.nn.Conv2d(16, 16, 7, stride=1, padding=3)\n    def forward(self, x):\n        v1 = self.conv1(x)\n        v2 = v1 + x\n        v3 = torch.relu(v2)\n        v4 = self.conv2(v3)\n        v5 = v4 + x\n        v6 = torch.relu(v5)\n        v7 = self.conv3(v6)\n        v8 = v7 + self.conv1(x)\n        v9 = torch.relu(v8)\n        return v9\n# Inputs to the model\nx = torch.randn(1, 16, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(16, 16, 7, stride=1, padding=3)\n        self.conv2 = torch.nn.Conv2d(16, 16, 7, stride=1, padding=3)\n        self.conv3 = torch.nn.Conv2d(16, 16, 7, stride=1, padding=3)\n    def forward(self, x):\n        v1 = self.conv1(x)\n        v2 = self.conv2(v1)\n        v3 = v1 + x\n        v4 = torch.relu(v3)\n        v5 = self.conv3(v2)\n        v6 = v5 + x\n        v7 = torch.relu(v6)\n        return v7\n# INPUTS TO THE MODEL\nx = torch.randn(1, 16, 64, 64)\n# MODEL END",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(16, 16, 3, stride=1, padding=1)\n        self.conv2 = torch.nn.Conv2d(16, 16, 1, stride=1, padding=0)\n    def forward(self, x):\n        v1 = self.conv1(x)\n        v2 = self.conv2(v1)\n        return v2\n# Inputs to the model\nx = torch.randn(1, 16, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(16, 16, 7, stride=2, padding=3)\n        self.conv2 = torch.nn.Conv2d(16, 16, 3, stride=2, padding=1)\n    def forward(self, x):\n        v1 = self.conv1(x)\n        v2 = self.conv2(x)\n        return v1 + v2\n# Inputs to the model\nx = torch.randn(1, 16, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(16, 16, 7, stride=1, padding=3)\n        self.conv2 = torch.nn.Conv2d(16, 16, 7, stride=1, padding=3)\n        self.conv3 = torch.nn.Conv2d(16, 16, 7, stride=1, padding=3)\n        self.conv4 = torch.nn.Conv2d(16, 16, 7, stride=1, padding=3)\n    def forward(self, x):\n        v1 = self.conv1(x)\n        v2 = v1 + x\n        v3 = self.conv2(x)\n        v4 = v2 + v3\n        v5 = torch.relu(v4)\n        v6 = self.conv3(x)\n        v7 = v6 + x\n        v8 = self.conv4(x)\n        v9 = v7 + v8\n        v10 = torch.relu(v9)\n        return v10\n# Inputs to the model\nx = torch.randn(1, 16, 64, 64)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(16, 16, 7, stride=1, padding=3)\n        self.conv2 = torch.nn.Conv2d(16, 16, 7, stride=1, padding=3)\n        self.conv3 = torch.nn.Conv2d(16, 16, 7, stride=1, padding=3)\n    def forward(self, x):\n        v1 = self.conv1(x)\n        v2 = torch.relu(x)\n        v3 = v1 + v2\n        v4 = torch.relu(v3)\n        v5 = self.conv2(v4)\n        v6 = v5 + v2\n        v7 = torch.relu(v6)\n        v8 = torch.relu(v4)\n        v9 = v8 + v3\n        v10 = torch.relu(v9)\n        v11 = self.conv3(v10)\n        v12 = v11 + v3\n        v13 = torch.relu(v12)\n        return v13\n# Inputs to the model\nx = torch.randn(1, 16, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(16, 16, 1, stride=1)\n        self.maxpool1 = torch.nn.MaxPool2d(32, stride=1)\n    def forward(self, x1, x2):\n        v1 = self.conv1(x1)\n        v2 = self.conv1(x2)\n        v3 = v1 + v2\n        v4 = self.maxpool1(v3)\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 16, 64, 64)\nx2 = torch.randn(1, 16, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 8, 3, stride=1, padding=3)\n        self.conv2 = torch.nn.Conv2d(8, 64, 3, stride=1, padding=3)\n    def forward(self, x):\n        v1 = self.conv1(x)\n        v2 = v1 + v1\n        v3 = self.conv2(v2)\n        v4 = v3 + x\n        return v4\n# Inputs to the model\nx = torch.randn(1, 3, 224, 224)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(16, 16, 7, stride=1, padding=3)\n        self.conv2 = torch.nn.Conv2d(16, 16, 7, stride=1, padding=3)\n        self.conv3 = torch.nn.Conv2d(16, 16, 7, stride=1, padding=3)\n    def forward(self, x, k):\n        v1 = self.conv1(x)\n        v2 = v1 + x\n        v3 = torch.relu(v2)\n        v4 = self.conv2(v3)\n        v5 = v4 + k\n        v6 = torch.relu(v5)\n        v7 = self.conv3(v6)\n        v8 = v7 + k\n        v9 = torch.relu(v8)\n        return v9\n# Inputs to the model\nx = torch.randn(1, 16, 64, 64)\nk = torch.randn(1, 16, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(16, 16, 7, stride=1, padding=3)\n        self.conv2 = torch.nn.Conv2d(16, 16, 7, stride=1, padding=3)\n        self.conv3 = torch.nn.Conv2d(16, 16, 7, stride=1, padding=3)\n    def forward(self, x):\n        v1 = self.conv1(x)\n        v2 = x + x\n        v3 = torch.relu(v2)\n        v4 = self.conv2(v3)\n        v5 = v4 * v1\n        v6 = torch.relu(v5)\n        v7 = self.conv3(v6)\n        v8 = self.conv2(v4)\n        v9 = v8 + v1\n        v10 = torch.relu(v9)\n        return v9\n# Inputs to the model\nx = torch.randn(1, 16, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, a):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(16, 16, 7, stride=1, padding=3)\n        self.conv2 = torch.nn.Conv2d(16, 16, 7, stride=1, padding=3)\n        self.conv3 = torch.nn.Conv2d(16, 16, 7, stride=1, padding=3)\n    def forward(self, x):\n        v1 = self.conv1(x)\n        v2 = v1 + x\n        v3 = torch.relu(v2)\n        v4 = self.conv2(v3)\n        v5 = v4 + x\n        v6 = torch.relu(v5)\n        v7 = self.conv3(v6)\n        v8 = v7 + self.conv1(x)\n        v9 = torch.relu(v8)\n        return v9\n# Inputs to the model\nx = torch.randn(1, 16, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(16, 16, 7, stride=1, padding=3)\n        self.conv2 = torch.nn.Conv2d(16, 16, 7, stride=1, padding=3)\n        self.conv3 = torch.nn.Conv2d(16, 16, 7, stride=1, padding=3)\n    def forward(self, x):\n        v1 = self.conv1(x)\n        v2 = self.conv2(v1)\n        v3 = v1 + x\n        v4 = torch.relu(v3)\n        v5 = self.conv3(v2)\n        v6 = v5 + x\n        v7 = torch.relu(v6)\n        return v7\n# INPUTS TO THE MODEL\nx = torch.randn(1, 16, 64, 64)\n# MODEL END",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(16, 16, 3, stride=1, padding=1)\n        self.conv2 = torch.nn.Conv2d(16, 16, 1, stride=1, padding=0)\n    def forward(self, x):\n        v1 = self.conv1(x)\n        v2 = self.conv2(v1)\n        return v2\n# Inputs to the model\nx = torch.randn(1, 16, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(16, 16, 7, stride=2, padding=3)\n        self.conv2 = torch.nn.Conv2d(16, 16, 3, stride=2, padding=1)\n    def forward(self, x):\n        v1 = self.conv1(x)\n        v2 = self.conv2(x)\n        return v1 + v2\n# Inputs to the model\nx = torch.randn(1, 16, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(16, 16, 7, stride=1, padding=3)\n        self.conv2 = torch.nn.Conv2d(16, 16, 7, stride=1, padding=3)\n        self.conv3 = torch.nn.Conv2d(16, 16, 7, stride=1, padding=3)\n        self.conv4 = torch.nn.Conv2d(16, 16, 7, stride=1, padding=3)\n    def forward(self, x):\n        v1 = self.conv1(x)\n        v2 = v1 + x\n        v3 = self.conv2(x)\n        v4 = v2 + v3\n        v5 = torch.relu(v4)\n        v6 = self.conv3(x)\n        v7 = v6 + x\n        v8 = self.conv4(x)\n        v9 = v7 + v8\n        v10 = torch.relu(v9)\n        return v10\n# Inputs to the model\nx = torch.randn(1, 16, 64, 64)\n"
            ],
            "g_time": 11.314014196395874
        }
    }
}
{
    "opt": "unary=5",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(47, 32, 2, stride=2, padding=1)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 47, 23, 23)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(9, 8, 3, stride=2, padding=(1, 0), groups=2, dilation=(2, 2))\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 9, 11, 23)\n",
                "\ntry:\n    x1 = torch.randn(1)\n    class Model(torch.nn.Module):\n        def __init__(self):\n            super().__init__()\n            self.conv_transpose = torch.nn.ConvTranspose1d(1, 1, 2, stride=2, padding=0)\n        def forward(self, _1):\n            _10 = self.conv_transpose(_1)\n            _20 = _10 * 0.5\n            _30 = _10 * 0.7071067811865476\n            _40 = torch.erf(_30)\n            _50 = _40 + 1\n            _60 = _20 * _50\n            return _60\n    y = Model()(x1)\n    print(y)\nexcept TypeError:\n    pass\n\ntry:\n    x1 = torch.randn(1)\n    class Model(torch.nn.Module):\n        def __init__(self):\n            super().__init__()\n            self.conv_transpose = torch.nn.ConvTranspose1d(1, 1, 4, stride=4, padding=3)\n        def forward(self, _1):\n            _10 = self.conv_transpose(_1)\n            _20 = _10 * 0.5\n            _30 = _10 * 0.7071067811865476\n            _40 = torch.erf(_30)\n            _50 = _40 + 1\n            _60 = _20 * _50\n            return _60\n    y = Model()(x1)\n    print(y)\nexcept IndexError:\n    pass\n\ntry:\n    x1 = torch.randn(1)\n    class Model(torch.nn.Module):\n        def __init__(self):\n            super().__init__()\n            self.conv_transpose = torch.nn.ConvTranspose1d(11, 11, 17, stride=17, padding=0)\n        def forward(self, _1):\n            _10 = self.conv_transpose(_1)\n            _20 = _10 * 0.5\n            _30 = _10 * 0.7071067811865476\n            _40 = torch.erf(_30)\n            _50 = _40 + 1\n            _60 = _20 * _50\n            return _60\n    y = Model()(x1)\n    print(y)\nexcept ValueError:\n    pass\n\nx1 = torch.randn(1)\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose1d(1, 1, 5, stride=5, padding=5)\n    def forward(self, _1):\n        _10 = self.conv_transpose(_1)\n        _20 = _10 * 0.5\n        _30 = _10 * 0.7071067811865476\n        _40 = torch.erf(_30)\n        _50 = _40 + 1\n        _60 = _20 * _50\n        return _60\ny = Model()(x1)\nprint(y)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(3, 1, (6, 9), stride=(3, 2), padding=(2, 1))\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 3, 55, 54)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(64, 32, (3, 3), stride=(3, 3), padding=(1, 1))\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 64, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose2d = torch.nn.ConvTranspose2d(8, 2, (4, 9), stride=(4, 6), padding=(1, 2))\n    def forward(self, x1):\n        v1 = self.conv_transpose2d(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 8, 29, 29)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(54, 34, 11, stride=2, padding=3)\n        self.max_pool = torch.nn.MaxPool2d((7, 6), stride=2, padding=3)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        v7 = self.max_pool(v6)\n        return v7\n# Inputs to the model\nx1 = torch.randn(1, 54, 77, 77)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(75, 60, (1, 20), stride=(1, 9), padding=4)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 75, 47, 89)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(3, 8, 2, stride=2, padding=1)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 3, 60, 60)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(9, 25, 5, stride=2, padding=0, dilation=2)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 9, 23, 23)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(47, 32, 2, stride=2, padding=1)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 47, 23, 23)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(9, 8, 3, stride=2, padding=(1, 0), groups=2, dilation=(2, 2))\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 9, 11, 23)\n",
                "\ntry:\n    x1 = torch.randn(1)\n    class Model(torch.nn.Module):\n        def __init__(self):\n            super().__init__()\n            self.conv_transpose = torch.nn.ConvTranspose1d(1, 1, 2, stride=2, padding=0)\n        def forward(self, _1):\n            _10 = self.conv_transpose(_1)\n            _20 = _10 * 0.5\n            _30 = _10 * 0.7071067811865476\n            _40 = torch.erf(_30)\n            _50 = _40 + 1\n            _60 = _20 * _50\n            return _60\n    y = Model()(x1)\n    print(y)\nexcept TypeError:\n    pass\n\ntry:\n    x1 = torch.randn(1)\n    class Model(torch.nn.Module):\n        def __init__(self):\n            super().__init__()\n            self.conv_transpose = torch.nn.ConvTranspose1d(1, 1, 4, stride=4, padding=3)\n        def forward(self, _1):\n            _10 = self.conv_transpose(_1)\n            _20 = _10 * 0.5\n            _30 = _10 * 0.7071067811865476\n            _40 = torch.erf(_30)\n            _50 = _40 + 1\n            _60 = _20 * _50\n            return _60\n    y = Model()(x1)\n    print(y)\nexcept IndexError:\n    pass\n\ntry:\n    x1 = torch.randn(1)\n    class Model(torch.nn.Module):\n        def __init__(self):\n            super().__init__()\n            self.conv_transpose = torch.nn.ConvTranspose1d(11, 11, 17, stride=17, padding=0)\n        def forward(self, _1):\n            _10 = self.conv_transpose(_1)\n            _20 = _10 * 0.5\n            _30 = _10 * 0.7071067811865476\n            _40 = torch.erf(_30)\n            _50 = _40 + 1\n            _60 = _20 * _50\n            return _60\n    y = Model()(x1)\n    print(y)\nexcept ValueError:\n    pass\n\nx1 = torch.randn(1)\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose1d(1, 1, 5, stride=5, padding=5)\n    def forward(self, _1):\n        _10 = self.conv_transpose(_1)\n        _20 = _10 * 0.5\n        _30 = _10 * 0.7071067811865476\n        _40 = torch.erf(_30)\n        _50 = _40 + 1\n        _60 = _20 * _50\n        return _60\ny = Model()(x1)\nprint(y)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(3, 1, (6, 9), stride=(3, 2), padding=(2, 1))\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 3, 55, 54)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(64, 32, (3, 3), stride=(3, 3), padding=(1, 1))\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 64, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose2d = torch.nn.ConvTranspose2d(8, 2, (4, 9), stride=(4, 6), padding=(1, 2))\n    def forward(self, x1):\n        v1 = self.conv_transpose2d(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 8, 29, 29)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(54, 34, 11, stride=2, padding=3)\n        self.max_pool = torch.nn.MaxPool2d((7, 6), stride=2, padding=3)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        v7 = self.max_pool(v6)\n        return v7\n# Inputs to the model\nx1 = torch.randn(1, 54, 77, 77)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(75, 60, (1, 20), stride=(1, 9), padding=4)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 75, 47, 89)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(3, 8, 2, stride=2, padding=1)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 3, 60, 60)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_transpose = torch.nn.ConvTranspose2d(9, 25, 5, stride=2, padding=0, dilation=2)\n    def forward(self, x1):\n        v1 = self.conv_transpose(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 9, 23, 23)\n"
            ],
            "g_time": 23.649532079696655
        }
    }
}
{
    "opt": "cat_addmm",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layers = nn.Linear(4, 8)\n        self.stack = torch.stack\n    def forward(self, x):\n        x = torch.cat((x, x, x, x), dim=1)\n        x = self.layers(x)\n        x = self.stack((x, x), dim=1)\n        x = torch.flatten(x, start_dim=1)\n        return x\n# Inputs to the model\nx = torch.randn(4, 4)\n",
                "\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layers = nn.Linear(1, 1)\n    def forward(self, x):\n        x = self.layers(x)\n        return torch.squeeze(x)\n# Inputs to the model\nx = torch.randn(3, 2, 1)\n",
                "\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layers = nn.Linear(4, 5)\n        self.permute = torch.transpose\n        self.stack = torch.stack\n    def forward(self, x):\n        x = self.layers(x)\n        x = self.permute(x, 0, 1)\n        x = self.permute(x, 1, 2)\n        x = self.stack((x, x), dim=1)\n        x = x.transpose(1, 2)\n        x = x.squeeze()\n        return x\n# Inputs to the model\nx = torch.randn(2, 2, 4)\n",
                "\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layers = nn.Linear(4, 2)\n    def forward(self, x):\n        x = self.layers(x)\n        x = torch.stack((x, x), dim=1)\n        x = x.transpose(1, 2)\n        x = torch.flatten(x, start_dim=1)\n        return x\n# Inputs to the model\nx = torch.randn(4, 4)\n",
                "\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layers = nn.Linear(7, 6)\n        self.stack = torch.stack\n    def forward(self, x):\n        x = self.layers(x)\n        x = self.stack((x, x), dim=1)\n        x = x.flatten(start_dim=1)\n        if bool(random.getrandbits(1)):\n            x = x.T\n        return x\n# Inputs to the model\nx = torch.randn(7, 7)\n",
                "\nclass Model(nn.Module):\n    def forward(self, x):\n        x = torch.squeeze(x, dim=1)\n        x = torch.tanh(x)\n        return x\n# Inputs to the model\nx = torch.randn(2, 2)\n",
                "\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layers = nn.Linear(3, 4)\n    def forward(self, x):\n        x = torch.rand_like(x) * self.layers(x)\n        return torch.cat([x, x, x, x], dim=1)\n# Inputs to the model\nx = torch.randn(2, 3)\n",
                "\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layers = nn.Linear(3, 5)\n        self.dropout = nn.Dropout()\n        self.avg_pool1d = nn.AvgPool2d((3, 1))\n        self.mean = torch.mean\n    def forward(self, x):\n        x = self.layers(x)\n        x = self.dropout(x)\n        x = x.unsqueeze(3)\n        x = self.avg_pool1d(x)\n        x = x.squeeze(3)\n        x = self.mean(x, dim=1)\n        return x\n# Inputs to the model\nx = torch.randn(2, 3)\n",
                "\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x, y, z):\n        x = x * y * z\n        return x\n# Inputs to the model\nx = torch.ones(2, 2)\ny = torch.ones(2, 2)\nz = torch.ones(2, 2)\n",
                "\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layers = nn.Linear(8, 4)\n        self.stack = torch.stack\n    def forward(self, x):\n        x = self.layers(x)\n        x = self.stack((x, x), dim=1)\n        x = torch.flatten(x, start_dim=1)\n        return x\n# Inputs to the model\nx = torch.randn(2, 8)\n"
            ],
            "code": [
                "\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layers = nn.Linear(4, 8)\n        self.stack = torch.stack\n    def forward(self, x):\n        x = torch.cat((x, x, x, x), dim=1)\n        x = self.layers(x)\n        x = self.stack((x, x), dim=1)\n        x = torch.flatten(x, start_dim=1)\n        return x\n# Inputs to the model\nx = torch.randn(4, 4)\n",
                "\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layers = nn.Linear(1, 1)\n    def forward(self, x):\n        x = self.layers(x)\n        return torch.squeeze(x)\n# Inputs to the model\nx = torch.randn(3, 2, 1)\n",
                "\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layers = nn.Linear(4, 5)\n        self.permute = torch.transpose\n        self.stack = torch.stack\n    def forward(self, x):\n        x = self.layers(x)\n        x = self.permute(x, 0, 1)\n        x = self.permute(x, 1, 2)\n        x = self.stack((x, x), dim=1)\n        x = x.transpose(1, 2)\n        x = x.squeeze()\n        return x\n# Inputs to the model\nx = torch.randn(2, 2, 4)\n",
                "\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layers = nn.Linear(4, 2)\n    def forward(self, x):\n        x = self.layers(x)\n        x = torch.stack((x, x), dim=1)\n        x = x.transpose(1, 2)\n        x = torch.flatten(x, start_dim=1)\n        return x\n# Inputs to the model\nx = torch.randn(4, 4)\n",
                "\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layers = nn.Linear(7, 6)\n        self.stack = torch.stack\n    def forward(self, x):\n        x = self.layers(x)\n        x = self.stack((x, x), dim=1)\n        x = x.flatten(start_dim=1)\n        if bool(random.getrandbits(1)):\n            x = x.T\n        return x\n# Inputs to the model\nx = torch.randn(7, 7)\n",
                "\nclass Model(nn.Module):\n    def forward(self, x):\n        x = torch.squeeze(x, dim=1)\n        x = torch.tanh(x)\n        return x\n# Inputs to the model\nx = torch.randn(2, 2)\n",
                "\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layers = nn.Linear(3, 4)\n    def forward(self, x):\n        x = torch.rand_like(x) * self.layers(x)\n        return torch.cat([x, x, x, x], dim=1)\n# Inputs to the model\nx = torch.randn(2, 3)\n",
                "\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layers = nn.Linear(3, 5)\n        self.dropout = nn.Dropout()\n        self.avg_pool1d = nn.AvgPool2d((3, 1))\n        self.mean = torch.mean\n    def forward(self, x):\n        x = self.layers(x)\n        x = self.dropout(x)\n        x = x.unsqueeze(3)\n        x = self.avg_pool1d(x)\n        x = x.squeeze(3)\n        x = self.mean(x, dim=1)\n        return x\n# Inputs to the model\nx = torch.randn(2, 3)\n",
                "\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x, y, z):\n        x = x * y * z\n        return x\n# Inputs to the model\nx = torch.ones(2, 2)\ny = torch.ones(2, 2)\nz = torch.ones(2, 2)\n",
                "\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layers = nn.Linear(8, 4)\n        self.stack = torch.stack\n    def forward(self, x):\n        x = self.layers(x)\n        x = self.stack((x, x), dim=1)\n        x = torch.flatten(x, start_dim=1)\n        return x\n# Inputs to the model\nx = torch.randn(2, 8)\n"
            ],
            "g_time": 5.506932020187378
        }
    }
}
{
    "opt": "binary=8",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 32, 1, stride=1, padding=1)\n        self.conv2 = torch.nn.Conv2d(3, 32, 1, stride=1, padding=1)\n    def forward(self, x1, x2):\n        l1 = self.conv1(x1)\n        m1 = self.conv2(x2)\n        h1 = torch.add(l1, m1)\n        l2 = self.conv1(x1.sub(torch.add(x1, m1)))\n        m2 = self.conv2(x2.mul(x1))\n        h2 = torch.add(l2, m2)\n        l3 = self.conv1(x2)\n        m3 = self.conv2(x1)\n        h3 = torch.add(l3, m3)\n        l4 = self.conv1(x2.sub(torch.add(x2, m3)))\n        m4 = self.conv2(x1.mul(x2))\n        h4 = torch.add(l4, m4)\n        h5 = h1 + h2\n        h6 = h3 + h4\n        return h2 + h5\n# Inputs to the model\nx1 = torch.randn(2, 3, 64, 64)\nx2 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(4, 32, 1, stride=1, padding=1)\n        self.conv2 = torch.nn.Conv2d(8, 16, 1, stride=1, padding=1)\n    def forward(self, x1, x2):\n        out_x1 = self.conv1(x1)\n        out_x2 = self.conv1(x2)\n        out = torch.cat([out_x1, out_x2], dim=1)\n        out = self.conv2(out)\n        return out\n# Inputs to the model\nx1 = torch.randn(1, 4, 32, 32)\nx2 = torch.randn(1, 8, 32, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n        self.conv2 = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n        self.conv3 = torch.nn.Conv2d(3, 16, 3, stride=1, padding=1)\n    def forward(self, x1, x2, x3):\n        v1 = self.conv1(x1)\n        v2 = self.conv2(x2)\n        v3 = self.conv3(x3)\n        v4 = v2 + v1\n        v5 = v4.add(v1)\n        v6 = v4 + v1\n        v7 = v3 + v4\n        v8 = v3 + x3\n        v9 = v5 + v7\n        v10 = v6 + x1\n        return v9\n# Inputs to the model\nx1 = torch.randn(1, 3, 16, 16)\nx2 = torch.randn(1, 3, 64, 64)\nx3 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n        self.conv2 = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n        self.bn1 = torch.nn.BatchNorm2d(8)\n        self.bn2 = torch.nn.BatchNorm2d(8)\n    def forward(self, x1, x2):\n        v1 = self.conv1(torch.add(x1, x2))\n        v2 = self.conv1(torch.add(x1, x2))\n        v3 = v1 + v2\n        v4 = self.bn1(v3)\n        v5 = v4 + v3\n        v6 = v5 + v2.add(v1)\n        v7 = self.bn1(v5)\n        v8 = v5 + v4\n        v9 = v6 + v7\n        return v8 + v9\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\nx2 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n        self.conv2 = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n        self.conv3 = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n        self.conv3_1 = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv1(x1[0])\n        v2 = self.conv1(x1[1])\n        v3 = v1 + v2\n        v4 = self.conv3_1(x1[2])\n        v5 = v1 + v4\n        v6 = self.conv2(x1[0])\n        v7 = self.conv2(x1[1])\n        v8 = v6 + v7\n        v9 = v5 + v8\n        return v9\n# Inputs to the model\nx1 = [(torch.randn(1, 3, 32, 32)),\n      (torch.randn(1, 3, 32, 32)),  \n      (torch.randn(1, 3, 32, 32))]\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n        self.conv2 = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n        self.conv3 = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n        self.conv4 = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n        self.conv5 = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n    def forward(self, x1, x2):\n        x1 = x1.add(x2)\n        v1 = self.conv1(x1)\n        v2 = self.conv2(x1)\n        v3 = self.conv3(x1)\n        v4 = self.conv4(x1)\n        v5 = self.conv5(x1)\n        v6 = v1 + v2\n        v7 = v2 + v3\n        v8 = v3 + v4\n        v9 = v4 + v5\n        return v6 + v7 + v8 + v9 + v5\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\nx2 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 1, 33, stride=2, padding=2)\n        self.conv2 = torch.nn.Conv2d(3, 1, 31, stride=1, padding=1)\n        self.conv3 = torch.nn.Conv2d(3, 1, 17, stride=3, padding=1)\n        self.conv4 = torch.nn.Conv2d(3, 1, 31, stride=1, padding=1)\n        self.conv5 = torch.nn.Conv2d(3, 1, 27, stride=1, padding=1)\n        self.conv6 = torch.nn.Conv2d(3, 1, 3, stride=1, padding=1)\n        self.conv7 = torch.nn.Conv2d(3, 1, 39, stride=1, padding=1)\n    def forward(self, x1):\n        x2 = self.conv1(x1)\n        x3 = self.conv1(x1)\n        x4 = (x2 + x3).add(x1)\n        x5 = self.conv2(x1)\n        x6 = self.conv3(x1)\n        x7 = (x6 - x5).add(x4)\n        x8 = self.conv4(x1)\n        x9 = self.conv5(x1)\n        x10 = (x8 - x9).add(x7)\n        x11 = self.conv6(x1)\n        x12 = self.conv7(x1)\n        x13 = (x12 - x11).add(x10)\n        return x13\n# Inputs to the model\nx1 = torch.randn(1, 3, 40, 20)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 16, 5, stride=1, padding=2)\n        self.relu1 = torch.nn.ReLU()\n    def forward(self, x1, x2):\n        v1 = self.conv1(torch.add(x1, x2))\n        v2 = v1 + v1\n        v3 = self.relu1(v2)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\nx2 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(7, 12, 1, stride=1, padding=1)\n        self.bn1 = torch.nn.BatchNorm2d(12)\n        self.conv2 = torch.nn.Conv2d(12, 20, 1, stride=1, padding=1)\n        self.conv3 = torch.nn.Conv2d(12, 20, 1, stride=1, padding=1)\n        self.bn2 = torch.nn.BatchNorm2d(20)\n        self.conv4 = torch.nn.Conv2d(20, 28, 1, stride=1, padding=1)\n        self.conv5 = torch.nn.Conv2d(20, 28, 1, stride=1, padding=1)\n        self.conv6 = torch.nn.Conv2d(20, 28, 1, stride=1, padding=1)\n        self.bn3 = torch.nn.BatchNorm2d(28)\n        self.conv7 = torch.nn.Conv2d(28, 16, 1, stride=1, padding=1)\n        self.conv8 = torch.nn.Conv2d(28, 16, 1, stride=1, padding=1)\n        self.conv9 = torch.nn.Conv2d(28, 16, 1, stride=1, padding=1)\n        self.bn4 = torch.nn.BatchNorm2d(16)\n        self.avgpool = torch.nn.AdaptiveAvgPool2d((1, 1))\n        self.flatten = torch.nn.Flatten()\n        self.fc = torch.nn.Linear(16, 10)\n    def forward(self, input):\n        v1 = self.conv1(input)\n        v2 = v1.add(v1.mean([2, 3], True))\n        v3 = self.bn1(v2)\n        v4 = self.conv2(v3)\n        v5 = self.conv3(v3)\n        v6 = v4.add_(v5.mean([2, 3], True))\n        v7 = self.bn2(v6)\n        v8 = self.conv4(v7)\n        v9 = self.conv5(v7)\n        v10 = self.conv6(v7)\n        v11 = v8.add_(v10.mean([2, 3], True))\n        v12 = self.bn3(v11)\n        v13 = self.conv7(v12)\n        v14 = self.conv8(v12)\n        v15 = self.conv9(v12)\n        v16 = v13.add_(v15.mean([2, 3], True))\n        v17 = self.bn4(v16)\n        v18 = self.avgpool(v17).flatten(1)\n        v19 = self.fc(v18)\n        return v19\n# Inputs to the model\ninput = torch.randn(1, 7, 32, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n        self.conv2 = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1, groups=2)\n    def forward(self, x1, x2):\n        v1 = self.conv1(x1)\n        v2 = self.conv2(x2)\n        v3 = v1 + v2\n        v4 = self.conv1(x1)\n        v5 = self.conv2(x2)\n        v6 = v4 + v5\n        return v3 + v6\n# Inputs to the model\nx1 = torch.randn(1, 3, 32, 32)\nx2 = torch.randn(1, 3, 32, 32)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 32, 1, stride=1, padding=1)\n        self.conv2 = torch.nn.Conv2d(3, 32, 1, stride=1, padding=1)\n    def forward(self, x1, x2):\n        l1 = self.conv1(x1)\n        m1 = self.conv2(x2)\n        h1 = torch.add(l1, m1)\n        l2 = self.conv1(x1.sub(torch.add(x1, m1)))\n        m2 = self.conv2(x2.mul(x1))\n        h2 = torch.add(l2, m2)\n        l3 = self.conv1(x2)\n        m3 = self.conv2(x1)\n        h3 = torch.add(l3, m3)\n        l4 = self.conv1(x2.sub(torch.add(x2, m3)))\n        m4 = self.conv2(x1.mul(x2))\n        h4 = torch.add(l4, m4)\n        h5 = h1 + h2\n        h6 = h3 + h4\n        return h2 + h5\n# Inputs to the model\nx1 = torch.randn(2, 3, 64, 64)\nx2 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(4, 32, 1, stride=1, padding=1)\n        self.conv2 = torch.nn.Conv2d(8, 16, 1, stride=1, padding=1)\n    def forward(self, x1, x2):\n        out_x1 = self.conv1(x1)\n        out_x2 = self.conv1(x2)\n        out = torch.cat([out_x1, out_x2], dim=1)\n        out = self.conv2(out)\n        return out\n# Inputs to the model\nx1 = torch.randn(1, 4, 32, 32)\nx2 = torch.randn(1, 8, 32, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n        self.conv2 = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n        self.conv3 = torch.nn.Conv2d(3, 16, 3, stride=1, padding=1)\n    def forward(self, x1, x2, x3):\n        v1 = self.conv1(x1)\n        v2 = self.conv2(x2)\n        v3 = self.conv3(x3)\n        v4 = v2 + v1\n        v5 = v4.add(v1)\n        v6 = v4 + v1\n        v7 = v3 + v4\n        v8 = v3 + x3\n        v9 = v5 + v7\n        v10 = v6 + x1\n        return v9\n# Inputs to the model\nx1 = torch.randn(1, 3, 16, 16)\nx2 = torch.randn(1, 3, 64, 64)\nx3 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n        self.conv2 = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n        self.bn1 = torch.nn.BatchNorm2d(8)\n        self.bn2 = torch.nn.BatchNorm2d(8)\n    def forward(self, x1, x2):\n        v1 = self.conv1(torch.add(x1, x2))\n        v2 = self.conv1(torch.add(x1, x2))\n        v3 = v1 + v2\n        v4 = self.bn1(v3)\n        v5 = v4 + v3\n        v6 = v5 + v2.add(v1)\n        v7 = self.bn1(v5)\n        v8 = v5 + v4\n        v9 = v6 + v7\n        return v8 + v9\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\nx2 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n        self.conv2 = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n        self.conv3 = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n        self.conv3_1 = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv1(x1[0])\n        v2 = self.conv1(x1[1])\n        v3 = v1 + v2\n        v4 = self.conv3_1(x1[2])\n        v5 = v1 + v4\n        v6 = self.conv2(x1[0])\n        v7 = self.conv2(x1[1])\n        v8 = v6 + v7\n        v9 = v5 + v8\n        return v9\n# Inputs to the model\nx1 = [(torch.randn(1, 3, 32, 32)),\n      (torch.randn(1, 3, 32, 32)),  \n      (torch.randn(1, 3, 32, 32))]\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n        self.conv2 = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n        self.conv3 = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n        self.conv4 = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n        self.conv5 = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n    def forward(self, x1, x2):\n        x1 = x1.add(x2)\n        v1 = self.conv1(x1)\n        v2 = self.conv2(x1)\n        v3 = self.conv3(x1)\n        v4 = self.conv4(x1)\n        v5 = self.conv5(x1)\n        v6 = v1 + v2\n        v7 = v2 + v3\n        v8 = v3 + v4\n        v9 = v4 + v5\n        return v6 + v7 + v8 + v9 + v5\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\nx2 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 1, 33, stride=2, padding=2)\n        self.conv2 = torch.nn.Conv2d(3, 1, 31, stride=1, padding=1)\n        self.conv3 = torch.nn.Conv2d(3, 1, 17, stride=3, padding=1)\n        self.conv4 = torch.nn.Conv2d(3, 1, 31, stride=1, padding=1)\n        self.conv5 = torch.nn.Conv2d(3, 1, 27, stride=1, padding=1)\n        self.conv6 = torch.nn.Conv2d(3, 1, 3, stride=1, padding=1)\n        self.conv7 = torch.nn.Conv2d(3, 1, 39, stride=1, padding=1)\n    def forward(self, x1):\n        x2 = self.conv1(x1)\n        x3 = self.conv1(x1)\n        x4 = (x2 + x3).add(x1)\n        x5 = self.conv2(x1)\n        x6 = self.conv3(x1)\n        x7 = (x6 - x5).add(x4)\n        x8 = self.conv4(x1)\n        x9 = self.conv5(x1)\n        x10 = (x8 - x9).add(x7)\n        x11 = self.conv6(x1)\n        x12 = self.conv7(x1)\n        x13 = (x12 - x11).add(x10)\n        return x13\n# Inputs to the model\nx1 = torch.randn(1, 3, 40, 20)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 16, 5, stride=1, padding=2)\n        self.relu1 = torch.nn.ReLU()\n    def forward(self, x1, x2):\n        v1 = self.conv1(torch.add(x1, x2))\n        v2 = v1 + v1\n        v3 = self.relu1(v2)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\nx2 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(7, 12, 1, stride=1, padding=1)\n        self.bn1 = torch.nn.BatchNorm2d(12)\n        self.conv2 = torch.nn.Conv2d(12, 20, 1, stride=1, padding=1)\n        self.conv3 = torch.nn.Conv2d(12, 20, 1, stride=1, padding=1)\n        self.bn2 = torch.nn.BatchNorm2d(20)\n        self.conv4 = torch.nn.Conv2d(20, 28, 1, stride=1, padding=1)\n        self.conv5 = torch.nn.Conv2d(20, 28, 1, stride=1, padding=1)\n        self.conv6 = torch.nn.Conv2d(20, 28, 1, stride=1, padding=1)\n        self.bn3 = torch.nn.BatchNorm2d(28)\n        self.conv7 = torch.nn.Conv2d(28, 16, 1, stride=1, padding=1)\n        self.conv8 = torch.nn.Conv2d(28, 16, 1, stride=1, padding=1)\n        self.conv9 = torch.nn.Conv2d(28, 16, 1, stride=1, padding=1)\n        self.bn4 = torch.nn.BatchNorm2d(16)\n        self.avgpool = torch.nn.AdaptiveAvgPool2d((1, 1))\n        self.flatten = torch.nn.Flatten()\n        self.fc = torch.nn.Linear(16, 10)\n    def forward(self, input):\n        v1 = self.conv1(input)\n        v2 = v1.add(v1.mean([2, 3], True))\n        v3 = self.bn1(v2)\n        v4 = self.conv2(v3)\n        v5 = self.conv3(v3)\n        v6 = v4.add_(v5.mean([2, 3], True))\n        v7 = self.bn2(v6)\n        v8 = self.conv4(v7)\n        v9 = self.conv5(v7)\n        v10 = self.conv6(v7)\n        v11 = v8.add_(v10.mean([2, 3], True))\n        v12 = self.bn3(v11)\n        v13 = self.conv7(v12)\n        v14 = self.conv8(v12)\n        v15 = self.conv9(v12)\n        v16 = v13.add_(v15.mean([2, 3], True))\n        v17 = self.bn4(v16)\n        v18 = self.avgpool(v17).flatten(1)\n        v19 = self.fc(v18)\n        return v19\n# Inputs to the model\ninput = torch.randn(1, 7, 32, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)\n        self.conv2 = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1, groups=2)\n    def forward(self, x1, x2):\n        v1 = self.conv1(x1)\n        v2 = self.conv2(x2)\n        v3 = v1 + v2\n        v4 = self.conv1(x1)\n        v5 = self.conv2(x2)\n        v6 = v4 + v5\n        return v3 + v6\n# Inputs to the model\nx1 = torch.randn(1, 3, 32, 32)\nx2 = torch.randn(1, 3, 32, 32)\n"
            ],
            "g_time": 24.351128816604614
        }
    }
}
{
    "opt": "sfdp=4",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, qK, v, mask):\n        q = torch.sigmoid(qk @ v.transpose(-2, -1) / math.sqrt(qk.size(-1)) + mask)\n        attn_weight = torch.softmax(q, dim=-1)\n        output = attn_weight @ v\n        return output\n# Inputs to the model\nqK2 = torch.randn(1, 64, 56, 56)\nv = torch.randn(1, 64, 56, 56)\nmask = (torch.rand(1, 56, 56) > 0.7).fill_(-1000000000.0)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, Q34, K1, V0, mask):\n        qK = Q34 @ K1.transpose(-2, -1) / math.sqrt(Q34.size(-1))\n        qK = qK + mask\n        attn_weight = torch.softmax(qK, dim=-1)\n        output = attn_weight @ V0\n        return output\n# Inputs to the model\nQK = torch.randn(1, 64, 56, 56)\nKV1 = torch.randn(1, 64, 56, 56)\nVlue300 = torch.randn(1, 64, 56, 56)\nmask = (torch.rand(1, 56, 56) > 0.7).fill_(-1000000000.0)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, W09, M8, v1, mask):\n        qk = W09 @ M8.transpose(-2, -1) / math.sqrt(W09.size(-1))\n        qk = qk + mask\n        attn_weight = torch.softmax(qk, dim=-1)\n        output = attn_weight @ v1\n        return output\n# Inputs to the model\nW6 = torch.randn(1, 64, 56, 56)\nM7 = torch.randn(1, 56, 56)\nV2 = torch.randn(1, 56, 56, 56)\nmask = (torch.rand(1, 56, 56) > 0.7).fill_(-1000000000.0)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, qk, v0, mask):\n        qK = qk @ v0.transpose(-2, -1) / math.sqrt(qk.size(-1))\n        qK = qK + mask\n        attn_weight = torch.softmax(qK, dim=-1)\n        output = qK @ v0\n        return output\n# Inputs to the model\nqk0 = torch.randn(1, 64, 192, 64)\nv3 = torch.randn(1, 1, 1, 1)\nmask = (torch.rand(1, 192, 64) > 0.7).fill_(-1000000000.0)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, qK, v3, mask):\n        qK_ = qK @ v3.transpose(-2, -1) / math.sqrt(qK.size(-1))\n        qK_ = qK_ + mask\n        attn_weight = torch.softmax(qK_, dim=-1)\n        output = attn_weight @ v3\n        return output\n# Inputs to the model\nqk1 = torch.randn(1, 64, 56, 56)\nv3 = torch.randn(1, 64, 56, 56)\nmask = (torch.rand(1, 56, 56) > 0.7).fill_(-1000000000.0)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, W22, K3, v3, mask):\n        qk = W22 @ K3.transpose(-2, -1) / math.sqrt(W22.size(-1))\n        qk = qk + mask\n        attn_weight = torch.softmax(qk, dim=-1)\n        output = attn_weight @ v3\n        return output\n# Inputs\nW22 = torch.randn(1, 64, 56, 56)\nK = torch.randn(1, 64, 56, 56)\nV3 = torch.randn(1, 64, 56, 56)\nmask = (torch.rand(1, 56, 56) > 0.7).fill_(-1000000000.0)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, W13, k4, v1, mask):\n        _qk = W13 @ k4.transpose(-2, -1) / math.sqrt(W13.size(-1))\n        qk = _qk + mask\n        attn_weight = torch.softmax(qk, dim=-1)\n        output = attn_weight @ v1\n        return output\n# Inputs to the model\nQ = torch.randn(1, 64, 56, 56)\nK = torch.randn(1, 64, 56, 56)\nV = torch.randn(1, 64, 56, 56)\nmask = (torch.rand(1, 56, 56) > 0.7).fill_(-1000000000.0)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, Q2, k4, V5, mask):\n        qk = Q2 @ k4.transpose(-2, -1) / math.sqrt(Q2.size(-1))\n        qk = qk + mask\n        K = torch.softmax(qk, dim=-1)\n        output = K @ V5\n        return output\n# Inputs to the model\nQ2 = torch.randn(1, 64, 56, 56)\nk4 = torch.randn(1, 64, 56, 56)\nV5 = torch.randn(1, 64, 56, 56)\nmask = (torch.rand(1, 56, 56) > 0.7).fill_(-1000000000.0)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, qk6, V3, mask):\n        q_K = qk6 @ V3.transpose(-2, -1) / math.sqrt(qk6.size(-1))\n        q_K = q_K + mask\n        attn_weight = torch.softmax(q_K, dim=-1)\n        output = attn_weight @ V3\n        return output\n# Inputs to the model\nQ3 = torch.randn(1, 64, 56, 56)\nqk5 = torch.randn(1, 64, 56, 56)\nmask = (torch.rand(1, 56, 56) > 0.7).fill_(-1000000000.0)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, query3, key, value1, mask,):\n        query_key = query3 @ key.transpose(-2, -1) / math.sqrt(query3.size(-1))\n        query_key = query_key + mask\n        attn_weight = torch.softmax(query_key, dim=-1)\n        output = attn_weight @ value1\n        return output\n# Inputs to the model\nQ13 = torch.randn(1, 64, 56, 56)\nK = torch.randn(1, 64, 56, 56)\nV = torch.randn(1, 64, 56, 56)\nmask = (torch.rand(1, 56, 56) > 0.7).fill_(-1000000000.0)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, qK, v, mask):\n        q = torch.sigmoid(qk @ v.transpose(-2, -1) / math.sqrt(qk.size(-1)) + mask)\n        attn_weight = torch.softmax(q, dim=-1)\n        output = attn_weight @ v\n        return output\n# Inputs to the model\nqK2 = torch.randn(1, 64, 56, 56)\nv = torch.randn(1, 64, 56, 56)\nmask = (torch.rand(1, 56, 56) > 0.7).fill_(-1000000000.0)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, Q34, K1, V0, mask):\n        qK = Q34 @ K1.transpose(-2, -1) / math.sqrt(Q34.size(-1))\n        qK = qK + mask\n        attn_weight = torch.softmax(qK, dim=-1)\n        output = attn_weight @ V0\n        return output\n# Inputs to the model\nQK = torch.randn(1, 64, 56, 56)\nKV1 = torch.randn(1, 64, 56, 56)\nVlue300 = torch.randn(1, 64, 56, 56)\nmask = (torch.rand(1, 56, 56) > 0.7).fill_(-1000000000.0)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, W09, M8, v1, mask):\n        qk = W09 @ M8.transpose(-2, -1) / math.sqrt(W09.size(-1))\n        qk = qk + mask\n        attn_weight = torch.softmax(qk, dim=-1)\n        output = attn_weight @ v1\n        return output\n# Inputs to the model\nW6 = torch.randn(1, 64, 56, 56)\nM7 = torch.randn(1, 56, 56)\nV2 = torch.randn(1, 56, 56, 56)\nmask = (torch.rand(1, 56, 56) > 0.7).fill_(-1000000000.0)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, qk, v0, mask):\n        qK = qk @ v0.transpose(-2, -1) / math.sqrt(qk.size(-1))\n        qK = qK + mask\n        attn_weight = torch.softmax(qK, dim=-1)\n        output = qK @ v0\n        return output\n# Inputs to the model\nqk0 = torch.randn(1, 64, 192, 64)\nv3 = torch.randn(1, 1, 1, 1)\nmask = (torch.rand(1, 192, 64) > 0.7).fill_(-1000000000.0)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, qK, v3, mask):\n        qK_ = qK @ v3.transpose(-2, -1) / math.sqrt(qK.size(-1))\n        qK_ = qK_ + mask\n        attn_weight = torch.softmax(qK_, dim=-1)\n        output = attn_weight @ v3\n        return output\n# Inputs to the model\nqk1 = torch.randn(1, 64, 56, 56)\nv3 = torch.randn(1, 64, 56, 56)\nmask = (torch.rand(1, 56, 56) > 0.7).fill_(-1000000000.0)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, W22, K3, v3, mask):\n        qk = W22 @ K3.transpose(-2, -1) / math.sqrt(W22.size(-1))\n        qk = qk + mask\n        attn_weight = torch.softmax(qk, dim=-1)\n        output = attn_weight @ v3\n        return output\n# Inputs\nW22 = torch.randn(1, 64, 56, 56)\nK = torch.randn(1, 64, 56, 56)\nV3 = torch.randn(1, 64, 56, 56)\nmask = (torch.rand(1, 56, 56) > 0.7).fill_(-1000000000.0)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, W13, k4, v1, mask):\n        _qk = W13 @ k4.transpose(-2, -1) / math.sqrt(W13.size(-1))\n        qk = _qk + mask\n        attn_weight = torch.softmax(qk, dim=-1)\n        output = attn_weight @ v1\n        return output\n# Inputs to the model\nQ = torch.randn(1, 64, 56, 56)\nK = torch.randn(1, 64, 56, 56)\nV = torch.randn(1, 64, 56, 56)\nmask = (torch.rand(1, 56, 56) > 0.7).fill_(-1000000000.0)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, Q2, k4, V5, mask):\n        qk = Q2 @ k4.transpose(-2, -1) / math.sqrt(Q2.size(-1))\n        qk = qk + mask\n        K = torch.softmax(qk, dim=-1)\n        output = K @ V5\n        return output\n# Inputs to the model\nQ2 = torch.randn(1, 64, 56, 56)\nk4 = torch.randn(1, 64, 56, 56)\nV5 = torch.randn(1, 64, 56, 56)\nmask = (torch.rand(1, 56, 56) > 0.7).fill_(-1000000000.0)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, qk6, V3, mask):\n        q_K = qk6 @ V3.transpose(-2, -1) / math.sqrt(qk6.size(-1))\n        q_K = q_K + mask\n        attn_weight = torch.softmax(q_K, dim=-1)\n        output = attn_weight @ V3\n        return output\n# Inputs to the model\nQ3 = torch.randn(1, 64, 56, 56)\nqk5 = torch.randn(1, 64, 56, 56)\nmask = (torch.rand(1, 56, 56) > 0.7).fill_(-1000000000.0)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, query3, key, value1, mask,):\n        query_key = query3 @ key.transpose(-2, -1) / math.sqrt(query3.size(-1))\n        query_key = query_key + mask\n        attn_weight = torch.softmax(query_key, dim=-1)\n        output = attn_weight @ value1\n        return output\n# Inputs to the model\nQ13 = torch.randn(1, 64, 56, 56)\nK = torch.randn(1, 64, 56, 56)\nV = torch.randn(1, 64, 56, 56)\nmask = (torch.rand(1, 56, 56) > 0.7).fill_(-1000000000.0)\n"
            ],
            "g_time": 8.654201984405518
        }
    }
}
{
    "opt": "binary_unary=8",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 8, 3, stride=1, padding=1)\n        self.conv2 = torch.nn.Conv2d(3, 8, 1, stride=1, padding=0)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = self.conv1(x1)\n        v3 = self.conv2(x1)\n        v4 = v1 + v2\n        v5 = v3 + v4\n        v6 = torch.relu(v5)\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(1, 8, 1, stride=1, padding=0)\n    def forward(self, x1, x2):\n        v1 = self.conv1(x1)\n        v2 = self.conv1(x2)\n        v3 = v1 + v2\n        v4 = torch.tanh(v3)\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 1, 64, 64)\nx2 = torch.randn(1, 1, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(1, 5, 3, stride=2, padding=1)\n        self.conv2 = torch.nn.Conv2d(5, 8, 3, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = self.conv2(v1)\n        v3 = self.conv2(v2)\n        v4 = self.conv1(v3)\n        v5 = self.conv2(v4)\n        v6 = self.conv1(v5)\n        v7 = v4 + v5 + v6\n        v8 = torch.relu(v7)\n        return v8\n# Inputs to the model\nx1 = torch.randn(1, 1, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(1, 1, 1, stride=1, padding=0)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v3 = torch.relu(v1)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 1, 32, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(1, 8, 4, stride=1, padding=0)\n    def forward(self, x1, x2):\n        v1 = torch.relu(self.conv1(x1))\n        v2 = torch.relu(self.conv1(x2))\n        v3 = v1 + v2\n        v4 = torch.relu(v3)\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 1, 64, 64)\nx2 = torch.randn(1, 1, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(1, 8, 1, stride=1, padding=0)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = v1[0]\n        v3 = v2[0]\n        v4 = v2[1]\n        v5 = v2[2]\n        v6 = v2[3]\n        v7 = v3 + v4 + v5 + v6\n        v8 = torch.relu(v7)\n        return v8\n# Inputs to the model\nx1 = torch.randn(1, 1, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(1, 5, 1, stride=1, padding=0)\n        self.conv2 = torch.nn.Conv2d(1, 5, 1, stride=1, padding=0)\n    def forward(self, x1, x2):\n        v1 = self.conv1(x1)\n        v2 = self.conv2(x2)\n        v3 = v1 + v2\n        v4 = torch.relu(v3)\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 1, 48, 48)\nx2 = torch.randn(1, 1, 48, 48)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, x2, x3, x4):\n        v1 = x1 + x2\n        v2 = x3 + x4\n        v3 = v1 + v2\n        v4 = torch.relu(v3)\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\nx2 = torch.randn(1, 3, 64, 64)\nx3 = torch.randn(1, 3, 64, 64)\nx4 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 3, 3, stride=1, padding=1)\n    def forward(self, x1, x2):\n        v1 = self.conv1(x1)\n        v2 = self.conv1(x2)\n        v3 = v1 + v2\n        v4 = torch.nn.functional.relu6(v3)\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\nx2 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(1, 16, 1, stride=1, padding=0)\n    def forward(self, x1, x2):\n        v1 = self.conv1(x1)\n        v2 = self.conv1(x2)\n        v3 = v1 + v2\n        v4 = torch.relu(v3)\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 1, 64, 64)\nx2 = torch.randn(1, 1, 64, 64)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 8, 3, stride=1, padding=1)\n        self.conv2 = torch.nn.Conv2d(3, 8, 1, stride=1, padding=0)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = self.conv1(x1)\n        v3 = self.conv2(x1)\n        v4 = v1 + v2\n        v5 = v3 + v4\n        v6 = torch.relu(v5)\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(1, 8, 1, stride=1, padding=0)\n    def forward(self, x1, x2):\n        v1 = self.conv1(x1)\n        v2 = self.conv1(x2)\n        v3 = v1 + v2\n        v4 = torch.tanh(v3)\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 1, 64, 64)\nx2 = torch.randn(1, 1, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(1, 5, 3, stride=2, padding=1)\n        self.conv2 = torch.nn.Conv2d(5, 8, 3, stride=1, padding=1)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = self.conv2(v1)\n        v3 = self.conv2(v2)\n        v4 = self.conv1(v3)\n        v5 = self.conv2(v4)\n        v6 = self.conv1(v5)\n        v7 = v4 + v5 + v6\n        v8 = torch.relu(v7)\n        return v8\n# Inputs to the model\nx1 = torch.randn(1, 1, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(1, 1, 1, stride=1, padding=0)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v3 = torch.relu(v1)\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 1, 32, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(1, 8, 4, stride=1, padding=0)\n    def forward(self, x1, x2):\n        v1 = torch.relu(self.conv1(x1))\n        v2 = torch.relu(self.conv1(x2))\n        v3 = v1 + v2\n        v4 = torch.relu(v3)\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 1, 64, 64)\nx2 = torch.randn(1, 1, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(1, 8, 1, stride=1, padding=0)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = v1[0]\n        v3 = v2[0]\n        v4 = v2[1]\n        v5 = v2[2]\n        v6 = v2[3]\n        v7 = v3 + v4 + v5 + v6\n        v8 = torch.relu(v7)\n        return v8\n# Inputs to the model\nx1 = torch.randn(1, 1, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(1, 5, 1, stride=1, padding=0)\n        self.conv2 = torch.nn.Conv2d(1, 5, 1, stride=1, padding=0)\n    def forward(self, x1, x2):\n        v1 = self.conv1(x1)\n        v2 = self.conv2(x2)\n        v3 = v1 + v2\n        v4 = torch.relu(v3)\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 1, 48, 48)\nx2 = torch.randn(1, 1, 48, 48)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1, x2, x3, x4):\n        v1 = x1 + x2\n        v2 = x3 + x4\n        v3 = v1 + v2\n        v4 = torch.relu(v3)\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\nx2 = torch.randn(1, 3, 64, 64)\nx3 = torch.randn(1, 3, 64, 64)\nx4 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 3, 3, stride=1, padding=1)\n    def forward(self, x1, x2):\n        v1 = self.conv1(x1)\n        v2 = self.conv1(x2)\n        v3 = v1 + v2\n        v4 = torch.nn.functional.relu6(v3)\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\nx2 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(1, 16, 1, stride=1, padding=0)\n    def forward(self, x1, x2):\n        v1 = self.conv1(x1)\n        v2 = self.conv1(x2)\n        v3 = v1 + v2\n        v4 = torch.relu(v3)\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 1, 64, 64)\nx2 = torch.randn(1, 1, 64, 64)\n"
            ],
            "g_time": 7.1937103271484375
        }
    }
}
{
    "opt": "splitwithsizes_cat_replace",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.features = torch.nn.Sequential(torch.nn.ReLU())\n    def forward(self, v1):\n        split_tensors = torch.split(v1, [1, 1, 1], dim=1)\n        concatenated_tensor = torch.cat(split_tensors, dim=1)\n        return (concatenated_tensor, torch.split(v1, [1, 1, 1], dim=1))\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Layer0(torch.nn.Module):\n    def __init__(self, inp, out):\n        super(Layer0, self).__init__()\n        self.block = Block()\n    def forward(self, inputs):\n        split_tensors = torch.split(inputs, [1, 1, 1], 1)\n        concatenated_tensor = torch.cat(split_tensors, dim=1)\n        return self.block(concatenated_tensor)\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.features = Layer0(3, 32)\n        self.extra = torch.nn.ReLU()\n    def forward(self, inputs):\n        split_tensors = torch.split(inputs, 10, 1)\n        concatenated_tensor = torch.cat(split_tensors, dim=1)\n        return concatenated_tensor\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 32)\n",
                "\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.blocks = nn.ModuleList(\n            [nn.Sequential(nn.Conv2d(3, 8, kernel_size=3), nn.ReLU(), nn.AdaptiveAvgPool2d(output_size=(5, 5)), nn.Dropout(),\n                           nn.Linear(168*8*8, 10), nn.Softmax())] * 16\n            )\n    def forward(self, x):\n        outputs = []\n        for i, block in enumerate(self.blocks):\n            x = block(x)\n            if i in {0, 4, 9, 13}:\n                outputs.append(x)\n        return tuple(outputs)\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass FwdBlock(torch.nn.Module):\n    def __init__(self, inp, out):\n        super(FwdBlock, self).__init__()\n        self.ops = torch.nn.Sequential(torch.nn.Conv1d(inp, out, 1, 1, 0), torch.nn.ReLU(),torch.nn.Conv1d(out, out, 1, 1, 0), torch.nn.ReLU(),torch.nn.Conv1d(out, out, 1, 1, 0), torch.nn.ReLU())\n    def forward(self, x):\n        return self.ops(x)\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        # Block1\n        self.features = FwdBlock(8, 16)\n        # self.layer3 = nn.Sequential(FwdBlock(16, 16), FwdBlock(16, 16))\n        # Block2\n        self.extra = FwdBlock(16, 16)\n    def forward(self, x):\n        split_tensors = torch.split(x, [1, 1, 1, 1, 1, 1, 1, 1], dim=1)\n        concatenated_tensor = torch.cat(split_tensors, dim=1)\n        return (concatenated_tensor, torch.split(x, [1, 1, 1, 1, 1, 1, 1, 1], dim=1))\n# Inputs to the model\nx1 = torch.randn(1, 8, 240)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.features = torch.nn.ModuleDict({'block0': torch.nn.Conv2d(3, 32, 3, 1, 1, bias=False), 'block1': torch.nn.Sequential(torch.nn.Conv2d(32, 32, 3, 1, 1, bias=False), torch.nn.Conv2d(32, 32, 3, 1, 1, bias=False))})\n    def forward(self, v1):\n        split_tensors = torch.split(v1, [1, 1, 1], dim=1)\n        concatenated_tensor = torch.cat(split_tensors, dim=1)\n        return (concatenated_tensor, torch.split(v1, [1, 1, 1], dim=1))\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, inp, hidden, out):\n        super(Model, self).__init__()\n        self.features = torch.nn.Sequential(torch.nn.Conv2d(inp, hidden, kernel_size=3, stride=1, padding=0, bias=False), torch.nn.ReLU(inplace=False), torch.nn.AvgPool2d(kernel_size=2, stride=2, padding=0, ceil_mode=False), torch.nn.BatchNorm2d(hidden, affine=False, track_running_stats=False), torch.nn.Conv2d(hidden, out, kernel_size=3, stride=1, padding=0, bias=False), torch.nn.Conv2d(hidden, out, kernel_size=5, stride=1, padding=2, bias=False))\n    def forward(self, v1):\n        split_tensors = torch.split(v1, [1, 1, 1, 1, 1, 1], dim=1)\n        concatenated_tensor = torch.cat(split_tensors, dim=1)\n        return (concatenated_tensor, torch.split(v1, [1, 1, 1, 1, 1, 1], dim=1))\n# Inputs to the model\nx1 = torch.randn(1, 3, 112, 112)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(32, 32, 3, 2, 1, bias=False)\n    def forward(self, x):\n        x = torch.split(x, [2,2,2,2], dim=1)[0]\n        x = torch.cat([x, x, x, x], dim=1) # pattern doesn't exist because dim=2 here\n        return self.conv(x)\n# Inputs to the model\nx = torch.randn(128, 32, 8, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.features = torch.nn.Sequential(list(torch.nn.ModuleDict({'block0': torch.nn.Conv2d(3, 32, 3, 1, 1, bias=False)}).values())[0], list(torch.nn.ModuleDict({'block1': torch.nn.Sequential(torch.nn.Conv2d(32, 32, 3, 1, 1, bias=False), torch.nn.Conv2d(32, 32, 3, 1, 1, bias=False))}).values())[0])\n    def forward(self, v1):\n        split_tensors = torch.split(v1, [1, 1, 1], dim=1)\n        concatenated_tensor = torch.cat(split_tensors, dim=1)\n        return (concatenated_tensor, torch.split(v1, [1, 1, 1], dim=1))\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.features1 = torch.nn.Sequential(torch.nn.Conv2d(3, 32, 3, 1, 1, bias=False), torch.nn.BatchNorm2d(32, affine=False, track_running_stats=False)),\n        self.features2 = torch.nn.Sequential(torch.nn.Conv2d(32, 32, 3, 1, 1, bias=False), torch.nn.BatchNorm2d(32, affine=False, track_running_stats=False)),\n        self.features3 = torch.nn.Sequential(torch.nn.Conv2d(32, 32, 3, 1, 1, bias=False), torch.nn.BatchNorm2d(32, affine=False, track_running_stats=False)),\n    def forward(self, v1, v2, v3):\n        split_tensors1 = torch.split(v1, [1, 1, 1], dim=1)\n        concatenated_tensor1 = torch.cat(split_tensors1, dim=1)\n        split_tensors2 = torch.split(v2, [1, 1], dim=1)\n        concatenated_tensor2 = torch.cat(split_tensors2, dim=1)\n        split_tensors3 = torch.split(v3, [1, 1, 1], dim=1)\n        concatenated_tensor3 = torch.cat(split_tensors3, dim=1)\n        return (concatenated_tensor1, concatenated_tensor2, concatenated_tensor3)\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\nx2 = torch.randn(1, 6, 64, 50)\nx3 = torch.randn(1, 9, 64, 50)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.features = torch.nn.ModuleDict({'block0': torch.nn.Conv2d(3, 32, 3, 1, 1, bias=False), 'block1': torch.nn.Sequential(torch.nn.Conv2d(32, 32, 3, 1, 1, bias=False), torch.nn.Conv2d(32, 32, 3, 1, 1, bias=False))})\n        self.extra = torch.nn.MaxPool2d(3, 2, 1)\n    def forward(self, v1):\n        split_tensors = torch.split(v1, [1, 1, 1], dim=1)\n        concatenated_tensor = torch.cat(split_tensors, dim=1)\n        return (concatenated_tensor, torch.split(v1, [1, 1, 1], dim=1))\n# Inputs to the model\nx1 = torch.randn(1, 3, 16, 16)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.features = torch.nn.Sequential(torch.nn.ReLU())\n    def forward(self, v1):\n        split_tensors = torch.split(v1, [1, 1, 1], dim=1)\n        concatenated_tensor = torch.cat(split_tensors, dim=1)\n        return (concatenated_tensor, torch.split(v1, [1, 1, 1], dim=1))\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Layer0(torch.nn.Module):\n    def __init__(self, inp, out):\n        super(Layer0, self).__init__()\n        self.block = Block()\n    def forward(self, inputs):\n        split_tensors = torch.split(inputs, [1, 1, 1], 1)\n        concatenated_tensor = torch.cat(split_tensors, dim=1)\n        return self.block(concatenated_tensor)\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.features = Layer0(3, 32)\n        self.extra = torch.nn.ReLU()\n    def forward(self, inputs):\n        split_tensors = torch.split(inputs, 10, 1)\n        concatenated_tensor = torch.cat(split_tensors, dim=1)\n        return concatenated_tensor\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 32)\n",
                "\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.blocks = nn.ModuleList(\n            [nn.Sequential(nn.Conv2d(3, 8, kernel_size=3), nn.ReLU(), nn.AdaptiveAvgPool2d(output_size=(5, 5)), nn.Dropout(),\n                           nn.Linear(168*8*8, 10), nn.Softmax())] * 16\n            )\n    def forward(self, x):\n        outputs = []\n        for i, block in enumerate(self.blocks):\n            x = block(x)\n            if i in {0, 4, 9, 13}:\n                outputs.append(x)\n        return tuple(outputs)\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass FwdBlock(torch.nn.Module):\n    def __init__(self, inp, out):\n        super(FwdBlock, self).__init__()\n        self.ops = torch.nn.Sequential(torch.nn.Conv1d(inp, out, 1, 1, 0), torch.nn.ReLU(),torch.nn.Conv1d(out, out, 1, 1, 0), torch.nn.ReLU(),torch.nn.Conv1d(out, out, 1, 1, 0), torch.nn.ReLU())\n    def forward(self, x):\n        return self.ops(x)\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        # Block1\n        self.features = FwdBlock(8, 16)\n        # self.layer3 = nn.Sequential(FwdBlock(16, 16), FwdBlock(16, 16))\n        # Block2\n        self.extra = FwdBlock(16, 16)\n    def forward(self, x):\n        split_tensors = torch.split(x, [1, 1, 1, 1, 1, 1, 1, 1], dim=1)\n        concatenated_tensor = torch.cat(split_tensors, dim=1)\n        return (concatenated_tensor, torch.split(x, [1, 1, 1, 1, 1, 1, 1, 1], dim=1))\n# Inputs to the model\nx1 = torch.randn(1, 8, 240)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.features = torch.nn.ModuleDict({'block0': torch.nn.Conv2d(3, 32, 3, 1, 1, bias=False), 'block1': torch.nn.Sequential(torch.nn.Conv2d(32, 32, 3, 1, 1, bias=False), torch.nn.Conv2d(32, 32, 3, 1, 1, bias=False))})\n    def forward(self, v1):\n        split_tensors = torch.split(v1, [1, 1, 1], dim=1)\n        concatenated_tensor = torch.cat(split_tensors, dim=1)\n        return (concatenated_tensor, torch.split(v1, [1, 1, 1], dim=1))\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, inp, hidden, out):\n        super(Model, self).__init__()\n        self.features = torch.nn.Sequential(torch.nn.Conv2d(inp, hidden, kernel_size=3, stride=1, padding=0, bias=False), torch.nn.ReLU(inplace=False), torch.nn.AvgPool2d(kernel_size=2, stride=2, padding=0, ceil_mode=False), torch.nn.BatchNorm2d(hidden, affine=False, track_running_stats=False), torch.nn.Conv2d(hidden, out, kernel_size=3, stride=1, padding=0, bias=False), torch.nn.Conv2d(hidden, out, kernel_size=5, stride=1, padding=2, bias=False))\n    def forward(self, v1):\n        split_tensors = torch.split(v1, [1, 1, 1, 1, 1, 1], dim=1)\n        concatenated_tensor = torch.cat(split_tensors, dim=1)\n        return (concatenated_tensor, torch.split(v1, [1, 1, 1, 1, 1, 1], dim=1))\n# Inputs to the model\nx1 = torch.randn(1, 3, 112, 112)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(32, 32, 3, 2, 1, bias=False)\n    def forward(self, x):\n        x = torch.split(x, [2,2,2,2], dim=1)[0]\n        x = torch.cat([x, x, x, x], dim=1) # pattern doesn't exist because dim=2 here\n        return self.conv(x)\n# Inputs to the model\nx = torch.randn(128, 32, 8, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.features = torch.nn.Sequential(list(torch.nn.ModuleDict({'block0': torch.nn.Conv2d(3, 32, 3, 1, 1, bias=False)}).values())[0], list(torch.nn.ModuleDict({'block1': torch.nn.Sequential(torch.nn.Conv2d(32, 32, 3, 1, 1, bias=False), torch.nn.Conv2d(32, 32, 3, 1, 1, bias=False))}).values())[0])\n    def forward(self, v1):\n        split_tensors = torch.split(v1, [1, 1, 1], dim=1)\n        concatenated_tensor = torch.cat(split_tensors, dim=1)\n        return (concatenated_tensor, torch.split(v1, [1, 1, 1], dim=1))\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.features1 = torch.nn.Sequential(torch.nn.Conv2d(3, 32, 3, 1, 1, bias=False), torch.nn.BatchNorm2d(32, affine=False, track_running_stats=False)),\n        self.features2 = torch.nn.Sequential(torch.nn.Conv2d(32, 32, 3, 1, 1, bias=False), torch.nn.BatchNorm2d(32, affine=False, track_running_stats=False)),\n        self.features3 = torch.nn.Sequential(torch.nn.Conv2d(32, 32, 3, 1, 1, bias=False), torch.nn.BatchNorm2d(32, affine=False, track_running_stats=False)),\n    def forward(self, v1, v2, v3):\n        split_tensors1 = torch.split(v1, [1, 1, 1], dim=1)\n        concatenated_tensor1 = torch.cat(split_tensors1, dim=1)\n        split_tensors2 = torch.split(v2, [1, 1], dim=1)\n        concatenated_tensor2 = torch.cat(split_tensors2, dim=1)\n        split_tensors3 = torch.split(v3, [1, 1, 1], dim=1)\n        concatenated_tensor3 = torch.cat(split_tensors3, dim=1)\n        return (concatenated_tensor1, concatenated_tensor2, concatenated_tensor3)\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\nx2 = torch.randn(1, 6, 64, 50)\nx3 = torch.randn(1, 9, 64, 50)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.features = torch.nn.ModuleDict({'block0': torch.nn.Conv2d(3, 32, 3, 1, 1, bias=False), 'block1': torch.nn.Sequential(torch.nn.Conv2d(32, 32, 3, 1, 1, bias=False), torch.nn.Conv2d(32, 32, 3, 1, 1, bias=False))})\n        self.extra = torch.nn.MaxPool2d(3, 2, 1)\n    def forward(self, v1):\n        split_tensors = torch.split(v1, [1, 1, 1], dim=1)\n        concatenated_tensor = torch.cat(split_tensors, dim=1)\n        return (concatenated_tensor, torch.split(v1, [1, 1, 1], dim=1))\n# Inputs to the model\nx1 = torch.randn(1, 3, 16, 16)\n"
            ],
            "g_time": 16.511908531188965
        }
    }
}
{
    "opt": "binary_unary=6",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 - 1\n        v3 = torch.nn.functional.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, input_size, output_size):\n        super().__init__()\n        self.linear = torch.nn.Linear(input_size, output_size, bias=True)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 - 0.5\n        v3 = F.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model(10, 19)\n\n# Inputs to the model\nx1 = torch.randn(1, 10)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(5, 8)\n \n    def forward(self, x3):\n        v3 = self.linear(x3)\n        v2 = v3 - 3\n        v4 = F.relu(v2)\n        return v4\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx3 = torch.randn(1, 5)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 4, bias=False)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 - 0.5\n        v3 = F.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(8, 1)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 - 0.5\n        v3 = v2.relu()\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(1, 1)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 - 95.3\n        v3 = v2.relu()\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 1)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear= torch.nn.Linear(8, 3)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 - 0.5420211607450407\n        v3 = F.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(32, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(4, 4)\n \n    def forward(self, x):\n        v1 = self.linear(x)\n        v2 = v1 - other\n        v3 = torch.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 4)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(8, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 - other\n        v3 = torch.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(23, 29)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 - 1.023796464034558\n        v3 = F.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 23)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 - 1\n        v3 = torch.nn.functional.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self, input_size, output_size):\n        super().__init__()\n        self.linear = torch.nn.Linear(input_size, output_size, bias=True)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 - 0.5\n        v3 = F.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model(10, 19)\n\n# Inputs to the model\nx1 = torch.randn(1, 10)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(5, 8)\n \n    def forward(self, x3):\n        v3 = self.linear(x3)\n        v2 = v3 - 3\n        v4 = F.relu(v2)\n        return v4\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx3 = torch.randn(1, 5)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 4, bias=False)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 - 0.5\n        v3 = F.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(8, 1)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 - 0.5\n        v3 = v2.relu()\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(1, 1)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 - 95.3\n        v3 = v2.relu()\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 1)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear= torch.nn.Linear(8, 3)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 - 0.5420211607450407\n        v3 = F.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(32, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(4, 4)\n \n    def forward(self, x):\n        v1 = self.linear(x)\n        v2 = v1 - other\n        v3 = torch.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 4)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(8, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 - other\n        v3 = torch.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(23, 29)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 - 1.023796464034558\n        v3 = F.relu(v2)\n        return v3\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 23)\n"
            ],
            "g_time": 5.490801811218262
        }
    }
}
{
    "opt": "sfdp=0",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.key = torch.nn.Parameter(torch.randn(72, 55, 87, 96))\n    def forward(self, x1):\n        q = x1\n        k = x1\n        v = x1\n        inv_scale = math.sqrt(k.size(1))\n        scaled_dot_product = torch.matmul(q, k.transpose(-2, -1)) / inv_scale\n        attention_weights = scaled_dot_product.softmax(dim=-1)\n        output = attention_weights.matmul(v)\n        return output\n# Inputs to the model\nx1 = torch.randn(25, 2, 84, 39)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.key = torch.nn.Parameter(torch.randn(47, 34, 70, 14))\n    def forward(self, x1):\n        q = x1\n        k = x1\n        v = x1\n        inv_scale = math.sqrt(k.size(1))\n        scaled_dot_product = torch.matmul(q, k.transpose(-2, -1)) / inv_scale\n        attention_weights = scaled_dot_product.softmax(dim=-1)\n        output = attention_weights.matmul(v)\n        return output\n# Inputs to the model\nx1 = torch.randn(72, 28, 63, 31)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.key = torch.nn.Parameter(torch.randn(74, 11, 48, 34))\n    def forward(self, x1):\n        q = x1\n        k = x1\n        v = x1\n        inv_scale = math.sqrt(k.size(1))\n        scaled_dot_product = torch.matmul(q, k.transpose(-2, -1)) / inv_scale\n        attention_weights = scaled_dot_product.softmax(dim=-1)\n        output = attention_weights.matmul(v)\n        return output\n# Inputs to the model\nx1 = torch.randn(75, 6, 61, 80)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.key = torch.nn.Parameter(torch.randn(45, 26, 65, 91))\n    def forward(self, x1):\n        q = x1\n        k = x1\n        v = x1\n        inv_scale = math.sqrt(k.size(1))\n        scaled_dot_product = torch.matmul(q, k.transpose(-2, -1)) / inv_scale\n        attention_weights = scaled_dot_product.softmax(dim=-1)\n        output = attention_weights.matmul(v)\n        return output\n# Inputs to the model\nx1 = torch.randn(57, 91, 19, 31)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.key = torch.nn.Parameter(torch.randn(84, 74, 45, 74))\n    def forward(self, x1):\n        q = x1\n        k = x1\n        v = x1\n        inv_scale = math.sqrt(k.size(1))\n        scaled_dot_product = torch.matmul(q, k.transpose(-2, -1)) / inv_scale\n        attention_weights = scaled_dot_product.softmax(dim=-1)\n        output = attention_weights.matmul(v)\n        return output\n# Inputs to the model\nx1 = torch.randn(6, 72, 69, 77)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.key = torch.nn.Parameter(torch.randn(16, 31, 51, 34))\n    def forward(self, x1):\n        q = x1\n        k = x1\n        v = x1\n        inv_scale = math.sqrt(k.size(1))\n        scaled_dot_product = torch.matmul(q, k.transpose(-2, -1)) / inv_scale\n        attention_weights = scaled_dot_product.softmax(dim=-1)\n        output = attention_weights.matmul(v)\n        return output\n# Inputs to the model\nx1 = torch.randn(59, 94, 67, 93)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.key = torch.nn.Parameter(torch.randn(27, 17, 8, 80))\n    def forward(self, x1):\n        q = x1\n        k = x1\n        v = x1\n        inv_scale = math.sqrt(k.size(1))\n        scaled_dot_product = torch.matmul(q, k.transpose(-2, -1)) / inv_scale\n        attention_weights = scaled_dot_product.softmax(dim=-1)\n        output = attention_weights.matmul(v)\n        return output\n# Inputs to the model\nx1 = torch.randn(13, 78, 50, 81)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.key = torch.nn.Parameter(torch.randn(92, 18, 97, 45))\n    def forward(self, x1):\n        q = x1\n        k = x1\n        v = x1\n        inv_scale = math.sqrt(k.size(1))\n        scaled_dot_product = torch.matmul(q, k.transpose(-2, -1)) / inv_scale\n        attention_weights = scaled_dot_product.softmax(dim=-1)\n        output = attention_weights.matmul(v)\n        return output\n# Inputs to the model\nx1 = torch.randn(18, 42, 69, 96)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.key = torch.nn.Parameter(torch.randn(8, 37, 71, 42))\n    def forward(self, x1):\n        q = x1\n        k = x1\n        v = x1\n        inv_scale = math.sqrt(k.size(1))\n        scaled_dot_product = torch.matmul(q, k.transpose(-2, -1)) / inv_scale\n        attention_weights = scaled_dot_product.softmax(dim=-1)\n        output = attention_weights.matmul(v)\n        return output\n# Inputs to the model\nx1 = torch.randn(35, 70, 19, 67)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.key = torch.nn.Parameter(torch.randn(68, 3, 24, 19))\n    def forward(self, x1):\n        q = x1\n        k = x1\n        v = x1\n        inv_scale = math.sqrt(k.size(1))\n        scaled_dot_product = torch.matmul(q, k.transpose(-2, -1)) / inv_scale\n        attention_weights = scaled_dot_product.softmax(dim=-1)\n        output = attention_weights.matmul(v)\n        return output\n# Inputs to the model\nx1 = torch.randn(15, 10, 86, 70)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.key = torch.nn.Parameter(torch.randn(72, 55, 87, 96))\n    def forward(self, x1):\n        q = x1\n        k = x1\n        v = x1\n        inv_scale = math.sqrt(k.size(1))\n        scaled_dot_product = torch.matmul(q, k.transpose(-2, -1)) / inv_scale\n        attention_weights = scaled_dot_product.softmax(dim=-1)\n        output = attention_weights.matmul(v)\n        return output\n# Inputs to the model\nx1 = torch.randn(25, 2, 84, 39)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.key = torch.nn.Parameter(torch.randn(47, 34, 70, 14))\n    def forward(self, x1):\n        q = x1\n        k = x1\n        v = x1\n        inv_scale = math.sqrt(k.size(1))\n        scaled_dot_product = torch.matmul(q, k.transpose(-2, -1)) / inv_scale\n        attention_weights = scaled_dot_product.softmax(dim=-1)\n        output = attention_weights.matmul(v)\n        return output\n# Inputs to the model\nx1 = torch.randn(72, 28, 63, 31)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.key = torch.nn.Parameter(torch.randn(74, 11, 48, 34))\n    def forward(self, x1):\n        q = x1\n        k = x1\n        v = x1\n        inv_scale = math.sqrt(k.size(1))\n        scaled_dot_product = torch.matmul(q, k.transpose(-2, -1)) / inv_scale\n        attention_weights = scaled_dot_product.softmax(dim=-1)\n        output = attention_weights.matmul(v)\n        return output\n# Inputs to the model\nx1 = torch.randn(75, 6, 61, 80)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.key = torch.nn.Parameter(torch.randn(45, 26, 65, 91))\n    def forward(self, x1):\n        q = x1\n        k = x1\n        v = x1\n        inv_scale = math.sqrt(k.size(1))\n        scaled_dot_product = torch.matmul(q, k.transpose(-2, -1)) / inv_scale\n        attention_weights = scaled_dot_product.softmax(dim=-1)\n        output = attention_weights.matmul(v)\n        return output\n# Inputs to the model\nx1 = torch.randn(57, 91, 19, 31)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.key = torch.nn.Parameter(torch.randn(84, 74, 45, 74))\n    def forward(self, x1):\n        q = x1\n        k = x1\n        v = x1\n        inv_scale = math.sqrt(k.size(1))\n        scaled_dot_product = torch.matmul(q, k.transpose(-2, -1)) / inv_scale\n        attention_weights = scaled_dot_product.softmax(dim=-1)\n        output = attention_weights.matmul(v)\n        return output\n# Inputs to the model\nx1 = torch.randn(6, 72, 69, 77)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.key = torch.nn.Parameter(torch.randn(16, 31, 51, 34))\n    def forward(self, x1):\n        q = x1\n        k = x1\n        v = x1\n        inv_scale = math.sqrt(k.size(1))\n        scaled_dot_product = torch.matmul(q, k.transpose(-2, -1)) / inv_scale\n        attention_weights = scaled_dot_product.softmax(dim=-1)\n        output = attention_weights.matmul(v)\n        return output\n# Inputs to the model\nx1 = torch.randn(59, 94, 67, 93)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.key = torch.nn.Parameter(torch.randn(27, 17, 8, 80))\n    def forward(self, x1):\n        q = x1\n        k = x1\n        v = x1\n        inv_scale = math.sqrt(k.size(1))\n        scaled_dot_product = torch.matmul(q, k.transpose(-2, -1)) / inv_scale\n        attention_weights = scaled_dot_product.softmax(dim=-1)\n        output = attention_weights.matmul(v)\n        return output\n# Inputs to the model\nx1 = torch.randn(13, 78, 50, 81)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.key = torch.nn.Parameter(torch.randn(92, 18, 97, 45))\n    def forward(self, x1):\n        q = x1\n        k = x1\n        v = x1\n        inv_scale = math.sqrt(k.size(1))\n        scaled_dot_product = torch.matmul(q, k.transpose(-2, -1)) / inv_scale\n        attention_weights = scaled_dot_product.softmax(dim=-1)\n        output = attention_weights.matmul(v)\n        return output\n# Inputs to the model\nx1 = torch.randn(18, 42, 69, 96)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.key = torch.nn.Parameter(torch.randn(8, 37, 71, 42))\n    def forward(self, x1):\n        q = x1\n        k = x1\n        v = x1\n        inv_scale = math.sqrt(k.size(1))\n        scaled_dot_product = torch.matmul(q, k.transpose(-2, -1)) / inv_scale\n        attention_weights = scaled_dot_product.softmax(dim=-1)\n        output = attention_weights.matmul(v)\n        return output\n# Inputs to the model\nx1 = torch.randn(35, 70, 19, 67)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.key = torch.nn.Parameter(torch.randn(68, 3, 24, 19))\n    def forward(self, x1):\n        q = x1\n        k = x1\n        v = x1\n        inv_scale = math.sqrt(k.size(1))\n        scaled_dot_product = torch.matmul(q, k.transpose(-2, -1)) / inv_scale\n        attention_weights = scaled_dot_product.softmax(dim=-1)\n        output = attention_weights.matmul(v)\n        return output\n# Inputs to the model\nx1 = torch.randn(15, 10, 86, 70)\n"
            ],
            "g_time": 6.763339042663574
        }
    }
}
{
    "opt": "pointless_cumsum_replacement",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1):\n        b = {}\n        a = {}\n        b['dtype'] = torch.float64\n        b['layout'] = torch.strided\n        b['device'] = torch.device('cuda:0')\n        a['dtype'] = torch.float32\n        a['layout'] = torch.strided\n        a['device'] = torch.device('cuda:0')\n        a['dtype_to'] = torch.float64\n        a['dtype_from'] = torch.float32\n        b['dtype_to'] = torch.float64\n        b['dtype_from'] = torch.float32\n        t1 = torch.full([32, 128], 1, dtype=b['dtype'], layout=b['layout'], device=b['device'], pin_memory=False)\n        t2 = t1.to(dtype=a['dtype'])\n        t3 = torch.cumsum(t2, 1)\n        return t3\n# Inputs to the model\nx1 = torch.randn(32, 128, device='cuda:0')\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1):\n        b = {}\n        a = {}\n        b['dtype'] = torch.bool\n        b['layout'] = torch.strided\n        b['device'] = torch.device('cuda:0')\n        a['dtype'] = torch.float16\n        a['layout'] = torch.strided\n        a['device'] = torch.device('cuda:0')\n        a['dtype_to'] = torch.bool\n        a['dtype_from'] = torch.float16\n        b['dtype_to'] = torch.float64\n        b['dtype_from'] = torch.bool\n        t1 = torch.full([33, 256], 1, dtype=b['dtype'], layout=b['layout'], device=b['device'], pin_memory=False)\n        t2 = t1.to(dtype=a['dtype'])\n        t3 = torch.cumsum(t2, 1)\n        return t3\n# Inputs to the model\nx1 = torch.randn(33, 256, device='cuda:0')\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1):\n        b = {}\n        a = {}\n        b['dtype'] = torch.int64\n        b['layout'] = torch.strided\n        b['device'] = torch.device('cuda:0')\n        a['dtype'] = torch.uint8\n        a['layout'] = torch.strided\n        a['device'] = torch.device('cuda:0')\n        a['dtype_to'] = torch.int64\n        a['dtype_from'] = torch.uint8\n        b['dtype_to'] = torch.int64\n        b['dtype_from'] = torch.uint8\n        t1 = torch.full([1536, 1536], 1, dtype=b['dtype'], layout=b['layout'], device=b['device'], pin_memory=False)\n        t2 = t1.to(dtype=a['dtype'])\n        t3 = torch.cumsum(t2, 0)\n        return t3\n# Inputs to the model\nx1 = torch.randn(1536, 1536, device='cuda:0')\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1):\n        b = {}\n        a = {}\n        b['dtype'] = torch.float64\n        b['layout'] = torch.strided\n        b['device'] = torch.device('cuda:0')\n        a['dtype'] = torch.bool\n        a['layout'] = torch.strided\n        a['device'] = torch.device('cuda:0')\n        a['dtype_to'] = torch.float64\n        a['dtype_from'] = torch.bool\n        b['dtype_to'] = torch.float64\n        b['dtype_from'] = torch.bool\n        t1 = torch.full([32, 32], 1, dtype=b['dtype'], layout=b['layout'], device=b['device'], pin_memory=False)\n        t2 = t1.to(dtype=a['dtype'])\n        t3 = torch.cumsum(t2, 1)\n        return t3\n# Inputs to the model\nx1 = torch.randn(32, 32, device='cuda:0')\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1):\n        b = {}\n        a = {}\n        b['dtype'] = torch.float32\n        b['layout'] = torch.strided\n        b['device'] = torch.device('cuda:0')\n        a['dtype'] = torch.float16\n        a['layout'] = torch.strided\n        a['device'] = torch.device('cuda:0')\n        a['dtype_to'] = torch.float16\n        a['dtype_from'] = torch.float32\n        b['dtype_to'] = torch.float16\n        b['dtype_from'] = torch.float32\n        t1 = torch.full([16, 64], 1, dtype=b['dtype'], layout=b['layout'], device=b['device'], pin_memory=False)\n        t2 = t1.to(dtype=a['dtype'])\n        t3 = torch.cumsum(t2, 1)\n        return t3\n# Inputs to the model\nx1 = torch.randn(16, 64, device='cuda:0')\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1):\n        b = {}\n        a = {}\n        b['dtype'] = torch.complex64\n        b['layout'] = torch.strided\n        b['device'] = torch.device('cuda:0')\n        a['dtype'] = torch.float32\n        a['layout'] = torch.strided\n        a['device'] = torch.device('cuda:0')\n        a['dtype_to'] = torch.complex64\n        a['dtype_from'] = torch.float32\n        b['dtype_to'] = torch.complex64\n        b['dtype_from'] = torch.float32\n        t1 = torch.full([94, 171], 1, dtype=b['dtype'], layout=b['layout'], device=b['device'], pin_memory=False)\n        t2 = t1.to(dtype=a['dtype'])\n        t3 = torch.cumsum(t2, 1)\n        return t3\n# Inputs to the model\nx1 = torch.randn(94, 171, device='cuda:0')\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1):\n        b = {}\n        a = {}\n        b['dtype'] = torch.uint8\n        b['layout'] = torch.strided\n        b['device'] = torch.device('cuda:0')\n        a['dtype'] = torch.uint8\n        a['layout'] = torch.strided\n        a['device'] = torch.device('cuda:0')\n        a['dtype_to'] = torch.uint8\n        a['dtype_from'] = torch.uint8\n        b['dtype_to'] = torch.uint8\n        b['dtype_from'] = torch.uint8\n        t1 = torch.full([4, 4], 1, dtype=b['dtype'], layout=b['layout'], device=b['device'], pin_memory=False)\n        t2 = t1.to(dtype=a['dtype'])\n        t3 = torch.cumsum(t2, 1)\n        return t3\n# Inputs to the model\nx1 = torch.randn(4, 4, device='cuda:0')\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1):\n        b = {}\n        a = {}\n        b['dtype'] = torch.float64\n        b['layout'] = torch.strided\n        b['device'] = torch.device('cuda:0')\n        a['dtype'] = torch.uint8\n        a['layout'] = torch.strided\n        a['device'] = torch.device('cuda:0')\n        a['dtype_to'] = torch.float64\n        a['dtype_from'] = torch.uint8\n        b['dtype_to'] = torch.float64\n        b['dtype_from'] = torch.uint8\n        t1 = torch.full([41, 8], 1, dtype=b['dtype'], layout=b['layout'], device=b['device'], pin_memory=False)\n        t2 = t1.to(dtype=a['dtype'])\n        t3 = torch.cumsum(t2, 1)\n        return t3\n# Inputs to the model\nx1 = torch.randn(41, 8, device='cuda:0')\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1):\n        b = {}\n        a = {}\n        b['dtype'] = torch.float32\n        b['layout'] = torch.strided\n        b['device'] = torch.device('cuda:0')\n        a['dtype'] = torch.int64\n        a['layout'] = torch.strided\n        a['device'] = torch.device('cuda:0')\n        a['dtype_to'] = torch.float32\n        a['dtype_from'] = torch.int64\n        b['dtype_to'] = torch.float32\n        b['dtype_from'] = torch.int64\n        t1 = torch.full([172, 252], 1, dtype=b['dtype'], layout=b['layout'], device=b['device'], pin_memory=False)\n        t2 = t1.to(dtype=a['dtype'])\n        t3 = torch.cumsum(t2, 1)\n        return t3\n# Inputs to the model\nx1 = torch.randn(172, 252, device='cuda:0')\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1):\n        b = {}\n        a = {}\n        b['dtype'] = torch.int64\n        b['layout'] = torch.strided\n        b['device'] = torch.device('cuda:0')\n        a['dtype'] = torch.long\n        a['layout'] = torch.strided\n        a['device'] = torch.device('cuda:0')\n        a['dtype_to'] = torch.int64\n        a['dtype_from'] = torch.long\n        b['dtype_to'] = torch.int64\n        b['dtype_from'] = torch.long\n        t1 = torch.full([64, 64], 1, dtype=b['dtype'], layout=b['layout'], device=b['device'], pin_memory=False)\n        t2 = t1.to(dtype=a['dtype'])\n        t3 = torch.cumsum(t2, 1)\n        return t3\n# Inputs to the model\nx1 = torch.randn(64, 64, device='cuda:0')\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1):\n        b = {}\n        a = {}\n        b['dtype'] = torch.float64\n        b['layout'] = torch.strided\n        b['device'] = torch.device('cuda:0')\n        a['dtype'] = torch.float32\n        a['layout'] = torch.strided\n        a['device'] = torch.device('cuda:0')\n        a['dtype_to'] = torch.float64\n        a['dtype_from'] = torch.float32\n        b['dtype_to'] = torch.float64\n        b['dtype_from'] = torch.float32\n        t1 = torch.full([32, 128], 1, dtype=b['dtype'], layout=b['layout'], device=b['device'], pin_memory=False)\n        t2 = t1.to(dtype=a['dtype'])\n        t3 = torch.cumsum(t2, 1)\n        return t3\n# Inputs to the model\nx1 = torch.randn(32, 128, device='cuda:0')\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1):\n        b = {}\n        a = {}\n        b['dtype'] = torch.bool\n        b['layout'] = torch.strided\n        b['device'] = torch.device('cuda:0')\n        a['dtype'] = torch.float16\n        a['layout'] = torch.strided\n        a['device'] = torch.device('cuda:0')\n        a['dtype_to'] = torch.bool\n        a['dtype_from'] = torch.float16\n        b['dtype_to'] = torch.float64\n        b['dtype_from'] = torch.bool\n        t1 = torch.full([33, 256], 1, dtype=b['dtype'], layout=b['layout'], device=b['device'], pin_memory=False)\n        t2 = t1.to(dtype=a['dtype'])\n        t3 = torch.cumsum(t2, 1)\n        return t3\n# Inputs to the model\nx1 = torch.randn(33, 256, device='cuda:0')\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1):\n        b = {}\n        a = {}\n        b['dtype'] = torch.int64\n        b['layout'] = torch.strided\n        b['device'] = torch.device('cuda:0')\n        a['dtype'] = torch.uint8\n        a['layout'] = torch.strided\n        a['device'] = torch.device('cuda:0')\n        a['dtype_to'] = torch.int64\n        a['dtype_from'] = torch.uint8\n        b['dtype_to'] = torch.int64\n        b['dtype_from'] = torch.uint8\n        t1 = torch.full([1536, 1536], 1, dtype=b['dtype'], layout=b['layout'], device=b['device'], pin_memory=False)\n        t2 = t1.to(dtype=a['dtype'])\n        t3 = torch.cumsum(t2, 0)\n        return t3\n# Inputs to the model\nx1 = torch.randn(1536, 1536, device='cuda:0')\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1):\n        b = {}\n        a = {}\n        b['dtype'] = torch.float64\n        b['layout'] = torch.strided\n        b['device'] = torch.device('cuda:0')\n        a['dtype'] = torch.bool\n        a['layout'] = torch.strided\n        a['device'] = torch.device('cuda:0')\n        a['dtype_to'] = torch.float64\n        a['dtype_from'] = torch.bool\n        b['dtype_to'] = torch.float64\n        b['dtype_from'] = torch.bool\n        t1 = torch.full([32, 32], 1, dtype=b['dtype'], layout=b['layout'], device=b['device'], pin_memory=False)\n        t2 = t1.to(dtype=a['dtype'])\n        t3 = torch.cumsum(t2, 1)\n        return t3\n# Inputs to the model\nx1 = torch.randn(32, 32, device='cuda:0')\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1):\n        b = {}\n        a = {}\n        b['dtype'] = torch.float32\n        b['layout'] = torch.strided\n        b['device'] = torch.device('cuda:0')\n        a['dtype'] = torch.float16\n        a['layout'] = torch.strided\n        a['device'] = torch.device('cuda:0')\n        a['dtype_to'] = torch.float16\n        a['dtype_from'] = torch.float32\n        b['dtype_to'] = torch.float16\n        b['dtype_from'] = torch.float32\n        t1 = torch.full([16, 64], 1, dtype=b['dtype'], layout=b['layout'], device=b['device'], pin_memory=False)\n        t2 = t1.to(dtype=a['dtype'])\n        t3 = torch.cumsum(t2, 1)\n        return t3\n# Inputs to the model\nx1 = torch.randn(16, 64, device='cuda:0')\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1):\n        b = {}\n        a = {}\n        b['dtype'] = torch.complex64\n        b['layout'] = torch.strided\n        b['device'] = torch.device('cuda:0')\n        a['dtype'] = torch.float32\n        a['layout'] = torch.strided\n        a['device'] = torch.device('cuda:0')\n        a['dtype_to'] = torch.complex64\n        a['dtype_from'] = torch.float32\n        b['dtype_to'] = torch.complex64\n        b['dtype_from'] = torch.float32\n        t1 = torch.full([94, 171], 1, dtype=b['dtype'], layout=b['layout'], device=b['device'], pin_memory=False)\n        t2 = t1.to(dtype=a['dtype'])\n        t3 = torch.cumsum(t2, 1)\n        return t3\n# Inputs to the model\nx1 = torch.randn(94, 171, device='cuda:0')\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1):\n        b = {}\n        a = {}\n        b['dtype'] = torch.uint8\n        b['layout'] = torch.strided\n        b['device'] = torch.device('cuda:0')\n        a['dtype'] = torch.uint8\n        a['layout'] = torch.strided\n        a['device'] = torch.device('cuda:0')\n        a['dtype_to'] = torch.uint8\n        a['dtype_from'] = torch.uint8\n        b['dtype_to'] = torch.uint8\n        b['dtype_from'] = torch.uint8\n        t1 = torch.full([4, 4], 1, dtype=b['dtype'], layout=b['layout'], device=b['device'], pin_memory=False)\n        t2 = t1.to(dtype=a['dtype'])\n        t3 = torch.cumsum(t2, 1)\n        return t3\n# Inputs to the model\nx1 = torch.randn(4, 4, device='cuda:0')\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1):\n        b = {}\n        a = {}\n        b['dtype'] = torch.float64\n        b['layout'] = torch.strided\n        b['device'] = torch.device('cuda:0')\n        a['dtype'] = torch.uint8\n        a['layout'] = torch.strided\n        a['device'] = torch.device('cuda:0')\n        a['dtype_to'] = torch.float64\n        a['dtype_from'] = torch.uint8\n        b['dtype_to'] = torch.float64\n        b['dtype_from'] = torch.uint8\n        t1 = torch.full([41, 8], 1, dtype=b['dtype'], layout=b['layout'], device=b['device'], pin_memory=False)\n        t2 = t1.to(dtype=a['dtype'])\n        t3 = torch.cumsum(t2, 1)\n        return t3\n# Inputs to the model\nx1 = torch.randn(41, 8, device='cuda:0')\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1):\n        b = {}\n        a = {}\n        b['dtype'] = torch.float32\n        b['layout'] = torch.strided\n        b['device'] = torch.device('cuda:0')\n        a['dtype'] = torch.int64\n        a['layout'] = torch.strided\n        a['device'] = torch.device('cuda:0')\n        a['dtype_to'] = torch.float32\n        a['dtype_from'] = torch.int64\n        b['dtype_to'] = torch.float32\n        b['dtype_from'] = torch.int64\n        t1 = torch.full([172, 252], 1, dtype=b['dtype'], layout=b['layout'], device=b['device'], pin_memory=False)\n        t2 = t1.to(dtype=a['dtype'])\n        t3 = torch.cumsum(t2, 1)\n        return t3\n# Inputs to the model\nx1 = torch.randn(172, 252, device='cuda:0')\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x1):\n        b = {}\n        a = {}\n        b['dtype'] = torch.int64\n        b['layout'] = torch.strided\n        b['device'] = torch.device('cuda:0')\n        a['dtype'] = torch.long\n        a['layout'] = torch.strided\n        a['device'] = torch.device('cuda:0')\n        a['dtype_to'] = torch.int64\n        a['dtype_from'] = torch.long\n        b['dtype_to'] = torch.int64\n        b['dtype_from'] = torch.long\n        t1 = torch.full([64, 64], 1, dtype=b['dtype'], layout=b['layout'], device=b['device'], pin_memory=False)\n        t2 = t1.to(dtype=a['dtype'])\n        t3 = torch.cumsum(t2, 1)\n        return t3\n# Inputs to the model\nx1 = torch.randn(64, 64, device='cuda:0')\n"
            ],
            "g_time": 10.030140161514282
        }
    }
}
{
    "opt": "unary=22",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(8, 8)\n\n    def forward(self, x):\n        v1 = self.linear(x)\n        v2 = torch.tanh(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx = torch.randn(1, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(8, 8)\n \n    def forward(self, x2):\n        v7 = self.linear(x2)\n        v8 = torch.tanh(v7)\n        return v8\n\n\n# Initializing the model\nm2 = Model()\n\n# Inputs to the model\nx2 = torch.randn(1, 8)\n__output__2 = m2(x2)",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.tanh(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(8, 4, bias=True)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.tanh(v1)\n        return y1\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(16, 1)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.tanh(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(128, 256)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        t1 = torch.nn.Linear(16, 32)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.tanh(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(4, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(224, 224)\n \n    def forward(self, x1): \n        v1 = self.linear(x1)\n        v2 = torch.tanh(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 224, 224)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(16, 64)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.tanh(v1)\n        return v2\n \n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(2, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.tanh(v1)\n        return v2\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(8, 32)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.tanh(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 8)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(8, 8)\n\n    def forward(self, x):\n        v1 = self.linear(x)\n        v2 = torch.tanh(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx = torch.randn(1, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(8, 8)\n \n    def forward(self, x2):\n        v7 = self.linear(x2)\n        v8 = torch.tanh(v7)\n        return v8\n\n\n# Initializing the model\nm2 = Model()\n\n# Inputs to the model\nx2 = torch.randn(1, 8)\n__output__2 = m2(x2)",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.tanh(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(8, 4, bias=True)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.tanh(v1)\n        return y1\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(16, 1)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.tanh(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(128, 256)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        t1 = torch.nn.Linear(16, 32)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.tanh(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(4, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(224, 224)\n \n    def forward(self, x1): \n        v1 = self.linear(x1)\n        v2 = torch.tanh(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 224, 224)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(16, 64)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.tanh(v1)\n        return v2\n \n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(2, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.tanh(v1)\n        return v2\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(8, 32)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = torch.tanh(v1)\n        return v2\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 8)\n"
            ],
            "g_time": 4.311503171920776
        }
    }
}
{
    "opt": "binary=0",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 17, 1, stride=1, padding=0)\n    def forward(self, x1, other=None, padding1=None):\n        v1 = self.conv(x1)\n        if other == None:\n            other = V(torch.rand(v1.shape))\n        v2 = v1 + other\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 3, 5, 5)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(15, 43, 5, stride=2, padding=0)\n        self.conv2 = torch.nn.Conv2d(43, 66, 3, stride=1, padding=0, dilation=2)\n    def forward(self, x1, other=None, padding1=None):\n        v1 = self.conv1(x1)\n        v2 = self.conv2(v1)\n        if other == None:\n            other = torch.randn(v2.shape)\n        v3 = v2 + other\n        return v3\n# Inputs to the model\nx1 = torch.randn(5, 15, 24, 18)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(66, 3, 1, stride=1, padding=1)\n    def forward(self, x1, other=None, padding1=None):\n        v1 = self.conv(x1)\n        if other == None:\n            other = torch.randn(v1.shape)\n        v2 = v1 + other\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 66, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(66, 92, 1, stride=1, padding=0)\n        self.conv2 = torch.nn.Conv2d(15, 34, 1, stride=1, padding=0)\n    def forward(self, x1, other=None, padding1=None):\n        v1 = self.conv1(x1)\n        v2 = self.conv2(x1)\n        v3 = v1 + v2\n        if other == None:\n            other = torch.randn(v1.shape)\n        v4 = v3 + other\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 66, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(66, 2, 1, stride=1, padding=0)\n    def forward(self, x1, other=torch.randn(1, 2, 18, 18), padding1=None):\n        v1 = self.conv(x1)\n        v2 = v1 + other\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 66, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 64, 1, stride=1, padding=1)\n    def forward(self, x1, other=0, padding1=None):\n        v1 = self.conv1(x1)\n        v2 = v1 + other\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(32, 64, 3, stride=1, padding=0)\n    def forward(self, x1, padding1=False):\n        v1 = self.conv(x1)\n        if padding1 == True:\n            padding1 = torch.randn(v1.shape)\n        v2 = v1 + 0\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 32, 64, 64)\nx2 = torch.randn(x1.shape)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(26, 7, 1, stride=1, padding=2)\n    def forward(self, x1, other=0, padding1=0):\n        v1 = self.conv(x1)\n        if padding1 == 0:\n            padding1 = torch.randn(v1.shape)\n        v2 = v1 + other\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 26, 16, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.avgpool = torch.nn.AvgPool2d(3, stride=1, padding=1)\n        self.conv = torch.nn.Conv2d(64, 2, 1, stride=1, padding=0)\n    def forward(self, x1, other=None, padding1=None):\n        v1 = self.avgpool(x1)\n        if other == None:\n            other = torch.randn(v1.shape)\n        v2 = self.conv(v1)\n        v3 = v2 + other\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 64, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(57, 64, 1, stride=1, padding=0)\n        self.add = torch.nn.add\n    def forward(self, x1, other=0, padding1=None):\n        v1 = self.conv(x1)\n        v2 = self.add(v1, other)\n        return v2\n# Inputs to the model\nx1 = torch.randn(3, 57, 64, 64)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(3, 17, 1, stride=1, padding=0)\n    def forward(self, x1, other=None, padding1=None):\n        v1 = self.conv(x1)\n        if other == None:\n            other = V(torch.rand(v1.shape))\n        v2 = v1 + other\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 3, 5, 5)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(15, 43, 5, stride=2, padding=0)\n        self.conv2 = torch.nn.Conv2d(43, 66, 3, stride=1, padding=0, dilation=2)\n    def forward(self, x1, other=None, padding1=None):\n        v1 = self.conv1(x1)\n        v2 = self.conv2(v1)\n        if other == None:\n            other = torch.randn(v2.shape)\n        v3 = v2 + other\n        return v3\n# Inputs to the model\nx1 = torch.randn(5, 15, 24, 18)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(66, 3, 1, stride=1, padding=1)\n    def forward(self, x1, other=None, padding1=None):\n        v1 = self.conv(x1)\n        if other == None:\n            other = torch.randn(v1.shape)\n        v2 = v1 + other\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 66, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(66, 92, 1, stride=1, padding=0)\n        self.conv2 = torch.nn.Conv2d(15, 34, 1, stride=1, padding=0)\n    def forward(self, x1, other=None, padding1=None):\n        v1 = self.conv1(x1)\n        v2 = self.conv2(x1)\n        v3 = v1 + v2\n        if other == None:\n            other = torch.randn(v1.shape)\n        v4 = v3 + other\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 66, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(66, 2, 1, stride=1, padding=0)\n    def forward(self, x1, other=torch.randn(1, 2, 18, 18), padding1=None):\n        v1 = self.conv(x1)\n        v2 = v1 + other\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 66, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 64, 1, stride=1, padding=1)\n    def forward(self, x1, other=0, padding1=None):\n        v1 = self.conv1(x1)\n        v2 = v1 + other\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(32, 64, 3, stride=1, padding=0)\n    def forward(self, x1, padding1=False):\n        v1 = self.conv(x1)\n        if padding1 == True:\n            padding1 = torch.randn(v1.shape)\n        v2 = v1 + 0\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 32, 64, 64)\nx2 = torch.randn(x1.shape)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(26, 7, 1, stride=1, padding=2)\n    def forward(self, x1, other=0, padding1=0):\n        v1 = self.conv(x1)\n        if padding1 == 0:\n            padding1 = torch.randn(v1.shape)\n        v2 = v1 + other\n        return v2\n# Inputs to the model\nx1 = torch.randn(1, 26, 16, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.avgpool = torch.nn.AvgPool2d(3, stride=1, padding=1)\n        self.conv = torch.nn.Conv2d(64, 2, 1, stride=1, padding=0)\n    def forward(self, x1, other=None, padding1=None):\n        v1 = self.avgpool(x1)\n        if other == None:\n            other = torch.randn(v1.shape)\n        v2 = self.conv(v1)\n        v3 = v2 + other\n        return v3\n# Inputs to the model\nx1 = torch.randn(1, 64, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(57, 64, 1, stride=1, padding=0)\n        self.add = torch.nn.add\n    def forward(self, x1, other=0, padding1=None):\n        v1 = self.conv(x1)\n        v2 = self.add(v1, other)\n        return v2\n# Inputs to the model\nx1 = torch.randn(3, 57, 64, 64)\n"
            ],
            "g_time": 6.951510190963745
        }
    }
}
{
    "opt": "unary=15",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.prelu1 = torch.nn.PReLU()\n        self.prelu2 = torch.nn.PReLU()\n        self.prelu3 = torch.nn.PReLU()\n        self.prelu4 = torch.nn.PReLU()\n        self.prelu5 = torch.nn.PReLU()\n        self.prelu6 = torch.nn.PReLU()\n        self.prelu7 = torch.nn.PReLU()\n    def forward(self, x1):\n        v1 = self.prelu1(x1)\n        v2 = self.prelu2(v1)\n        v3 = self.prelu3(v2)\n        v4 = self.prelu4(v3)\n        v5 = self.prelu5(v4)\n        v6 = self.prelu6(v5)\n        v7 = self.prelu7(v6)\n        return v7\n# Inputs to the model\nx1 = torch.randn(1, 3, 224, 224)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 6, 5, stride=1, padding=0)\n        self.conv2 = torch.nn.Conv2d(6, 16, 5, stride=1, padding=0)\n        self.conv3 = torch.nn.Conv2d(16, 120, 5, stride=1, padding=0)\n        self.conv4 = torch.nn.Conv2d(120, 84, 5, stride=1, padding=0)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = torch.relu(v1)\n        v3 = torch.max_pool2d(v2, kernel_size=2, stride=2, padding=0)\n        v4 = self.conv2(v3)\n        v5 = torch.relu(v4)\n        v6 = torch.max_pool2d(v5, kernel_size=2, stride=2, padding=0)\n        v7 = self.conv3(v6)\n        v8 = torch.sigmoid(v7)\n        v9 = self.conv4(v8)\n        v10 = torch.sigmoid(v9)\n        return v10\n# Inputs to the model\nx1 = torch.randn(1, 3, 28, 28)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear0 = torch.nn.Linear(7, 5, bias=True)\n        self.linear1 = torch.nn.Linear(5, 32, bias=True)\n        self.linear2 = torch.nn.Linear(32, 55, bias=True)\n        self.linear3 = torch.nn.Linear(55, 120, bias=True)\n        self.linear4 = torch.nn.Linear(120, 28, bias=True)\n    def forward(self, x0):\n            v0 = x0.permute(0, 2, 3, 1)\n            v2 = self.linear0(v0)\n            v1 = x0.permute(0, 2, 3, 1)\n            v3 = self.linear1(v1)\n            v5 = self.linear2(v3)\n            v4 = x0.permute(0, 2, 3, 1)\n            v6 = self.linear3(v4)\n            v8 = self.linear4(v6)\n            return v8\n# Inputs to the model\nx0 = torch.randn(1, 7, 9, 9)\n",
                "\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.Conv1 = Conv(6, 256, 1, 1, 0, bias=False)\n        self.Conv2 = Conv(1, 64, 7, 2, 3, groups=1, bias=False)\n    def forward(self, x1):\n        t0 = x1\n        t1 = self.Conv1.forward(t0)\n        t2 = torch.sigmoid(t1)\n        t3 = self.Conv2.forward(t2)\n        t4 = torch.sigmoid(t3)\n        return t4\n# Inputs to the model\nx1 = torch.randn(1, 1, 32, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 3, 1, stride=1, padding=0)\n        self.conv2 = torch.nn.Conv2d(3, 3, 1, stride=1, padding=0)\n        self.conv3 = torch.nn.Conv2d(3, 3, 1, stride=1, padding=0)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = torch.relu(v1)\n        v3 = self.conv2(v2)\n        v4 = torch.relu(v3)\n        v5 = self.conv3(v4)\n        v6 = torch.relu(v5)\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 3, 28, 28)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(1, 16, 5, stride=1, padding=0)\n        self.conv2 = torch.nn.Conv2d(16, 6, 5, stride=1, padding=0)\n        self.conv3 = torch.nn.Conv2d(6, 120, 5, stride=1, padding=0)\n        self.conv4 = torch.nn.Conv2d(120, 84, 5, stride=1, padding=0)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = torch.relu(v1)\n        v3 = self.conv2(v2)\n        v4 = torch.relu(v3)\n        v5 = self.conv3(v4)\n        v6 = torch.sigmoid(v5)\n        v7 = self.conv4(v6)\n        v8 = torch.sigmoid(v7)\n        return v8\n# Inputs to the model\nx1 = torch.randn(1, 1, 28, 28)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(1, 6, 5, stride=1, padding=2)\n        self.conv2 = torch.nn.Conv2d(6, 16, 5, stride=1, padding=0)\n        self.conv3 = torch.nn.Conv2d(16, 120, 5, stride=1, padding=0)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = torch.avg_pool2d(v1, 2, stride=2, padding=0, ceil_mode=False)\n        v3 = self.conv2(v2)\n        v4 = torch.flatten(v3, 1)\n        v5 = self.conv3(v4)\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 1, 28, 28)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(23, 19, 13, stride=5, padding=0)\n        self.conv2 = torch.nn.Conv2d(17, 47, 13, stride=7, padding=0)\n        self.conv3 = torch.nn.Conv2d(82, 47, 13, stride=15, padding=0)\n        self.conv4 = torch.nn.Conv2d(85, 46, 13, stride=1, padding=0)\n        self.conv5 = torch.nn.Conv2d(86, 34, 13, stride=1, padding=0)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = self.conv2(v2)\n        v4 = torch.sigmoid(v3)\n        v5 = self.conv3(v4)\n        v6 = torch.sigmoid(v5)\n        v7 = self.conv4(v6)\n        v8 = torch.sigmoid(v7)\n        v9 = self.conv5(v8)\n        return v9\n# Inputs to the model\nx1 = torch.randn(1, 23, 32, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 8, 5, stride=1, padding=2)\n        self.conv2 = torch.nn.Conv2d(8, 8, 5, stride=1, padding=2)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = torch.relu(v1)\n        v3 = self.conv2(v2)\n        v4 = torch.sigmoid(v3)\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 3, 32, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(1, 6, 5, stride=1, padding=0)\n        self.conv2 = torch.nn.Conv2d(6, 16, 5, stride=1, padding=0)\n        self.conv3 = torch.nn.Conv2d(16, 120, 5, stride=1, padding=0)\n        self.conv4 = torch.nn.Conv2d(120, 84, 5, stride=1, padding=0)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = torch.relu(v1)\n        v3 = self.conv2(v2)\n        v4 = torch.sigmoid(v3)\n        v5 = self.conv3(v4)\n        v6 = torch.sigmoid(v5)\n        v7 = self.conv4(v6)\n        return v7\n# Inputs to the model\nx1 = torch.randn(1, 1, 28, 28)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.prelu1 = torch.nn.PReLU()\n        self.prelu2 = torch.nn.PReLU()\n        self.prelu3 = torch.nn.PReLU()\n        self.prelu4 = torch.nn.PReLU()\n        self.prelu5 = torch.nn.PReLU()\n        self.prelu6 = torch.nn.PReLU()\n        self.prelu7 = torch.nn.PReLU()\n    def forward(self, x1):\n        v1 = self.prelu1(x1)\n        v2 = self.prelu2(v1)\n        v3 = self.prelu3(v2)\n        v4 = self.prelu4(v3)\n        v5 = self.prelu5(v4)\n        v6 = self.prelu6(v5)\n        v7 = self.prelu7(v6)\n        return v7\n# Inputs to the model\nx1 = torch.randn(1, 3, 224, 224)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 6, 5, stride=1, padding=0)\n        self.conv2 = torch.nn.Conv2d(6, 16, 5, stride=1, padding=0)\n        self.conv3 = torch.nn.Conv2d(16, 120, 5, stride=1, padding=0)\n        self.conv4 = torch.nn.Conv2d(120, 84, 5, stride=1, padding=0)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = torch.relu(v1)\n        v3 = torch.max_pool2d(v2, kernel_size=2, stride=2, padding=0)\n        v4 = self.conv2(v3)\n        v5 = torch.relu(v4)\n        v6 = torch.max_pool2d(v5, kernel_size=2, stride=2, padding=0)\n        v7 = self.conv3(v6)\n        v8 = torch.sigmoid(v7)\n        v9 = self.conv4(v8)\n        v10 = torch.sigmoid(v9)\n        return v10\n# Inputs to the model\nx1 = torch.randn(1, 3, 28, 28)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear0 = torch.nn.Linear(7, 5, bias=True)\n        self.linear1 = torch.nn.Linear(5, 32, bias=True)\n        self.linear2 = torch.nn.Linear(32, 55, bias=True)\n        self.linear3 = torch.nn.Linear(55, 120, bias=True)\n        self.linear4 = torch.nn.Linear(120, 28, bias=True)\n    def forward(self, x0):\n            v0 = x0.permute(0, 2, 3, 1)\n            v2 = self.linear0(v0)\n            v1 = x0.permute(0, 2, 3, 1)\n            v3 = self.linear1(v1)\n            v5 = self.linear2(v3)\n            v4 = x0.permute(0, 2, 3, 1)\n            v6 = self.linear3(v4)\n            v8 = self.linear4(v6)\n            return v8\n# Inputs to the model\nx0 = torch.randn(1, 7, 9, 9)\n",
                "\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.Conv1 = Conv(6, 256, 1, 1, 0, bias=False)\n        self.Conv2 = Conv(1, 64, 7, 2, 3, groups=1, bias=False)\n    def forward(self, x1):\n        t0 = x1\n        t1 = self.Conv1.forward(t0)\n        t2 = torch.sigmoid(t1)\n        t3 = self.Conv2.forward(t2)\n        t4 = torch.sigmoid(t3)\n        return t4\n# Inputs to the model\nx1 = torch.randn(1, 1, 32, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 3, 1, stride=1, padding=0)\n        self.conv2 = torch.nn.Conv2d(3, 3, 1, stride=1, padding=0)\n        self.conv3 = torch.nn.Conv2d(3, 3, 1, stride=1, padding=0)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = torch.relu(v1)\n        v3 = self.conv2(v2)\n        v4 = torch.relu(v3)\n        v5 = self.conv3(v4)\n        v6 = torch.relu(v5)\n        return v6\n# Inputs to the model\nx1 = torch.randn(1, 3, 28, 28)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(1, 16, 5, stride=1, padding=0)\n        self.conv2 = torch.nn.Conv2d(16, 6, 5, stride=1, padding=0)\n        self.conv3 = torch.nn.Conv2d(6, 120, 5, stride=1, padding=0)\n        self.conv4 = torch.nn.Conv2d(120, 84, 5, stride=1, padding=0)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = torch.relu(v1)\n        v3 = self.conv2(v2)\n        v4 = torch.relu(v3)\n        v5 = self.conv3(v4)\n        v6 = torch.sigmoid(v5)\n        v7 = self.conv4(v6)\n        v8 = torch.sigmoid(v7)\n        return v8\n# Inputs to the model\nx1 = torch.randn(1, 1, 28, 28)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(1, 6, 5, stride=1, padding=2)\n        self.conv2 = torch.nn.Conv2d(6, 16, 5, stride=1, padding=0)\n        self.conv3 = torch.nn.Conv2d(16, 120, 5, stride=1, padding=0)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = torch.avg_pool2d(v1, 2, stride=2, padding=0, ceil_mode=False)\n        v3 = self.conv2(v2)\n        v4 = torch.flatten(v3, 1)\n        v5 = self.conv3(v4)\n        return v5\n# Inputs to the model\nx1 = torch.randn(1, 1, 28, 28)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(23, 19, 13, stride=5, padding=0)\n        self.conv2 = torch.nn.Conv2d(17, 47, 13, stride=7, padding=0)\n        self.conv3 = torch.nn.Conv2d(82, 47, 13, stride=15, padding=0)\n        self.conv4 = torch.nn.Conv2d(85, 46, 13, stride=1, padding=0)\n        self.conv5 = torch.nn.Conv2d(86, 34, 13, stride=1, padding=0)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = torch.sigmoid(v1)\n        v3 = self.conv2(v2)\n        v4 = torch.sigmoid(v3)\n        v5 = self.conv3(v4)\n        v6 = torch.sigmoid(v5)\n        v7 = self.conv4(v6)\n        v8 = torch.sigmoid(v7)\n        v9 = self.conv5(v8)\n        return v9\n# Inputs to the model\nx1 = torch.randn(1, 23, 32, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(3, 8, 5, stride=1, padding=2)\n        self.conv2 = torch.nn.Conv2d(8, 8, 5, stride=1, padding=2)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = torch.relu(v1)\n        v3 = self.conv2(v2)\n        v4 = torch.sigmoid(v3)\n        return v4\n# Inputs to the model\nx1 = torch.randn(1, 3, 32, 32)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(1, 6, 5, stride=1, padding=0)\n        self.conv2 = torch.nn.Conv2d(6, 16, 5, stride=1, padding=0)\n        self.conv3 = torch.nn.Conv2d(16, 120, 5, stride=1, padding=0)\n        self.conv4 = torch.nn.Conv2d(120, 84, 5, stride=1, padding=0)\n    def forward(self, x1):\n        v1 = self.conv1(x1)\n        v2 = torch.relu(v1)\n        v3 = self.conv2(v2)\n        v4 = torch.sigmoid(v3)\n        v5 = self.conv3(v4)\n        v6 = torch.sigmoid(v5)\n        v7 = self.conv4(v6)\n        return v7\n# Inputs to the model\nx1 = torch.randn(1, 1, 28, 28)\n"
            ],
            "g_time": 12.651797771453857
        }
    }
}
{
    "opt": "unary=4",
    "response": {
        "0": {
            "raw": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Linear(64, 64)\n \n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(15, 20)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 15)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(16, 32)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(16, 32)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(8, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(8, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(512, 10)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 512)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.tanh(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 1)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(16, 16)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 16)\n"
            ],
            "code": [
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = torch.nn.Linear(64, 64)\n \n    def forward(self, x1):\n        v1 = self.conv(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(15, 20)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 15)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(16, 32)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(16, 32)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 16)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(8, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3, 64, 64)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(8, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 8)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(512, 10)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 512)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 8)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.tanh(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 1)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 3)\n",
                "\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(16, 16)\n \n    def forward(self, x1):\n        v1 = self.linear(x1)\n        v2 = v1 * 0.5\n        v3 = v1 * 0.7071067811865476\n        v4 = torch.erf(v3)\n        v5 = v4 + 1\n        v6 = v2 * v5\n        return v6\n\n# Initializing the model\nm = Model()\n\n# Inputs to the model\nx1 = torch.randn(1, 16)\n"
            ],
            "g_time": 6.672938108444214
        }
    }
}

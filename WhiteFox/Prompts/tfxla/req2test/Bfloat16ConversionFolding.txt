### Please generate one valid TensorFlow model that satisfies requirements below.
You should only use public TensorFlow APIs. The model can be used as the input to trigger the optimization pass `ReshapeReshapeForwarding` in TensorFlow XLA.

# Description
The model should contain the following pattern:
```
t1 = tf.reshape(input_tensor, ...)
t2 = tf.reshape(t1, input_tensor.shape)
```
The pattern describes that there are two reshape operators in the model. The first `reshape` operator transforms a tensor input `input_tensor` from `input_tensor.shape` to any new shape, and the second `reshape` operator transforms the output of first `reshape` back to `input_tensor.shape`.


# Model
class Model(tf.keras.Model):

  def __init__(self):
    super(Model, self).__init__()

  def call(self, x1):
    x2 = tf.reshape(x1, [2,2])
    return tf.reshape(x2, [4])

# Initializing the model
m = Model()

# Inputs to the model
input_shape = [4]
x1 = tf.constant([4.,5.,6.,7.], shape=input_shape)

# Call model
y = m(x1)


### Please generate one valid TensorFlow model that satisfies requirements below.
You should only use public TensorFlow APIs. The model can be used as the input to trigger the optimization pass `Bfloat16ConversionFolding` in TensorFlow XLA.

# Description
The model should contain the following pattern:

```python
t1 = tf.cast(input_tensor, tf.bfloat16)
t2 = tf.cast(t1, tf.float32)
```

The pattern describes that there are two cast operators in the model. The first `cast` operator transforms a tensor input `input_tensor` from its original data type to `bfloat16`, and the second `cast` operator transforms the output of the first `cast` back to `float32`.

The optimization pass `Bfloat16ConversionFolding` is triggered when all the users of a `float32` tensor are converting it to `bfloat16`. The optimization pass will fold the conversions by changing the data type of the tensor to `bfloat16` and replacing all uses of the conversion with the tensor itself. 

The optimization pass also folds the conversions of the operands of an instruction if the operand is a conversion from `bfloat16` to `float32` and the instruction supports `bfloat16` input directly at the operand index. The conversion is folded by replacing the operand with the input of the conversion.

The optimization pass does not fold `bfloat16` conversions for instructions related to tuples, entry and exit of a computation, fusion, convert, side-effecting instructions, in-place operations, and control flow. If the instruction is the root instruction, folding can only happen when it supports mixed precision so that its operands can be changed.

# Model
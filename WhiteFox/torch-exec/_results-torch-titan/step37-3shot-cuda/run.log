Start testing
Successfully create atemp.py

LmfuzzTestcase 0 addmm addmm_1 JIT_FAIL 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 1 addmm addmm_10 Success 420 succeed
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 2 addmm addmm_2 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 3 addmm addmm_3 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 4 addmm addmm_4 Success 420 succeed
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 5 addmm addmm_5 Success 420 succeed
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 6 addmm addmm_6 Success 420 succeed
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 7 addmm addmm_7 Success 420 succeed
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 8 addmm addmm_8 Success 420 succeed
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 9 addmm addmm_9 Success 420 succeed
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 10 binary=0 binary=0_1 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 11 binary=0 binary=0_10 JIT_FAIL 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 12 binary=0 binary=0_2 JIT_FAIL 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 13 binary=0 binary=0_3 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 14 binary=0 binary=0_4 JIT_FAIL 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 15 binary=0 binary=0_5 JIT_FAIL 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 16 binary=0 binary=0_6 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 17 binary=0 binary=0_7 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 18 binary=0 binary=0_8 JIT_FAIL 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 19 binary=0 binary=0_9 JIT_FAIL 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 20 binary=10 binary=10_1 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 21 binary=10 binary=10_10 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 22 binary=10 binary=10_2 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 23 binary=10 binary=10_3 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 24 binary=10 binary=10_4 JIT_FAIL 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 25 binary=10 binary=10_5 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 26 binary=10 binary=10_6 JIT_FAIL 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 27 binary=10 binary=10_7 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 28 binary=10 binary=10_8 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 29 binary=10 binary=10_9 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 30 binary=2 binary=2_1 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 31 binary=2 binary=2_10 JIT_FAIL 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 32 binary=2 binary=2_2 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 33 binary=2 binary=2_3 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 34 binary=2 binary=2_4 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 35 binary=2 binary=2_5 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 36 binary=2 binary=2_6 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 37 binary=2 binary=2_7 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 38 binary=2 binary=2_8 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 39 binary=2 binary=2_9 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 40 binary=4 binary=4_1 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 41 binary=4 binary=4_10 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 42 binary=4 binary=4_2 JIT_FAIL 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 43 binary=4 binary=4_3 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 44 binary=4 binary=4_4 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 45 binary=4 binary=4_5 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 46 binary=4 binary=4_6 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 47 binary=4 binary=4_7 invalid syntax 420 ('<unknown>', 5, 46, '        self.linear = torch.nn.Linear(32, 8) 8 is a feature length.\n')
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 48 binary=4 binary=4_8 JIT_FAIL 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 49 binary=4 binary=4_9 JIT_FAIL 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 50 binary=6 binary=6_1 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 51 binary=6 binary=6_10 JIT_FAIL 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 52 binary=6 binary=6_2 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 53 binary=6 binary=6_3 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 54 binary=6 binary=6_4 JIT_FAIL 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 55 binary=6 binary=6_5 JIT_FAIL 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 56 binary=6 binary=6_6 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 57 binary=6 binary=6_7 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 58 binary=6 binary=6_8 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 59 binary=6 binary=6_9 unexpected indent 420 ('<unknown>', 1, 1, ' and inputs\n')
----------------------------------


LmfuzzTestcase 60 binary=8 binary=8_1 unexpected indent 420 ('<unknown>', 1, 1, ' with dynamic shape\n')
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 61 binary=8 binary=8_10 JIT_FAIL 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 62 binary=8 binary=8_2 JIT_FAIL 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 63 binary=8 binary=8_3 JIT_FAIL 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 64 binary=8 binary=8_4 JIT_FAIL 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 65 binary=8 binary=8_5 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 66 binary=8 binary=8_6 JIT_FAIL 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 67 binary=8 binary=8_7 JIT_FAIL 420 Catch
----------------------------------

Successfully create atemp.py
out_channels must be divisible by groups
import os
import torch
import torch.nn.functional as F
import torch.nn as nn
import numpy as np
from torch.autograd import Variable
import math
import torch as th
import torch.linalg as la
from torch.nn import Parameter
import torch.linalg as linalg



class Model(torch.nn.Module):

    def __init__(self):
        super().__init__()
        self.conv1 = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1, groups=3)

    def forward(self, x):
        v1 = self.conv1(x)
        return v1




func = Model().to('cuda')



x = torch.randn(1, 3, 64, 64)


test_inputs = [x]


LmfuzzTestcase 68 binary=8 binary=8_8 SKIP 420 void
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 69 binary=8 binary=8_9 JIT_FAIL 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 70 binary_unary=0 binary_unary=0_1 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 71 binary_unary=0 binary_unary=0_10 JIT_FAIL 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 72 binary_unary=0 binary_unary=0_2 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 73 binary_unary=0 binary_unary=0_3 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 74 binary_unary=0 binary_unary=0_4 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 75 binary_unary=0 binary_unary=0_5 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 76 binary_unary=0 binary_unary=0_6 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 77 binary_unary=0 binary_unary=0_7 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 78 binary_unary=0 binary_unary=0_8 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 79 binary_unary=0 binary_unary=0_9 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 80 binary_unary=10 binary_unary=10_1 JIT_FAIL 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 81 binary_unary=10 binary_unary=10_10 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 82 binary_unary=10 binary_unary=10_2 JIT_FAIL 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 83 binary_unary=10 binary_unary=10_3 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 84 binary_unary=10 binary_unary=10_4 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 85 binary_unary=10 binary_unary=10_5 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 86 binary_unary=10 binary_unary=10_6 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 87 binary_unary=10 binary_unary=10_7 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 88 binary_unary=10 binary_unary=10_8 JIT_FAIL 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 89 binary_unary=10 binary_unary=10_9 JIT_FAIL 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 90 binary_unary=2 binary_unary=2_1 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 91 binary_unary=2 binary_unary=2_10 JIT_FAIL 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 92 binary_unary=2 binary_unary=2_2 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py
__init__() got multiple values for argument 'stride'
import os
import torch
import torch.nn.functional as F
import torch.nn as nn
import numpy as np
from torch.autograd import Variable
import math
import torch as th
import torch.linalg as la
from torch.nn import Parameter
import torch.linalg as linalg



class Model(torch.nn.Module):

    def __init__(self):
        super().__init__()
        self.maxpool = torch.nn.MaxPool2d(3, 2, stride=2, padding=1)

    def forward(self, x1):
        v1 = self.maxpool(x1)
        v2 = (v1 - 100)
        v3 = F.relu(v2)
        return v3




func = Model().to('cuda')



x1 = torch.randn(1, 3, 64, 64)


test_inputs = [x1]


LmfuzzTestcase 93 binary_unary=2 binary_unary=2_3 SKIP 420 void
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 94 binary_unary=2 binary_unary=2_4 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 95 binary_unary=2 binary_unary=2_5 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 96 binary_unary=2 binary_unary=2_6 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 97 binary_unary=2 binary_unary=2_7 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 98 binary_unary=2 binary_unary=2_8 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 99 binary_unary=2 binary_unary=2_9 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 100 binary_unary=4 binary_unary=4_1 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 101 binary_unary=4 binary_unary=4_10 invalid syntax 420 ('<unknown>', 9, 18, '        __return t2__\n')
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 102 binary_unary=4 binary_unary=4_2 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 103 binary_unary=4 binary_unary=4_3 JIT_FAIL 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 104 binary_unary=4 binary_unary=4_4 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 105 binary_unary=4 binary_unary=4_5 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 106 binary_unary=4 binary_unary=4_6 JIT_FAIL 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 107 binary_unary=4 binary_unary=4_7 JIT_FAIL 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 108 binary_unary=4 binary_unary=4_8 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 109 binary_unary=4 binary_unary=4_9 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 110 binary_unary=6 binary_unary=6_1 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 111 binary_unary=6 binary_unary=6_10 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 112 binary_unary=6 binary_unary=6_2 JIT_FAIL 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 113 binary_unary=6 binary_unary=6_3 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 114 binary_unary=6 binary_unary=6_4 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 115 binary_unary=6 binary_unary=6_5 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 116 binary_unary=6 binary_unary=6_6 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 117 binary_unary=6 binary_unary=6_7 JIT_FAIL 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 118 binary_unary=6 binary_unary=6_8 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 119 binary_unary=6 binary_unary=6_9 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 120 binary_unary=8 binary_unary=8_1 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 121 binary_unary=8 binary_unary=8_10 JIT_FAIL 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 122 binary_unary=8 binary_unary=8_2 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 123 binary_unary=8 binary_unary=8_3 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py
unsupported operand type(s) for %: 'tuple' and 'int'
import os
import torch
import torch.nn.functional as F
import torch.nn as nn
import numpy as np
from torch.autograd import Variable
import math
import torch as th
import torch.linalg as la
from torch.nn import Parameter
import torch.linalg as linalg



class Model(torch.nn.Module):

    def __init__(self):
        super().__init__()
        self.conv1 = torch.nn.Conv2d(1, 8, 1, stride=1, padding=1)
        self.conv2 = torch.nn.Conv2d((1, 8, 8), 7, 1, stride=1, padding=1)

    def forward(self, x1):
        v1 = self.conv1(x1)
        v2 = self.conv2(x1)
        v3 = (v1 + v2)
        v4 = torch.relu(v3)
        return v4




func = Model().to('cuda')



x1 = torch.randn(1, 1, 64, 64)


test_inputs = [x1]


LmfuzzTestcase 124 binary_unary=8 binary_unary=8_4 SKIP 420 void
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 125 binary_unary=8 binary_unary=8_5 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 126 binary_unary=8 binary_unary=8_6 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 127 binary_unary=8 binary_unary=8_7 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 128 binary_unary=8 binary_unary=8_8 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 129 binary_unary=8 binary_unary=8_9 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 130 cat_addmm cat_addmm_1 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 131 cat_addmm cat_addmm_10 JIT_FAIL 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 132 cat_addmm cat_addmm_2 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 133 cat_addmm cat_addmm_3 unexpected EOF while parsing 420 ('<unknown>', 3, 5, '    \n')
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 134 cat_addmm cat_addmm_4 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 135 cat_addmm cat_addmm_5 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 136 cat_addmm cat_addmm_6 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 137 cat_addmm cat_addmm_7 Success 420 succeed
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 138 cat_addmm cat_addmm_8 JIT_FAIL 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 139 cat_addmm cat_addmm_9 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py
Successfully create atemp.py

LmfuzzTestcase 150 cat_slice_cat cat_slice_cat_1 JIT_FAIL 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 151 cat_slice_cat cat_slice_cat_10 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 152 cat_slice_cat cat_slice_cat_2 JIT_FAIL 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 153 cat_slice_cat cat_slice_cat_3 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 154 cat_slice_cat cat_slice_cat_4 JIT_FAIL 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 155 cat_slice_cat cat_slice_cat_5 JIT_FAIL 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 156 cat_slice_cat cat_slice_cat_6 JIT_FAIL 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 157 cat_slice_cat cat_slice_cat_7 JIT_FAIL 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 158 cat_slice_cat cat_slice_cat_8 JIT_FAIL 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 159 cat_slice_cat cat_slice_cat_9 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 160 fuse_conv_bn fuse_conv_bn_1 JIT_FAIL 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 161 fuse_conv_bn fuse_conv_bn_10 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py
__init__() missing 1 required positional argument: 'kernel_size'
import os
import torch
import torch.nn.functional as F
import torch.nn as nn
import numpy as np
from torch.autograd import Variable
import math
import torch as th
import torch.linalg as la
from torch.nn import Parameter
import torch.linalg as linalg



class Model(torch.nn.Module):

    def __init__(self):
        super().__init__()
        conv = torch.nn.Conv1d
        bn = torch.nn.BatchNorm1d
        relu = torch.nn.ReLU(inplace=True)
        self.feature = torch.nn.Sequential(conv(11, 3, padding=4, stride=2), conv(3, 64, kernel_size=3, stride=1, padding=1), conv(64, 64, stride=2), conv(64, 64, groups=64, kernel_size=3, padding=1, stride=1), bn(64, momentum=0.5), relu, conv(64, 64, groups=64, kernel_size=3, padding=1), conv(64, 64, groups=64, kernel_size=3, padding=1), conv(64, 64, groups=64, kernel_size=3, padding=1), bn(64, momentum=0.5), relu, conv(64, 96, groups=32, kernel_size=2, padding=1), bn(96, momentum=0.5), relu)
        self.classifier = torch.nn.Sequential(conv(96, 256, kernel_size=2, padding=1), bn(256, momentum=0.5), relu, conv(256, 256, kernel_size=2, padding=1), conv(256, 256, kernel_size=1), bn(256, momentum=0.5), relu, conv(256, 20, kernel_size=2, padding=1))

    def forward(self, x):
        x = self.feature(x)
        x = self.classifier(x)
        return x




func = Model().to('cuda')



x = torch.randn(1, 11, 26)


test_inputs = [x]


LmfuzzTestcase 162 fuse_conv_bn fuse_conv_bn_2 SKIP 420 void
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 163 fuse_conv_bn fuse_conv_bn_3 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 164 fuse_conv_bn fuse_conv_bn_4 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py
module 'torch.nn' has no attribute 'ConvXd'
import os
import torch
import torch.nn.functional as F
import torch.nn as nn
import numpy as np
from torch.autograd import Variable
import math
import torch as th
import torch.linalg as la
from torch.nn import Parameter
import torch.linalg as linalg



class Model(torch.nn.Module):

    def __init__(self):
        super().__init__()
        conv = torch.nn.ConvXd
        bn = torch.nn.BatchNormXd
        self.conv1 = conv(3, 16, kernel_size=(1, 3), padding=(0, 1))
        self.bn1 = bn(16, eps=1e-05)

    def forward(self, x):
        x1 = self.conv1(x)
        x2 = self.bn1(x1)
        return x2




func = Model().to('cuda')



x = torch.randn(1, 3, 4, 8)


test_inputs = [x]


LmfuzzTestcase 165 fuse_conv_bn fuse_conv_bn_5 SKIP 420 void
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 166 fuse_conv_bn fuse_conv_bn_6 JIT_FAIL 420 Catch
----------------------------------

Successfully create atemp.py
'tuple' object has no attribute 'to'
import os
import torch
import torch.nn.functional as F
import torch.nn as nn
import numpy as np
from torch.autograd import Variable
import math
import torch as th
import torch.linalg as la
from torch.nn import Parameter
import torch.linalg as linalg



func = ().to('cuda')



x = torch.randn(1, 4, 8, 8)


test_inputs = [x]


LmfuzzTestcase 167 fuse_conv_bn fuse_conv_bn_7 SKIP 420 void
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 168 fuse_conv_bn fuse_conv_bn_8 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 169 fuse_conv_bn fuse_conv_bn_9 JIT_FAIL 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 170 linear_permute_fusion linear_permute_fusion_1 JIT_FAIL 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 171 linear_permute_fusion linear_permute_fusion_10 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 172 linear_permute_fusion linear_permute_fusion_2 Success 420 succeed
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 173 linear_permute_fusion linear_permute_fusion_3 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 174 linear_permute_fusion linear_permute_fusion_4 JIT_FAIL 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 175 linear_permute_fusion linear_permute_fusion_5 JIT_FAIL 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 176 linear_permute_fusion linear_permute_fusion_6 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 177 linear_permute_fusion linear_permute_fusion_7 JIT_FAIL 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 178 linear_permute_fusion linear_permute_fusion_8 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 179 linear_permute_fusion linear_permute_fusion_9 Success 420 succeed
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 180 mm_plus_mm mm_plus_mm_1 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 181 mm_plus_mm mm_plus_mm_10 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 182 mm_plus_mm mm_plus_mm_2 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 183 mm_plus_mm mm_plus_mm_3 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 184 mm_plus_mm mm_plus_mm_4 JIT_FAIL 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 185 mm_plus_mm mm_plus_mm_5 Success 420 succeed
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 186 mm_plus_mm mm_plus_mm_6 Success 420 succeed
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 187 mm_plus_mm mm_plus_mm_7 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 188 mm_plus_mm mm_plus_mm_8 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 189 mm_plus_mm mm_plus_mm_9 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py
'Model' object has no attribute '_modules'
import os
import torch
import torch.nn.functional as F
import torch.nn as nn
import numpy as np
from torch.autograd import Variable
import math
import torch as th
import torch.linalg as la
from torch.nn import Parameter
import torch.linalg as linalg



class Model(torch.nn.Module):

    def __init__(self):
        super().__init__()
        self.linear = torch.nn.Linear(3, 3)

    def forward(self, x1):
        v1 = x1.permute(0, 2, 1)
        v2 = torch.nn.functional.linear(v1, self.linear.weight, self.linear.bias)
        v2 = torch.sign(v2)
        v3 = torch.min(v2, dim=(- 1))[1]
        x2 = torch.min(v3, dim=(- 1))[1]
        x3 = x2.unsqueeze(dim=(- 1))
        v3 = (v3 + x3.to(v3.dtype))
        v3 = torch.mean(v3.T)
        return (v1[0][0] == v3.item()).to(torch.float32)




class Model(torch.nn.Module):

    def __init__(self):
        pass

    def forward(self):
        m = torch.nn.Softmax()
        return m.weight




func = Model().to('cuda')



x1 = torch.randn(2, 2, 3)


test_inputs = [x1]


LmfuzzTestcase 190 permute_linear_fusion permute_linear_fusion_1 SKIP 420 void
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 191 permute_linear_fusion permute_linear_fusion_10 JIT_FAIL 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 192 permute_linear_fusion permute_linear_fusion_2 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 193 permute_linear_fusion permute_linear_fusion_3 JIT_FAIL 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 194 permute_linear_fusion permute_linear_fusion_4 JIT_FAIL 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 195 permute_linear_fusion permute_linear_fusion_5 JIT_FAIL 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 196 permute_linear_fusion permute_linear_fusion_6 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 197 permute_linear_fusion permute_linear_fusion_7 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 198 permute_linear_fusion permute_linear_fusion_8 JIT_FAIL 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 199 permute_linear_fusion permute_linear_fusion_9 JIT_FAIL 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 200 permute_matmul_fusion permute_matmul_fusion_1 Success 420 succeed
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 201 permute_matmul_fusion permute_matmul_fusion_10 Success 420 succeed
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 202 permute_matmul_fusion permute_matmul_fusion_2 Success 420 succeed
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 203 permute_matmul_fusion permute_matmul_fusion_3 Success 420 succeed
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 204 permute_matmul_fusion permute_matmul_fusion_4 Success 420 succeed
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 205 permute_matmul_fusion permute_matmul_fusion_5 Success 420 succeed
----------------------------------

Successfully create atemp.py
'tuple' object has no attribute 'to'
import os
import torch
import torch.nn.functional as F
import torch.nn as nn
import numpy as np
from torch.autograd import Variable
import math
import torch as th
import torch.linalg as la
from torch.nn import Parameter
import torch.linalg as linalg



func = ().to('cuda')

input_tensor = torch.randn(1, 1, 1)

test_inputs = [input_tensor]


LmfuzzTestcase 206 permute_matmul_fusion permute_matmul_fusion_6 SKIP 420 void
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 207 permute_matmul_fusion permute_matmul_fusion_7 Success 420 succeed
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 208 permute_matmul_fusion permute_matmul_fusion_8 Success 420 succeed
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 209 permute_matmul_fusion permute_matmul_fusion_9 Success 420 succeed
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 210 pointless_cumsum_replacement pointless_cumsum_replacement_1 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 211 pointless_cumsum_replacement pointless_cumsum_replacement_10 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 212 pointless_cumsum_replacement pointless_cumsum_replacement_2 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 213 pointless_cumsum_replacement pointless_cumsum_replacement_3 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 214 pointless_cumsum_replacement pointless_cumsum_replacement_4 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 215 pointless_cumsum_replacement pointless_cumsum_replacement_5 JIT_FAIL 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 216 pointless_cumsum_replacement pointless_cumsum_replacement_6 JIT_FAIL 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 217 pointless_cumsum_replacement pointless_cumsum_replacement_7 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 218 pointless_cumsum_replacement pointless_cumsum_replacement_8 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 219 pointless_cumsum_replacement pointless_cumsum_replacement_9 Success 420 succeed
----------------------------------

Successfully create atemp.py
'tuple' object has no attribute 'to'
import os
import torch
import torch.nn.functional as F
import torch.nn as nn
import numpy as np
from torch.autograd import Variable
import math
import torch as th
import torch.linalg as la
from torch.nn import Parameter
import torch.linalg as linalg



func = ().to('cuda')



x1 = torch.randn(2)


test_inputs = [x1]


LmfuzzTestcase 220 replace_fx replace_fx_1 SKIP 420 void
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 221 replace_fx replace_fx_10 JIT_FAIL 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 222 replace_fx replace_fx_2 JIT_FAIL 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 223 replace_fx replace_fx_3 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 224 replace_fx replace_fx_4 JIT_FAIL 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 225 replace_fx replace_fx_5 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 226 replace_fx replace_fx_6 JIT_FAIL 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 227 replace_fx replace_fx_7 JIT_FAIL 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 228 replace_fx replace_fx_8 JIT_FAIL 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 229 replace_fx replace_fx_9 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 230 sfdp=0 sfdp=0_1 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 231 sfdp=0 sfdp=0_10 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 232 sfdp=0 sfdp=0_2 Success 420 succeed
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 233 sfdp=0 sfdp=0_3 Success 420 succeed
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 234 sfdp=0 sfdp=0_4 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 235 sfdp=0 sfdp=0_5 Success 420 succeed
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 236 sfdp=0 sfdp=0_6 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 237 sfdp=0 sfdp=0_7 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 238 sfdp=0 sfdp=0_8 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 239 sfdp=0 sfdp=0_9 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 240 sfdp=1 sfdp=1_1 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py
name 'latent_dim' is not defined
import os
import torch
import torch.nn.functional as F
import torch.nn as nn
import numpy as np
from torch.autograd import Variable
import math
import torch as th
import torch.linalg as la
from torch.nn import Parameter
import torch.linalg as linalg



class Model(torch.nn.Module):

    def __init__(self, dropout_p, num_heads, num_heads_per_partition, num_partitions):
        super().__init__()
        self.dropout_p = dropout_p
        self.num_heads = num_heads
        self.num_partitions = num_partitions
        self.num_heads_per_partition = num_heads_per_partition
        self.scale_factor = (1 / (np.sqrt(latent_dim) * num_heads_per_partition))
        self.to_k = nn.Parameter(torch.randn((num_heads * num_heads_per_partition), latent_dim, latent_dim))
        self.to_q = nn.Parameter(torch.randn((num_heads * num_heads_per_partition), latent_dim, latent_dim))
        self.to_v = nn.Parameter(torch.randn((num_heads * num_heads_per_partition), latent_dim, latent_dim))

    def forward(self, keys, queries, values, training=False):
        keys = rearrange(keys, 'b n (h p) d -> (b h p) n d', h=self.num_heads, p=self.num_heads_per_partition)
        queries = rearrange(queries, 'b n (h p) d -> (b h p) n d', h=self.num_heads, p=self.num_heads_per_partition)
        values = rearrange(values, 'b n (h p) d -> (b h p) n d', h=self.num_heads, p=self.num_heads_per_partition)
        qk = torch.matmul(queries, torch.transpose(keys, (- 2), (- 1)))
        scaled_qk = qk.div(self.scale_factor)
        softmax_qk = scaled_qk.softmax(dim=(- 1))
        attention = torch.nn.functional.dropout(softmax_qk, p=self.dropout_p)
        output = torch.matmul(attention, values)
        output = rearrange(output, '(b h p) n d -> b n (h p) d', b=1, h=self.num_heads, p=self.num_heads_per_partition)
        return output



dropout_p = 1
num_heads = 1
num_heads_per_partition = 1
num_partitions = 1
func = Model(dropout_p, num_heads, num_heads_per_partition, num_partitions).to('cuda')

keys = 1
queries = 1
values = 1

test_inputs = [keys, queries, values]


LmfuzzTestcase 241 sfdp=1 sfdp=1_10 SKIP 420 void
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 242 sfdp=1 sfdp=1_2 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 243 sfdp=1 sfdp=1_3 JIT_FAIL 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 244 sfdp=1 sfdp=1_4 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 245 sfdp=1 sfdp=1_5 unexpected EOF while parsing 420 ('<unknown>', 3, 5, '    \n')
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 246 sfdp=1 sfdp=1_6 JIT_FAIL 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 247 sfdp=1 sfdp=1_7 JIT_FAIL 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 248 sfdp=1 sfdp=1_8 JIT_FAIL 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 249 sfdp=1 sfdp=1_9 JIT_FAIL 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 250 sfdp=2 sfdp=2_1 JIT_FAIL 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 251 sfdp=2 sfdp=2_10 JIT_FAIL 420 Catch
----------------------------------

Successfully create atemp.py
'tuple' object has no attribute 'to'
import os
import torch
import torch.nn.functional as F
import torch.nn as nn
import numpy as np
from torch.autograd import Variable
import math
import torch as th
import torch.linalg as la
from torch.nn import Parameter
import torch.linalg as linalg



func = ().to('cuda')



x1 = torch.randn(4, 1024, 3)



x2 = torch.randn(4, 1024, 8)




x3 = torch.randint(1024, (4, 1024, 1), dtype=torch.long)


test_inputs = [x1, x2, x3]


LmfuzzTestcase 252 sfdp=2 sfdp=2_2 SKIP 420 void
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 253 sfdp=2 sfdp=2_3 JIT_FAIL 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 254 sfdp=2 sfdp=2_4 JIT_FAIL 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 255 sfdp=2 sfdp=2_5 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py
'Model' object has no attribute '_modules'
import os
import torch
import torch.nn.functional as F
import torch.nn as nn
import numpy as np
from torch.autograd import Variable
import math
import torch as th
import torch.linalg as la
from torch.nn import Parameter
import torch.linalg as linalg



class Model(torch.nn.Module):

    def __init__(self):
        pass

    def forward(self, x1, x2):
        a1 = torch.matmul(x1, x2.transpose((- 2), (- 1)))
        a2 = a1.div(1)
        a3 = torch.nn.functional.softmax(a2, dim=(- 1))
        a4 = torch.nn.functional.dropout(a3, p=0.2)




func = Model().to('cuda')

x1 = 1
x2 = 1

test_inputs = [x1, x2]


LmfuzzTestcase 256 sfdp=2 sfdp=2_6 SKIP 420 void
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 257 sfdp=2 sfdp=2_7 JIT_FAIL 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 258 sfdp=2 sfdp=2_8 JIT_FAIL 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 259 sfdp=2 sfdp=2_9 JIT_FAIL 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 260 sfdp=3 sfdp=3_1 JIT_FAIL 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 261 sfdp=3 sfdp=3_10 JIT_FAIL 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 262 sfdp=3 sfdp=3_2 JIT_FAIL 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 263 sfdp=3 sfdp=3_3 JIT_FAIL 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 264 sfdp=3 sfdp=3_4 JIT_FAIL 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 265 sfdp=3 sfdp=3_5 JIT_FAIL 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 266 sfdp=3 sfdp=3_6 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 267 sfdp=3 sfdp=3_7 JIT_FAIL 420 Catch
----------------------------------

Successfully create atemp.py
name 'dim_head' is not defined
import os
import torch
import torch.nn.functional as F
import torch.nn as nn
import numpy as np
from torch.autograd import Variable
import math
import torch as th
import torch.linalg as la
from torch.nn import Parameter
import torch.linalg as linalg



class Model(torch.nn.Module):

    def __init__(self):
        super().__init__()
        self.scale_factor = math.sqrt(dim_head)

    def forward(self, query, key, value, dropout_p):
        qk_mul = torch.matmul(query, key.transpose((- 2), (- 1)))
        scaled_qk = (qk_mul * self.scale_factor)
        softmax_qk = scaled_qk.softmax(dim=(- 1))
        dropout_qk = torch.nn.functional.dropout(softmax_qk, p=dropout_p)
        output = dropout_qk.matmul(value)
        return output



func = Model().to('cuda')

query = 1
key = 1
value = 1
dropout_p = 1

test_inputs = [query, key, value, dropout_p]


LmfuzzTestcase 268 sfdp=3 sfdp=3_8 SKIP 420 void
----------------------------------

Successfully create atemp.py
name 'Query' is not defined
import os
import torch
import torch.nn.functional as F
import torch.nn as nn
import numpy as np
from torch.autograd import Variable
import math
import torch as th
import torch.linalg as la
from torch.nn import Parameter
import torch.linalg as linalg



class Model(torch.nn.Module):

    def __init__(self):
        super().__init__()
        self.query = Query()
        self.key = Key()
        self.value = Value()

    def forward(self, x1):
        q = self.query(x1)
        k = self.key(x1)
        v = self.value(x1)
        qk = torch.matmul(q, k.transpose((- 2), (- 1)))
        scaled_qk = qk.mul(scale_factor)
        softmax_qk = scaled_qk.softmax(dim=(- 1))
        dropout_qk = torch.nn.functional.dropout(softmax_qk, p=dropout_p)
        output = dropout_qk.matmul(v)
        return output



func = Model().to('cuda')



x1 = torch.randn(1, 4, 256, 256)


test_inputs = [x1]


LmfuzzTestcase 269 sfdp=3 sfdp=3_9 SKIP 420 void
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 270 sfdp=4 sfdp=4_1 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 271 sfdp=4 sfdp=4_10 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 272 sfdp=4 sfdp=4_2 JIT_FAIL 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 273 sfdp=4 sfdp=4_3 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 274 sfdp=4 sfdp=4_4 JIT_FAIL 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 275 sfdp=4 sfdp=4_5 JIT_FAIL 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 276 sfdp=4 sfdp=4_6 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 277 sfdp=4 sfdp=4_7 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 278 sfdp=4 sfdp=4_8 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 279 sfdp=4 sfdp=4_9 JIT_FAIL 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 280 sfdp=5 sfdp=5_1 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 281 sfdp=5 sfdp=5_10 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 282 sfdp=5 sfdp=5_2 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 283 sfdp=5 sfdp=5_3 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 284 sfdp=5 sfdp=5_4 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 285 sfdp=5 sfdp=5_5 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 286 sfdp=5 sfdp=5_6 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py
integer division or modulo by zero
import os
import torch
import torch.nn.functional as F
import torch.nn as nn
import numpy as np
from torch.autograd import Variable
import math
import torch as th
import torch.linalg as la
from torch.nn import Parameter
import torch.linalg as linalg



class Model(torch.nn.Module):

    def __init__(self):
        super().__init__()
        self.heads = 0
        self.seq_len = 0
        self.dim = (0 // self.heads)

    def forward(self, query, key, value, attn_mask):
        qk = ((query @ key.transpose((- 2), (- 1))) / math.sqrt(query.size((- 1))))
        qk = (qk + attn_mask)
        attn_weight = torch.softmax(qk, dim=(- 1))
        attn_weight = torch.dropout(attn_weight, 0.1, True)
        output = (attn_weight @ value)
        return output




func = Model().to('cuda')



query = torch.randn(1, 8, 8192, 2102)



key = torch.randn(1, 8, 8192, 2102)



value = torch.randn(1, 8, 8192, 2102)



attn_mask = torch.randn(1, 1, 8192, 8192)


test_inputs = [query, key, value, attn_mask]


LmfuzzTestcase 287 sfdp=5 sfdp=5_7 SKIP 420 void
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 288 sfdp=5 sfdp=5_8 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 289 sfdp=5 sfdp=5_9 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 290 sink_cat_after_pointwise sink_cat_after_pointwise_1 JIT_FAIL 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 291 sink_cat_after_pointwise sink_cat_after_pointwise_10 JIT_FAIL 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 292 sink_cat_after_pointwise sink_cat_after_pointwise_2 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 293 sink_cat_after_pointwise sink_cat_after_pointwise_3 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 294 sink_cat_after_pointwise sink_cat_after_pointwise_4 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 295 sink_cat_after_pointwise sink_cat_after_pointwise_5 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 296 sink_cat_after_pointwise sink_cat_after_pointwise_6 Success 420 succeed
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 297 sink_cat_after_pointwise sink_cat_after_pointwise_7 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 298 sink_cat_after_pointwise sink_cat_after_pointwise_8 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 299 sink_cat_after_pointwise sink_cat_after_pointwise_9 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 300 splitwithsizes_cat_replace splitwithsizes_cat_replace_1 Success 420 succeed
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 301 splitwithsizes_cat_replace splitwithsizes_cat_replace_10 JIT_FAIL 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 302 splitwithsizes_cat_replace splitwithsizes_cat_replace_2 Success 420 succeed
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 303 splitwithsizes_cat_replace splitwithsizes_cat_replace_3 Success 420 succeed
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 304 splitwithsizes_cat_replace splitwithsizes_cat_replace_4 Success 420 succeed
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 305 splitwithsizes_cat_replace splitwithsizes_cat_replace_5 Success 420 succeed
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 306 splitwithsizes_cat_replace splitwithsizes_cat_replace_6 Success 420 succeed
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 307 splitwithsizes_cat_replace splitwithsizes_cat_replace_7 Success 420 succeed
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 308 splitwithsizes_cat_replace splitwithsizes_cat_replace_8 JIT_FAIL 420 Catch
----------------------------------

Successfully create atemp.py
in_channels must be divisible by groups
import os
import torch
import torch.nn.functional as F
import torch.nn as nn
import numpy as np
from torch.autograd import Variable
import math
import torch as th
import torch.linalg as la
from torch.nn import Parameter
import torch.linalg as linalg



class Model(torch.nn.Module):

    def __init__(self):
        super().__init__()
        self.features = torch.nn.Conv2d(3, 32, 3, 1, groups=2, bias=True)

    def forward(self, v1):
        split_tensors = torch.split(v1, [1, 1, 1], dim=1)
        concatenated_tensor = torch.cat(split_tensors, dim=1)
        return (concatenated_tensor, torch.split(v1, [1, 1, 1], dim=1))




func = Model().to('cuda')



x1 = torch.randn(1, 3, 64, 64)


test_inputs = [x1]


LmfuzzTestcase 309 splitwithsizes_cat_replace splitwithsizes_cat_replace_9 SKIP 420 void
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 310 unary=0 unary=0_1 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 311 unary=0 unary=0_10 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 312 unary=0 unary=0_2 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 313 unary=0 unary=0_3 JIT_FAIL 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 314 unary=0 unary=0_4 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 315 unary=0 unary=0_5 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py
__init__() got an unexpected keyword argument 'size'
import os
import torch
import torch.nn.functional as F
import torch.nn as nn
import numpy as np
from torch.autograd import Variable
import math
import torch as th
import torch.linalg as la
from torch.nn import Parameter
import torch.linalg as linalg



class Model(torch.nn.Module):

    def __init__(self):
        super().__init__()
        self.conv = torch.nn.Conv2d(2, 10, 9, stride=1, padding=11)
        self.tconv1 = torch.nn.ConvTranspose2d(10, 1, 23, size=None, stride=1, padding=7)
        self.tconv2 = torch.nn.ConvTranspose2d(28, 76, 11, size=(22, 22), stride=1, padding=8)
        self.tconv3 = torch.nn.ConvTranspose2d(68, 1, 9, size=(22, 22), stride=1, padding=10)

    def forward(self, x6):
        v1 = self.conv(x6)
        v2 = (v1 * 0.5)
        v3 = (v1 * v1)
        v4 = (v3 * v1)
        v5 = (v4 * 0.044715)
        v6 = (v1 + v5)
        v7 = (v6 * 0.7978845608028654)
        v8 = torch.tanh(v7)
        v9 = (v8 + 1)
        v10 = (v2 * v9)
        v11 = self.tconv1(v10)
        v12 = self.tconv2(v8)
        v13 = self.tconv3(v12)
        v14 = (v11 * v13)
        return v14




func = Model().to('cuda')



x6 = torch.randn(1, 2, 32, 32)


test_inputs = [x6]


LmfuzzTestcase 316 unary=0 unary=0_6 SKIP 420 void
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 317 unary=0 unary=0_7 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 318 unary=0 unary=0_8 JIT_FAIL 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 319 unary=0 unary=0_9 JIT_FAIL 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 320 unary=1 unary=1_1 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 321 unary=1 unary=1_10 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 322 unary=1 unary=1_2 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 323 unary=1 unary=1_3 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 324 unary=1 unary=1_4 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 325 unary=1 unary=1_5 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 326 unary=1 unary=1_6 JIT_FAIL 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 327 unary=1 unary=1_7 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 328 unary=1 unary=1_8 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 329 unary=1 unary=1_9 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 330 unary=10 unary=10_1 JIT_FAIL 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 331 unary=10 unary=10_10 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 332 unary=10 unary=10_2 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 333 unary=10 unary=10_3 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 334 unary=10 unary=10_4 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 335 unary=10 unary=10_5 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 336 unary=10 unary=10_6 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 337 unary=10 unary=10_7 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 338 unary=10 unary=10_8 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 339 unary=10 unary=10_9 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 340 unary=11 unary=11_1 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 341 unary=11 unary=11_10 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 342 unary=11 unary=11_2 JIT_FAIL 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 343 unary=11 unary=11_3 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 344 unary=11 unary=11_4 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 345 unary=11 unary=11_5 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 346 unary=11 unary=11_6 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 347 unary=11 unary=11_7 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 348 unary=11 unary=11_8 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py
'tuple' object has no attribute 'to'
import os
import torch
import torch.nn.functional as F
import torch.nn as nn
import numpy as np
from torch.autograd import Variable
import math
import torch as th
import torch.linalg as la
from torch.nn import Parameter
import torch.linalg as linalg



func = ().to('cuda')

input_tensor = torch.randn(1, 1, 1)

test_inputs = [input_tensor]


LmfuzzTestcase 349 unary=11 unary=11_9 SKIP 420 void
----------------------------------

Successfully create atemp.py
out_channels must be divisible by groups
import os
import torch
import torch.nn.functional as F
import torch.nn as nn
import numpy as np
from torch.autograd import Variable
import math
import torch as th
import torch.linalg as la
from torch.nn import Parameter
import torch.linalg as linalg



class Model(torch.nn.Module):

    def __init__(self):
        super().__init__()
        self.conv = torch.nn.Conv2d(16, 3, 3, dilation=1, groups=16)
        self.sigmoid = torch.nn.Sigmoid()

    def forward(self, x1):
        v1 = self.conv(x1)
        v2 = self.sigmoid(v1)
        v3 = (v1 * v2)
        return v3




func = Model().to('cuda')



x1 = torch.randn(1, 16, 64, 64)


test_inputs = [x1]


LmfuzzTestcase 350 unary=12 unary=12_1 SKIP 420 void
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 351 unary=12 unary=12_10 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 352 unary=12 unary=12_2 JIT_FAIL 420 Catch
----------------------------------

Successfully create atemp.py
in_channels must be divisible by groups
import os
import torch
import torch.nn.functional as F
import torch.nn as nn
import numpy as np
from torch.autograd import Variable
import math
import torch as th
import torch.linalg as la
from torch.nn import Parameter
import torch.linalg as linalg



class Model(torch.nn.Module):

    def __init__(self):
        super().__init__()
        self.conv = torch.nn.Conv2d(11, 19, 11, stride=1, padding=(11 // 2), groups=5)
        self.sigmoid = torch.nn.Sigmoid()

    def forward(self, x1):
        v1 = self.conv(x1)
        v2 = self.sigmoid(v1)
        v3 = (v1 * v2)
        return v3




func = Model().to('cuda')



x1 = torch.randn(1, 11, 64, 64)


test_inputs = [x1]


LmfuzzTestcase 353 unary=12 unary=12_3 SKIP 420 void
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 354 unary=12 unary=12_4 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 355 unary=12 unary=12_5 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 356 unary=12 unary=12_6 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 357 unary=12 unary=12_7 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 358 unary=12 unary=12_8 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 359 unary=12 unary=12_9 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 360 unary=13 unary=13_1 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py
__init__() got an unexpected keyword argument 'stride'
import os
import torch
import torch.nn.functional as F
import torch.nn as nn
import numpy as np
from torch.autograd import Variable
import math
import torch as th
import torch.linalg as la
from torch.nn import Parameter
import torch.linalg as linalg



class Model(torch.nn.Module):

    def __init__(self):
        super().__init__()
        self.linear = torch.nn.Linear(3, 8, 1, stride=1, padding=1)

    def forward(self, x1):
        v1 = self.linear(x1)
        v2 = torch.sigmoid(v1)
        v3 = (v1 * v2)
        return v3



func = Model().to('cuda')



x1 = torch.randn(1, 3, 8, 8)


test_inputs = [x1]


LmfuzzTestcase 361 unary=13 unary=13_10 SKIP 420 void
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 362 unary=13 unary=13_2 JIT_FAIL 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 363 unary=13 unary=13_3 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 364 unary=13 unary=13_4 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 365 unary=13 unary=13_5 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 366 unary=13 unary=13_6 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 367 unary=13 unary=13_7 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py
__init__() got an unexpected keyword argument 'stride'
import os
import torch
import torch.nn.functional as F
import torch.nn as nn
import numpy as np
from torch.autograd import Variable
import math
import torch as th
import torch.linalg as la
from torch.nn import Parameter
import torch.linalg as linalg



class Model(torch.nn.Module):

    def __init__(self):
        super().__init__()
        self.linear = torch.nn.Linear(3, 8, 1, stride=1, padding=1)

    def forward(self, x1):
        v1 = self.linear(x1)
        v2 = torch.sigmoid(v1)
        v3 = (v1 * v2)
        return v3



func = Model().to('cuda')



x1 = torch.randn(1, 3, 64, 64)


test_inputs = [x1]


LmfuzzTestcase 368 unary=13 unary=13_8 SKIP 420 void
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 369 unary=13 unary=13_9 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 370 unary=14 unary=14_1 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 371 unary=14 unary=14_10 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 372 unary=14 unary=14_2 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 373 unary=14 unary=14_3 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 374 unary=14 unary=14_4 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 375 unary=14 unary=14_5 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 376 unary=14 unary=14_6 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 377 unary=14 unary=14_7 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 378 unary=14 unary=14_8 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 379 unary=14 unary=14_9 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 380 unary=15 unary=15_1 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 381 unary=15 unary=15_10 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 382 unary=15 unary=15_2 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 383 unary=15 unary=15_3 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 384 unary=15 unary=15_4 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 385 unary=15 unary=15_5 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 386 unary=15 unary=15_6 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 387 unary=15 unary=15_7 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 388 unary=15 unary=15_8 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 389 unary=15 unary=15_9 JIT_FAIL 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 390 unary=16 unary=16_1 JIT_FAIL 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 391 unary=16 unary=16_10 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py
empty() received an invalid combination of arguments - got (tuple, dtype=NoneType, device=NoneType), but expected one of:
 * (tuple of ints size, *, tuple of names names, torch.memory_format memory_format, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)
 * (tuple of ints size, *, torch.memory_format memory_format, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)

import os
import torch
import torch.nn.functional as F
import torch.nn as nn
import numpy as np
from torch.autograd import Variable
import math
import torch as th
import torch.linalg as la
from torch.nn import Parameter
import torch.linalg as linalg



class Model(torch.nn.Module):

    def __init__(self):
        super().__init__()
        self.linear = torch.nn.Linear(1000.0, 1000.0)

    def forward(self, x1):
        v1 = self.linear(x1)
        v2 = torch.relu(v1)
        return v2



func = Model().to('cuda')



x1 = torch.randn(1, 1000)


test_inputs = [x1]


LmfuzzTestcase 392 unary=16 unary=16_2 SKIP 420 void
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 393 unary=16 unary=16_3 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 394 unary=16 unary=16_4 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 395 unary=16 unary=16_5 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py
name '__shape1__' is not defined
import os
import torch
import torch.nn.functional as F
import torch.nn as nn
import numpy as np
from torch.autograd import Variable
import math
import torch as th
import torch.linalg as la
from torch.nn import Parameter
import torch.linalg as linalg



class Model(torch.nn.Module):

    def __init__(self):
        super().__init__()
        self.linear = torch.nn.Linear(__shape1__, __shape2__)

    def forward(self, x1):
        v1 = self.linear(x1)
        v2 = torch.relu(v1)
        return v2



func = Model().to('cuda')

x1 = 1

test_inputs = [x1]


LmfuzzTestcase 396 unary=16 unary=16_6 SKIP 420 void
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 397 unary=16 unary=16_7 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 398 unary=16 unary=16_8 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 399 unary=16 unary=16_9 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 400 unary=17 unary=17_1 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 401 unary=17 unary=17_10 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py
in_channels must be divisible by groups
import os
import torch
import torch.nn.functional as F
import torch.nn as nn
import numpy as np
from torch.autograd import Variable
import math
import torch as th
import torch.linalg as la
from torch.nn import Parameter
import torch.linalg as linalg



class Model(torch.nn.Module):

    def __init__(self):
        super().__init__()
        self.conv = torch.nn.ConvTranspose2d(3, 393, 2, 2, 1, 2, 5, 27)

    def forward(self, x1):
        v1 = self.conv(x1)
        v2 = torch.sigmoid(v1)
        return v2




func = Model().to('cuda')



x1 = torch.randn(1, 3, 28, 28)


test_inputs = [x1]


LmfuzzTestcase 402 unary=17 unary=17_2 SKIP 420 void
----------------------------------


LmfuzzTestcase 403 unary=17 unary=17_3 unexpected EOF while parsing 420 ('<unknown>', 2, 16, 'model_string =\\\n')
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 404 unary=17 unary=17_4 JIT_FAIL 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 405 unary=17 unary=17_5 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 406 unary=17 unary=17_6 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 407 unary=17 unary=17_7 JIT_FAIL 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 408 unary=17 unary=17_8 JIT_FAIL 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 409 unary=17 unary=17_9 JIT_FAIL 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 410 unary=18 unary=18_1 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 411 unary=18 unary=18_10 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 412 unary=18 unary=18_2 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 413 unary=18 unary=18_3 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 414 unary=18 unary=18_4 JIT_FAIL 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 415 unary=18 unary=18_5 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 416 unary=18 unary=18_6 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 417 unary=18 unary=18_7 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 418 unary=18 unary=18_8 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 419 unary=18 unary=18_9 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 420 unary=19 unary=19_1 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 421 unary=19 unary=19_10 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 422 unary=19 unary=19_2 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 423 unary=19 unary=19_3 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 424 unary=19 unary=19_4 JIT_FAIL 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 425 unary=19 unary=19_5 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 426 unary=19 unary=19_6 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 427 unary=19 unary=19_7 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 428 unary=19 unary=19_8 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 429 unary=19 unary=19_9 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 430 unary=2 unary=2_1 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 431 unary=2 unary=2_10 JIT_FAIL 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 432 unary=2 unary=2_2 JIT_FAIL 420 Catch
----------------------------------

Successfully create atemp.py
module 'torch.nn' has no attribute 'Conv3dTranspose'
import os
import torch
import torch.nn.functional as F
import torch.nn as nn
import numpy as np
from torch.autograd import Variable
import math
import torch as th
import torch.linalg as la
from torch.nn import Parameter
import torch.linalg as linalg



class Module0(torch.nn.Module):

    def __init__(self, dim0):
        super().__init__()
        self.conv_transpose3d = torch.nn.ConvTranspose3d(128, 32, (3, 3, 3), (1, 1, 1), (1, 1, 1), (1, 1, 1))
        self.conv3dtranspose3d = torch.nn.Conv3dTranspose(1, 2, 4, 2, 1, 2, 1)

    def forward(self, x, x3):
        (v37, v28) = self.conv_transpose3d.forward(x)
        (v3, v2, v1) = x3.size()
        (v32, v23) = self.conv3dtranspose3d.forward(v37.view((- 1), v2, v3, v1))
        ret0 = v32.view((- 1), v32.size()[2], v32.size()[3], v32.size()[4])
        return ret0




class Module1(torch.nn.Module):

    def __init__(self, dim0):
        super().__init__()
        self.module0_0 = Module0(dim0)
        self.conv_transpose3d = torch.nn.ConvTranspose3d(32, 2, (3, 3, 3), (1, 1, 1), (1, 1, 1), (1, 1, 1))

    def forward(self, x0):
        v4 = self.module0_0.forward(x0, x0)
        (v43, v14) = self.conv_transpose3d.forward(v4)
        ret1 = v43
        return ret1




class Model(torch.nn.Module):

    def __init__(self, dim0):
        super().__init__()
        self.module1_0 = Module1(dim0)
        self.conv_transpose2d = torch.nn.ConvTranspose2d(14, 1, 4, stride=2)
        self.relu = torch.nn.ReLU()

    def forward(self, x):
        v34 = self.module1_0.forward(x)
        (v11, v12) = v34.size()
        v18 = v34.view((- 1), v11, v12)
        v13 = self.conv_transpose2d(v18)
        v19 = self.relu(v13)
        return v19



dim0 = 1

func = Model(dim0).to('cuda')



x = torch.randn(1, 3, 224, 224)


test_inputs = [x]


LmfuzzTestcase 433 unary=2 unary=2_3 SKIP 420 void
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 434 unary=2 unary=2_4 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 435 unary=2 unary=2_5 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 436 unary=2 unary=2_6 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 437 unary=2 unary=2_7 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 438 unary=2 unary=2_8 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 439 unary=2 unary=2_9 JIT_FAIL 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 440 unary=20 unary=20_1 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 441 unary=20 unary=20_10 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 442 unary=20 unary=20_2 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 443 unary=20 unary=20_3 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py
in_channels must be divisible by groups
import os
import torch
import torch.nn.functional as F
import torch.nn as nn
import numpy as np
from torch.autograd import Variable
import math
import torch as th
import torch.linalg as la
from torch.nn import Parameter
import torch.linalg as linalg



class Model(torch.nn.Module):

    def __init__(self):
        super().__init__()
        self.sigmoid1 = torch.nn.Sigmoid()
        self.sigmoid2 = torch.nn.Sigmoid()
        self.conv_t = torch.nn.ConvTranspose2d(in_channels=3, out_channels=3, kernel_size=(1, 1), padding=(1, 1), groups=2, bias=True)

    def forward(self, x1):
        v1 = self.conv_t(x1)
        v2 = self.sigmoid1(v1)
        v3 = self.sigmoid2(v2)
        v4 = self.sigmoid1(v3)
        return v4




func = Model().to('cuda')



x1 = torch.randn(1, 3, 3, 3)


test_inputs = [x1]


LmfuzzTestcase 444 unary=20 unary=20_4 SKIP 420 void
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 445 unary=20 unary=20_5 Success 420 succeed
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 446 unary=20 unary=20_6 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py
in_channels must be divisible by groups
import os
import torch
import torch.nn.functional as F
import torch.nn as nn
import numpy as np
from torch.autograd import Variable
import math
import torch as th
import torch.linalg as la
from torch.nn import Parameter
import torch.linalg as linalg



class Model(torch.nn.Module):

    def __init__(self):
        super(Model, self).__init__()
        self.conv = torch.nn.ConvTranspose2d(in_channels=1, out_channels=3, kernel_size=(13, 1), stride=(1, 1), padding=(0, 0), groups=4, bias=True)
        self.conv1 = torch.nn.ConvTranspose2d(in_channels=3, out_channels=5, kernel_size=(11, 2), stride=(12, 21), padding=(4, 4), dilation=8, groups=6, bias=False)
        self.conv2 = torch.nn.ConvTranspose2d(in_channels=3, out_channels=1, kernel_size=(1, 2, 3), stride=(1, 2), padding=(3, 4), bias=False)

    def forward(self, x1):
        v1 = self.conv(x1)
        v2 = self.conv1(v1)
        v3 = self.conv2(v2)
        return v3




func = Model().to('cuda')



x1 = torch.randn(2, 1, 52, 52)


test_inputs = [x1]


LmfuzzTestcase 447 unary=20 unary=20_7 SKIP 420 void
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 448 unary=20 unary=20_8 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 449 unary=20 unary=20_9 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 450 unary=21 unary=21_1 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 451 unary=21 unary=21_10 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 452 unary=21 unary=21_2 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 453 unary=21 unary=21_3 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 454 unary=21 unary=21_4 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 455 unary=21 unary=21_5 JIT_FAIL 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 456 unary=21 unary=21_6 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 457 unary=21 unary=21_7 JIT_FAIL 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 458 unary=21 unary=21_8 JIT_FAIL 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 459 unary=21 unary=21_9 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 460 unary=22 unary=22_1 JIT_FAIL 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 461 unary=22 unary=22_10 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 462 unary=22 unary=22_2 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 463 unary=22 unary=22_3 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 464 unary=22 unary=22_4 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 465 unary=22 unary=22_5 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 466 unary=22 unary=22_6 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 467 unary=22 unary=22_7 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 468 unary=22 unary=22_8 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 469 unary=22 unary=22_9 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 470 unary=23 unary=23_1 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 471 unary=23 unary=23_10 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 472 unary=23 unary=23_2 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 473 unary=23 unary=23_3 JIT_FAIL 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 474 unary=23 unary=23_4 JIT_FAIL 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 475 unary=23 unary=23_5 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 476 unary=23 unary=23_6 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 477 unary=23 unary=23_7 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 478 unary=23 unary=23_8 JIT_FAIL 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 479 unary=23 unary=23_9 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 480 unary=24 unary=24_1 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 481 unary=24 unary=24_10 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 482 unary=24 unary=24_2 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 483 unary=24 unary=24_3 JIT_FAIL 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 484 unary=24 unary=24_4 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 485 unary=24 unary=24_5 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 486 unary=24 unary=24_6 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 487 unary=24 unary=24_7 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 488 unary=24 unary=24_8 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 489 unary=24 unary=24_9 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 490 unary=25 unary=25_1 JIT_FAIL 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 491 unary=25 unary=25_10 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 492 unary=25 unary=25_2 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 493 unary=25 unary=25_3 JIT_FAIL 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 494 unary=25 unary=25_4 JIT_FAIL 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 495 unary=25 unary=25_5 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 496 unary=25 unary=25_6 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 497 unary=25 unary=25_7 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py
'Model' object has no attribute '_modules'
import os
import torch
import torch.nn.functional as F
import torch.nn as nn
import numpy as np
from torch.autograd import Variable
import math
import torch as th
import torch.linalg as la
from torch.nn import Parameter
import torch.linalg as linalg



class Model(torch.nn.Module):

    def __init__(self, negative_slope):
        super(Model).__init__()
        self.negative_slope = negative_slope

    def forward(self, x1):
        v1 = x1.shape[1]
        v2 = math.exp(negative_slope)
        v3 = torch.zeros(v1)
        v4 = torch.add(v2, 1)
        v5 = torch.multiply(v4, v3)
        v6 = v5[0:Negative_slope]
        v7 = v6[0:Negative_slope[0:Negative_slope]]
        v8 = torch.nn.functional.max_pool2d(x1, kernel_size=Kernel_Size, stride=Stride, padding=Padding, dilation=Dilation, ceil_mode=Ceil_Mode)
        v9 = v7.flatten(start_dim=Start_Dim, end_dim=(- 1))
        v10 = (v8 + v9)
        return v10



negative_slope = 1

func = Model(negative_slope).to('cuda')

x1 = 1

test_inputs = [x1]


LmfuzzTestcase 498 unary=25 unary=25_8 SKIP 420 void
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 499 unary=25 unary=25_9 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 500 unary=26 unary=26_1 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 501 unary=26 unary=26_10 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 502 unary=26 unary=26_2 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 503 unary=26 unary=26_3 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 504 unary=26 unary=26_4 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 505 unary=26 unary=26_5 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 506 unary=26 unary=26_6 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 507 unary=26 unary=26_7 JIT_FAIL 420 Catch
----------------------------------

Successfully create atemp.py
__init__() got multiple values for argument 'kernel_size'
import os
import torch
import torch.nn.functional as F
import torch.nn as nn
import numpy as np
from torch.autograd import Variable
import math
import torch as th
import torch.linalg as la
from torch.nn import Parameter
import torch.linalg as linalg



class Model(torch.nn.Module):

    def __init__(self):
        super().__init__()
        self.conv_t = torch.nn.ConvTranspose2d(185, 184, 12, kernel_size=(7, 2), padding=2, output_padding=0, stride=3, bias=True)

    def forward(self, x11):
        q1 = self.conv_t(x11)
        q2 = (q1 > 0)
        q3 = (q1 * (- 0.5))
        q4 = torch.where(q2, q1, q3)
        return torch.nn.functional.max_pool2d(q4, (2, 1))




func = Model().to('cuda')



x11 = torch.randn(5, 185, 25, 34)


test_inputs = [x11]


LmfuzzTestcase 508 unary=26 unary=26_8 SKIP 420 void
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 509 unary=26 unary=26_9 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 510 unary=27 unary=27_1 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 511 unary=27 unary=27_10 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 512 unary=27 unary=27_2 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 513 unary=27 unary=27_3 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 514 unary=27 unary=27_4 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 515 unary=27 unary=27_5 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 516 unary=27 unary=27_6 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 517 unary=27 unary=27_7 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 518 unary=27 unary=27_8 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 519 unary=27 unary=27_9 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 520 unary=28 unary=28_1 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 521 unary=28 unary=28_10 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 522 unary=28 unary=28_2 JIT_FAIL 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 523 unary=28 unary=28_3 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 524 unary=28 unary=28_4 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py
name '__min_value__' is not defined
import os
import torch
import torch.nn.functional as F
import torch.nn as nn
import numpy as np
from torch.autograd import Variable
import math
import torch as th
import torch.linalg as la
from torch.nn import Parameter
import torch.linalg as linalg



class Model_L1_L2(torch.nn.Module):

    def __init__(self, min_value=(- 1), max_value=1):
        super().__init__()
        if (min_value >= max_value):
            raise ValueError('Minimum value must be less than maximum value')
        self.min_value = min_value
        self.max_value = max_value

    def forward(self, x1):
        v1 = torch.flatten(x1, start_dim=1)
        v2 = torch.clamp_min(v1, self.min_value)
        v3 = torch.clamp_max(v2, self.max_value)
        return v3.view(x1.size())



func = Model_L1_L2(__min_value__, __max_value__).to('cuda')



x1 = torch.randn(1, 2, 16, 16)


test_inputs = [x1]


LmfuzzTestcase 525 unary=28 unary=28_5 SKIP 420 void
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 526 unary=28 unary=28_6 JIT_FAIL 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 527 unary=28 unary=28_7 JIT_FAIL 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 528 unary=28 unary=28_8 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 529 unary=28 unary=28_9 JIT_FAIL 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 530 unary=29 unary=29_1 JIT_FAIL 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 531 unary=29 unary=29_10 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 532 unary=29 unary=29_2 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py
module 'torch.nn' has no attribute 'Sqrt'
import os
import torch
import torch.nn.functional as F
import torch.nn as nn
import numpy as np
from torch.autograd import Variable
import math
import torch as th
import torch.linalg as la
from torch.nn import Parameter
import torch.linalg as linalg



class Model(torch.nn.Module):

    def __init__(self, min_value=(- 3.7), max_value=(- 2.2)):
        super().__init__()
        self.sqrt = torch.nn.Sqrt()
        self.conv = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)
        self.min_value = min_value
        self.max_value = max_value

    def forward(self, x1):
        v1 = self.conv(x1)
        v2 = torch.clamp_min(v1, self.min_value)
        v3 = torch.clamp_max(v2, self.max_value)
        v4 = self.sqrt(v3)
        return v4




func = Model().to('cuda')



x1 = torch.randn(1, 3, 64, 64)


test_inputs = [x1]


LmfuzzTestcase 533 unary=29 unary=29_3 SKIP 420 void
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 534 unary=29 unary=29_4 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 535 unary=29 unary=29_5 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 536 unary=29 unary=29_6 closing parenthesis ']' does not match opening parenthesis '(' 420 ('<unknown>', 3, 56, '    def __init__(self, min_value=-17.2, max_value=-16.7]):')
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 537 unary=29 unary=29_7 JIT_FAIL 420 Catch
----------------------------------

Successfully create atemp.py
module 'torch.nn' has no attribute 'Clamp'
import os
import torch
import torch.nn.functional as F
import torch.nn as nn
import numpy as np
from torch.autograd import Variable
import math
import torch as th
import torch.linalg as la
from torch.nn import Parameter
import torch.linalg as linalg



class Model(torch.nn.Module):

    def __init__(self, min_value=(- 0.7), max_value=3):
        super().__init__()
        self.sigmoid = torch.nn.Sigmoid()
        self.clamp = torch.nn.Clamp(min=min_value, max=max_value)
        self.conv_transpose2d = torch.nn.ConvTranspose2d(3, 8, 1, stride=1, padding=1)

    def forward(self, x1):
        v1 = self.conv_transpose2d(x1)
        v2 = self.clamp(v1)
        v3 = self.sigmoid(v2)
        return v3




func = Model().to('cuda')



x1 = torch.randn(1, 3, 64, 64)


test_inputs = [x1]


LmfuzzTestcase 538 unary=29 unary=29_8 SKIP 420 void
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 539 unary=29 unary=29_9 JIT_FAIL 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 540 unary=3 unary=3_1 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 541 unary=3 unary=3_10 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 542 unary=3 unary=3_2 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 543 unary=3 unary=3_3 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 544 unary=3 unary=3_4 JIT_FAIL 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 545 unary=3 unary=3_5 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py
module 'torch.nn.quantized' has no attribute 'ConvReLU2d'
import os
import torch
import torch.nn.functional as F
import torch.nn as nn
import numpy as np
from torch.autograd import Variable
import math
import torch as th
import torch.linalg as la
from torch.nn import Parameter
import torch.linalg as linalg



class Model(torch.nn.Module):

    def __init__(self):
        super().__init__()
        self.conv1a = torch.nn.quantized.ConvReLU2d(256, 544, (1, 7), stride=1, padding=(0, 3))
        self.conv1r = torch.nn.quantized.Conv2d(256, 384, (1, 5), stride=1, padding=(0, 2))
        self.conv2a = torch.nn.quantized.ConvReLU2d(384, 512, (3, 3), stride=1, padding=(1, 1))
        self.conv2r = torch.nn.quantized.Conv2d(384, 256, (1, 3), stride=1, padding=(0, 1))

    def forward(self, x1):
        v1 = self.conv1a(x1)
        v2 = self.conv1r(x1)
        v3 = (v1 * 0.5)
        v4 = (v2 * 0.5)
        v5 = (v3 * 0.7071067811865476)
        v6 = (v4 * 0.7071067811865476)
        v7 = torch.erf(v5)
        v8 = (v7 + 1)
        v9 = torch.erf(v6)
        v10 = (v8 * v9)
        v11 = (v2 * 0.5)
        v12 = (v1 * 0.5)
        v13 = (v12 * 0.7071067811865476)
        v14 = (v11 * 0.7071067811865476)
        v15 = torch.erf(v13)
        v16 = (v15 + 1)
        v17 = torch.erf(v14)
        v18 = (v16 * v17)
        v19 = (v10 + v18)
        v20 = (v3 * 0.5)
        v21 = (v4 * 0.5)
        v22 = (v20 * 0.7071067811865476)
        v23 = (v21 * 0.7071067811865476)
        v24 = torch.erf(v22)
        v25 = (v24 + 1)
        v26 = torch.erf(v23)
        v27 = (v25 * v26)
        v28 = (v27 + v20)
        v29 = (v22 + 1)
        v30 = (v23 + 1)
        v31 = torch.erf(v30)
        v32 = (v31 * v32)
        v33 = (v32 + v23)
        v34 = (v22 - 1)
        v35 = torch.exp(v34)
        v36 = (v35 + 1)
        v37 = (v23 - 1)
        v38 = torch.exp(v37)
        v39 = (v38 + 1)
        v40 = (v38 / v39)
        v41 = (v36 * v40)
        v42 = (v27 * v25)
        v43 = (v20 * v16)
        v44 = (v21 * v15)
        v45 = (v42 + v41)
        v46 = (v21 - 1)
        v47 = torch.exp(v46)
        v48 = (v47 + 1)
        v49 = (v20 - 1)
        v50 = torch.exp(v49)
        v51 = (v50 + 1)
        v52 = (v32 / v51)
        v53 = (v48 * v52)
        v54 = (v18 * v10)
        v55 = (v12 * v8)
        v56 = (v44 + v33)
        v57 = (v43 + v54)
        v58 = (v11 - 1)
        v59 = torch.exp(v58)
        v60 = (v59 + 1)
        v61 = (v33 / v60)
        v62 = (v13 - 1)
        v63 = torch.exp(v62)
        v64 = (v63 + 1)
        v65 = (v64 / v61)
        v66 = (v9 * v65)
        v67 = (v66 + v55)
        v68 = (v8 * v53)
        v69 = (v65 * v63)
        v70 = (v56 + v68)
        v71 = torch.quantize_per_tensor(v67, 0.0, 122.0, torch.quint8)
        v72 = torch.quantize_per_tensor(v70, 0.0, 98.0, torch.quint8)
        v73 = torch.dequantize(v71)
        v74 = torch.dequantize(v72)
        v75 = self.conv2a(v73)
        v76 = self.conv2r(v74)
        v77 = (v75 * 0.5)
        v78 = (v76 * 0.5)
        v79 = (v77 * 0.7071067811865476)
        v80 = (v78 * 0.7071067811865476)
        v81 = torch.erf(v79)
        v82 = (v81 + 1)
        v83 = torch.erf(v80)
        v84 = torch.quantize_per_tensor(v82, 0.0, 122.0, torch.quint8)
        v85 = torch.quantize_per_tensor(v83, 0.0, 122.0, torch.quint8)
        v86 = torch.dequantize(v84)
        v87 = torch.dequantize(v85)
        v88 = (v78 + 1.0)
        v89 = (v77 + 1.0)
        v90 = (v78 - 1.0)
        v91 = torch.exp(v90)
        v92 = (v91 + 1)
        v93 = (v77 - 1.0)
        v94 = torch.exp(v93)
        v95 = (v94 + 1)
        v96 = (v89 / v95)
        v97 = (v92 * v96)
        v98 = (v82 * v83)
        v99 = (v98 * 0.5)
        v100 = (v88 * 0.5)
        v101 = (v99 * 0.7071067811865476)
        v102 = (v100 * 0.7071067811865476)
        v103 = torch.erf(v101)
        v104 = (v103 + 1)
        v105 = torch.erf(v102)
        v106 = (v94 / v105)
        v107 = (v91 / v106)
        v108 = (v97 + (v107 * v104))
        v109 = (v108 * 123.0)
        v110 = (v101 + 1)
        v111 = (v99 + 1)
        v112 = torch.erf(v111)
        v113 = (v112 * v113)
        v114 = (v113 - 1.0)
        v115 = torch.exp(v114)
        v116 = (v115 + 1)
        v117 = (v115 / v100)
        v118 = (v110 * v117)
        v119 = (v103 - 1.0)
        v120 = torch.exp(v119)
        v121 = (v120 + 1)
        v122 = (v120 / v99)
        v123 = (v122 * 123.0)
        v124 = (v104 + 1.0)
        v125 = (v100 + 1.0)
        v126 = torch.erf(v125)
        v127 = (v126 * v127)
        v128 = (v102 + 1.0)
        v129 = (v102 - 1.0)
        v130 = torch.exp(v129)
        v131 = (v130 + 1)
        v132 = (v131 / v101)
        v133 = (v132 / v88)
        v134 = (v79 + 1.0)
        v135 = (v134 + 1.0)
        v136 = torch.erf(v135)
        v137 = (v136 * v137)
        v138 = (v129 - 1.0)
        v139 = torch.exp(v138)
        v140 = (v139 + 1)
        v141 = (v139 / v77)
        v142 = (v88 * v102)
        v143 = (v142 * v122)
        v144 = (v128 / v133)
        v145 = (v143 + v123)
        v146 = (v141 / v140)
        v147 = (v131 * v146)
        v148 = (v78 / v144)
        v149 = (v148 * v97)
        v150 = (v147 + v144)
        v151 = (v91 * v90)
        v152 = (v151 + 1)
        v153 = (v121 / v150)
        v154 = (v147 * 123.0)
        v155 = (v124 * v120)
        v156 = (v79 - 1.0)
        v157 = torch.exp(v156)
        v158 = (v157 + 1)
        v159 = (v157 - 1.0)
        v160 = torch.exp(v159)
        v161 = (v160 + 1)
        v162 = (v160 / v134)
        v163 = (v93 / v162)
        v164 = (v163 * 123.0)
        v165 = (v124 * v110)
        v166 = (v158 * v116)
        v167 = (v158 * v119)
        v168 = (v165 + v166)
        v169 = (v157 * v117)
        v170 = (v129 * v137)
        v171 = (v169 * 123.0)
        v172 = (v168 * 123.0)
        v173 = (v134 * v132)
        v174 = (v173 * v170)
        v175 = (v153 * 123.0)
        v176 = (v90 * v162)
        v177 = (v153 * v159)
        v178 = (v143 + v167)
        v179 = (v141 / v155)
        v180 = (v179 / v164)
        v181 = (v155 * v151)
        v182 = (v175 + (v133 * v161))
        v183 = (v171 + v172)
        v184 = (v170 + v178)
        v185 = (v174 + (v180 * v181))
        v186 = self.conv2a(v183)
        v187 = self.conv2r(v184)
        v188 = (v186 * 0.5)
        v189 = (v187 * 0.5)
        v190 = (v188 * 0.7071067811865476)
        v191 = (v189 * 0.7071067811865476)
        v192 = torch.erf(v190)
        v193 = (v192 + 1)
        v194 = torch.erf(v191)
        v195 = (v193 + (v181 * 123.0))
        v196 = torch.quantize_per_tensor(v195, 0.0, 122.0, torch.quint8)
        v197 = torch.dequantize(v196)
        v198 = self.conv2a(v188)
        v199 = self.conv2r(v191)
        v200 = (v198 * 0.5)
        v201 = (v199 * 0.5)
        v202 = (v200 * 0.7071067811865476)
        v203 = (v201 * 0.7071067811865476)
        v204 = torch.erf(v202)
        v205 = (v204 + 1)
        v206 = torch.erf(v203)
        v207 = (v205 + (v180 * 123.0))
        v208 = (v206 / 123.0)
        v209 = (v195 * v188)
        v210 = torch.quantize_per_tensor(v209, 0.0, 122.0, torch.quint8)
        v211 = torch.dequantize(v210)
        v212 = (v195 * v191)
        v213 = torch.quantize_per_tensor(v212, 0.0, 122.0, torch.quint8)
        v214 = torch.dequantize(v213)
        v215 = (v208 + v211)
        v216 = (v208 + v214)
        v217 = (v211 + v214)
        v218 = self.conv2a(v215)
        v219 = self.conv2r(v217)
        v220 = (v218 * 0.5)
        v221 = (v219 * 0.5)
        v222 = (v220 * 0.7071067811865476)
        v223 = (v221 * 0.7071067811865476)
        v224 = torch.erf(v222)
        v225 = (v224 + 1)
        v226 = torch.erf(v223)
        v227 = (v225 + (v180 * 33.0))
        v228 = torch.quantize_per_tensor(v227, 0.0, 122.0, torch.quint8)
        v229 = torch.dequantize(v228)
        v230 = self.conv2a(v220)
        v231 = self.conv2r(v223)
        v232 = (v230 * 0.5)
        v233 = (v231 * 0.5)
        v234 = (v232 * 0.7071067811865476)
        v235 = (v233 * 0.7071067811865476)
        v236 = torch.erf(v234)
        v237 = (v236 + 1)
        v238 = torch.erf(v235)
        v239 = (v237 + (v181 * 123.0))
        v240 = (v238 / 123.0)
        v241 = (v227 * v220)
        v242 = torch.quantize_per_tensor(v241, 0.0, 122.0, torch.quint8)
        v243 = torch.dequantize(v242)
        v244 = (v227 * v223)
        v245 = torch.quantize_per_tensor(v244, 0.0, 122.0, torch.quint8)
        v246 = torch.dequantize(v245)
        v247 = (v240 + v243)
        v248 = (v240 + v246)
        v249 = (v243 + v246)
        v250 = self.conv2a(v247)
        v251 = self.conv2r(v249)
        v252 = (v250 * 0.5)
        v253 = (v251 * 0.5)
        v254 = (v252 * 0.7071067811865476)
        v255 = (v253 * 0.7071067811865476)
        v256 = torch.erf(v254)
        v257 = (v256 + 1)
        v258 = torch.erf(v255)
        v259 = (v257 + (v180 * 33.0))
        v260 = torch.quantize_per_tensor(v259, 0.0, 122.0, torch.quint8)
        v261 = torch.dequantize(v260)
        v260 = self.conv2a(v252)
        v261 = self.conv2r(v255)
        v262 = (v260 * 0.5)
        v263 = (v261 * 0.5)
        v264 = (v262 * 0.7071067811865476)
        v265 = (v263 * 0.7071067811865476)
        v266 = torch.erf(v264)
        v267 = (v266 + 1)
        v268 = torch.erf(v265)
        v269 = (v267 + (v181 * 123.0))
        v270 = (v268 / 123.0)
        x = v270
        return x




func = Model().to('cuda')



x1 = torch.randn([1, 256, 88, 88])


test_inputs = [x1]


LmfuzzTestcase 546 unary=3 unary=3_6 SKIP 420 void
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 547 unary=3 unary=3_7 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py
conv2d() received an invalid combination of arguments - got (int, int, tuple, padding=int, stride=int), but expected one of:
 * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, tuple of ints padding, tuple of ints dilation, int groups)
 * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, str padding, tuple of ints dilation, int groups)

import os
import torch
import torch.nn.functional as F
import torch.nn as nn
import numpy as np
from torch.autograd import Variable
import math
import torch as th
import torch.linalg as la
from torch.nn import Parameter
import torch.linalg as linalg



class Model(torch.nn.Module):

    def __init__(self):
        super().__init__()
        self.conv2 = torch.nn.functional.conv2d
        self.conv = self.conv2(128, 256, (17, 17), stride=2, padding=0)
        self.conv1 = self.conv2(256, 128, (17, 17), stride=1, padding=0)
        self.conv3 = self.conv2((- 1), (- 1), (1, 1), stride=1, padding=0)
        self.conv4 = self.conv2(128, 10, (1, 1), stride=1, padding=0)

    def forward(self, x1):
        v2 = (x1 * 0.5)
        v21 = (x1 * 0.7071067811865476)
        v3 = torch.erf(v21)
        v4 = (v3 + 1)
        v5 = self.conv(v4)
        v6 = (v5 * 0.5)
        v7 = (v5 * 0.7071067811865476)
        v8 = torch.erf(v7)
        v9 = (v8 + 1)
        v10 = self.conv1(v9)
        v11 = (v6 * v10)
        v11 = torch.tanh(v11)
        v12 = self.conv3(v11)
        v13 = self.conv4(v12)
        return v13




func = Model().to('cuda')



x1 = torch.randn(128, 1, 17, 17)


test_inputs = [x1]


LmfuzzTestcase 548 unary=3 unary=3_8 SKIP 420 void
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 549 unary=3 unary=3_9 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 550 unary=4 unary=4_1 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 551 unary=4 unary=4_10 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 552 unary=4 unary=4_2 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 553 unary=4 unary=4_3 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 554 unary=4 unary=4_4 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 555 unary=4 unary=4_5 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 556 unary=4 unary=4_6 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 557 unary=4 unary=4_7 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 558 unary=4 unary=4_8 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 559 unary=4 unary=4_9 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py
out_channels must be divisible by groups
import os
import torch
import torch.nn.functional as F
import torch.nn as nn
import numpy as np
from torch.autograd import Variable
import math
import torch as th
import torch.linalg as la
from torch.nn import Parameter
import torch.linalg as linalg



class Model(torch.nn.Module):

    def __init__(self):
        super().__init__()
        self.conv_transpose = torch.nn.ConvTranspose2d(27, 38, 3, stride=1, padding=1, dilation=3, groups=3)

    def forward(self, x1):
        v1 = self.conv_transpose(x1)
        v2 = (v1 * 0.5)
        v3 = (v1 * 0.7071067811865476)
        v4 = torch.erf(v3)
        v5 = (v4 + 1)
        v6 = (v2 * v5)
        return v6




func = Model().to('cuda')



x1 = torch.randn(1, 27, 64, 64)


test_inputs = [x1]


LmfuzzTestcase 560 unary=5 unary=5_1 SKIP 420 void
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 561 unary=5 unary=5_10 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 562 unary=5 unary=5_2 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 563 unary=5 unary=5_3 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py
name 'padding_num' is not defined
import os
import torch
import torch.nn.functional as F
import torch.nn as nn
import numpy as np
from torch.autograd import Variable
import math
import torch as th
import torch.linalg as la
from torch.nn import Parameter
import torch.linalg as linalg



class Model(torch.nn.Module):

    def __init__(self):
        super().__init__()
        self.conv_transpose = torch.nn.ConvTranspose1d(16, 16, 2, stride=1, padding=padding_num)

    def forward(self, x1):
        v1 = self.conv_transpose(x1)
        v2 = (v1 * 0.5)
        v3 = (v1 * 0.7071067811865476)
        v4 = torch.erf(v3)
        v5 = (v4 + 1)
        v6 = (v2 * v5)
        return v6




func = Model().to('cuda')



x1 = torch.randn(1, 16, 64)


test_inputs = [x1]


LmfuzzTestcase 564 unary=5 unary=5_4 SKIP 420 void
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 565 unary=5 unary=5_5 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 566 unary=5 unary=5_6 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 567 unary=5 unary=5_7 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 568 unary=5 unary=5_8 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 569 unary=5 unary=5_9 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 570 unary=6 unary=6_1 JIT_FAIL 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 571 unary=6 unary=6_10 JIT_FAIL 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 572 unary=6 unary=6_2 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 573 unary=6 unary=6_3 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 574 unary=6 unary=6_4 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 575 unary=6 unary=6_5 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 576 unary=6 unary=6_6 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 577 unary=6 unary=6_7 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 578 unary=6 unary=6_8 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 579 unary=6 unary=6_9 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 580 unary=7 unary=7_1 JIT_FAIL 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 581 unary=7 unary=7_10 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 582 unary=7 unary=7_2 positional argument follows keyword argument 420 ('<unknown>', 9, 40, None)
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 583 unary=7 unary=7_3 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 584 unary=7 unary=7_4 Success 420 succeed
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 585 unary=7 unary=7_5 JIT_FAIL 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 586 unary=7 unary=7_6 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 587 unary=7 unary=7_7 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 588 unary=7 unary=7_8 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 589 unary=7 unary=7_9 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 590 unary=8 unary=8_1 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 591 unary=8 unary=8_10 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 592 unary=8 unary=8_2 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 593 unary=8 unary=8_3 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 594 unary=8 unary=8_4 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 595 unary=8 unary=8_5 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py
Only "zeros" padding mode is supported for ConvTranspose2d
import os
import torch
import torch.nn.functional as F
import torch.nn as nn
import numpy as np
from torch.autograd import Variable
import math
import torch as th
import torch.linalg as la
from torch.nn import Parameter
import torch.linalg as linalg



class Model(torch.nn.Module):

    def __init__(self):
        super().__init__()
        self.conv_transpose = torch.nn.ConvTranspose2d(1, 3, 2, stride=2, padding=(1, 1), padding_mode='circular')

    def forward(self, x1):
        v1 = self.conv_transpose(x1)
        v2 = (v1 + 3)
        v3 = torch.clamp(v2, min=0)
        v4 = torch.clamp(v3, max=6)
        v5 = (v1 * v4)
        v6 = (v5 / 6)
        return v6




func = Model().to('cuda')



x1 = torch.randn(1, 1, 65, 129)


test_inputs = [x1]


LmfuzzTestcase 596 unary=8 unary=8_6 SKIP 420 void
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 597 unary=8 unary=8_7 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 598 unary=8 unary=8_8 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 599 unary=8 unary=8_9 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py
in_channels must be divisible by groups
import os
import torch
import torch.nn.functional as F
import torch.nn as nn
import numpy as np
from torch.autograd import Variable
import math
import torch as th
import torch.linalg as la
from torch.nn import Parameter
import torch.linalg as linalg



class Model(torch.nn.Module):

    def __init__(self):
        super().__init__()
        self.conv = torch.nn.Conv2d(3, 8, 9, padding=1, groups=5)

    def forward(self, x1):
        v1 = (3 + self.conv(x1))
        v2 = v1.clamp(min=0, max=6)
        v3 = v2.div(6)
        v4 = (v1 - 3)
        v5 = v4.clamp(min=0, max=6)
        v6 = v5.div(6)
        return v6




func = Model().to('cuda')



x1 = torch.randn(1, 3, 64, 64)


test_inputs = [x1]


LmfuzzTestcase 600 unary=9 unary=9_1 SKIP 420 void
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 601 unary=9 unary=9_10 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 602 unary=9 unary=9_2 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 603 unary=9 unary=9_3 JIT_FAIL 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 604 unary=9 unary=9_4 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 605 unary=9 unary=9_5 JIT_FAIL 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 606 unary=9 unary=9_6 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 607 unary=9 unary=9_7 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 608 unary=9 unary=9_8 JIT_STATUS 420 Catch
----------------------------------

Successfully create atemp.py

LmfuzzTestcase 609 unary=9 unary=9_9 JIT_FAIL 420 Catch
----------------------------------


Used time: 924.756514787674
Start testing

LmfuzzTestcase 0 addmm addmm_1 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 1 addmm addmm_10 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 2 addmm addmm_2 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 3 addmm addmm_3 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 4 addmm addmm_4 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 5 addmm addmm_5 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 6 addmm addmm_6 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 7 addmm addmm_7 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 8 addmm addmm_8 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 9 addmm addmm_9 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 10 binary=0 binary=0_1 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 11 binary=0 binary=0_10 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 12 binary=0 binary=0_2 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 13 binary=0 binary=0_3 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 14 binary=0 binary=0_4 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 15 binary=0 binary=0_5 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 16 binary=0 binary=0_6 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 17 binary=0 binary=0_7 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 18 binary=0 binary=0_8 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 19 binary=0 binary=0_9 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 20 binary=10 binary=10_1 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 21 binary=10 binary=10_10 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 22 binary=10 binary=10_2 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 23 binary=10 binary=10_3 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 24 binary=10 binary=10_4 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 25 binary=10 binary=10_5 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 26 binary=10 binary=10_6 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 27 binary=10 binary=10_7 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 28 binary=10 binary=10_8 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 29 binary=10 binary=10_9 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 30 binary=2 binary=2_1 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 31 binary=2 binary=2_10 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 32 binary=2 binary=2_2 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 33 binary=2 binary=2_3 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 34 binary=2 binary=2_4 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 35 binary=2 binary=2_5 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 36 binary=2 binary=2_6 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 37 binary=2 binary=2_7 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 38 binary=2 binary=2_8 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 39 binary=2 binary=2_9 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 40 binary=4 binary=4_1 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 41 binary=4 binary=4_10 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 42 binary=4 binary=4_2 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 43 binary=4 binary=4_3 JIT_STATUS 420 Catch
----------------------------------

name 'x3' is not defined
import os
import torch
import torch.nn.functional as F
import torch.nn as nn
import numpy as np
from torch.autograd import Variable
import math
import torch as th
import torch.linalg as la
from torch.nn import Parameter
import torch.linalg as linalg

class Model(torch.nn.Module):

    def __init__(self, x2, x3):
        super().__init__()
        self.fc1 = torch.nn.Linear(2, 2)

    def forward(self, input_tensor, x2=x2, x3=x3):
        v1 = self.fc1(input_tensor)
        v2 = x2 + x3
        v3 = v1 + v2
        return v3


x2 = torch.randn(1, 1, 1, 256)
x3 = torch.randn(1, 1, 1, 256)
func = Model(x2, x3).to('cuda')


x2 = torch.randn(1, 1, 1, 256)

x3 = torch.randn(1, 1, 1, 256)
input_tensor = 1

test_inputs = [x2, x3, input_tensor]


LmfuzzTestcase 44 binary=4 binary=4_4 SKIP 420 void
----------------------------------

'Model' object has no attribute 'n_out'
import os
import torch
import torch.nn.functional as F
import torch.nn as nn
import numpy as np
from torch.autograd import Variable
import math
import torch as th
import torch.linalg as la
from torch.nn import Parameter
import torch.linalg as linalg

class Model(torch.nn.Module):

    def __init__(self, other_value: torch.Tensor=None):
        super().__init__()
        if other_value is None:
            other_value = torch.zeros(self.n_out, self.n_out)
        self.n_out = other_value.shape[0]
        self.t2 = other_value

    def forward(self, x1):
        v1 = x1.shape[1]
        v2 = torch.matmul(x1, torch.randn(v1, v1))
        v3 = v2 + self.t2
        return v3


func = Model().to('cuda')


x1 = torch.randn(1, 3, 64, 64)

test_inputs = [x1]


LmfuzzTestcase 45 binary=4 binary=4_5 SKIP 420 void
----------------------------------


LmfuzzTestcase 46 binary=4 binary=4_6 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 47 binary=4 binary=4_7 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 48 binary=4 binary=4_8 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 49 binary=4 binary=4_9 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 50 binary=6 binary=6_1 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 51 binary=6 binary=6_10 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 52 binary=6 binary=6_2 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 53 binary=6 binary=6_3 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 54 binary=6 binary=6_4 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 55 binary=6 binary=6_5 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 56 binary=6 binary=6_6 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 57 binary=6 binary=6_7 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 58 binary=6 binary=6_8 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 59 binary=6 binary=6_9 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 60 binary=8 binary=8_1 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 61 binary=8 binary=8_10 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 62 binary=8 binary=8_2 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 63 binary=8 binary=8_3 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 64 binary=8 binary=8_4 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 65 binary=8 binary=8_5 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 66 binary=8 binary=8_6 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 67 binary=8 binary=8_7 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 68 binary=8 binary=8_8 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 69 binary=8 binary=8_9 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 70 binary_unary=0 binary_unary=0_1 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 71 binary_unary=0 binary_unary=0_10 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 72 binary_unary=0 binary_unary=0_2 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 73 binary_unary=0 binary_unary=0_3 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 74 binary_unary=0 binary_unary=0_4 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 75 binary_unary=0 binary_unary=0_5 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 76 binary_unary=0 binary_unary=0_6 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 77 binary_unary=0 binary_unary=0_7 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 78 binary_unary=0 binary_unary=0_8 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 79 binary_unary=0 binary_unary=0_9 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 80 binary_unary=10 binary_unary=10_1 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 81 binary_unary=10 binary_unary=10_10 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 82 binary_unary=10 binary_unary=10_2 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 83 binary_unary=10 binary_unary=10_3 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 84 binary_unary=10 binary_unary=10_4 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 85 binary_unary=10 binary_unary=10_5 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 86 binary_unary=10 binary_unary=10_6 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 87 binary_unary=10 binary_unary=10_7 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 88 binary_unary=10 binary_unary=10_8 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 89 binary_unary=10 binary_unary=10_9 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 90 binary_unary=2 binary_unary=2_1 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 91 binary_unary=2 binary_unary=2_10 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 92 binary_unary=2 binary_unary=2_2 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 93 binary_unary=2 binary_unary=2_3 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 94 binary_unary=2 binary_unary=2_4 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 95 binary_unary=2 binary_unary=2_5 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 96 binary_unary=2 binary_unary=2_6 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 97 binary_unary=2 binary_unary=2_7 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 98 binary_unary=2 binary_unary=2_8 JIT_STATUS 420 Catch
----------------------------------

in_channels must be divisible by groups
import os
import torch
import torch.nn.functional as F
import torch.nn as nn
import numpy as np
from torch.autograd import Variable
import math
import torch as th
import torch.linalg as la
from torch.nn import Parameter
import torch.linalg as linalg

class Model(torch.nn.Module):

    def __init__(self):
        super().__init__()
        self.conv1 = torch.nn.Conv2d(1, 3, 3, stride=1, groups=1, padding=1)
        self.conv2 = torch.nn.Conv2d(1, 3, 3, stride=1, groups=2, padding=1)

    def forward(self, x1):
        v1 = self.conv1(x1)
        v2 = self.conv2(x1)
        v3 = torch.add(v1, v2)
        return torch.max(v3, 0.0)



func = Model().to('cuda')


x1 = torch.randn(1, 1, 64, 64)

test_inputs = [x1]


LmfuzzTestcase 99 binary_unary=2 binary_unary=2_9 SKIP 420 void
----------------------------------


LmfuzzTestcase 100 binary_unary=4 binary_unary=4_1 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 101 binary_unary=4 binary_unary=4_10 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 102 binary_unary=4 binary_unary=4_2 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 103 binary_unary=4 binary_unary=4_3 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 104 binary_unary=4 binary_unary=4_4 JIT_STATUS 420 Catch
----------------------------------

__init__() missing 2 required positional arguments: 'in_features' and 'out_features'
import os
import torch
import torch.nn.functional as F
import torch.nn as nn
import numpy as np
from torch.autograd import Variable
import math
import torch as th
import torch.linalg as la
from torch.nn import Parameter
import torch.linalg as linalg

class Model(torch.nn.Module):

    def __init__(self, other):
        super().__init__()
        self.linear = torch.nn.Linear()

    def forward(self, x1, other=None):
        v1 = self.linear(x1)
        v2 = v1 + other
        v3 = torch.relu(v2)
        return v3


other = 1

func = Model(other).to('cuda')


x1 = torch.randn(1, 3, 64, 64)

test_inputs = [x1]


LmfuzzTestcase 105 binary_unary=4 binary_unary=4_5 SKIP 420 void
----------------------------------


LmfuzzTestcase 106 binary_unary=4 binary_unary=4_6 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 107 binary_unary=4 binary_unary=4_7 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 108 binary_unary=4 binary_unary=4_8 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 109 binary_unary=4 binary_unary=4_9 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 110 binary_unary=6 binary_unary=6_1 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 111 binary_unary=6 binary_unary=6_10 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 112 binary_unary=6 binary_unary=6_2 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 113 binary_unary=6 binary_unary=6_3 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 114 binary_unary=6 binary_unary=6_4 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 115 binary_unary=6 binary_unary=6_5 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 116 binary_unary=6 binary_unary=6_6 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 117 binary_unary=6 binary_unary=6_7 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 118 binary_unary=6 binary_unary=6_8 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 119 binary_unary=6 binary_unary=6_9 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 120 binary_unary=8 binary_unary=8_1 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 121 binary_unary=8 binary_unary=8_10 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 122 binary_unary=8 binary_unary=8_2 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 123 binary_unary=8 binary_unary=8_3 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 124 binary_unary=8 binary_unary=8_4 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 125 binary_unary=8 binary_unary=8_5 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 126 binary_unary=8 binary_unary=8_6 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 127 binary_unary=8 binary_unary=8_7 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 128 binary_unary=8 binary_unary=8_8 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 129 binary_unary=8 binary_unary=8_9 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 130 cat_addmm cat_addmm_1 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 131 cat_addmm cat_addmm_10 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 132 cat_addmm cat_addmm_2 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 133 cat_addmm cat_addmm_3 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 134 cat_addmm cat_addmm_4 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 135 cat_addmm cat_addmm_5 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 136 cat_addmm cat_addmm_6 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 137 cat_addmm cat_addmm_7 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 138 cat_addmm cat_addmm_8 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 139 cat_addmm cat_addmm_9 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 140 cat_mm cat_mm_1 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 141 cat_mm cat_mm_10 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 142 cat_mm cat_mm_2 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 143 cat_mm cat_mm_3 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 144 cat_mm cat_mm_4 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 145 cat_mm cat_mm_5 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 146 cat_mm cat_mm_6 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 147 cat_mm cat_mm_7 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 148 cat_mm cat_mm_8 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 149 cat_mm cat_mm_9 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 150 cat_slice_cat cat_slice_cat_1 closing parenthesis ')' does not match opening parenthesis '[' 420 ('<unknown>', 10, 41, '        v2 = v1[:, 0:9223372036854775807)]')
----------------------------------


LmfuzzTestcase 151 cat_slice_cat cat_slice_cat_10 closing parenthesis ')' does not match opening parenthesis '[' 420 ('<unknown>', 11, 41, '        s1 = c1[:, 0:9223372036854775807)]')
----------------------------------


LmfuzzTestcase 152 cat_slice_cat cat_slice_cat_2 closing parenthesis ')' does not match opening parenthesis '[' 420 ('<unknown>', 10, 40, '        v2 = v1[:, :9223372036854775807)]')
----------------------------------


LmfuzzTestcase 153 cat_slice_cat cat_slice_cat_3 closing parenthesis ')' does not match opening parenthesis '[' 420 ('<unknown>', 7, 25, '        v2 = v1[:, (- 1))]')
----------------------------------


LmfuzzTestcase 154 cat_slice_cat cat_slice_cat_4 closing parenthesis ')' does not match opening parenthesis '[' 420 ('<unknown>', 10, 41, '        v2 = v1[:, 0:9223372036854775807)]')
----------------------------------


LmfuzzTestcase 155 cat_slice_cat cat_slice_cat_5 closing parenthesis ')' does not match opening parenthesis '[' 420 ('<unknown>', 11, 41, '        v2 = v1[:, 0:9223372036854775807)]')
----------------------------------


LmfuzzTestcase 156 cat_slice_cat cat_slice_cat_6 closing parenthesis ')' does not match opening parenthesis '[' 420 ('<unknown>', 7, 23, '        v2 = v1[:, ::2)]')
----------------------------------


LmfuzzTestcase 157 cat_slice_cat cat_slice_cat_7 closing parenthesis ')' does not match opening parenthesis '[' 420 ('<unknown>', 11, 24, '        t3 = t2[:, :250)]')
----------------------------------


LmfuzzTestcase 158 cat_slice_cat cat_slice_cat_8 closing parenthesis ')' does not match opening parenthesis '[' 420 ('<unknown>', 7, 41, '        t2 = t1[:, 0:9223372036854775807)]')
----------------------------------


LmfuzzTestcase 159 cat_slice_cat cat_slice_cat_9 closing parenthesis ')' does not match opening parenthesis '[' 420 ('<unknown>', 16, 29, '        v4 = v3[:, 0:4194303)]')
----------------------------------


LmfuzzTestcase 160 fuse_conv_bn fuse_conv_bn_1 JIT_STATUS 420 Catch
----------------------------------

'tuple' object has no attribute 'to'
import os
import torch
import torch.nn.functional as F
import torch.nn as nn
import numpy as np
from torch.autograd import Variable
import math
import torch as th
import torch.linalg as la
from torch.nn import Parameter
import torch.linalg as linalg



func = ().to('cuda')

input_tensor = torch.randn(1, 1, 1)

test_inputs = [input_tensor]


LmfuzzTestcase 161 fuse_conv_bn fuse_conv_bn_10 SKIP 420 void
----------------------------------


LmfuzzTestcase 162 fuse_conv_bn fuse_conv_bn_2 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 163 fuse_conv_bn fuse_conv_bn_3 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 164 fuse_conv_bn fuse_conv_bn_4 JIT_STATUS 420 Catch
----------------------------------

module 'torch.nn' has no attribute 'ConvTransfom'
import os
import torch
import torch.nn.functional as F
import torch.nn as nn
import numpy as np
from torch.autograd import Variable
import math
import torch as th
import torch.linalg as la
from torch.nn import Parameter
import torch.linalg as linalg

class Model(torch.nn.Module):

    def __init__(self):
        super().__init__()
        self.conv = torch.nn.ConvTransfom(3, 1, 1)
        self.bn = torch.nn.BatchNorm1d(3)

    def forward(self, x):
        x = self.conv(x)
        x = self.bn(x)
        return x



func = Model().to('cuda')


x = torch.randn(1, 3, 1, 1)

test_inputs = [x]


LmfuzzTestcase 165 fuse_conv_bn fuse_conv_bn_5 SKIP 420 void
----------------------------------


LmfuzzTestcase 166 fuse_conv_bn fuse_conv_bn_6 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 167 fuse_conv_bn fuse_conv_bn_7 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 168 fuse_conv_bn fuse_conv_bn_8 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 169 fuse_conv_bn fuse_conv_bn_9 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 170 linear_permute_fusion linear_permute_fusion_1 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 171 linear_permute_fusion linear_permute_fusion_10 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 172 linear_permute_fusion linear_permute_fusion_2 JIT_FAIL 420 Catch
----------------------------------

'tuple' object has no attribute 'to'
import os
import torch
import torch.nn.functional as F
import torch.nn as nn
import numpy as np
from torch.autograd import Variable
import math
import torch as th
import torch.linalg as la
from torch.nn import Parameter
import torch.linalg as linalg



func = ().to('cuda')

input_tensor = torch.randn(1, 1, 1)

test_inputs = [input_tensor]


LmfuzzTestcase 173 linear_permute_fusion linear_permute_fusion_3 SKIP 420 void
----------------------------------


LmfuzzTestcase 174 linear_permute_fusion linear_permute_fusion_4 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 175 linear_permute_fusion linear_permute_fusion_5 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 176 linear_permute_fusion linear_permute_fusion_6 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 177 linear_permute_fusion linear_permute_fusion_7 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 178 linear_permute_fusion linear_permute_fusion_8 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 179 linear_permute_fusion linear_permute_fusion_9 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 180 mm_plus_mm mm_plus_mm_1 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 181 mm_plus_mm mm_plus_mm_10 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 182 mm_plus_mm mm_plus_mm_2 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 183 mm_plus_mm mm_plus_mm_3 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 184 mm_plus_mm mm_plus_mm_4 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 185 mm_plus_mm mm_plus_mm_5 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 186 mm_plus_mm mm_plus_mm_6 Success 420 succeed
----------------------------------


LmfuzzTestcase 187 mm_plus_mm mm_plus_mm_7 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 188 mm_plus_mm mm_plus_mm_8 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 189 mm_plus_mm mm_plus_mm_9 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 190 permute_linear_fusion permute_linear_fusion_1 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 191 permute_linear_fusion permute_linear_fusion_10 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 192 permute_linear_fusion permute_linear_fusion_2 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 193 permute_linear_fusion permute_linear_fusion_3 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 194 permute_linear_fusion permute_linear_fusion_4 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 195 permute_linear_fusion permute_linear_fusion_5 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 196 permute_linear_fusion permute_linear_fusion_6 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 197 permute_linear_fusion permute_linear_fusion_7 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 198 permute_linear_fusion permute_linear_fusion_8 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 199 permute_linear_fusion permute_linear_fusion_9 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 200 permute_matmul_fusion permute_matmul_fusion_1 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 201 permute_matmul_fusion permute_matmul_fusion_10 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 202 permute_matmul_fusion permute_matmul_fusion_2 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 203 permute_matmul_fusion permute_matmul_fusion_3 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 204 permute_matmul_fusion permute_matmul_fusion_4 JIT_FAIL 420 Catch
----------------------------------

Can't get source for <function Model.forward at 0x7bf61e849b80>. TorchScript requires source access in order to carry out compilation, make sure original .py files are available.
import os
import torch
import torch.nn.functional as F
import torch.nn as nn
import numpy as np
from torch.autograd import Variable
import math
import torch as th
import torch.linalg as la
from torch.nn import Parameter
import torch.linalg as linalg

class Model(torch.jit.ScriptModule):

    @torch.jit.script_method
    def forward(self, x1, x2):
        v1 = x1.permute(0, 2, 1)
        v2 = torch.matmul(v1, x2)
        return v2



func = Model().to('cuda')


x1 = torch.randn(1, 2, 2)

x2 = torch.randn(1, 2, 2)

test_inputs = [x1, x2]


LmfuzzTestcase 205 permute_matmul_fusion permute_matmul_fusion_5 SKIP 420 void
----------------------------------


LmfuzzTestcase 206 permute_matmul_fusion permute_matmul_fusion_6 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 207 permute_matmul_fusion permute_matmul_fusion_7 JIT_STATUS 420 Catch
----------------------------------

module 'torch.nn' has no attribute 'Conv0D'
import os
import torch
import torch.nn.functional as F
import torch.nn as nn
import numpy as np
from torch.autograd import Variable
import math
import torch as th
import torch.linalg as la
from torch.nn import Parameter
import torch.linalg as linalg

class Model(torch.nn.Module):

    def __init__(self):
        super().__init__()
        self.conv = torch.nn.Conv0D(1, 1, 1)

    def forward(self, x1, x2):
        v1 = x1.permute(0, 2, 1)
        v3 = x2.permute(0, 2, 1)
        v2 = torch.bmm(v1, v3)
        return v2



func = Model().to('cuda')


x1 = torch.randn(1, 1, 1)

x2 = torch.randn(1, 1, 1)

test_inputs = [x1, x2]


LmfuzzTestcase 208 permute_matmul_fusion permute_matmul_fusion_8 SKIP 420 void
----------------------------------


LmfuzzTestcase 209 permute_matmul_fusion permute_matmul_fusion_9 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 210 pointless_cumsum_replacement pointless_cumsum_replacement_1 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 211 pointless_cumsum_replacement pointless_cumsum_replacement_10 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 212 pointless_cumsum_replacement pointless_cumsum_replacement_2 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 213 pointless_cumsum_replacement pointless_cumsum_replacement_3 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 214 pointless_cumsum_replacement pointless_cumsum_replacement_4 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 215 pointless_cumsum_replacement pointless_cumsum_replacement_5 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 216 pointless_cumsum_replacement pointless_cumsum_replacement_6 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 217 pointless_cumsum_replacement pointless_cumsum_replacement_7 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 218 pointless_cumsum_replacement pointless_cumsum_replacement_8 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 219 pointless_cumsum_replacement pointless_cumsum_replacement_9 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 220 replace_fx replace_fx_1 Success 420 succeed
----------------------------------


LmfuzzTestcase 221 replace_fx replace_fx_10 Success 420 succeed
----------------------------------


LmfuzzTestcase 222 replace_fx replace_fx_2 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 223 replace_fx replace_fx_3 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 224 replace_fx replace_fx_4 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 225 replace_fx replace_fx_5 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 226 replace_fx replace_fx_6 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 227 replace_fx replace_fx_7 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 228 replace_fx replace_fx_8 JIT_STATUS 420 Catch
----------------------------------

module 'torch.nn' has no attribute 'module'
import os
import torch
import torch.nn.functional as F
import torch.nn as nn
import numpy as np
from torch.autograd import Variable
import math
import torch as th
import torch.linalg as la
from torch.nn import Parameter
import torch.linalg as linalg

class model(torch.nn.module):

    def __init__(self):
        super().__init__()
        self.x = torch.rand(8, requires_grad=True)

    def forward(self, x):
        v1 = torch.nn.functional.dropout(self.x, p=0.2)
        v2 = torch.rand_like(x)
        return v1 + v2



func = model().to('cuda')


x1 = torch.randn(1, 2)

test_inputs = [x1]


LmfuzzTestcase 229 replace_fx replace_fx_9 SKIP 420 void
----------------------------------


LmfuzzTestcase 230 sfdp=0 sfdp=0_1 JIT_FAIL 420 Catch
----------------------------------

name 'query_dim' is not defined
import os
import torch
import torch.nn.functional as F
import torch.nn as nn
import numpy as np
from torch.autograd import Variable
import math
import torch as th
import torch.linalg as la
from torch.nn import Parameter
import torch.linalg as linalg

class AttentionModel(torch.nn.Module):

    def __init__(self):
        super().__init__()
        self.query = torch.nn.Linear(32, query_dim, bias=False)
        self.key = torch.nn.Linear(48, key_dim, bias=False)
        self.value = torch.nn.Linear(16, value_dim, bias=False)

    def forward(self, x1, x2):
        q = self.query(x1)
        k = self.key(x2)
        v = self.value(x2)
        scaled_dot_product = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(query_dim)
        attention_weights = scaled_dot_product.softmax(dim=-1)
        output = attention_weights.matmul(v)
        return output


func = AttentionModel().to('cuda')


batch_size = 1
x1 = torch.randn(batch_size, 32)

batch_size = 1
x2 = torch.randn(batch_size, 48)

test_inputs = [x1, x2]


LmfuzzTestcase 231 sfdp=0 sfdp=0_10 SKIP 420 void
----------------------------------


LmfuzzTestcase 232 sfdp=0 sfdp=0_2 JIT_FAIL 420 Catch
----------------------------------

module 'torch.nn' has no attribute 'Dense'
import os
import torch
import torch.nn.functional as F
import torch.nn as nn
import numpy as np
from torch.autograd import Variable
import math
import torch as th
import torch.linalg as la
from torch.nn import Parameter
import torch.linalg as linalg

class Attention(torch.nn.Module):

    def __init__(self, num_heads=1, hidden_size=128):
        super().__init__()
        self.num_heads = num_heads
        self.hidden_size = hidden_size
        self.head_size = hidden_size // num_heads
        self.scale = torch.sqrt(torch.tensor(self.head_size).float())

    def construct(self, query_vector, key_vector, value_vector):
        q = self.query(query_vector).view(-1, self.num_heads, self.head_size).transpose(0, 1)
        k = self.key(key_vector).view(-1, self.num_heads, self.head_size).transpose(0, 1)
        v = self.value(value_vector).view(-1, self.num_heads, self.head_size).transpose(0, 1)
        attn_weights = torch.matmul(q, k.transpose(-2, -1)) / self.scale
        attn_weights = F.softmax(attn_weights, dim=-1)
        head = torch.matmul(attn_weights, v)
        head = head.transpose(0, 1).contiguous().view(-1, self.num_heads * self.head_size)
        output = self.output(head)
        return output

    def construct_qkv(self, query_vector, key_vector, value_vector):
        q = self.query(query_vector).view(-1, self.num_heads, self.head_size).transpose(0, 1)
        k = self.key(key_vector).view(-1, self.num_heads, self.head_size).transpose(0, 1)
        v = self.value(value_vector).view(-1, self.num_heads, self.head_size).transpose(0, 1)
        return (q, k, v)

    def construct_output(self, head):
        return self.output(head)

    def query(self):
        raise NotImplementedError

    def key(self):
        raise NotImplementedError

    def value(self):
        raise NotImplementedError

    def output(self):
        raise NotImplementedError

class MHA(Attention):

    def __init__(self, num_heads, hidden_size, use_bias=False):
        super().__init__(num_heads, hidden_size)
        self.query = nn.Dense(in_channels=hidden_size, out_channels=hidden_size, has_bias=use_bias, activation=nn.ReLU())
        self.key = nn.Dense(in_channels=hidden_size, out_channels=hidden_size, has_bias=use_bias, activation=nn.ReLU())
        self.value = nn.Dense(in_channels=hidden_size, out_channels=hidden_size, has_bias=use_bias, activation=nn.ReLU())
        self.output = nn.Dense(in_channels=hidden_size, out_channels=hidden_size, has_bias=use_bias, activation=nn.ReLU())


num_heads = 1
hidden_size = 1
func = MHA(2, 8).to('cuda')


x1 = torch.randn(1, 8)

x2 = torch.randn(1, 8)

x3 = torch.randn(1, 8)

test_inputs = [x1, x2, x3]


LmfuzzTestcase 233 sfdp=0 sfdp=0_3 SKIP 420 void
----------------------------------

cannot assign module before Module.__init__() call
import os
import torch
import torch.nn.functional as F
import torch.nn as nn
import numpy as np
from torch.autograd import Variable
import math
import torch as th
import torch.linalg as la
from torch.nn import Parameter
import torch.linalg as linalg

class Model(torch.nn.Module):

    def __init__(self, dim=768):
        self.qkv = torch.nn.Linear(dim, dim * 3)
        self.out = torch.nn.Linear(dim, dim)
        self.qkv.apply(self._init_weights)
        self.out.apply(self._init_weights)
        self.dim = d

    def _init_weights(self, module):
        if isinstance(module, torch.nn.Linear):
            torch.nn.init.xavier_uniform(module.weight)
            if isinstance(module, torch.nn.Linear) and module.bias is not None:
                constant_(module.bias, 0.0)
        elif isinstance(module, torch.nn.LayerNorm):
            torch.nn.init.constant_(module.bias, 0.0)
            torch.nn.init.constant_(module.weight, 1.0)

    def forward(self, x):
        (q, k, v) = self.qkv(x).chunk(3, dim=-1)
        q /= math.sqrt(self.dim)
        attention = torch.matmul(q, k.transpose(-2, -1))
        attention = torch.softmax(attention, dim=-1)
        out = torch.matmul(attention, v)
        out = self.out(out)
        return out


func = Model().to('cuda')


x1 = torch.randn(1, 3, 768)

test_inputs = [x1]


LmfuzzTestcase 234 sfdp=0 sfdp=0_4 SKIP 420 void
----------------------------------


LmfuzzTestcase 235 sfdp=0 sfdp=0_5 JIT_FAIL 420 Catch
----------------------------------

name 'MultiHeadAttention' is not defined
import os
import torch
import torch.nn.functional as F
import torch.nn as nn
import numpy as np
from torch.autograd import Variable
import math
import torch as th
import torch.linalg as la
from torch.nn import Parameter
import torch.linalg as linalg

class Model(torch.nn.Module):

    def __init__(self, num_heads=8, hidden_key_size=8, hidden_value_size=8):
        super(Model, self).__init__()
        self.attn = MultiHeadAttention(num_heads, hidden_key_size, hidden_value_size)

    def forward(self, query, key, value, attn_mask=None, key_padding_mask=None):
        return self.attn(query, key, value, attn_mask, key_padding_mask)


func = Model().to('cuda')


x1 = torch.rand(1, 64, 32)

x2 = torch.rand(1, 64, 48)

x3 = torch.rand(1, 64, 32)
query = 1
key = 1

test_inputs = [x1, x2, x3, query, key]


LmfuzzTestcase 236 sfdp=0 sfdp=0_6 SKIP 420 void
----------------------------------


LmfuzzTestcase 237 sfdp=0 sfdp=0_7 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 238 sfdp=0 sfdp=0_8 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 239 sfdp=0 sfdp=0_9 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 240 sfdp=1 sfdp=1_1 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 241 sfdp=1 sfdp=1_10 closing parenthesis ')' does not match opening parenthesis '[' 420 ('<unknown>', 22, 29, 'attention_mask[:, :, 128:256)] = 1')
----------------------------------


LmfuzzTestcase 242 sfdp=1 sfdp=1_2 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 243 sfdp=1 sfdp=1_3 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 244 sfdp=1 sfdp=1_4 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 245 sfdp=1 sfdp=1_5 closing parenthesis ')' does not match opening parenthesis '[' 420 ('<unknown>', 12, 29, '        q = v1[:, :query_dim)]')
----------------------------------


LmfuzzTestcase 246 sfdp=1 sfdp=1_6 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 247 sfdp=1 sfdp=1_7 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 248 sfdp=1 sfdp=1_8 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 249 sfdp=1 sfdp=1_9 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 250 sfdp=2 sfdp=2_1 unindent does not match any outer indentation level 420 ('<unknown>', 15, 46, '        def forward(self, query, key, value):\n')
----------------------------------


LmfuzzTestcase 251 sfdp=2 sfdp=2_10 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 252 sfdp=2 sfdp=2_2 JIT_FAIL 420 Catch
----------------------------------

'tuple' object has no attribute 'to'
import os
import torch
import torch.nn.functional as F
import torch.nn as nn
import numpy as np
from torch.autograd import Variable
import math
import torch as th
import torch.linalg as la
from torch.nn import Parameter
import torch.linalg as linalg



func = ().to('cuda')


x1 = torch.randn(1, 3)

x2 = torch.randn(2, 2)

test_inputs = [x1, x2]


LmfuzzTestcase 253 sfdp=2 sfdp=2_3 SKIP 420 void
----------------------------------

name 'BertConfig' is not defined
import os
import torch
import torch.nn.functional as F
import torch.nn as nn
import numpy as np
from torch.autograd import Variable
import math
import torch as th
import torch.linalg as la
from torch.nn import Parameter
import torch.linalg as linalg

class BertSelfAttention(nn.Module):

    def __init__(self, config):
        super().__init__()
        if config.hidden_size % config.num_attention_heads != 0 and (not hasattr(config, 'embedding_size')):
            raise ValueError('The hidden size (%d) is not a multiple of the number of attention heads (%d)' % (config.hidden_size, config.num_attention_heads))
        self.output_attentions = config.output_attentions
        self.num_attention_heads = config.num_attention_heads
        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)
        self.all_head_size = self.num_attention_heads * self.attention_head_size
        self.query = nn.Linear(config.hidden_size, self.all_head_size)
        self.key = nn.Linear(config.hidden_size, self.all_head_size)
        self.value = nn.Linear(config.hidden_size, self.all_head_size)
        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)

    def transpose_for_scores(self, x):
        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)
        x = x.view(*new_x_shape)
        return x.permute(0, 2, 1, 3)

    def forward(self, hidden_states, attention_mask, head_mask=None):
        mixed_query_layer = self.query(hidden_states)
        mixed_key_layer = self.key(hidden_states)
        mixed_value_layer = self.value(hidden_states)
        query_layer = self.transpose_for_scores(mixed_query_layer)
        key_layer = self.transpose_for_scores(mixed_key_layer)
        value_layer = self.transpose_for_scores(mixed_value_layer)
        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))
        attention_scores = attention_scores / math.sqrt(self.attention_head_size)
        attention_scores = attention_scores + attention_mask
        attention_probs = nn.Softmax(dim=-1)(attention_scores)
        attention_probs = self.dropout(attention_probs)
        if head_mask is not None:
            attention_probs = attention_probs * head_mask
        context_layer = torch.matmul(attention_probs, value_layer)
        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()
        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)
        context_layer = context_layer.view(*new_context_layer_shape)
        outputs = (context_layer, attention_probs) if self.output_attentions else (context_layer,)
        return outputs


config = BertConfig(hidden_size=768, num_attention_heads=12)
func = BertSelfAttention(config).to('cuda')


x1 = torch.randn(2, 64, 768)

mask = torch.ones(2, 64, 64)
hidden_states = 1

test_inputs = [x1, mask, hidden_states]


LmfuzzTestcase 254 sfdp=2 sfdp=2_4 SKIP 420 void
----------------------------------


LmfuzzTestcase 255 sfdp=2 sfdp=2_5 JIT_FAIL 420 Catch
----------------------------------

'tuple' object has no attribute 'to'
import os
import torch
import torch.nn.functional as F
import torch.nn as nn
import numpy as np
from torch.autograd import Variable
import math
import torch as th
import torch.linalg as la
from torch.nn import Parameter
import torch.linalg as linalg



func = ().to('cuda')

input_tensor = torch.randn(1, 1, 1)

test_inputs = [input_tensor]


LmfuzzTestcase 256 sfdp=2 sfdp=2_6 SKIP 420 void
----------------------------------

name 'AttentionCore' is not defined
import os
import torch
import torch.nn.functional as F
import torch.nn as nn
import numpy as np
from torch.autograd import Variable
import math
import torch as th
import torch.linalg as la
from torch.nn import Parameter
import torch.linalg as linalg

class Model(torch.nn.Module):

    def __init__(self, query_channels, key_channels, n_units, n_heads, dropout):
        super().__init__()
        self.attn = AttentionCore(query_channels, key_channels, n_units, n_heads, dropout)

    def forward(self, query, key, value, scale_factor=1, dropout_p=0.2):
        v = self.attn(query, key, value, scale_factor, dropout_p)
        return v


query_channels = 1
key_channels = 1
n_units = 1
n_heads = 1
dropout = 1

func = Model(query_channels, key_channels, n_units, n_heads, dropout).to('cuda')


query = torch.randn(1, 32, 5, 32)

key = torch.randn(1, 32, 4, 32)

value = torch.randn(1, 32, 5, 32)

test_inputs = [query, key, value]


LmfuzzTestcase 257 sfdp=2 sfdp=2_7 SKIP 420 void
----------------------------------


LmfuzzTestcase 258 sfdp=2 sfdp=2_8 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 259 sfdp=2 sfdp=2_9 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 260 sfdp=3 sfdp=3_1 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 261 sfdp=3 sfdp=3_10 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 262 sfdp=3 sfdp=3_2 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 263 sfdp=3 sfdp=3_3 JIT_STATUS 420 Catch
----------------------------------

name 'k' is not defined
import os
import torch
import torch.nn.functional as F
import torch.nn as nn
import numpy as np
from torch.autograd import Variable
import math
import torch as th
import torch.linalg as la
from torch.nn import Parameter
import torch.linalg as linalg

class Model(torch.nn.Module):

    def __init__(self, query, key, value, scale_factor, dropout_p):
        super().__init__()
        self.query = torch.nn.Parameter(torch.randn(query))
        self.key = torch.nn.Parameter(torch.randn(key))
        self.value = torch.nn.Parameter(torch.randn(value))

    def forward(self, q, k, v):
        qk = torch.matmul(q, k.transpose(-2, -1))
        scaled_qk = qk.mul(scale_factor)
        softmax_qk = scaled_qk.softmax(dim=-1)
        dropout_qk = torch.nn.functional.dropout(softmax_qk, p=dropout_p)
        output = dropout_qk.matmul(v)
        return output


query = 1
key = 1
value = 1
scale_factor = 1 / math.sqrt(k.size(-1))
dropout_p = 0.1

func = Model(query, key, value, scale_factor, dropout_p).to('cuda')


q = torch.randn(1, 512, 384)

k = torch.randn(1, 512, 384)

v = torch.randn(1, 512, 384)

test_inputs = [q, k, v]


LmfuzzTestcase 264 sfdp=3 sfdp=3_4 SKIP 420 void
----------------------------------

module 'torch.nn' has no attribute 'MatMul'
import os
import torch
import torch.nn.functional as F
import torch.nn as nn
import numpy as np
from torch.autograd import Variable
import math
import torch as th
import torch.linalg as la
from torch.nn import Parameter
import torch.linalg as linalg

class Model(torch.nn.Module):

    def __init__(self):
        super().__init__()
        self.dropout = torch.nn.Dropout2d(p=0.0)
        self.softmax = torch.nn.Softmax(dim=-1)
        self.matmul1 = torch.nn.MatMul()
        self.matmul2 = torch.nn.MatMul()
        self.mul = torch.mul
        self.add = torch.add

    def forward(self, q, k, v, scale_factor):
        qk = self.matmul1(q, k)
        scaled_qk = self.mul(qk, scale_factor)
        softmax_qk = self.softmax(scaled_qk)
        dropout_qk = self.dropout(softmax_qk)
        output = self.matmul2(dropout_qk, v)
        return output


func = Model().to('cuda')


q = torch.randn(10, 5)

k = torch.randn(10, 5)

v = torch.randn(10, 5)
scale_factor = 1

test_inputs = [q, k, v, scale_factor]


LmfuzzTestcase 265 sfdp=3 sfdp=3_5 SKIP 420 void
----------------------------------


LmfuzzTestcase 266 sfdp=3 sfdp=3_6 JIT_STATUS 420 Catch
----------------------------------

name 'in_features' is not defined
import os
import torch
import torch.nn.functional as F
import torch.nn as nn
import numpy as np
from torch.autograd import Variable
import math
import torch as th
import torch.linalg as la
from torch.nn import Parameter
import torch.linalg as linalg

class Model(torch.nn.Module):

    def __init__(self, scale_factor, dropout_p, num_heads):
        super().__init__()
        self.scale_factor = scale_factor
        self.dropout_p = dropout_p
        self.linear_weight = torch.nn.Parameter(torch.randn((in_features, num_heads, head_features)))
        self.linear_bias = torch.nn.Parameter(torch.randn((in_features, num_heads, head_features)))

    def forward(m, inputs):
        (batch_size, seq_len, in_features) = inputs.shape
        weights = torch.nn.functional.softmax(self.weight, dim=-1)
        weights = torch.nn.functional.dropout(weights, p=self.dropout_p)
        bias = torch.nn.functional.dropout(self.bias, p=self.dropout_p)
        outputs = torch.einsum('ibj,jbf->ibf', weights, bias)
        return outputs


scale_factor = 1
dropout_p = 1
num_heads = 1

func = Model(scale_factor, dropout_p, num_heads).to('cuda')

inputs = 1

test_inputs = [inputs]


LmfuzzTestcase 267 sfdp=3 sfdp=3_7 SKIP 420 void
----------------------------------


LmfuzzTestcase 268 sfdp=3 sfdp=3_8 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 269 sfdp=3 sfdp=3_9 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 270 sfdp=4 sfdp=4_1 expected an indented block 420 ('<unknown>', 16, 0, 'def linear(input, weight, bias):\n')
----------------------------------


LmfuzzTestcase 271 sfdp=4 sfdp=4_10 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 272 sfdp=4 sfdp=4_2 closing parenthesis ')' does not match opening parenthesis '[' 420 ('<unknown>', 9, 27, '        q = __input__[:, 0)]')
----------------------------------


LmfuzzTestcase 273 sfdp=4 sfdp=4_3 closing parenthesis ')' does not match opening parenthesis '[' 420 ('<unknown>', 15, 42, '                attn_mask[:, :, i:(i + 1))] = 0')
----------------------------------


LmfuzzTestcase 274 sfdp=4 sfdp=4_4 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 275 sfdp=4 sfdp=4_5 closing parenthesis ')' does not match opening parenthesis '[' 420 ('<unknown>', 10, 51, '        attn_mask = ((torch.triu(x2[:(- 1), :(- 1))]) + torch.tril(x2[:(- 1), :(- 1))])) * (- 10000.0))')
----------------------------------


LmfuzzTestcase 276 sfdp=4 sfdp=4_6 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 277 sfdp=4 sfdp=4_7 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 278 sfdp=4 sfdp=4_8 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 279 sfdp=4 sfdp=4_9 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 280 sfdp=5 sfdp=5_1 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 281 sfdp=5 sfdp=5_10 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 282 sfdp=5 sfdp=5_2 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 283 sfdp=5 sfdp=5_3 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 284 sfdp=5 sfdp=5_4 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 285 sfdp=5 sfdp=5_5 closing parenthesis ')' does not match opening parenthesis '[' 420 ('<unknown>', 9, 23, '    angle_rads[:, 0::2)] = torch.sin(angle_rads[:, 0::2)])')
----------------------------------

'tuple' object has no attribute 'to'
import os
import torch
import torch.nn.functional as F
import torch.nn as nn
import numpy as np
from torch.autograd import Variable
import math
import torch as th
import torch.linalg as la
from torch.nn import Parameter
import torch.linalg as linalg



func = ().to('cuda')


query = torch.randn(1, 5, 128)

key = torch.randn(1, 5, 128)

attn_mask = torch.randn(1, 5)

test_inputs = [query, key, attn_mask]


LmfuzzTestcase 286 sfdp=5 sfdp=5_6 SKIP 420 void
----------------------------------


LmfuzzTestcase 287 sfdp=5 sfdp=5_7 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 288 sfdp=5 sfdp=5_8 JIT_FAIL 420 Catch
----------------------------------

name 'd_model' is not defined
import os
import torch
import torch.nn.functional as F
import torch.nn as nn
import numpy as np
from torch.autograd import Variable
import math
import torch as th
import torch.linalg as la
from torch.nn import Parameter
import torch.linalg as linalg

class Model(torch.nn.Module):

    def __init__(self, num_layers):
        super().__init__()
        self.multihead_attn = torch.nn.MultiheadAttention(d_model, num_heads)

    def forward(self, x1, x2, x3, x4, x5):
        v1 = self.multihead_attn(x1, x2, x3)
        v2 = torch.sigmoid(v1)
        v3 = v4 + v5
        return v6


num_layers = 1

func = Model(num_layers).to('cuda')


x1 = torch.randint(10, (1, 4, 1024))
x1 = torch.randint(10, (1, 4, 1024))
x2 = x1.clone()
x3 = (x1 - x2) % 13


x4 = torch.triu(torch.tril(x3.transpose(0, 1).transpose(1, 2), diagonal=10).transpose(0, 1).transpose(1, 2), diagonal=-10)
x3 = 1
x5 = 1

test_inputs = [x1, x2, x4, x3, x5]


LmfuzzTestcase 289 sfdp=5 sfdp=5_9 SKIP 420 void
----------------------------------


LmfuzzTestcase 290 sink_cat_after_pointwise sink_cat_after_pointwise_1 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 291 sink_cat_after_pointwise sink_cat_after_pointwise_10 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 292 sink_cat_after_pointwise sink_cat_after_pointwise_2 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 293 sink_cat_after_pointwise sink_cat_after_pointwise_3 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 294 sink_cat_after_pointwise sink_cat_after_pointwise_4 closing parenthesis ')' does not match opening parenthesis '[' 420 ('<unknown>', 9, 25, '        y = x[:, :, None)]')
----------------------------------


LmfuzzTestcase 295 sink_cat_after_pointwise sink_cat_after_pointwise_5 closing parenthesis ')' does not match opening parenthesis '[' 420 ('<unknown>', 9, 32, '        y = torch.cat([x[:, 0:1)], x[:, 1:4)]], dim=1)')
----------------------------------


LmfuzzTestcase 296 sink_cat_after_pointwise sink_cat_after_pointwise_6 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 297 sink_cat_after_pointwise sink_cat_after_pointwise_7 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 298 sink_cat_after_pointwise sink_cat_after_pointwise_8 closing parenthesis ')' does not match opening parenthesis '[' 420 ('<unknown>', 10, 19, '        y = x[:, 0)]')
----------------------------------


LmfuzzTestcase 299 sink_cat_after_pointwise sink_cat_after_pointwise_9 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 300 splitwithsizes_cat_replace splitwithsizes_cat_replace_1 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 301 splitwithsizes_cat_replace splitwithsizes_cat_replace_10 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 302 splitwithsizes_cat_replace splitwithsizes_cat_replace_2 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 303 splitwithsizes_cat_replace splitwithsizes_cat_replace_3 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 304 splitwithsizes_cat_replace splitwithsizes_cat_replace_4 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 305 splitwithsizes_cat_replace splitwithsizes_cat_replace_5 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 306 splitwithsizes_cat_replace splitwithsizes_cat_replace_6 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 307 splitwithsizes_cat_replace splitwithsizes_cat_replace_7 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 308 splitwithsizes_cat_replace splitwithsizes_cat_replace_8 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 309 splitwithsizes_cat_replace splitwithsizes_cat_replace_9 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 310 unary=0 unary=0_1 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 311 unary=0 unary=0_10 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 312 unary=0 unary=0_2 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 313 unary=0 unary=0_3 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 314 unary=0 unary=0_4 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 315 unary=0 unary=0_5 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 316 unary=0 unary=0_6 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 317 unary=0 unary=0_7 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 318 unary=0 unary=0_8 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 319 unary=0 unary=0_9 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 320 unary=1 unary=1_1 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 321 unary=1 unary=1_10 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 322 unary=1 unary=1_2 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 323 unary=1 unary=1_3 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 324 unary=1 unary=1_4 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 325 unary=1 unary=1_5 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 326 unary=1 unary=1_6 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 327 unary=1 unary=1_7 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 328 unary=1 unary=1_8 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 329 unary=1 unary=1_9 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 330 unary=10 unary=10_1 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 331 unary=10 unary=10_10 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 332 unary=10 unary=10_2 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 333 unary=10 unary=10_3 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 334 unary=10 unary=10_4 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 335 unary=10 unary=10_5 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 336 unary=10 unary=10_6 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 337 unary=10 unary=10_7 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 338 unary=10 unary=10_8 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 339 unary=10 unary=10_9 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 340 unary=11 unary=11_1 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 341 unary=11 unary=11_10 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 342 unary=11 unary=11_2 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 343 unary=11 unary=11_3 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 344 unary=11 unary=11_4 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 345 unary=11 unary=11_5 JIT_STATUS 420 Catch
----------------------------------

out_channels must be divisible by groups
import os
import torch
import torch.nn.functional as F
import torch.nn as nn
import numpy as np
from torch.autograd import Variable
import math
import torch as th
import torch.linalg as la
from torch.nn import Parameter
import torch.linalg as linalg

class Model(torch.nn.Module):

    def __init__(self):
        super().__init__()
        self.conv_transpose = torch.nn.ConvTranspose2d(8, 1, 5, stride=2, padding=2, groups=2)

    def forward(self, x1):
        v1 = self.conv_transpose(x1)
        v2 = v1 + 3
        v3 = torch.clamp_min(v2, 0)
        v4 = torch.clamp_max(v3, 6)
        v5 = v4 / 6
        return v5



func = Model().to('cuda')


x1 = torch.randn(1, 8, 64, 64)

test_inputs = [x1]


LmfuzzTestcase 346 unary=11 unary=11_6 SKIP 420 void
----------------------------------


LmfuzzTestcase 347 unary=11 unary=11_7 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 348 unary=11 unary=11_8 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 349 unary=11 unary=11_9 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 350 unary=12 unary=12_1 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 351 unary=12 unary=12_10 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 352 unary=12 unary=12_2 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 353 unary=12 unary=12_3 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 354 unary=12 unary=12_4 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 355 unary=12 unary=12_5 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 356 unary=12 unary=12_6 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 357 unary=12 unary=12_7 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 358 unary=12 unary=12_8 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 359 unary=12 unary=12_9 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 360 unary=13 unary=13_1 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 361 unary=13 unary=13_10 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 362 unary=13 unary=13_2 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 363 unary=13 unary=13_3 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 364 unary=13 unary=13_4 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 365 unary=13 unary=13_5 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 366 unary=13 unary=13_6 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 367 unary=13 unary=13_7 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 368 unary=13 unary=13_8 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 369 unary=13 unary=13_9 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 370 unary=14 unary=14_1 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 371 unary=14 unary=14_10 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 372 unary=14 unary=14_2 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 373 unary=14 unary=14_3 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 374 unary=14 unary=14_4 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 375 unary=14 unary=14_5 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 376 unary=14 unary=14_6 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 377 unary=14 unary=14_7 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 378 unary=14 unary=14_8 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 379 unary=14 unary=14_9 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 380 unary=15 unary=15_1 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 381 unary=15 unary=15_10 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 382 unary=15 unary=15_2 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 383 unary=15 unary=15_3 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 384 unary=15 unary=15_4 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 385 unary=15 unary=15_5 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 386 unary=15 unary=15_6 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 387 unary=15 unary=15_7 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 388 unary=15 unary=15_8 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 389 unary=15 unary=15_9 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 390 unary=16 unary=16_1 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 391 unary=16 unary=16_10 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 392 unary=16 unary=16_2 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 393 unary=16 unary=16_3 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 394 unary=16 unary=16_4 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 395 unary=16 unary=16_5 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 396 unary=16 unary=16_6 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 397 unary=16 unary=16_7 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 398 unary=16 unary=16_8 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 399 unary=16 unary=16_9 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 400 unary=17 unary=17_1 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 401 unary=17 unary=17_10 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 402 unary=17 unary=17_2 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 403 unary=17 unary=17_3 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 404 unary=17 unary=17_4 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 405 unary=17 unary=17_5 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 406 unary=17 unary=17_6 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 407 unary=17 unary=17_7 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 408 unary=17 unary=17_8 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 409 unary=17 unary=17_9 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 410 unary=18 unary=18_1 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 411 unary=18 unary=18_10 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 412 unary=18 unary=18_2 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 413 unary=18 unary=18_3 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 414 unary=18 unary=18_4 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 415 unary=18 unary=18_5 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 416 unary=18 unary=18_6 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 417 unary=18 unary=18_7 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 418 unary=18 unary=18_8 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 419 unary=18 unary=18_9 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 420 unary=19 unary=19_1 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 421 unary=19 unary=19_10 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 422 unary=19 unary=19_2 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 423 unary=19 unary=19_3 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 424 unary=19 unary=19_4 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 425 unary=19 unary=19_5 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 426 unary=19 unary=19_6 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 427 unary=19 unary=19_7 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 428 unary=19 unary=19_8 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 429 unary=19 unary=19_9 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 430 unary=2 unary=2_1 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 431 unary=2 unary=2_10 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 432 unary=2 unary=2_2 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 433 unary=2 unary=2_3 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 434 unary=2 unary=2_4 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 435 unary=2 unary=2_5 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 436 unary=2 unary=2_6 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 437 unary=2 unary=2_7 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 438 unary=2 unary=2_8 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 439 unary=2 unary=2_9 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 440 unary=20 unary=20_1 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 441 unary=20 unary=20_10 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 442 unary=20 unary=20_2 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 443 unary=20 unary=20_3 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 444 unary=20 unary=20_4 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 445 unary=20 unary=20_5 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 446 unary=20 unary=20_6 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 447 unary=20 unary=20_7 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 448 unary=20 unary=20_8 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 449 unary=20 unary=20_9 JIT_FAIL 420 Catch
----------------------------------

__init__() got an unexpected keyword argument 'in_channels'
import os
import torch
import torch.nn.functional as F
import torch.nn as nn
import numpy as np
from torch.autograd import Variable
import math
import torch as th
import torch.linalg as la
from torch.nn import Parameter
import torch.linalg as linalg

class ModelTanh(torch.nn.Module):

    def __init__(self):
        super().__init__()
        self.linear = torch.nn.Linear(in_channels=3, out_channels=257, bias=True)
        self.tanh = torch.nn.Tanh()

    def forward(self, x):
        v1 = self.linear(x)
        v2 = torch.tanh(v1)
        return v2



func = ModelTanh().to('cuda')


x = torch.randn(1, 3)

test_inputs = [x]


LmfuzzTestcase 450 unary=21 unary=21_1 SKIP 420 void
----------------------------------


LmfuzzTestcase 451 unary=21 unary=21_10 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 452 unary=21 unary=21_2 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 453 unary=21 unary=21_3 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 454 unary=21 unary=21_4 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 455 unary=21 unary=21_5 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 456 unary=21 unary=21_6 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 457 unary=21 unary=21_7 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 458 unary=21 unary=21_8 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 459 unary=21 unary=21_9 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 460 unary=22 unary=22_1 JIT_STATUS 420 Catch
----------------------------------

descriptor '__init__' of 'super' object needs an argument
import os
import torch
import torch.nn.functional as F
import torch.nn as nn
import numpy as np
from torch.autograd import Variable
import math
import torch as th
import torch.linalg as la
from torch.nn import Parameter
import torch.linalg as linalg

class Model(torch.nn.Module):

    def __init__(self):
        super.__init__()
        self.w = torch.rand(16, 32)
        self.linear = torch.nn.Linear(16, 32)

    def forward(self, x1):
        v1 = self.linear(x1)
        v2 = torch.tanh(v1)
        return v2


func = Model().to('cuda')


x1 = torch.randn(1, 16)

test_inputs = [x1]


LmfuzzTestcase 461 unary=22 unary=22_10 SKIP 420 void
----------------------------------


LmfuzzTestcase 462 unary=22 unary=22_2 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 463 unary=22 unary=22_3 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 464 unary=22 unary=22_4 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 465 unary=22 unary=22_5 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 466 unary=22 unary=22_6 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 467 unary=22 unary=22_7 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 468 unary=22 unary=22_8 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 469 unary=22 unary=22_9 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 470 unary=23 unary=23_1 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 471 unary=23 unary=23_10 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 472 unary=23 unary=23_2 JIT_FAIL 420 Catch
----------------------------------

in_channels must be divisible by groups
import os
import torch
import torch.nn.functional as F
import torch.nn as nn
import numpy as np
from torch.autograd import Variable
import math
import torch as th
import torch.linalg as la
from torch.nn import Parameter
import torch.linalg as linalg

class Model(torch.nn.Module):

    def __init__(self):
        super().__init__()
        self.conv_transpose = torch.nn.ConvTranspose2d(5, 2, 1, stride=1, padding=0, groups=2)

    def forward(self, x1):
        v1 = self.conv_transpose(x1)
        v2 = torch.tanh(v1)
        return v2



func = Model().to('cuda')


x1 = torch.randn(1, 5, 2, 1)

test_inputs = [x1]


LmfuzzTestcase 473 unary=23 unary=23_3 SKIP 420 void
----------------------------------


LmfuzzTestcase 474 unary=23 unary=23_4 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 475 unary=23 unary=23_5 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 476 unary=23 unary=23_6 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 477 unary=23 unary=23_7 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 478 unary=23 unary=23_8 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 479 unary=23 unary=23_9 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 480 unary=24 unary=24_1 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 481 unary=24 unary=24_10 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 482 unary=24 unary=24_2 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 483 unary=24 unary=24_3 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 484 unary=24 unary=24_4 JIT_FAIL 420 Catch
----------------------------------

cannot assign module before Module.__init__() call
import os
import torch
import torch.nn.functional as F
import torch.nn as nn
import numpy as np
from torch.autograd import Variable
import math
import torch as th
import torch.linalg as la
from torch.nn import Parameter
import torch.linalg as linalg

class Model(torch.nn.Module):

    def __init__(self, negative_slope):
        super(Model).__init__()
        self.conv = torch.nn.Conv2d(3, 8, 1, stride=1, padding=1)
        self.negative_slope = negative_slope

    def forward(self, x):
        v1 = self.conv(x)
        v2 = v1 > 0
        v3 = self.negative_slope * v1
        v4 = torch.where(v2, v1, v3)
        return v4


negative_slope = 1

func = Model(negative_slope).to('cuda')


x1 = torch.randn(1, 3, 64, 64)

test_inputs = [x1]


LmfuzzTestcase 485 unary=24 unary=24_5 SKIP 420 void
----------------------------------


LmfuzzTestcase 486 unary=24 unary=24_6 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 487 unary=24 unary=24_7 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 488 unary=24 unary=24_8 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 489 unary=24 unary=24_9 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 490 unary=25 unary=25_1 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 491 unary=25 unary=25_10 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 492 unary=25 unary=25_2 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 493 unary=25 unary=25_3 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 494 unary=25 unary=25_4 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 495 unary=25 unary=25_5 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 496 unary=25 unary=25_6 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 497 unary=25 unary=25_7 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 498 unary=25 unary=25_8 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 499 unary=25 unary=25_9 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 500 unary=26 unary=26_1 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 501 unary=26 unary=26_10 JIT_FAIL 420 Catch
----------------------------------

in_channels must be divisible by groups
import os
import torch
import torch.nn.functional as F
import torch.nn as nn
import numpy as np
from torch.autograd import Variable
import math
import torch as th
import torch.linalg as la
from torch.nn import Parameter
import torch.linalg as linalg

class Model(torch.nn.Module):

    def __init__(self):
        super().__init__()
        self.conv_t = torch.nn.ConvTranspose2d(19, 64, 1, stride=1, padding=0, groups=7)

    def forward(self, x2):
        v1 = self.conv_t(x2)
        v2 = v1 > 0
        v3 = v1 * -0.25
        v4 = torch.where(v2, v1, v3)
        return v4



func = Model().to('cuda')


x2 = torch.randn(8, 19, 4, 4)

test_inputs = [x2]


LmfuzzTestcase 502 unary=26 unary=26_2 SKIP 420 void
----------------------------------


LmfuzzTestcase 503 unary=26 unary=26_3 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 504 unary=26 unary=26_4 JIT_STATUS 420 Catch
----------------------------------

in_channels must be divisible by groups
import os
import torch
import torch.nn.functional as F
import torch.nn as nn
import numpy as np
from torch.autograd import Variable
import math
import torch as th
import torch.linalg as la
from torch.nn import Parameter
import torch.linalg as linalg

class Model(torch.nn.Module):

    def __init__(self):
        super().__init__()
        self.conv_transpose = torch.nn.ConvTranspose2d(19, 64, 1, stride=1, padding=0, dilation=3, groups=2, output_padding=1)

    def forward(self, x2):
        v1 = self.conv_transpose(x2)
        v2 = v1 > 0
        v3 = v1 * -0.1
        v4 = torch.where(v2, v1, v3)
        return v4



func = Model().to('cuda')


x2 = torch.randn(8, 19, 4, 4)

test_inputs = [x2]


LmfuzzTestcase 505 unary=26 unary=26_5 SKIP 420 void
----------------------------------


LmfuzzTestcase 506 unary=26 unary=26_6 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 507 unary=26 unary=26_7 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 508 unary=26 unary=26_8 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 509 unary=26 unary=26_9 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 510 unary=27 unary=27_1 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 511 unary=27 unary=27_10 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 512 unary=27 unary=27_2 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 513 unary=27 unary=27_3 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 514 unary=27 unary=27_4 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 515 unary=27 unary=27_5 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 516 unary=27 unary=27_6 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 517 unary=27 unary=27_7 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 518 unary=27 unary=27_8 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 519 unary=27 unary=27_9 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 520 unary=28 unary=28_1 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 521 unary=28 unary=28_10 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 522 unary=28 unary=28_2 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 523 unary=28 unary=28_3 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 524 unary=28 unary=28_4 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 525 unary=28 unary=28_5 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 526 unary=28 unary=28_6 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 527 unary=28 unary=28_7 JIT_STATUS 420 Catch
----------------------------------

name 'in_features' is not defined
import os
import torch
import torch.nn.functional as F
import torch.nn as nn
import numpy as np
from torch.autograd import Variable
import math
import torch as th
import torch.linalg as la
from torch.nn import Parameter
import torch.linalg as linalg

class Model(torch.nn.Module):

    def __init__(self):
        super().__init__()
        self.linear = torch.nn.Linear(in_features, out_features)
        self.min_value = torch.randn(1, in_features)
        self.max_value = torch.randn(1, in_features)

    def forward(self, x):
        y = self.linear(x)
        z = torch.clamp_min(y, self.min_value)
        return torch.clamp_max(z, self.max_value)


func = Model().to('cuda')

x = 1

test_inputs = [x]


LmfuzzTestcase 528 unary=28 unary=28_8 SKIP 420 void
----------------------------------


LmfuzzTestcase 529 unary=28 unary=28_9 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 530 unary=29 unary=29_1 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 531 unary=29 unary=29_10 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 532 unary=29 unary=29_2 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 533 unary=29 unary=29_3 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 534 unary=29 unary=29_4 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 535 unary=29 unary=29_5 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 536 unary=29 unary=29_6 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 537 unary=29 unary=29_7 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 538 unary=29 unary=29_8 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 539 unary=29 unary=29_9 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 540 unary=3 unary=3_1 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 541 unary=3 unary=3_10 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 542 unary=3 unary=3_2 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 543 unary=3 unary=3_3 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 544 unary=3 unary=3_4 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 545 unary=3 unary=3_5 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 546 unary=3 unary=3_6 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 547 unary=3 unary=3_7 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 548 unary=3 unary=3_8 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 549 unary=3 unary=3_9 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 550 unary=4 unary=4_1 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 551 unary=4 unary=4_10 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 552 unary=4 unary=4_2 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 553 unary=4 unary=4_3 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 554 unary=4 unary=4_4 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 555 unary=4 unary=4_5 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 556 unary=4 unary=4_6 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 557 unary=4 unary=4_7 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 558 unary=4 unary=4_8 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 559 unary=4 unary=4_9 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 560 unary=5 unary=5_1 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 561 unary=5 unary=5_10 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 562 unary=5 unary=5_2 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 563 unary=5 unary=5_3 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 564 unary=5 unary=5_4 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 565 unary=5 unary=5_5 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 566 unary=5 unary=5_6 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 567 unary=5 unary=5_7 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 568 unary=5 unary=5_8 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 569 unary=5 unary=5_9 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 570 unary=6 unary=6_1 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 571 unary=6 unary=6_10 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 572 unary=6 unary=6_2 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 573 unary=6 unary=6_3 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 574 unary=6 unary=6_4 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 575 unary=6 unary=6_5 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 576 unary=6 unary=6_6 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 577 unary=6 unary=6_7 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 578 unary=6 unary=6_8 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 579 unary=6 unary=6_9 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 580 unary=7 unary=7_1 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 581 unary=7 unary=7_10 positional argument follows keyword argument 420 ('<unknown>', 7, 42, '        return clamp(min=0, max=6, l1 + 3) * l1 / 6\n')
----------------------------------


LmfuzzTestcase 582 unary=7 unary=7_2 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 583 unary=7 unary=7_3 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 584 unary=7 unary=7_4 positional argument follows keyword argument 420 ('<unknown>', 9, 49, '        v2 = v1 * torch.clamp(min=0, max=6, v1+3)\n')
----------------------------------


LmfuzzTestcase 585 unary=7 unary=7_5 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 586 unary=7 unary=7_6 positional argument follows keyword argument 420 ('<unknown>', 9, 45, '        l2 = l1 * clamp(min=0, max=6, l1 + 3)\n')
----------------------------------


LmfuzzTestcase 587 unary=7 unary=7_7 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 588 unary=7 unary=7_8 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 589 unary=7 unary=7_9 JIT_STATUS 420 Catch
----------------------------------

out_channels must be divisible by groups
import os
import torch
import torch.nn.functional as F
import torch.nn as nn
import numpy as np
from torch.autograd import Variable
import math
import torch as th
import torch.linalg as la
from torch.nn import Parameter
import torch.linalg as linalg

class Model(torch.nn.Module):

    def __init__(self):
        super().__init__()
        self.conv = torch.nn.Conv2d(3, 8, 4, stride=2, padding=2, dilation=2)
        self.conv_transpose = torch.nn.ConvTranspose2d(8, 4, 4, stride=2, output_padding=2, groups=8)

    def forward(self, x1):
        v1 = self.conv(x1)
        v2 = self.conv_transpose(v1)
        v3 = v2 + 3
        v4 = torch.clamp(v3, min=0)
        v5 = torch.clamp(v4, max=6)
        v6 = v2 * v5
        v7 = v6 / 6
        return v7



func = Model().to('cuda')


x1 = torch.randn(1, 3, 64, 64)

test_inputs = [x1]


LmfuzzTestcase 590 unary=8 unary=8_1 SKIP 420 void
----------------------------------


LmfuzzTestcase 591 unary=8 unary=8_10 JIT_FAIL 420 Catch
----------------------------------

out_channels must be divisible by groups
import os
import torch
import torch.nn.functional as F
import torch.nn as nn
import numpy as np
from torch.autograd import Variable
import math
import torch as th
import torch.linalg as la
from torch.nn import Parameter
import torch.linalg as linalg

class Model(torch.nn.Module):

    def __init__(self):
        super().__init__()
        m1 = torch.nn.AvgPool2d(3, stride=2, padding=2, count_include_pad=True)
        m2 = torch.nn.Conv2d(8, 26, 5, stride=2, padding=0, groups=4, dilation=2)
        m3 = torch.nn.ConvTranspose2d(26, 6, 4, stride=2, padding=2, output_padding=2, groups=1, dilation=1)
        self.m123 = torch.nn.Sequential(m1, m2, m3)

    def forward(self, x1):
        v1 = self.m123(x1)
        v2 = v1 + 3
        v3 = torch.clamp(v2, min=0)
        v4 = torch.clamp(v3, max=6)
        v5 = v1 * v4
        v6 = v5 / 6
        return v6



func = Model().to('cuda')


x1 = torch.randn(1, 3, 64, 64)

test_inputs = [x1]


LmfuzzTestcase 592 unary=8 unary=8_2 SKIP 420 void
----------------------------------


LmfuzzTestcase 593 unary=8 unary=8_3 expected an indented block 420 ('<unknown>', 3, 5, '    \n')
----------------------------------


LmfuzzTestcase 594 unary=8 unary=8_4 JIT_FAIL 420 Catch
----------------------------------

out_channels must be divisible by groups
import os
import torch
import torch.nn.functional as F
import torch.nn as nn
import numpy as np
from torch.autograd import Variable
import math
import torch as th
import torch.linalg as la
from torch.nn import Parameter
import torch.linalg as linalg

class Model(torch.nn.Module):

    def __init__(self):
        super().__init__()
        self.conv = torch.nn.Conv2d(64, 10, 3, padding=1, groups=32)

    def forward(self, x1):
        v1 = self.conv(x1)
        v2 = v1 + 3.0
        v3 = v2 / 100
        v4 = torch.clamp(v3, min=-5, max=6)
        return v4



func = Model().to('cuda')


x1 = torch.randn(1, 64, 64, 64)

test_inputs = [x1]


LmfuzzTestcase 595 unary=8 unary=8_5 SKIP 420 void
----------------------------------


LmfuzzTestcase 596 unary=8 unary=8_6 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 597 unary=8 unary=8_7 JIT_STATUS 420 Catch
----------------------------------

in_channels must be divisible by groups
import os
import torch
import torch.nn.functional as F
import torch.nn as nn
import numpy as np
from torch.autograd import Variable
import math
import torch as th
import torch.linalg as la
from torch.nn import Parameter
import torch.linalg as linalg

class Model(torch.nn.Module):

    def __init__(self):
        super().__init__()
        self.conv_transpose = torch.nn.ConvTranspose2d(3, 8, 7, stride=2, output_padding=0, groups=4)

    def forward(self, x1):
        v1 = self.conv_transpose(x1)
        v2 = v1 + 3
        v3 = torch.clamp(v2, min=0)
        v4 = torch.clamp(v3, max=6)
        v5 = v1 * v4
        v6 = v5 / 6
        return v6



func = Model().to('cuda')


x1 = torch.randn(1, 3, 64, 64)

test_inputs = [x1]


LmfuzzTestcase 598 unary=8 unary=8_8 SKIP 420 void
----------------------------------


LmfuzzTestcase 599 unary=8 unary=8_9 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 600 unary=9 unary=9_1 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 601 unary=9 unary=9_10 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 602 unary=9 unary=9_2 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 603 unary=9 unary=9_3 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 604 unary=9 unary=9_4 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 605 unary=9 unary=9_5 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 606 unary=9 unary=9_6 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 607 unary=9 unary=9_7 JIT_FAIL 420 Catch
----------------------------------


LmfuzzTestcase 608 unary=9 unary=9_8 JIT_STATUS 420 Catch
----------------------------------


LmfuzzTestcase 609 unary=9 unary=9_9 unexpected indent 420 ('<unknown>', 1, 1, ' -- this model contains two convolution operations.\n')
----------------------------------


Used time: 509.79991030693054
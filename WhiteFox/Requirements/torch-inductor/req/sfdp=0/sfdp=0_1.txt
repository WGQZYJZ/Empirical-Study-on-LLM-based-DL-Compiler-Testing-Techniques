The model should contain the following pattern:
```
scaled_dot_product = torch.matmul(query, key.transpose(-2, -1)) / inv_scale
attention_weights = scaled_dot_product.softmax(dim=-1)
output = attention_weights.matmul(value)
```
This pattern characterizes the Scaled Dot-Product Attention mechanism, which is a key component of Transformer models. In this mechanism, the attention weights are computed as the softmax of the scaled dot product of the query and key tensors. These weights are then used to compute a weighted sum of the value tensor. The scaling factor `inv_scale` is typically the square root of the dimension of the key/query vectors, which helps to stabilize the gradients especially when the dimensions are large.
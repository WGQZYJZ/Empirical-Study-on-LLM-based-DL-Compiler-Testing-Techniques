1
Cov: 0 -> 0
Cov: 0 -> 0
2
Cov: 0 -> 17
Cov: 17 -> 17
3
Cov: 17 -> 17
Cov: 17 -> 17
4
Cov: 17 -> 40
Cov: 40 -> 40
5
Cov: 40 -> 81
Cov: 81 -> 81
6
Cov: 81 -> 83
Cov: 83 -> 83
7
Cov: 83 -> 83
Cov: 83 -> 83
8
Cov: 83 -> 265
Cov: 265 -> 265
9
Cov: 265 -> 265
Cov: 265 -> 265
10
Cov: 265 -> 265
Cov: 265 -> 265
11
Cov: 265 -> 265
Cov: 265 -> 265
12
Cov: 265 -> 296
Cov: 296 -> 296
13
Cov: 296 -> 296
Cov: 296 -> 296
14
Cov: 296 -> 296
Cov: 296 -> 296
15
Cov: 296 -> 296
Cov: 296 -> 296
16
Cov: 296 -> 333
Cov: 333 -> 333
17
Cov: 333 -> 333
Cov: 333 -> 333
18
Cov: 333 -> 333
Cov: 333 -> 333
19
Cov: 333 -> 349
Cov: 349 -> 349
20
Cov: 349 -> 349
Cov: 349 -> 349
21
Cov: 349 -> 359
Cov: 359 -> 359
22
Cov: 359 -> 359
Cov: 359 -> 359
23
Cov: 359 -> 376
Cov: 376 -> 376
24
Cov: 376 -> 376
Cov: 376 -> 376
25
Cov: 376 -> 376
Cov: 376 -> 376
26
Cov: 376 -> 376
Cov: 376 -> 376
27
Cov: 376 -> 376
Cov: 376 -> 376
28
Cov: 376 -> 377
Cov: 377 -> 377
29
Cov: 377 -> 377
Cov: 377 -> 377
30
Cov: 377 -> 377
Cov: 377 -> 377
31
Cov: 377 -> 386
Cov: 386 -> 386
32
Cov: 386 -> 391
Cov: 391 -> 391
33
Cov: 391 -> 392
Cov: 392 -> 392
34
Cov: 392 -> 420
Cov: 420 -> 420
35
Cov: 420 -> 420
Cov: 420 -> 420
36
Cov: 420 -> 420
Cov: 420 -> 420
37
Cov: 420 -> 420
Cov: 420 -> 420
38
Cov: 420 -> 421
Cov: 421 -> 421
39
Cov: 421 -> 425
Cov: 425 -> 425
40
{"exception": "RuntimeError", "msg": "any: You passed a dimname (string) to this op in place of a dimension index but it does not yet support this behavior. Please pass a dimension index to work around this."}
41
Cov: 425 -> 425
Cov: 425 -> 425
42
Cov: 425 -> 425
Cov: 425 -> 425
43
Cov: 425 -> 427
Cov: 427 -> 427
44
Cov: 427 -> 427
Cov: 427 -> 427
45
Cov: 427 -> 427
Cov: 427 -> 427
46
Cov: 427 -> 427
Cov: 427 -> 427
47
Cov: 427 -> 487
Cov: 487 -> 487
48
Cov: 487 -> 487
Cov: 487 -> 487
49
{"exception": "RuntimeError", "msg": "This function was deprecated since version 1.9 and is now removed. `torch.solve` is deprecated in favor of `torch.linalg.solve`. `torch.linalg.solve` has its arguments reversed and does not return the LU factorization.\n\nTo get the LU factorization see `torch.lu`, which can be used with `torch.lu_solve` or `torch.lu_unpack`.\nX = torch.solve(B, A).solution should be replaced with:\nX = torch.linalg.solve(A, B)"}
50
Cov: 487 -> 492
Cov: 492 -> 492
51
Cov: 492 -> 510
Cov: 510 -> 510
52
Cov: 510 -> 510
Cov: 510 -> 510
53
Cov: 510 -> 510
Cov: 510 -> 510
54
Cov: 510 -> 510
Cov: 510 -> 510
55
Cov: 510 -> 510
Cov: 510 -> 510
56
Cov: 510 -> 510
Cov: 510 -> 510
57
Cov: 510 -> 510
Cov: 510 -> 510
58
Cov: 510 -> 510
Cov: 510 -> 510
59
Cov: 510 -> 520
Cov: 520 -> 520
60
Cov: 520 -> 521
Cov: 521 -> 521
61
Cov: 521 -> 521
Cov: 521 -> 521
62
Cov: 521 -> 522
Cov: 522 -> 522
63
Cov: 522 -> 522
Cov: 522 -> 522
64
Cov: 522 -> 522
Cov: 522 -> 522
65
{"exception": "NotImplementedError", "msg": "Could not run 'aten::q_per_channel_zero_points' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::q_per_channel_zero_points' is only available for these backends: [QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nQuantizedCPU: registered at aten/src/ATen/RegisterQuantizedCPU.cpp:944 [kernel]\nQuantizedCUDA: registered at aten/src/ATen/RegisterQuantizedCUDA.cpp:459 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:153 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:498 [backend fallback]\nFunctionalize: registered at ../aten/src/ATen/FunctionalizeFallbackKernel.cpp:290 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:86 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradMTIA: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_1.cpp:16002 [kernel]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:382 [backend fallback]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:249 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:710 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:203 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:161 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:494 [backend fallback]\nPreDispatch: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:165 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:157 [backend fallback]\n"}
66
Cov: 522 -> 531
Cov: 531 -> 531
67
Cov: 531 -> 534
Cov: 534 -> 534
68
Cov: 534 -> 534
Cov: 534 -> 534
69
Cov: 534 -> 534
Cov: 534 -> 534
70
Cov: 534 -> 534
Cov: 534 -> 534
71
Cov: 534 -> 534
Cov: 534 -> 534
72
Cov: 534 -> 535
Cov: 535 -> 535
73
Cov: 535 -> 535
Cov: 535 -> 535
74
Cov: 535 -> 535
Cov: 535 -> 535
75
Cov: 535 -> 535
Cov: 535 -> 535
76
Cov: 535 -> 548
Cov: 548 -> 548
77
Cov: 548 -> 548
Cov: 548 -> 548
78
{"exception": "RuntimeError", "msg": "tensor.sspaddmm(...) can only be called on sparse tensors"}
79
Cov: 548 -> 549
Cov: 549 -> 549
80
Cov: 549 -> 549
Cov: 549 -> 549
81
Cov: 549 -> 556
Cov: 556 -> 556
82
Cov: 556 -> 560
Cov: 560 -> 560
83
Cov: 560 -> 561
Cov: 561 -> 561
84
{"exception": "RuntimeError", "msg": "result type Float can't be cast to the desired output type Long"}
85
Cov: 561 -> 726
Cov: 726 -> 726
86
Cov: 726 -> 726
Cov: 726 -> 726
87
Cov: 726 -> 734
Cov: 734 -> 734
88
Cov: 734 -> 735
Cov: 735 -> 735
89
Cov: 735 -> 735
Cov: 735 -> 735
90
Cov: 735 -> 735
Cov: 735 -> 735
91
Cov: 735 -> 735
Cov: 735 -> 735
92
Cov: 735 -> 735
Cov: 735 -> 735
93
Cov: 735 -> 912
Cov: 912 -> 912
94
Cov: 912 -> 912
Cov: 912 -> 912
95
Cov: 912 -> 912
Cov: 912 -> 912
96
Cov: 912 -> 912
Cov: 912 -> 912
97
Cov: 912 -> 918
Cov: 918 -> 918
98
{"exception": "ModuleNotFoundError", "msg": "No module named 'PIL'"}
99
Cov: 918 -> 12078
Cov: 12078 -> 12078
100
Cov: 12078 -> 12078
Cov: 12078 -> 12078
101
Cov: 12078 -> 12078
Cov: 12078 -> 12078
102
Cov: 12078 -> 12078
Cov: 12078 -> 12078
103
Cov: 12078 -> 12079
Cov: 12079 -> 12079
104
Cov: 12079 -> 12079
Cov: 12079 -> 12079
105
Cov: 12079 -> 12079
Cov: 12079 -> 12079
106
Cov: 12079 -> 12079
Cov: 12079 -> 12079
107
{"exception": "RuntimeError", "msg": "This function was deprecated since version 1.9 and is now removed. `torch.solve` is deprecated in favor of `torch.linalg.solve`. `torch.linalg.solve` has its arguments reversed and does not return the LU factorization.\n\nTo get the LU factorization see `torch.lu`, which can be used with `torch.lu_solve` or `torch.lu_unpack`.\nX = torch.solve(B, A).solution should be replaced with:\nX = torch.linalg.solve(A, B)"}
108
Cov: 12079 -> 12080
Cov: 12080 -> 12080
109
Cov: 12080 -> 12080
Cov: 12080 -> 12080
110
Cov: 12080 -> 12080
Cov: 12080 -> 12080
111
Cov: 12080 -> 12121
Cov: 12121 -> 12121
112
Cov: 12121 -> 12122
Cov: 12122 -> 12122
113
Cov: 12122 -> 12122
Cov: 12122 -> 12122
114
Cov: 12122 -> 12125
Cov: 12125 -> 12125
115
Cov: 12125 -> 12125
Cov: 12125 -> 12125
116
Cov: 12125 -> 12125
Cov: 12125 -> 12125
117
{"exception": "TypeError", "msg": "heaviside(): argument 'values' (position 1) must be Tensor, not int"}
118
Cov: 12125 -> 12178
Cov: 12178 -> 12178
119
Cov: 12178 -> 12178
Cov: 12178 -> 12178
120
Cov: 12178 -> 12183
Cov: 12183 -> 12183
121
Cov: 12183 -> 12191
Cov: 12191 -> 12191
122
Cov: 12191 -> 12191
Cov: 12191 -> 12191
123
Cov: 12191 -> 12191
Cov: 12191 -> 12191
124
Cov: 12191 -> 12191
Cov: 12191 -> 12191
125
Cov: 12191 -> 12191
Cov: 12191 -> 12191
126
Cov: 12191 -> 12191
Cov: 12191 -> 12191
127
Cov: 12191 -> 12195
Cov: 12195 -> 12195
128
Cov: 12195 -> 12195
Cov: 12195 -> 12195
129
Cov: 12195 -> 12195
Cov: 12195 -> 12195
130
Cov: 12195 -> 12195
Cov: 12195 -> 12195
131
Cov: 12195 -> 12195
Cov: 12195 -> 12195
132
Cov: 12195 -> 12214
Cov: 12214 -> 12214
133
Cov: 12214 -> 12214
Cov: 12214 -> 12214
134
Cov: 12214 -> 12221
Cov: 12221 -> 12221
135
Cov: 12221 -> 12221
Cov: 12221 -> 12221
136
Cov: 12221 -> 12221
Cov: 12221 -> 12221
137
Cov: 12221 -> 12221
Cov: 12221 -> 12221
138
Cov: 12221 -> 12222
Cov: 12222 -> 12222
139
Cov: 12222 -> 12222
Cov: 12222 -> 12222
140
Cov: 12222 -> 12222
Cov: 12222 -> 12222
141
Cov: 12222 -> 12222
Cov: 12222 -> 12222
142
Cov: 12222 -> 12247
Cov: 12247 -> 12247
143
Cov: 12247 -> 12247
Cov: 12247 -> 12247
144
Cov: 12247 -> 12247
Cov: 12247 -> 12247
145
Cov: 12247 -> 12247
Cov: 12247 -> 12247
146
Cov: 12247 -> 12247
Cov: 12247 -> 12247
147
Cov: 12247 -> 12247
Cov: 12247 -> 12247
148
Cov: 12247 -> 12247
Cov: 12247 -> 12247
149
Cov: 12247 -> 12247
Cov: 12247 -> 12247
150
Cov: 12247 -> 12247
Cov: 12247 -> 12247
151
Cov: 12247 -> 12254
Cov: 12254 -> 12254
152
Cov: 12254 -> 12254
Cov: 12254 -> 12254
153
Cov: 12254 -> 12255
Cov: 12255 -> 12255
154
Cov: 12255 -> 12277
Cov: 12277 -> 12277
155
Cov: 12277 -> 12294
Cov: 12294 -> 12294
156
{"exception": "NameError", "msg": "name 'TensorDataset' is not defined"}
157
Cov: 12294 -> 12294
Cov: 12294 -> 12294
158
Cov: 12294 -> 12295
Cov: 12295 -> 12295
159
Cov: 12295 -> 12295
Cov: 12295 -> 12295
160
Cov: 12295 -> 12309
Cov: 12309 -> 12309
161
Cov: 12309 -> 12309
Cov: 12309 -> 12309
162
Cov: 12309 -> 12339
Cov: 12339 -> 12339
163
Cov: 12339 -> 12339
Cov: 12339 -> 12339
164
Cov: 12339 -> 12339
Cov: 12339 -> 12339
165
Cov: 12339 -> 12354
Cov: 12354 -> 12354
166
Cov: 12354 -> 12354
Cov: 12354 -> 12354
167
Cov: 12354 -> 12354
Cov: 12354 -> 12354
168
Cov: 12354 -> 12354
Cov: 12354 -> 12354
169
Cov: 12354 -> 12354
Cov: 12354 -> 12354
170
Cov: 12354 -> 12354
Cov: 12354 -> 12354
171
Cov: 12354 -> 12354
Cov: 12354 -> 12354
172
Cov: 12354 -> 12354
Cov: 12354 -> 12354
173
Cov: 12354 -> 12354
Cov: 12354 -> 12354
174
Cov: 12354 -> 12354
Cov: 12354 -> 12354
175
Cov: 12354 -> 12354
Cov: 12354 -> 12354
176
Cov: 12354 -> 12354
Cov: 12354 -> 12354
177
Cov: 12354 -> 12354
Cov: 12354 -> 12354
178
Cov: 12354 -> 12354
Cov: 12354 -> 12354
179
Cov: 12354 -> 12354
Cov: 12354 -> 12354
180
Cov: 12354 -> 12354
Cov: 12354 -> 12354
181
Cov: 12354 -> 12354
Cov: 12354 -> 12354
182
Cov: 12354 -> 12354
Cov: 12354 -> 12354
183
Cov: 12354 -> 12368
Cov: 12368 -> 12368
184
Cov: 12368 -> 12415
Cov: 12415 -> 12415
185
Cov: 12415 -> 12415
Cov: 12415 -> 12415
186
Cov: 12415 -> 12415
Cov: 12415 -> 12415
187
Cov: 12415 -> 12415
Cov: 12415 -> 12415
188
Cov: 12415 -> 12415
Cov: 12415 -> 12415
189
Cov: 12415 -> 12421
Cov: 12421 -> 12421
190
Cov: 12421 -> 12421
Cov: 12421 -> 12421
191
Cov: 12421 -> 12421
Cov: 12421 -> 12421
192
Cov: 12421 -> 12421
Cov: 12421 -> 12421
193
Cov: 12421 -> 12421
Cov: 12421 -> 12421
194
Cov: 12421 -> 12421
Cov: 12421 -> 12421
195
Cov: 12421 -> 12421
Cov: 12421 -> 12421
196
Cov: 12421 -> 12423
Cov: 12423 -> 12423
197
Cov: 12423 -> 12423
Cov: 12423 -> 12423
198
Cov: 12423 -> 12423
Cov: 12423 -> 12423
199
Cov: 12423 -> 12429
Cov: 12429 -> 12429
200
{"exception": "TypeError", "msg": "normal_(): argument 'mean' must be float, not Tensor"}
201
Cov: 12429 -> 12429
Cov: 12429 -> 12429
202
Cov: 12429 -> 12429
Cov: 12429 -> 12429
203
Cov: 12429 -> 12429
Cov: 12429 -> 12429
204
Cov: 12429 -> 12429
Cov: 12429 -> 12429
205
Cov: 12429 -> 12429
Cov: 12429 -> 12429
206
Cov: 12429 -> 12432
Cov: 12432 -> 12432
207
Cov: 12432 -> 12432
Cov: 12432 -> 12432
208
Cov: 12432 -> 12432
Cov: 12432 -> 12432
209
Cov: 12432 -> 12432
Cov: 12432 -> 12432
210
Cov: 12432 -> 12432
Cov: 12432 -> 12432
211
Cov: 12432 -> 12441
Cov: 12441 -> 12441
212
Cov: 12441 -> 12441
Cov: 12441 -> 12441
213
Cov: 12441 -> 12441
Cov: 12441 -> 12441
214
{"exception": "RuntimeError", "msg": "The expanded size of the tensor (5) must match the existing size (4) at non-singleton dimension 2.  Target sizes: [10, 3, 5].  Tensor sizes: [10, 3, 4]"}
215
Cov: 12441 -> 12441
Cov: 12441 -> 12441
216
Cov: 12441 -> 12441
Cov: 12441 -> 12441
217
Cov: 12441 -> 12442
Cov: 12442 -> 12442
218
{"exception": "RuntimeError", "msg": "the base given to float_power_ has dtype Float but the operation's result requires dtype Double"}
219
Cov: 12442 -> 12442
Cov: 12442 -> 12442
220
Cov: 12442 -> 12442
Cov: 12442 -> 12442
221
{"exception": "RuntimeError", "msg": "vector + matrix @ vector expected, got 2, 2, 1"}
222
Cov: 12442 -> 12442
Cov: 12442 -> 12442
223
Cov: 12442 -> 12450
Cov: 12450 -> 12450
224
Cov: 12450 -> 12464
Cov: 12464 -> 12464
225
Cov: 12464 -> 12469
Cov: 12469 -> 12469
226
Cov: 12469 -> 12469
Cov: 12469 -> 12469
227
Cov: 12469 -> 12470
Cov: 12470 -> 12470
228
Cov: 12470 -> 12470
Cov: 12470 -> 12470
229
Cov: 12470 -> 12470
Cov: 12470 -> 12470
230
Cov: 12470 -> 12470
Cov: 12470 -> 12470
231
Cov: 12470 -> 12470
Cov: 12470 -> 12470
232
Cov: 12470 -> 12470
Cov: 12470 -> 12470
233
Cov: 12470 -> 12474
Cov: 12474 -> 12474
234
Cov: 12474 -> 12474
Cov: 12474 -> 12474
235
Cov: 12474 -> 12474
Cov: 12474 -> 12474
236
Cov: 12474 -> 12474
Cov: 12474 -> 12474
237
Cov: 12474 -> 12474
Cov: 12474 -> 12474
238
Cov: 12474 -> 12474
Cov: 12474 -> 12474
239
Cov: 12474 -> 12474
Cov: 12474 -> 12474
240
Cov: 12474 -> 12474
Cov: 12474 -> 12474
241
Cov: 12474 -> 12474
Cov: 12474 -> 12474
242
Cov: 12474 -> 12474
Cov: 12474 -> 12474
243
Cov: 12474 -> 12474
Cov: 12474 -> 12474
244
Cov: 12474 -> 12474
Cov: 12474 -> 12474
245
Cov: 12474 -> 12474
Cov: 12474 -> 12474
246
Cov: 12474 -> 12474
Cov: 12474 -> 12474
247
Cov: 12474 -> 12474
Cov: 12474 -> 12474
248
Cov: 12474 -> 12474
Cov: 12474 -> 12474
249
{"exception": "TypeError", "msg": "index_add_() missing 1 required positional arguments: \"source\""}
250
Cov: 12474 -> 12474
Cov: 12474 -> 12474
251
Cov: 12474 -> 12474
Cov: 12474 -> 12474
252
Cov: 12474 -> 12474
Cov: 12474 -> 12474
253
Cov: 12474 -> 12474
Cov: 12474 -> 12474
254
Cov: 12474 -> 12474
Cov: 12474 -> 12474
255
Cov: 12474 -> 12474
Cov: 12474 -> 12474
256
Cov: 12474 -> 12474
Cov: 12474 -> 12474
257
Cov: 12474 -> 12475
Cov: 12475 -> 12475
258
Cov: 12475 -> 12475
Cov: 12475 -> 12475
259
Cov: 12475 -> 12475
Cov: 12475 -> 12475
260
Cov: 12475 -> 12475
Cov: 12475 -> 12475
261
Cov: 12475 -> 12475
Cov: 12475 -> 12475
262
Cov: 12475 -> 12516
Cov: 12516 -> 12516
263
Cov: 12516 -> 12516
Cov: 12516 -> 12516
264
Cov: 12516 -> 12516
Cov: 12516 -> 12516
265
Cov: 12516 -> 12516
Cov: 12516 -> 12516
266
Cov: 12516 -> 12530
Cov: 12530 -> 12530
267
Cov: 12530 -> 12537
Cov: 12537 -> 12537
268
Cov: 12537 -> 12537
Cov: 12537 -> 12537
269
Cov: 12537 -> 12537
Cov: 12537 -> 12537
270
Cov: 12537 -> 12537
Cov: 12537 -> 12537
271
Cov: 12537 -> 12537
Cov: 12537 -> 12537
272
Cov: 12537 -> 12537
Cov: 12537 -> 12537
273
Cov: 12537 -> 12537
Cov: 12537 -> 12537
274
Cov: 12537 -> 12537
Cov: 12537 -> 12537
275
Cov: 12537 -> 12537
Cov: 12537 -> 12537
276
Cov: 12537 -> 12537
Cov: 12537 -> 12537
277
Cov: 12537 -> 12537
Cov: 12537 -> 12537
278
Cov: 12537 -> 12537
Cov: 12537 -> 12537
279
Cov: 12537 -> 12537
Cov: 12537 -> 12537
280
Cov: 12537 -> 12537
Cov: 12537 -> 12537
281
{"exception": "RuntimeError", "msg": "For integral input tensors, argument alpha must not be a floating point number."}
282
Cov: 12537 -> 12537
Cov: 12537 -> 12537
283
Cov: 12537 -> 12537
Cov: 12537 -> 12537
284
Cov: 12537 -> 12537
Cov: 12537 -> 12537
285
Cov: 12537 -> 12537
Cov: 12537 -> 12537
286
Cov: 12537 -> 12537
Cov: 12537 -> 12537
287
Cov: 12537 -> 12537
Cov: 12537 -> 12537
288
Cov: 12537 -> 12543
Cov: 12543 -> 12543
289
Cov: 12543 -> 12543
Cov: 12543 -> 12543
290
Cov: 12543 -> 12543
Cov: 12543 -> 12543
291
Cov: 12543 -> 12543
Cov: 12543 -> 12543
292
Cov: 12543 -> 12543
Cov: 12543 -> 12543
293
Cov: 12543 -> 12543
Cov: 12543 -> 12543
294
Cov: 12543 -> 12543
Cov: 12543 -> 12543
295
Cov: 12543 -> 12543
Cov: 12543 -> 12543
296
Cov: 12543 -> 12547
Cov: 12547 -> 12547
297
Cov: 12547 -> 12548
Cov: 12548 -> 12548
298
Cov: 12548 -> 12548
Cov: 12548 -> 12548
299
Cov: 12548 -> 12548
Cov: 12548 -> 12548
300
Cov: 12548 -> 12548
Cov: 12548 -> 12548
301
Cov: 12548 -> 12548
Cov: 12548 -> 12548
302
Cov: 12548 -> 12548
Cov: 12548 -> 12548
303
{"exception": "TypeError", "msg": "sum_to_size() got an unexpected keyword argument 'out'"}
304
Cov: 12548 -> 12548
Cov: 12548 -> 12548
305
Cov: 12548 -> 12548
Cov: 12548 -> 12548
306
Cov: 12548 -> 12548
Cov: 12548 -> 12548
307
Cov: 12548 -> 12548
Cov: 12548 -> 12548
308
Cov: 12548 -> 12548
Cov: 12548 -> 12548
309
Cov: 12548 -> 12548
Cov: 12548 -> 12548
310
Cov: 12548 -> 12548
Cov: 12548 -> 12548
311
{"exception": "RuntimeError", "msg": "torch.ormqr: other.shape[-2] must be equal to input.shape[-2]"}
312
Cov: 12548 -> 12548
Cov: 12548 -> 12548
313
Cov: 12548 -> 12606
Cov: 12606 -> 12606
314
Cov: 12606 -> 12622
Cov: 12622 -> 12622
315
Cov: 12622 -> 12622
Cov: 12622 -> 12622
316
Cov: 12622 -> 12623
Cov: 12623 -> 12623
317
Cov: 12623 -> 12627
Cov: 12627 -> 12627
318
Cov: 12627 -> 12627
Cov: 12627 -> 12627
319
Cov: 12627 -> 12642
Cov: 12642 -> 12642
320
Cov: 12642 -> 12642
Cov: 12642 -> 12642
321
Cov: 12642 -> 12642
Cov: 12642 -> 12642
322
Cov: 12642 -> 12692
Cov: 12692 -> 12692
323
Cov: 12692 -> 12692
Cov: 12692 -> 12692
324
Cov: 12692 -> 12692
Cov: 12692 -> 12692
325
{"exception": "RuntimeError", "msg": "expand(torch.FloatTensor{[10, 3, 4]}, size=[3, 5]): the number of sizes provided (2) must be greater or equal to the number of dimensions in the tensor (3)"}
326
Cov: 12692 -> 12692
Cov: 12692 -> 12692
327
Cov: 12692 -> 12692
Cov: 12692 -> 12692
328
Cov: 12692 -> 12692
Cov: 12692 -> 12692
329
Cov: 12692 -> 12692
Cov: 12692 -> 12692
330
Cov: 12692 -> 12693
Cov: 12693 -> 12693
331
Cov: 12693 -> 12694
Cov: 12694 -> 12694
332
Cov: 12694 -> 12694
Cov: 12694 -> 12694
333
Cov: 12694 -> 12694
Cov: 12694 -> 12694
334
Cov: 12694 -> 12695
Cov: 12695 -> 12695
335
Cov: 12695 -> 12695
Cov: 12695 -> 12695
336
Cov: 12695 -> 12695
Cov: 12695 -> 12695
337
Cov: 12695 -> 12695
Cov: 12695 -> 12695
338
Cov: 12695 -> 12700
Cov: 12700 -> 12700
339
Cov: 12700 -> 12701
Cov: 12701 -> 12701
340
Cov: 12701 -> 12701
Cov: 12701 -> 12701
341
Cov: 12701 -> 12701
Cov: 12701 -> 12701
342
Cov: 12701 -> 13152
Cov: 13152 -> 13152
343
Cov: 13152 -> 13152
Cov: 13152 -> 13152
344
Cov: 13152 -> 13152
Cov: 13152 -> 13152
345
{"exception": "RuntimeError", "msg": "a leaf Variable that requires grad is being used in an in-place operation."}
346
Cov: 13152 -> 13152
Cov: 13152 -> 13152
347
Cov: 13152 -> 13152
Cov: 13152 -> 13152
348
Cov: 13152 -> 13152
Cov: 13152 -> 13152
349
Cov: 13152 -> 13199
Cov: 13199 -> 13199
350
Cov: 13199 -> 13204
Cov: 13204 -> 13204
351
Cov: 13204 -> 13204
Cov: 13204 -> 13204
352
Cov: 13204 -> 13204
Cov: 13204 -> 13204
353
{"exception": "TypeError", "msg": "heaviside(): argument 'values' (position 1) must be Tensor, not float"}
354
Cov: 13204 -> 13204
Cov: 13204 -> 13204
355
Cov: 13204 -> 13204
Cov: 13204 -> 13204
356
Cov: 13204 -> 13204
Cov: 13204 -> 13204
357
Cov: 13204 -> 13218
Cov: 13218 -> 13218
358
{"exception": "TypeError", "msg": "addbmm() missing 1 required positional arguments: \"batch2\""}
359
Cov: 13218 -> 13218
Cov: 13218 -> 13218
360
Cov: 13218 -> 13222
Cov: 13222 -> 13222
361
Cov: 13222 -> 13222
Cov: 13222 -> 13222
362
Cov: 13222 -> 13222
Cov: 13222 -> 13222
363
Cov: 13222 -> 13222
Cov: 13222 -> 13222
364
Cov: 13222 -> 13223
Cov: 13223 -> 13223
365
{"exception": "RuntimeError", "msg": "This function was deprecated since version 1.9 and is now removed. `torch.solve` is deprecated in favor of `torch.linalg.solve`. `torch.linalg.solve` has its arguments reversed and does not return the LU factorization.\n\nTo get the LU factorization see `torch.lu`, which can be used with `torch.lu_solve` or `torch.lu_unpack`.\nX = torch.solve(B, A).solution should be replaced with:\nX = torch.linalg.solve(A, B)"}
366
Cov: 13223 -> 13224
Cov: 13224 -> 13224
367
Cov: 13224 -> 13224
Cov: 13224 -> 13224
368
Cov: 13224 -> 13224
Cov: 13224 -> 13224
369
Cov: 13224 -> 13224
Cov: 13224 -> 13224
370
Cov: 13224 -> 13224
Cov: 13224 -> 13224
371
Cov: 13224 -> 13224
Cov: 13224 -> 13224
372
Cov: 13224 -> 13224
Cov: 13224 -> 13224
373
Cov: 13224 -> 13224
Cov: 13224 -> 13224
374
Cov: 13224 -> 13225
Cov: 13225 -> 13225
375
Cov: 13225 -> 13287
Cov: 13287 -> 13287
376
Cov: 13287 -> 13288
Cov: 13288 -> 13288
377
Cov: 13288 -> 13288
Cov: 13288 -> 13288
378
Cov: 13288 -> 13288
Cov: 13288 -> 13288
379
{"exception": "TypeError", "msg": "igammac_(): argument 'other' (position 1) must be Tensor, not float"}
380
Cov: 13288 -> 13288
Cov: 13288 -> 13288
381
Cov: 13288 -> 13288
Cov: 13288 -> 13288
382
Cov: 13288 -> 13289
Cov: 13289 -> 13289
383
Cov: 13289 -> 13289
Cov: 13289 -> 13289
384
Cov: 13289 -> 13289
Cov: 13289 -> 13289
385
Cov: 13289 -> 13289
Cov: 13289 -> 13289
386
Cov: 13289 -> 13289
Cov: 13289 -> 13289
387
{"exception": "TypeError", "msg": "arctanh_() takes no keyword arguments"}
388
Cov: 13289 -> 13289
Cov: 13289 -> 13289
389
Cov: 13289 -> 13289
Cov: 13289 -> 13289
390
Cov: 13289 -> 13289
Cov: 13289 -> 13289
391
Cov: 13289 -> 13289
Cov: 13289 -> 13289
392
Cov: 13289 -> 13312
Cov: 13312 -> 13312
393
Cov: 13312 -> 13312
Cov: 13312 -> 13312
394
Cov: 13312 -> 13313
Cov: 13313 -> 13313
395
Cov: 13313 -> 13330
Cov: 13330 -> 13330
396
Cov: 13330 -> 13334
Cov: 13334 -> 13334
397
Cov: 13334 -> 13334
Cov: 13334 -> 13334
398
Cov: 13334 -> 13334
Cov: 13334 -> 13334
399
Cov: 13334 -> 13334
Cov: 13334 -> 13334
400
Cov: 13334 -> 13334
Cov: 13334 -> 13334
401
Cov: 13334 -> 13338
Cov: 13338 -> 13338
402
Cov: 13338 -> 13338
Cov: 13338 -> 13338
403
Cov: 13338 -> 13339
Cov: 13339 -> 13339
404
Cov: 13339 -> 13345
Cov: 13345 -> 13345
405
Cov: 13345 -> 13345
Cov: 13345 -> 13345
406
Cov: 13345 -> 13345
Cov: 13345 -> 13345
407
Cov: 13345 -> 13350
Cov: 13350 -> 13350
408
Cov: 13350 -> 13360
Cov: 13360 -> 13360
409
{"exception": "TypeError", "msg": "'getset_descriptor' object is not callable"}
410
Cov: 13360 -> 13372
Cov: 13372 -> 13372
411
Cov: 13372 -> 13391
Cov: 13391 -> 13391
412
Cov: 13391 -> 13391
Cov: 13391 -> 13391
413
Cov: 13391 -> 13391
Cov: 13391 -> 13391
414
Cov: 13391 -> 13391
Cov: 13391 -> 13391
415
Cov: 13391 -> 13408
Cov: 13408 -> 13408
416
Cov: 13408 -> 13408
Cov: 13408 -> 13408
417
Cov: 13408 -> 13409
Cov: 13409 -> 13409
418
Cov: 13409 -> 13409
Cov: 13409 -> 13409
419
Cov: 13409 -> 13409
Cov: 13409 -> 13409
420
Cov: 13409 -> 13409
Cov: 13409 -> 13409
421
Cov: 13409 -> 13422
Cov: 13422 -> 13422
422
Cov: 13422 -> 13422
Cov: 13422 -> 13422
423
Cov: 13422 -> 13428
Cov: 13428 -> 13428
424
Cov: 13428 -> 13428
Cov: 13428 -> 13428
425
Cov: 13428 -> 13428
Cov: 13428 -> 13428
426
Cov: 13428 -> 13428
Cov: 13428 -> 13428
427
Cov: 13428 -> 13428
Cov: 13428 -> 13428
428
{"exception": "RuntimeError", "msg": "\"lshift_cpu\" not implemented for 'Float'"}
429
Cov: 13428 -> 13429
Cov: 13429 -> 13429
430
Cov: 13429 -> 13429
Cov: 13429 -> 13429
431
{"exception": "TypeError", "msg": "arctanh_() takes no keyword arguments"}
432
Cov: 13429 -> 13429
Cov: 13429 -> 13429
433
Cov: 13429 -> 13429
Cov: 13429 -> 13429
434
Cov: 13429 -> 13431
Cov: 13431 -> 13431
435
Cov: 13431 -> 13431
Cov: 13431 -> 13431
436
Cov: 13431 -> 13431
Cov: 13431 -> 13431
437
Cov: 13431 -> 13431
Cov: 13431 -> 13431
438
Cov: 13431 -> 13431
Cov: 13431 -> 13431
439
Cov: 13431 -> 13432
Cov: 13432 -> 13432
440
{"exception": "TypeError", "msg": "'getset_descriptor' object is not callable"}
441
Cov: 13432 -> 13611
Cov: 13611 -> 13611
442
Cov: 13611 -> 13611
Cov: 13611 -> 13611
443
Cov: 13611 -> 13611
Cov: 13611 -> 13611
444
Cov: 13611 -> 13613
Cov: 13613 -> 13613
445
Cov: 13613 -> 13613
Cov: 13613 -> 13613
446
Cov: 13613 -> 13613
Cov: 13613 -> 13613
447
Cov: 13613 -> 13614
Cov: 13614 -> 13614
448
Cov: 13614 -> 13614
Cov: 13614 -> 13614
449
Cov: 13614 -> 13614
Cov: 13614 -> 13614
450
Cov: 13614 -> 13659
Cov: 13659 -> 13659
451
Cov: 13659 -> 13659
Cov: 13659 -> 13659
452
Cov: 13659 -> 13659
Cov: 13659 -> 13659
453
Cov: 13659 -> 13659
Cov: 13659 -> 13659
454
Cov: 13659 -> 13659
Cov: 13659 -> 13659
455
Cov: 13659 -> 13717
Cov: 13717 -> 13717
456
Cov: 13717 -> 13717
Cov: 13717 -> 13717
457
Cov: 13717 -> 13717
Cov: 13717 -> 13717
458
Cov: 13717 -> 13727
Cov: 13727 -> 13727
459
Cov: 13727 -> 13754
Cov: 13754 -> 13754
460
Cov: 13754 -> 13754
Cov: 13754 -> 13754
461
Cov: 13754 -> 13754
Cov: 13754 -> 13754
462
{"exception": "RuntimeError", "msg": "masked_scatter_ only supports boolean masks, but got mask with dtype Byte"}
463
Cov: 13754 -> 13754
Cov: 13754 -> 13754
464
Cov: 13754 -> 13758
Cov: 13758 -> 13758
465
Cov: 13758 -> 13758
Cov: 13758 -> 13758
466
Cov: 13758 -> 13758
Cov: 13758 -> 13758
467
Cov: 13758 -> 13760
Cov: 13760 -> 13760
468
Cov: 13760 -> 13760
Cov: 13760 -> 13760
469
Cov: 13760 -> 13760
Cov: 13760 -> 13760
470
Cov: 13760 -> 13760
Cov: 13760 -> 13760
471
Cov: 13760 -> 13760
Cov: 13760 -> 13760
472
Cov: 13760 -> 13760
Cov: 13760 -> 13760
473
Cov: 13760 -> 13760
Cov: 13760 -> 13760
474
Cov: 13760 -> 13760
Cov: 13760 -> 13760
475
Cov: 13760 -> 13760
Cov: 13760 -> 13760
476
Cov: 13760 -> 13760
Cov: 13760 -> 13760
477
{"exception": "RuntimeError", "msg": "Index tensor must have the same number of dimensions as self tensor"}
478
Cov: 13760 -> 13760
Cov: 13760 -> 13760
479
Cov: 13760 -> 13760
Cov: 13760 -> 13760
480
Cov: 13760 -> 13760
Cov: 13760 -> 13760
481
Cov: 13760 -> 13760
Cov: 13760 -> 13760
482
Cov: 13760 -> 13760
Cov: 13760 -> 13760
483
Cov: 13760 -> 13760
Cov: 13760 -> 13760
484
Cov: 13760 -> 13760
Cov: 13760 -> 13760
485
Cov: 13760 -> 13760
Cov: 13760 -> 13760
486
Cov: 13760 -> 13760
Cov: 13760 -> 13760
487
{"exception": "RuntimeError", "msg": "\"lcm_cpu\" not implemented for 'Float'"}
488
Cov: 13760 -> 13760
Cov: 13760 -> 13760
489
Cov: 13760 -> 13760
Cov: 13760 -> 13760
490
{"exception": "RuntimeError", "msg": "A must be batches of square matrices, but they are 2 by 1 matrices"}
491
Cov: 13760 -> 13760
Cov: 13760 -> 13760
492
Cov: 13760 -> 13760
Cov: 13760 -> 13760
493
Cov: 13760 -> 13761
Cov: 13761 -> 13761
494
Cov: 13761 -> 13761
Cov: 13761 -> 13761
495
Cov: 13761 -> 13761
Cov: 13761 -> 13761
496
Cov: 13761 -> 13761
Cov: 13761 -> 13761
497
Cov: 13761 -> 13761
Cov: 13761 -> 13761
498
Cov: 13761 -> 13761
Cov: 13761 -> 13761
499
Cov: 13761 -> 13761
Cov: 13761 -> 13761
500
Cov: 13761 -> 13761
Cov: 13761 -> 13761
501
Cov: 13761 -> 13762
Cov: 13762 -> 13762
502
Cov: 13762 -> 13763
Cov: 13763 -> 13763
503
{"exception": "RuntimeError", "msg": "This function was deprecated since version 1.9 and is now removed. `torch.lstsq` is deprecated in favor of `torch.linalg.lstsq`.\n`torch.linalg.lstsq` has reversed arguments and does not return the QR decomposition in the returned tuple (although it returns other information about the problem).\n\nTo get the QR decomposition consider using `torch.linalg.qr`.\n\nThe returned solution in `torch.lstsq` stored the residuals of the solution in the last m - n columns of the returned value whenever m > n. In torch.linalg.lstsq, the residuals are in the field 'residuals' of the returned named tuple.\n\nThe unpacking of the solution, as in\nX, _ = torch.lstsq(B, A).solution[:A.size(1)]\nshould be replaced with:\nX = torch.linalg.lstsq(A, B).solution"}
504
Cov: 13763 -> 13768
Cov: 13768 -> 13768
505
Cov: 13768 -> 13768
Cov: 13768 -> 13768
506
Cov: 13768 -> 13768
Cov: 13768 -> 13768
507
Cov: 13768 -> 13771
Cov: 13771 -> 13771
508
Cov: 13771 -> 13772
Cov: 13772 -> 13772
509
Cov: 13772 -> 13772
Cov: 13772 -> 13772
510
Cov: 13772 -> 13772
Cov: 13772 -> 13772
511
{"exception": "TypeError", "msg": "index_put_(): argument 'indices' (position 1) must be tuple of Tensors, not Tensor"}
512
{"exception": "RuntimeError", "msg": "result type Float can't be cast to the desired output type Long"}
513
Cov: 13772 -> 13772
Cov: 13772 -> 13772
514
Cov: 13772 -> 13772
Cov: 13772 -> 13772
515
Cov: 13772 -> 13772
Cov: 13772 -> 13772
516
Cov: 13772 -> 13772
Cov: 13772 -> 13772
517
Cov: 13772 -> 13772
Cov: 13772 -> 13772
518
Cov: 13772 -> 13772
Cov: 13772 -> 13772
519
Cov: 13772 -> 13773
Cov: 13773 -> 13773
520
Cov: 13773 -> 13773
Cov: 13773 -> 13773
521
Cov: 13773 -> 13773
Cov: 13773 -> 13773
522
Cov: 13773 -> 13773
Cov: 13773 -> 13773
523
Cov: 13773 -> 13782
Cov: 13782 -> 13782
524
Cov: 13782 -> 13782
Cov: 13782 -> 13782
525
Cov: 13782 -> 13782
Cov: 13782 -> 13782
526
Cov: 13782 -> 13782
Cov: 13782 -> 13782
527
Cov: 13782 -> 13782
Cov: 13782 -> 13782
528
Cov: 13782 -> 13783
Cov: 13783 -> 13783
529
{"exception": "RuntimeError", "msg": "masked_scatter: expected self and source to have same dtypes but gotFloat and Long"}
530
Cov: 13783 -> 13783
Cov: 13783 -> 13783
531
Cov: 13783 -> 13783
Cov: 13783 -> 13783
532
Cov: 13783 -> 21143
Cov: 21143 -> 21143
533
Cov: 21143 -> 21143
Cov: 21143 -> 21143
534
Cov: 21143 -> 21150
Cov: 21150 -> 21150
535
Cov: 21150 -> 21150
Cov: 21150 -> 21150
536
Cov: 21150 -> 21150
Cov: 21150 -> 21150
537
Cov: 21150 -> 21150
Cov: 21150 -> 21150
538
Cov: 21150 -> 21150
Cov: 21150 -> 21150
539
Cov: 21150 -> 21150
Cov: 21150 -> 21150
540
Cov: 21150 -> 21150
Cov: 21150 -> 21150
541
Cov: 21150 -> 21150
Cov: 21150 -> 21150
542
Cov: 21150 -> 21150
Cov: 21150 -> 21150
543
Cov: 21150 -> 21150
Cov: 21150 -> 21150
544
{"exception": "RuntimeError", "msg": "scatter(): Expected self.dtype to be equal to src.dtype"}
545
Cov: 21150 -> 21154
Cov: 21154 -> 21154
546
Cov: 21154 -> 21154
Cov: 21154 -> 21154
547
Cov: 21154 -> 21158
Cov: 21158 -> 21158
548
Cov: 21158 -> 21158
Cov: 21158 -> 21158
549
Cov: 21158 -> 21158
Cov: 21158 -> 21158
550
Cov: 21158 -> 21159
Cov: 21159 -> 21159
551
Cov: 21159 -> 21159
Cov: 21159 -> 21159
552
Cov: 21159 -> 21159
Cov: 21159 -> 21159
553
{"exception": "NotImplementedError", "msg": "Could not run 'aten::q_per_channel_axis' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::q_per_channel_axis' is only available for these backends: [QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nQuantizedCPU: registered at aten/src/ATen/RegisterQuantizedCPU.cpp:944 [kernel]\nQuantizedCUDA: registered at aten/src/ATen/RegisterQuantizedCUDA.cpp:459 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:153 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:498 [backend fallback]\nFunctionalize: registered at ../aten/src/ATen/FunctionalizeFallbackKernel.cpp:290 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:86 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradMTIA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_4.cpp:13056 [kernel]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:382 [backend fallback]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:249 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:710 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:203 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:161 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:494 [backend fallback]\nPreDispatch: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:165 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:157 [backend fallback]\n"}
554
Cov: 21159 -> 21160
Cov: 21160 -> 21160
555
Cov: 21160 -> 21160
Cov: 21160 -> 21160
556
Cov: 21160 -> 21160
Cov: 21160 -> 21160
557
Cov: 21160 -> 21160
Cov: 21160 -> 21160
558
{"exception": "TypeError", "msg": "'getset_descriptor' object is not callable"}
559
Cov: 21160 -> 21160
Cov: 21160 -> 21160
560
Cov: 21160 -> 21160
Cov: 21160 -> 21160
561
Cov: 21160 -> 21160
Cov: 21160 -> 21160
562
Cov: 21160 -> 21160
Cov: 21160 -> 21160
563
{"exception": "RuntimeError", "msg": "masked_scatter_ only supports boolean masks, but got mask with dtype Byte"}
564
Cov: 21160 -> 21163
Cov: 21163 -> 21163
565
Cov: 21163 -> 21163
Cov: 21163 -> 21163
566
Cov: 21163 -> 21163
Cov: 21163 -> 21163
567
Cov: 21163 -> 21163
Cov: 21163 -> 21163
568
Cov: 21163 -> 21163
Cov: 21163 -> 21163
569
Cov: 21163 -> 21179
Cov: 21179 -> 21179
570
Cov: 21179 -> 21179
Cov: 21179 -> 21179
571
{"exception": "NotImplementedError", "msg": "Could not run 'aten::q_per_channel_axis' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::q_per_channel_axis' is only available for these backends: [QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nQuantizedCPU: registered at aten/src/ATen/RegisterQuantizedCPU.cpp:944 [kernel]\nQuantizedCUDA: registered at aten/src/ATen/RegisterQuantizedCUDA.cpp:459 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:153 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:498 [backend fallback]\nFunctionalize: registered at ../aten/src/ATen/FunctionalizeFallbackKernel.cpp:290 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:86 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradMTIA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_4.cpp:13056 [kernel]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:382 [backend fallback]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:249 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:710 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:203 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:161 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:494 [backend fallback]\nPreDispatch: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:165 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:157 [backend fallback]\n"}
572
Cov: 21179 -> 21179
Cov: 21179 -> 21179
573
Cov: 21179 -> 21199
Cov: 21199 -> 21199
574
{"exception": "TypeError", "msg": "descriptor 'new_tensor' for 'torch._C._TensorBase' objects doesn't apply to a 'numpy.ndarray' object"}
575
Cov: 21199 -> 21202
Cov: 21202 -> 21202
576
Cov: 21202 -> 21202
Cov: 21202 -> 21202
577
Cov: 21202 -> 21202
Cov: 21202 -> 21202
578
Cov: 21202 -> 21202
Cov: 21202 -> 21202
579
Cov: 21202 -> 21202
Cov: 21202 -> 21202
580
Cov: 21202 -> 21202
Cov: 21202 -> 21202
581
Cov: 21202 -> 21203
Cov: 21203 -> 21203
582
Cov: 21203 -> 21203
Cov: 21203 -> 21203
583
Cov: 21203 -> 21203
Cov: 21203 -> 21203
584
Cov: 21203 -> 21209
Cov: 21209 -> 21209
585
Cov: 21209 -> 21210
Cov: 21210 -> 21210
586
Cov: 21210 -> 21210
Cov: 21210 -> 21210
587
Cov: 21210 -> 21210
Cov: 21210 -> 21210
588
Cov: 21210 -> 21222
Cov: 21222 -> 21222
589
Cov: 21222 -> 21222
Cov: 21222 -> 21222
590
Cov: 21222 -> 21223
Cov: 21223 -> 21223
591
Cov: 21223 -> 21223
Cov: 21223 -> 21223
592
Cov: 21223 -> 21224
Cov: 21224 -> 21224
593
Cov: 21224 -> 21224
Cov: 21224 -> 21224
594
Cov: 21224 -> 21224
Cov: 21224 -> 21224
595
Cov: 21224 -> 21224
Cov: 21224 -> 21224
596
Cov: 21224 -> 21224
Cov: 21224 -> 21224
597
Cov: 21224 -> 21224
Cov: 21224 -> 21224
598
{"exception": "TypeError", "msg": "'getset_descriptor' object is not callable"}
599
Cov: 21224 -> 21225
Cov: 21225 -> 21225
600
Cov: 21225 -> 21225
Cov: 21225 -> 21225
601
Cov: 21225 -> 21226
Cov: 21226 -> 21226
602
Cov: 21226 -> 21226
Cov: 21226 -> 21226
603
Cov: 21226 -> 21226
Cov: 21226 -> 21226
604
Cov: 21226 -> 21226
Cov: 21226 -> 21226
605
Cov: 21226 -> 21226
Cov: 21226 -> 21226
606
Cov: 21226 -> 21226
Cov: 21226 -> 21226
607
Cov: 21226 -> 21226
Cov: 21226 -> 21226
608
Cov: 21226 -> 21226
Cov: 21226 -> 21226
609
Cov: 21226 -> 21226
Cov: 21226 -> 21226
610
Cov: 21226 -> 21226
Cov: 21226 -> 21226
611
Cov: 21226 -> 21229
Cov: 21229 -> 21229
612
Cov: 21229 -> 21242
Cov: 21242 -> 21242
613
Cov: 21242 -> 21252
Cov: 21252 -> 21252
614
Cov: 21252 -> 21252
Cov: 21252 -> 21252
615
{"exception": "RuntimeError", "msg": "scatter(): Expected self.dtype to be equal to src.dtype"}
616
Cov: 21252 -> 21252
Cov: 21252 -> 21252
617
Cov: 21252 -> 21252
Cov: 21252 -> 21252
618
Cov: 21252 -> 21252
Cov: 21252 -> 21252
619
Cov: 21252 -> 21252
Cov: 21252 -> 21252
620
Cov: 21252 -> 21252
Cov: 21252 -> 21252
621
Cov: 21252 -> 21253
Cov: 21253 -> 21253
622
Cov: 21253 -> 21253
Cov: 21253 -> 21253
623
Cov: 21253 -> 21253
Cov: 21253 -> 21253
624
Cov: 21253 -> 21253
Cov: 21253 -> 21253
625
Cov: 21253 -> 21253
Cov: 21253 -> 21253
626
{"exception": "NotImplementedError", "msg": "Could not run 'aten::q_per_channel_zero_points' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::q_per_channel_zero_points' is only available for these backends: [QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nQuantizedCPU: registered at aten/src/ATen/RegisterQuantizedCPU.cpp:944 [kernel]\nQuantizedCUDA: registered at aten/src/ATen/RegisterQuantizedCUDA.cpp:459 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:153 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:498 [backend fallback]\nFunctionalize: registered at ../aten/src/ATen/FunctionalizeFallbackKernel.cpp:290 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:86 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradMTIA: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_1.cpp:16002 [kernel]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:382 [backend fallback]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:249 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:710 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:203 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:161 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:494 [backend fallback]\nPreDispatch: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:165 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:157 [backend fallback]\n"}
627
Cov: 21253 -> 21253
Cov: 21253 -> 21253
628
Cov: 21253 -> 21253
Cov: 21253 -> 21253
629
Cov: 21253 -> 21253
Cov: 21253 -> 21253
630
Cov: 21253 -> 21254
Cov: 21254 -> 21254
631
Cov: 21254 -> 21255
Cov: 21255 -> 21255
632
Cov: 21255 -> 21255
Cov: 21255 -> 21255
633
Cov: 21255 -> 21268
Cov: 21268 -> 21268
634
Cov: 21268 -> 21268
Cov: 21268 -> 21268
635
{"exception": "RuntimeError", "msg": "This function was deprecated since version 1.9 and is now removed. `torch.linalg.eig` returns complex tensors of dtype `cfloat` or `cdouble` rather than real tensors mimicking complex tensors.\n\nL, _ = torch.eig(A) should be replaced with:\nL_complex = torch.linalg.eigvals(A)\n\nand\n\nL, V = torch.eig(A, eigenvectors=True) should be replaced with:\nL_complex, V_complex = torch.linalg.eig(A)"}
636
Cov: 21268 -> 21271
Cov: 21271 -> 21271
637
Cov: 21271 -> 21271
Cov: 21271 -> 21271
638
Cov: 21271 -> 21273
Cov: 21273 -> 21273
639
Cov: 21273 -> 21273
Cov: 21273 -> 21273
640
{"exception": "TypeError", "msg": "'getset_descriptor' object is not callable"}
641
Cov: 21273 -> 21273
Cov: 21273 -> 21273
642
Cov: 21273 -> 21273
Cov: 21273 -> 21273
643
Cov: 21273 -> 21274
Cov: 21274 -> 21274
644
Cov: 21274 -> 21297
Cov: 21297 -> 21297
645
Cov: 21297 -> 21297
Cov: 21297 -> 21297
646
Cov: 21297 -> 21301
Cov: 21301 -> 21301
647
Cov: 21301 -> 21301
Cov: 21301 -> 21301
648
Cov: 21301 -> 21301
Cov: 21301 -> 21301
649
Cov: 21301 -> 21301
Cov: 21301 -> 21301
650
Cov: 21301 -> 21301
Cov: 21301 -> 21301
651
Cov: 21301 -> 21301
Cov: 21301 -> 21301
652
{"exception": "RuntimeError", "msg": "Expected nested_tensorlist[0].size() > 0 to be true, but got false.  (Could this error message be improved?  If so, please report an enhancement request to PyTorch.)"}
653
Cov: 21301 -> 21314
Cov: 21314 -> 21314
654
Cov: 21314 -> 21331
Cov: 21331 -> 21331
655
Cov: 21331 -> 21331
Cov: 21331 -> 21331
656
{"exception": "TypeError", "msg": "random_() missing 2 required positional argument: \"from\", \"to\""}
657
Cov: 21331 -> 21533
Cov: 21533 -> 21533
658
Cov: 21533 -> 21565
Cov: 21565 -> 21565
659
Cov: 21565 -> 21565
Cov: 21565 -> 21565
660
Cov: 21565 -> 21565
Cov: 21565 -> 21565
661
Cov: 21565 -> 21565
Cov: 21565 -> 21565
662
Cov: 21565 -> 21565
Cov: 21565 -> 21565
663
Cov: 21565 -> 21565
Cov: 21565 -> 21565
664
Cov: 21565 -> 21565
Cov: 21565 -> 21565
665
Cov: 21565 -> 21565
Cov: 21565 -> 21565
666
Cov: 21565 -> 21565
Cov: 21565 -> 21565
667
{"exception": "RuntimeError", "msg": "source tensor shape must match self tensor shape, excluding the specified dimension. Got self.shape = [2, 3, 3] source.shape = [2, 3]"}
668
Cov: 21565 -> 21565
Cov: 21565 -> 21565
669
Cov: 21565 -> 21565
Cov: 21565 -> 21565
670
Cov: 21565 -> 21565
Cov: 21565 -> 21565
671
Cov: 21565 -> 21565
Cov: 21565 -> 21565
672
{"exception": "RuntimeError", "msg": "This function was deprecated since version 1.9 and is now removed.\nPlease use the `torch.linalg.matrix_rank` function instead. The parameter 'symmetric' was renamed in `torch.linalg.matrix_rank()` to 'hermitian'."}
673
Cov: 21565 -> 21568
Cov: 21568 -> 21568
674
Cov: 21568 -> 21568
Cov: 21568 -> 21568
675
Cov: 21568 -> 21568
Cov: 21568 -> 21568
676
Cov: 21568 -> 21568
Cov: 21568 -> 21568
677
Cov: 21568 -> 21569
Cov: 21569 -> 21569
678
Cov: 21569 -> 21570
Cov: 21570 -> 21570
679
Cov: 21570 -> 21570
Cov: 21570 -> 21570
680
Cov: 21570 -> 21577
Cov: 21577 -> 21577
681
Cov: 21577 -> 21577
Cov: 21577 -> 21577
682
Cov: 21577 -> 21577
Cov: 21577 -> 21577
683
Cov: 21577 -> 21577
Cov: 21577 -> 21577
684
Cov: 21577 -> 21579
Cov: 21579 -> 21579
685
Cov: 21579 -> 21580
Cov: 21580 -> 21580
686
{"exception": "TypeError", "msg": "map_(): argument 'other' (position 1) must be Tensor, not builtin_function_or_method"}
687
Cov: 21580 -> 21580
Cov: 21580 -> 21580
688
Cov: 21580 -> 21580
Cov: 21580 -> 21580
689
Cov: 21580 -> 21580
Cov: 21580 -> 21580
690
Cov: 21580 -> 21580
Cov: 21580 -> 21580
691
Cov: 21580 -> 21580
Cov: 21580 -> 21580
692
Cov: 21580 -> 21580
Cov: 21580 -> 21580
693
Cov: 21580 -> 21580
Cov: 21580 -> 21580
694
Cov: 21580 -> 21580
Cov: 21580 -> 21580
695
Cov: 21580 -> 21580
Cov: 21580 -> 21580
696
Cov: 21580 -> 21580
Cov: 21580 -> 21580
697
Cov: 21580 -> 21580
Cov: 21580 -> 21580
698
Cov: 21580 -> 21580
Cov: 21580 -> 21580
699
Cov: 21580 -> 21597
Cov: 21597 -> 21597
700
{"exception": "RuntimeError", "msg": "scatter(): Expected self.dtype to be equal to src.dtype"}
701
Cov: 21597 -> 21597
Cov: 21597 -> 21597
702
Cov: 21597 -> 21597
Cov: 21597 -> 21597
703
Cov: 21597 -> 21599
Cov: 21599 -> 21599
704
Cov: 21599 -> 21599
Cov: 21599 -> 21599
705
Cov: 21599 -> 21599
Cov: 21599 -> 21599
706
Cov: 21599 -> 21599
Cov: 21599 -> 21599
707
{"exception": "RuntimeError", "msg": "Please look up dimensions by name, got: name = None."}
708
Cov: 21599 -> 21599
Cov: 21599 -> 21599
709
Cov: 21599 -> 21599
Cov: 21599 -> 21599
710
Cov: 21599 -> 21599
Cov: 21599 -> 21599
711
{"exception": "TypeError", "msg": "scatter_() received an invalid combination of arguments - got (int, Tensor, Tensor, NoneType), but expected one of:\n * (int dim, Tensor index, Tensor src)\n * (int dim, Tensor index, Tensor src, *, str reduce)\n * (int dim, Tensor index, Number value)\n * (int dim, Tensor index, Number value, *, str reduce)\n"}
712
Cov: 21599 -> 21599
Cov: 21599 -> 21599
713
Cov: 21599 -> 21599
Cov: 21599 -> 21599
714
Cov: 21599 -> 21600
Cov: 21600 -> 21600
715
Cov: 21600 -> 21600
Cov: 21600 -> 21600
716
Cov: 21600 -> 21600
Cov: 21600 -> 21600
717
Cov: 21600 -> 21602
Cov: 21602 -> 21602
718
Cov: 21602 -> 21602
Cov: 21602 -> 21602
719
{"exception": "RuntimeError", "msg": "any: You passed a dimname (string) to this op in place of a dimension index but it does not yet support this behavior. Please pass a dimension index to work around this."}
720
Cov: 21602 -> 21602
Cov: 21602 -> 21602
721
Cov: 21602 -> 21602
Cov: 21602 -> 21602
722
Cov: 21602 -> 21602
Cov: 21602 -> 21602
723
Cov: 21602 -> 21603
Cov: 21603 -> 21603
724
Cov: 21603 -> 21609
Cov: 21609 -> 21609
725
Cov: 21609 -> 21609
Cov: 21609 -> 21609
726
Cov: 21609 -> 21609
Cov: 21609 -> 21609
727
Cov: 21609 -> 21609
Cov: 21609 -> 21609
728
Cov: 21609 -> 21610
Cov: 21610 -> 21610
729
Cov: 21610 -> 21610
Cov: 21610 -> 21610
730
Cov: 21610 -> 21610
Cov: 21610 -> 21610
731
Cov: 21610 -> 21610
Cov: 21610 -> 21610
732
Cov: 21610 -> 21610
Cov: 21610 -> 21610
733
{"exception": "TypeError", "msg": "addbmm() missing 1 required positional arguments: \"batch2\""}
734
Cov: 21610 -> 21610
Cov: 21610 -> 21610
735
Cov: 21610 -> 21610
Cov: 21610 -> 21610
736
Cov: 21610 -> 21610
Cov: 21610 -> 21610
737
Cov: 21610 -> 21845
Cov: 21845 -> 21845
738
Cov: 21845 -> 21845
Cov: 21845 -> 21845
739
Cov: 21845 -> 21962
Cov: 21962 -> 21962
740
Cov: 21962 -> 21962
Cov: 21962 -> 21962
741
{"exception": "TypeError", "msg": "index_put_(): argument 'indices' (position 1) must be tuple of Tensors, not Tensor"}
742
{"exception": "RuntimeError", "msg": "This function was deprecated since version 1.9 and is now removed.\nPlease use the `torch.linalg.matrix_rank` function instead. The parameter 'symmetric' was renamed in `torch.linalg.matrix_rank()` to 'hermitian'."}
743
Cov: 21962 -> 21971
Cov: 21971 -> 21971
744
Cov: 21971 -> 22010
Cov: 22010 -> 22010
745
Cov: 22010 -> 22010
Cov: 22010 -> 22010
746
Cov: 22010 -> 22020
Cov: 22020 -> 22020
747
Cov: 22020 -> 22033
Cov: 22033 -> 22033
748
Cov: 22033 -> 22033
Cov: 22033 -> 22033
749
Cov: 22033 -> 22035
Cov: 22035 -> 22035
750
Cov: 22035 -> 22035
Cov: 22035 -> 22035
751
Cov: 22035 -> 22035
Cov: 22035 -> 22035
752
{"exception": "RuntimeError", "msg": "vector + matrix @ vector expected, got 2, 2, 1"}
753
Cov: 22035 -> 22035
Cov: 22035 -> 22035
754
Cov: 22035 -> 22036
Cov: 22036 -> 22036
755
Cov: 22036 -> 22041
Cov: 22041 -> 22041
756
Cov: 22041 -> 22041
Cov: 22041 -> 22041
757
Cov: 22041 -> 22041
Cov: 22041 -> 22041
758
Cov: 22041 -> 22042
Cov: 22042 -> 22042
759
Cov: 22042 -> 22042
Cov: 22042 -> 22042
760
Cov: 22042 -> 22042
Cov: 22042 -> 22042
761
Cov: 22042 -> 22042
Cov: 22042 -> 22042
762
Cov: 22042 -> 22042
Cov: 22042 -> 22042
763
Cov: 22042 -> 22042
Cov: 22042 -> 22042
764
Cov: 22042 -> 22045
Cov: 22045 -> 22045
765
Cov: 22045 -> 22054
Cov: 22054 -> 22054
766
{"exception": "_LinAlgError", "msg": "torch.linalg.solve: The solver failed because the input matrix is singular."}
767
Cov: 22054 -> 22054
Cov: 22054 -> 22054
768
Cov: 22054 -> 22054
Cov: 22054 -> 22054
769
Cov: 22054 -> 22057
Cov: 22057 -> 22057
770
Cov: 22057 -> 22058
Cov: 22058 -> 22058
771
Cov: 22058 -> 22073
Cov: 22073 -> 22073
772
Cov: 22073 -> 22073
Cov: 22073 -> 22073
773
Cov: 22073 -> 22076
Cov: 22076 -> 22076
774
Cov: 22076 -> 22078
Cov: 22078 -> 22078
775
Cov: 22078 -> 22078
Cov: 22078 -> 22078
776
Cov: 22078 -> 22078
Cov: 22078 -> 22078
777
Cov: 22078 -> 22078
Cov: 22078 -> 22078
778
Cov: 22078 -> 22078
Cov: 22078 -> 22078
779
Cov: 22078 -> 22084
Cov: 22084 -> 22084
780
Cov: 22084 -> 22084
Cov: 22084 -> 22084
781
Cov: 22084 -> 22084
Cov: 22084 -> 22084
782
Cov: 22084 -> 22084
Cov: 22084 -> 22084
783
Cov: 22084 -> 22084
Cov: 22084 -> 22084
784
Cov: 22084 -> 22084
Cov: 22084 -> 22084
785
Cov: 22084 -> 22084
Cov: 22084 -> 22084
786
Cov: 22084 -> 22086
Cov: 22086 -> 22086
787
Cov: 22086 -> 22086
Cov: 22086 -> 22086
788
Cov: 22086 -> 22089
Cov: 22089 -> 22089
789
Cov: 22089 -> 22115
Cov: 22115 -> 22115
790
Cov: 22115 -> 22115
Cov: 22115 -> 22115
791
Cov: 22115 -> 22115
Cov: 22115 -> 22115
792
Cov: 22115 -> 22115
Cov: 22115 -> 22115
793
Cov: 22115 -> 22115
Cov: 22115 -> 22115
794
Cov: 22115 -> 22116
Cov: 22116 -> 22116
795
Cov: 22116 -> 22116
Cov: 22116 -> 22116
796
Cov: 22116 -> 22116
Cov: 22116 -> 22116
797
Cov: 22116 -> 22116
Cov: 22116 -> 22116
798
Cov: 22116 -> 22118
Cov: 22118 -> 22118
799
Cov: 22118 -> 22118
Cov: 22118 -> 22118
800
Cov: 22118 -> 22118
Cov: 22118 -> 22118
801
Cov: 22118 -> 22118
Cov: 22118 -> 22118
802
Cov: 22118 -> 22118
Cov: 22118 -> 22118
803
Cov: 22118 -> 22118
Cov: 22118 -> 22118
804
Cov: 22118 -> 22121
Cov: 22121 -> 22121
805
Cov: 22121 -> 22121
Cov: 22121 -> 22121
806
Cov: 22121 -> 22121
Cov: 22121 -> 22121
807
{"exception": "RuntimeError", "msg": "istft requires a complex-valued input tensor matching the output from stft with return_complex=True."}
808
Cov: 22121 -> 22134
Cov: 22134 -> 22134
809
{"exception": "TypeError", "msg": "to_sparse() received an invalid combination of arguments - got (sparse_dims=int, ), but expected one of:\n * (*, torch.layout layout, tuple of ints blocksize, int dense_dim)\n * (int sparse_dim)\n      didn't match because some of the keywords were incorrect: sparse_dims\n"}
810
Cov: 22134 -> 22134
Cov: 22134 -> 22134
811
Cov: 22134 -> 22135
Cov: 22135 -> 22135
812
Cov: 22135 -> 22135
Cov: 22135 -> 22135
813
Cov: 22135 -> 22399
Cov: 22399 -> 22399
814
Cov: 22399 -> 22401
Cov: 22401 -> 22401
815
{"exception": "TypeError", "msg": "mul_() missing 1 required positional arguments: \"other\""}
816
Cov: 22401 -> 22402
Cov: 22402 -> 22402
817
Cov: 22402 -> 22406
Cov: 22406 -> 22406
818
Cov: 22406 -> 22406
Cov: 22406 -> 22406
819
Cov: 22406 -> 22406
Cov: 22406 -> 22406
820
Cov: 22406 -> 22406
Cov: 22406 -> 22406
821
Cov: 22406 -> 22406
Cov: 22406 -> 22406
822
Cov: 22406 -> 22406
Cov: 22406 -> 22406
823
Cov: 22406 -> 22406
Cov: 22406 -> 22406
824
Cov: 22406 -> 22406
Cov: 22406 -> 22406
825
Cov: 22406 -> 22406
Cov: 22406 -> 22406
826
{"exception": "RuntimeError", "msg": "torch.ormqr: other.shape[-2] must be greater than or equal to tau.shape[-1]"}
827
Cov: 22406 -> 22406
Cov: 22406 -> 22406
828
Cov: 22406 -> 22406
Cov: 22406 -> 22406
829
Cov: 22406 -> 22407
Cov: 22407 -> 22407
830
Cov: 22407 -> 22407
Cov: 22407 -> 22407
831
Cov: 22407 -> 22407
Cov: 22407 -> 22407
832
Cov: 22407 -> 22407
Cov: 22407 -> 22407
833
{"exception": "RuntimeError", "msg": "all: You passed a dimname (string) to this op in place of a dimension index but it does not yet support this behavior. Please pass a dimension index to work around this."}
834
Cov: 22407 -> 22407
Cov: 22407 -> 22407
835
Cov: 22407 -> 22421
Cov: 22421 -> 22421
836
Cov: 22421 -> 22427
Cov: 22427 -> 22427
837
Cov: 22427 -> 22427
Cov: 22427 -> 22427
838
Cov: 22427 -> 22427
Cov: 22427 -> 22427
839
Cov: 22427 -> 22427
Cov: 22427 -> 22427
840
Cov: 22427 -> 22427
Cov: 22427 -> 22427
841
Cov: 22427 -> 22427
Cov: 22427 -> 22427
842
Cov: 22427 -> 22427
Cov: 22427 -> 22427
843
Cov: 22427 -> 22427
Cov: 22427 -> 22427
844
Cov: 22427 -> 22432
Cov: 22432 -> 22432
845
Cov: 22432 -> 22433
Cov: 22433 -> 22433
846
Cov: 22433 -> 22433
Cov: 22433 -> 22433
847
Cov: 22433 -> 22434
Cov: 22434 -> 22434
848
Cov: 22434 -> 22491
Cov: 22491 -> 22491
849
Cov: 22491 -> 22491
Cov: 22491 -> 22491
850
Cov: 22491 -> 22491
Cov: 22491 -> 22491
851
Cov: 22491 -> 22492
Cov: 22492 -> 22492
852
Cov: 22492 -> 22492
Cov: 22492 -> 22492
853
Cov: 22492 -> 22492
Cov: 22492 -> 22492
854
Cov: 22492 -> 22492
Cov: 22492 -> 22492
855
Cov: 22492 -> 22492
Cov: 22492 -> 22492
856
Cov: 22492 -> 22493
Cov: 22493 -> 22493
857
Cov: 22493 -> 22493
Cov: 22493 -> 22493
858
Cov: 22493 -> 22494
Cov: 22494 -> 22494
859
Cov: 22494 -> 22495
Cov: 22495 -> 22495
860
Cov: 22495 -> 22495
Cov: 22495 -> 22495
861
Cov: 22495 -> 22495
Cov: 22495 -> 22495
862
Cov: 22495 -> 22495
Cov: 22495 -> 22495
863
Cov: 22495 -> 22630
Cov: 22630 -> 22630
864
Cov: 22630 -> 22630
Cov: 22630 -> 22630
865
Cov: 22630 -> 22630
Cov: 22630 -> 22630
866
Cov: 22630 -> 22630
Cov: 22630 -> 22630
867
Cov: 22630 -> 22630
Cov: 22630 -> 22630
868
Cov: 22630 -> 22630
Cov: 22630 -> 22630
869
Cov: 22630 -> 22630
Cov: 22630 -> 22630
870
Cov: 22630 -> 22630
Cov: 22630 -> 22630
871
Cov: 22630 -> 22630
Cov: 22630 -> 22630
872
Cov: 22630 -> 22631
Cov: 22631 -> 22631
873
Cov: 22631 -> 22631
Cov: 22631 -> 22631
874
Cov: 22631 -> 22631
Cov: 22631 -> 22631
875
Cov: 22631 -> 22631
Cov: 22631 -> 22631
876
Cov: 22631 -> 22631
Cov: 22631 -> 22631
877
Cov: 22631 -> 22631
Cov: 22631 -> 22631
878
{"exception": "IndexError", "msg": "Dimension out of range (expected to be in range of [-1, 0], but got 1)"}
879
Cov: 22631 -> 22631
Cov: 22631 -> 22631
880
Cov: 22631 -> 22631
Cov: 22631 -> 22631
881
Cov: 22631 -> 22631
Cov: 22631 -> 22631
882
Cov: 22631 -> 22631
Cov: 22631 -> 22631
883
Cov: 22631 -> 22632
Cov: 22632 -> 22632
884
Cov: 22632 -> 22632
Cov: 22632 -> 22632
885
Cov: 22632 -> 22632
Cov: 22632 -> 22632
886
Cov: 22632 -> 22632
Cov: 22632 -> 22632
887
Cov: 22632 -> 22632
Cov: 22632 -> 22632
888
Cov: 22632 -> 22632
Cov: 22632 -> 22632
889
Cov: 22632 -> 22632
Cov: 22632 -> 22632
890
Cov: 22632 -> 22632
Cov: 22632 -> 22632
891
Cov: 22632 -> 22632
Cov: 22632 -> 22632
892
{"exception": "TypeError", "msg": "'getset_descriptor' object is not callable"}
893
Cov: 22632 -> 22632
Cov: 22632 -> 22632
894
Cov: 22632 -> 22632
Cov: 22632 -> 22632
895
Cov: 22632 -> 22632
Cov: 22632 -> 22632
896
Cov: 22632 -> 22632
Cov: 22632 -> 22632
897
Cov: 22632 -> 22633
Cov: 22633 -> 22633
898
Cov: 22633 -> 22633
Cov: 22633 -> 22633
899
Cov: 22633 -> 22637
Cov: 22637 -> 22637
900
Cov: 22637 -> 22637
Cov: 22637 -> 22637
901
Cov: 22637 -> 22637
Cov: 22637 -> 22637
902
Cov: 22637 -> 22637
Cov: 22637 -> 22637
903
Cov: 22637 -> 22637
Cov: 22637 -> 22637
904
Cov: 22637 -> 22638
Cov: 22638 -> 22638
905
Cov: 22638 -> 22638
Cov: 22638 -> 22638
906
Cov: 22638 -> 22638
Cov: 22638 -> 22638
907
Cov: 22638 -> 22638
Cov: 22638 -> 22638
908
Cov: 22638 -> 22638
Cov: 22638 -> 22638
909
Cov: 22638 -> 22638
Cov: 22638 -> 22638
910
Cov: 22638 -> 22638
Cov: 22638 -> 22638
911
Cov: 22638 -> 22638
Cov: 22638 -> 22638
912
Cov: 22638 -> 22638
Cov: 22638 -> 22638
913
Cov: 22638 -> 22638
Cov: 22638 -> 22638
914
Cov: 22638 -> 22638
Cov: 22638 -> 22638
915
Cov: 22638 -> 22638
Cov: 22638 -> 22638
916
Cov: 22638 -> 22638
Cov: 22638 -> 22638
917
Cov: 22638 -> 22638
Cov: 22638 -> 22638
918
Cov: 22638 -> 22638
Cov: 22638 -> 22638
919
Cov: 22638 -> 22639
Cov: 22639 -> 22639
920
Cov: 22639 -> 22639
Cov: 22639 -> 22639
921
Cov: 22639 -> 22639
Cov: 22639 -> 22639
922
Cov: 22639 -> 22640
Cov: 22640 -> 22640
923
Cov: 22640 -> 22641
Cov: 22641 -> 22641
924
Cov: 22641 -> 22641
Cov: 22641 -> 22641
925
Cov: 22641 -> 22675
Cov: 22675 -> 22675
926
{"exception": "RuntimeError", "msg": "put_ does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True)'. You can turn off determinism just for this operation, or you can use the 'warn_only=True' option, if that's acceptable for your application. You can also file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation."}
927
Cov: 22675 -> 22675
Cov: 22675 -> 22675
928
Cov: 22675 -> 22678
Cov: 22678 -> 22678
929
Cov: 22678 -> 22678
Cov: 22678 -> 22678
930
Cov: 22678 -> 22678
Cov: 22678 -> 22678
931
Cov: 22678 -> 22691
Cov: 22691 -> 22691
932
Cov: 22691 -> 22691
Cov: 22691 -> 22691
933
Cov: 22691 -> 22691
Cov: 22691 -> 22691
934
Cov: 22691 -> 22691
Cov: 22691 -> 22691
935
Cov: 22691 -> 22691
Cov: 22691 -> 22691
936
Cov: 22691 -> 22691
Cov: 22691 -> 22691
937
Cov: 22691 -> 22897
Cov: 22897 -> 22897
938
Cov: 22897 -> 22897
Cov: 22897 -> 22897
939
Cov: 22897 -> 22900
Cov: 22900 -> 22900
940
Cov: 22900 -> 22900
Cov: 22900 -> 22900
941
Cov: 22900 -> 22900
Cov: 22900 -> 22900
942
Cov: 22900 -> 22900
Cov: 22900 -> 22900
943
Cov: 22900 -> 22900
Cov: 22900 -> 22900
944
Cov: 22900 -> 22900
Cov: 22900 -> 22900
945
Cov: 22900 -> 22900
Cov: 22900 -> 22900
946
Cov: 22900 -> 22900
Cov: 22900 -> 22900
947
Cov: 22900 -> 22901
Cov: 22901 -> 22901
948
Cov: 22901 -> 22901
Cov: 22901 -> 22901
949
Cov: 22901 -> 22901
Cov: 22901 -> 22901
950
Cov: 22901 -> 22901
Cov: 22901 -> 22901
951
Cov: 22901 -> 22901
Cov: 22901 -> 22901
952
Cov: 22901 -> 22901
Cov: 22901 -> 22901
953
Cov: 22901 -> 22901
Cov: 22901 -> 22901
954
Cov: 22901 -> 22901
Cov: 22901 -> 22901
955
Cov: 22901 -> 22902
Cov: 22902 -> 22902
956
Cov: 22902 -> 22902
Cov: 22902 -> 22902
957
Cov: 22902 -> 22902
Cov: 22902 -> 22902
958
Cov: 22902 -> 22910
Cov: 22910 -> 22910
959
Cov: 22910 -> 22910
Cov: 22910 -> 22910
960
Cov: 22910 -> 22910
Cov: 22910 -> 22910
961
Cov: 22910 -> 22910
Cov: 22910 -> 22910
962
Cov: 22910 -> 22910
Cov: 22910 -> 22910
963
{"exception": "RuntimeError", "msg": "cannot sample n_sample > prob_dist.size(-1) samples without replacement"}
964
Cov: 22910 -> 22910
Cov: 22910 -> 22910
965
Cov: 22910 -> 22910
Cov: 22910 -> 22910
966
Cov: 22910 -> 22910
Cov: 22910 -> 22910
967
Cov: 22910 -> 22910
Cov: 22910 -> 22910
968
Cov: 22910 -> 22910
Cov: 22910 -> 22910
969
Cov: 22910 -> 22910
Cov: 22910 -> 22910
970
Cov: 22910 -> 22910
Cov: 22910 -> 22910
971
Cov: 22910 -> 22910
Cov: 22910 -> 22910
972
Cov: 22910 -> 22924
Cov: 22924 -> 22924
973
Cov: 22924 -> 22924
Cov: 22924 -> 22924
974
Cov: 22924 -> 22924
Cov: 22924 -> 22924
975
Cov: 22924 -> 22925
Cov: 22925 -> 22925
976
Cov: 22925 -> 22925
Cov: 22925 -> 22925
977
Cov: 22925 -> 22925
Cov: 22925 -> 22925
978
Cov: 22925 -> 22925
Cov: 22925 -> 22925
979
Cov: 22925 -> 22951
Cov: 22951 -> 22951
980
Cov: 22951 -> 22951
Cov: 22951 -> 22951
981
Cov: 22951 -> 22951
Cov: 22951 -> 22951
982
Cov: 22951 -> 22951
Cov: 22951 -> 22951
983
{"exception": "RuntimeError", "msg": "index_add_(): self (Float) and source (Long) must have the same scalar type"}
984
{"exception": "RuntimeError", "msg": "result type Float can't be cast to the desired output type Long"}
985
Cov: 22951 -> 22951
Cov: 22951 -> 22951
986
Cov: 22951 -> 22951
Cov: 22951 -> 22951
987
Cov: 22951 -> 22951
Cov: 22951 -> 22951
988
Cov: 22951 -> 22951
Cov: 22951 -> 22951
989
Cov: 22951 -> 22951
Cov: 22951 -> 22951
990
Cov: 22951 -> 22951
Cov: 22951 -> 22951
991
Cov: 22951 -> 22951
Cov: 22951 -> 22951
992
Cov: 22951 -> 22952
Cov: 22952 -> 22952
993
Cov: 22952 -> 22952
Cov: 22952 -> 22952
994
Cov: 22952 -> 22952
Cov: 22952 -> 22952
995
Cov: 22952 -> 22952
Cov: 22952 -> 22952
996
Cov: 22952 -> 22952
Cov: 22952 -> 22952
997
Cov: 22952 -> 22956
Cov: 22956 -> 22956
998
Cov: 22956 -> 22956
Cov: 22956 -> 22956
999
Cov: 22956 -> 22956
Cov: 22956 -> 22956
1000
Cov: 22956 -> 22956
Cov: 22956 -> 22956
1001
Cov: 22956 -> 22956
Cov: 22956 -> 22956
1002
Cov: 22956 -> 22956
Cov: 22956 -> 22956
1003
Cov: 22956 -> 22959
Cov: 22959 -> 22959
1004
Cov: 22959 -> 22959
Cov: 22959 -> 22959
1005
Cov: 22959 -> 22959
Cov: 22959 -> 22959
1006
Cov: 22959 -> 22959
Cov: 22959 -> 22959
1007
Cov: 22959 -> 22959
Cov: 22959 -> 22959
1008
Cov: 22959 -> 22959
Cov: 22959 -> 22959
1009
Cov: 22959 -> 22959
Cov: 22959 -> 22959
1010
Cov: 22959 -> 22959
Cov: 22959 -> 22959
1011
Cov: 22959 -> 22959
Cov: 22959 -> 22959
1012
Cov: 22959 -> 22959
Cov: 22959 -> 22959
1013
Cov: 22959 -> 22963
Cov: 22963 -> 22963
1014
Cov: 22963 -> 22972
Cov: 22972 -> 22972
1015
Cov: 22972 -> 22973
Cov: 22973 -> 22973
1016
Cov: 22973 -> 22973
Cov: 22973 -> 22973
1017
Cov: 22973 -> 22981
Cov: 22981 -> 22981
1018
{"exception": "RuntimeError", "msg": "Index tensor must have the same number of dimensions as self tensor"}
1019
Cov: 22981 -> 22981
Cov: 22981 -> 22981
1020
Cov: 22981 -> 22981
Cov: 22981 -> 22981
1021
Cov: 22981 -> 22981
Cov: 22981 -> 22981
1022
Cov: 22981 -> 22981
Cov: 22981 -> 22981
1023
Cov: 22981 -> 22981
Cov: 22981 -> 22981
1024
Cov: 22981 -> 22981
Cov: 22981 -> 22981
1025
Cov: 22981 -> 22981
Cov: 22981 -> 22981
1026
Cov: 22981 -> 22987
Cov: 22987 -> 22987
1027
Cov: 22987 -> 22990
Cov: 22990 -> 22990
1028
Cov: 22990 -> 22994
Cov: 22994 -> 22994
1029
Cov: 22994 -> 22994
Cov: 22994 -> 22994
1030
Cov: 22994 -> 22994
Cov: 22994 -> 22994
1031
Cov: 22994 -> 22994
Cov: 22994 -> 22994
1032
Cov: 22994 -> 22994
Cov: 22994 -> 22994
1033
Cov: 22994 -> 22994
Cov: 22994 -> 22994
1034
Cov: 22994 -> 22994
Cov: 22994 -> 22994
1035
Cov: 22994 -> 22994
Cov: 22994 -> 22994
1036
Cov: 22994 -> 22994
Cov: 22994 -> 22994
1037
{"exception": "TypeError", "msg": "new_full(): argument 'size' (position 1) must be tuple of ints, not int"}
1038
Cov: 22994 -> 22994
Cov: 22994 -> 22994
1039
Cov: 22994 -> 22994
Cov: 22994 -> 22994
1040
Cov: 22994 -> 22994
Cov: 22994 -> 22994
1041
Cov: 22994 -> 22994
Cov: 22994 -> 22994
1042
Cov: 22994 -> 22994
Cov: 22994 -> 22994
1043
{"exception": "TypeError", "msg": "'getset_descriptor' object is not callable"}
1044
Cov: 22994 -> 22994
Cov: 22994 -> 22994
1045
Cov: 22994 -> 22994
Cov: 22994 -> 22994
1046
Cov: 22994 -> 22994
Cov: 22994 -> 22994
1047
Cov: 22994 -> 22994
Cov: 22994 -> 22994
1048
Cov: 22994 -> 22994
Cov: 22994 -> 22994
1049
Cov: 22994 -> 23032
Cov: 23032 -> 23032
1050
Cov: 23032 -> 23032
Cov: 23032 -> 23032
1051
Cov: 23032 -> 23036
Cov: 23036 -> 23036
1052
Cov: 23036 -> 23036
Cov: 23036 -> 23036
1053
Cov: 23036 -> 23036
Cov: 23036 -> 23036
1054
Cov: 23036 -> 23036
Cov: 23036 -> 23036
1055
Cov: 23036 -> 23036
Cov: 23036 -> 23036
1056
Cov: 23036 -> 23036
Cov: 23036 -> 23036
1057
Cov: 23036 -> 23036
Cov: 23036 -> 23036
1058
Cov: 23036 -> 23036
Cov: 23036 -> 23036
1059
Cov: 23036 -> 23036
Cov: 23036 -> 23036
1060
Cov: 23036 -> 23036
Cov: 23036 -> 23036
1061
Cov: 23036 -> 23036
Cov: 23036 -> 23036
1062
Cov: 23036 -> 23036
Cov: 23036 -> 23036
1063
Cov: 23036 -> 23036
Cov: 23036 -> 23036
1064
Cov: 23036 -> 23036
Cov: 23036 -> 23036
1065
Cov: 23036 -> 23036
Cov: 23036 -> 23036
1066
Cov: 23036 -> 23036
Cov: 23036 -> 23036
1067
Cov: 23036 -> 23036
Cov: 23036 -> 23036
1068
Cov: 23036 -> 23037
Cov: 23037 -> 23037
1069
Cov: 23037 -> 23037
Cov: 23037 -> 23037
1070
Cov: 23037 -> 23037
Cov: 23037 -> 23037
1071
Cov: 23037 -> 23037
Cov: 23037 -> 23037
1072
Cov: 23037 -> 23037
Cov: 23037 -> 23037
1073
Cov: 23037 -> 23084
Cov: 23084 -> 23084
1074
Cov: 23084 -> 23085
Cov: 23085 -> 23085
1075
{"exception": "RuntimeError", "msg": "This function was deprecated since version 1.9 and is now removed. `torch.solve` is deprecated in favor of `torch.linalg.solve`. `torch.linalg.solve` has its arguments reversed and does not return the LU factorization.\n\nTo get the LU factorization see `torch.lu`, which can be used with `torch.lu_solve` or `torch.lu_unpack`.\nX = torch.solve(B, A).solution should be replaced with:\nX = torch.linalg.solve(A, B)"}
1076
{"exception": "RuntimeError", "msg": "This function was deprecated since version 1.9 and is now removed. `torch.lstsq` is deprecated in favor of `torch.linalg.lstsq`.\n`torch.linalg.lstsq` has reversed arguments and does not return the QR decomposition in the returned tuple (although it returns other information about the problem).\n\nTo get the QR decomposition consider using `torch.linalg.qr`.\n\nThe returned solution in `torch.lstsq` stored the residuals of the solution in the last m - n columns of the returned value whenever m > n. In torch.linalg.lstsq, the residuals are in the field 'residuals' of the returned named tuple.\n\nThe unpacking of the solution, as in\nX, _ = torch.lstsq(B, A).solution[:A.size(1)]\nshould be replaced with:\nX = torch.linalg.lstsq(A, B).solution"}
1077
Cov: 23085 -> 23087
Cov: 23087 -> 23087
1078
Cov: 23087 -> 23087
Cov: 23087 -> 23087
1079
Cov: 23087 -> 23087
Cov: 23087 -> 23087
1080
{"exception": "RuntimeError", "msg": "a leaf Variable that requires grad is being used in an in-place operation."}
1081
Cov: 23087 -> 23088
Cov: 23088 -> 23088
1082
Cov: 23088 -> 23088
Cov: 23088 -> 23088
1083
Cov: 23088 -> 23088
Cov: 23088 -> 23088
1084
Cov: 23088 -> 23088
Cov: 23088 -> 23088
1085
Cov: 23088 -> 23089
Cov: 23089 -> 23089
1086
Cov: 23089 -> 23089
Cov: 23089 -> 23089
1087
Cov: 23089 -> 23089
Cov: 23089 -> 23089
1088
Cov: 23089 -> 23089
Cov: 23089 -> 23089
1089
Cov: 23089 -> 23089
Cov: 23089 -> 23089
1090
Cov: 23089 -> 23089
Cov: 23089 -> 23089
1091
Cov: 23089 -> 23089
Cov: 23089 -> 23089
1092
Cov: 23089 -> 23089
Cov: 23089 -> 23089
1093
{"exception": "RuntimeError", "msg": "inner() the last dimension must match on both input tensors but got shapes [4, 3] and [3, 4]"}
1094
Cov: 23089 -> 23089
Cov: 23089 -> 23089
1095
Cov: 23089 -> 23090
Cov: 23090 -> 23090
1096
Cov: 23090 -> 23092
Cov: 23092 -> 23092
1097
Cov: 23092 -> 23092
Cov: 23092 -> 23092
1098
Cov: 23092 -> 23092
Cov: 23092 -> 23092
1099
Cov: 23092 -> 23092
Cov: 23092 -> 23092
1100
Cov: 23092 -> 23094
Cov: 23094 -> 23094
1101
Cov: 23094 -> 23095
Cov: 23095 -> 23095
1102
Cov: 23095 -> 23095
Cov: 23095 -> 23095
1103
Cov: 23095 -> 23096
Cov: 23096 -> 23096
1104
Cov: 23096 -> 23096
Cov: 23096 -> 23096
1105
Cov: 23096 -> 23096
Cov: 23096 -> 23096
1106
Cov: 23096 -> 23096
Cov: 23096 -> 23096
1107
Cov: 23096 -> 23096
Cov: 23096 -> 23096
1108
Cov: 23096 -> 23096
Cov: 23096 -> 23096
1109
Cov: 23096 -> 23096
Cov: 23096 -> 23096
1110
Cov: 23096 -> 23113
Cov: 23113 -> 23113
1111
Cov: 23113 -> 23114
Cov: 23114 -> 23114
1112
Cov: 23114 -> 23114
Cov: 23114 -> 23114
1113
Cov: 23114 -> 23114
Cov: 23114 -> 23114
1114
Cov: 23114 -> 23114
Cov: 23114 -> 23114
1115
Cov: 23114 -> 23114
Cov: 23114 -> 23114
1116
Cov: 23114 -> 23114
Cov: 23114 -> 23114
1117
Cov: 23114 -> 23114
Cov: 23114 -> 23114
1118
Cov: 23114 -> 23114
Cov: 23114 -> 23114
1119
Cov: 23114 -> 23116
Cov: 23116 -> 23116
1120
Cov: 23116 -> 23117
Cov: 23117 -> 23117
1121
Cov: 23117 -> 23123
Cov: 23123 -> 23123
1122
Cov: 23123 -> 23123
Cov: 23123 -> 23123
1123
Cov: 23123 -> 23123
Cov: 23123 -> 23123
1124
Cov: 23123 -> 23123
Cov: 23123 -> 23123
1125
Cov: 23123 -> 23123
Cov: 23123 -> 23123
1126
Cov: 23123 -> 23123
Cov: 23123 -> 23123
1127
Cov: 23123 -> 23123
Cov: 23123 -> 23123
1128
Cov: 23123 -> 23123
Cov: 23123 -> 23123
1129
Cov: 23123 -> 23123
Cov: 23123 -> 23123
1130
Cov: 23123 -> 23123
Cov: 23123 -> 23123
1131
Cov: 23123 -> 23127
Cov: 23127 -> 23127
1132
Cov: 23127 -> 23127
Cov: 23127 -> 23127
1133
Cov: 23127 -> 23127
Cov: 23127 -> 23127
1134
Cov: 23127 -> 23127
Cov: 23127 -> 23127
1135
Cov: 23127 -> 23178
Cov: 23178 -> 23178
1136
Cov: 23178 -> 23179
Cov: 23179 -> 23179
1137
Cov: 23179 -> 23179
Cov: 23179 -> 23179
1138
Cov: 23179 -> 23179
Cov: 23179 -> 23179
1139
Cov: 23179 -> 23180
Cov: 23180 -> 23180
1140
Cov: 23180 -> 23186
Cov: 23186 -> 23186
1141
Cov: 23186 -> 23186
Cov: 23186 -> 23186
1142
Cov: 23186 -> 23186
Cov: 23186 -> 23186
1143
Cov: 23186 -> 23188
Cov: 23188 -> 23188
1144
Cov: 23188 -> 23188
Cov: 23188 -> 23188
1145
Cov: 23188 -> 23188
Cov: 23188 -> 23188
1146
Cov: 23188 -> 23188
Cov: 23188 -> 23188
1147
Cov: 23188 -> 23188
Cov: 23188 -> 23188
1148
Cov: 23188 -> 23188
Cov: 23188 -> 23188
1149
{"exception": "RuntimeError", "msg": "\"rshift_cpu\" not implemented for 'Float'"}
1150
{"exception": "RuntimeError", "msg": "A must be batches of square matrices, but they are 3 by 4 matrices"}
1151
Cov: 23188 -> 23188
Cov: 23188 -> 23188
1152
Cov: 23188 -> 23189
Cov: 23189 -> 23189
1153
Cov: 23189 -> 23189
Cov: 23189 -> 23189
1154
Cov: 23189 -> 23189
Cov: 23189 -> 23189
1155
Cov: 23189 -> 23189
Cov: 23189 -> 23189
1156
Cov: 23189 -> 23189
Cov: 23189 -> 23189
1157
Cov: 23189 -> 23189
Cov: 23189 -> 23189
1158
Cov: 23189 -> 23189
Cov: 23189 -> 23189
1159
Cov: 23189 -> 23189
Cov: 23189 -> 23189
1160
Cov: 23189 -> 23189
Cov: 23189 -> 23189
1161
Cov: 23189 -> 23190
Cov: 23190 -> 23190
1162
Cov: 23190 -> 23190
Cov: 23190 -> 23190
1163
Cov: 23190 -> 23190
Cov: 23190 -> 23190
1164
Cov: 23190 -> 23191
Cov: 23191 -> 23191
1165
Cov: 23191 -> 23191
Cov: 23191 -> 23191
1166
Cov: 23191 -> 23191
Cov: 23191 -> 23191
1167
Cov: 23191 -> 23191
Cov: 23191 -> 23191
1168
Cov: 23191 -> 23191
Cov: 23191 -> 23191
1169
Cov: 23191 -> 23197
Cov: 23197 -> 23197
1170
Cov: 23197 -> 23197
Cov: 23197 -> 23197
1171
Cov: 23197 -> 23198
Cov: 23198 -> 23198
1172
Cov: 23198 -> 23198
Cov: 23198 -> 23198
1173
Cov: 23198 -> 23198
Cov: 23198 -> 23198
1174
Cov: 23198 -> 23201
Cov: 23201 -> 23201
1175
Cov: 23201 -> 23201
Cov: 23201 -> 23201
1176
Cov: 23201 -> 23201
Cov: 23201 -> 23201
1177
Cov: 23201 -> 23201
Cov: 23201 -> 23201
1178
Cov: 23201 -> 23201
Cov: 23201 -> 23201
1179
Cov: 23201 -> 23201
Cov: 23201 -> 23201
1180
Cov: 23201 -> 23205
Cov: 23205 -> 23205
1181
Cov: 23205 -> 23205
Cov: 23205 -> 23205
1182
Cov: 23205 -> 23205
Cov: 23205 -> 23205
1183
Cov: 23205 -> 23205
Cov: 23205 -> 23205
1184
{"exception": "RuntimeError", "msg": "This function was deprecated since version 1.9 and is now removed. `torch.lstsq` is deprecated in favor of `torch.linalg.lstsq`.\n`torch.linalg.lstsq` has reversed arguments and does not return the QR decomposition in the returned tuple (although it returns other information about the problem).\n\nTo get the QR decomposition consider using `torch.linalg.qr`.\n\nThe returned solution in `torch.lstsq` stored the residuals of the solution in the last m - n columns of the returned value whenever m > n. In torch.linalg.lstsq, the residuals are in the field 'residuals' of the returned named tuple.\n\nThe unpacking of the solution, as in\nX, _ = torch.lstsq(B, A).solution[:A.size(1)]\nshould be replaced with:\nX = torch.linalg.lstsq(A, B).solution"}
1185
Cov: 23205 -> 23206
Cov: 23206 -> 23206
1186
Cov: 23206 -> 23206
Cov: 23206 -> 23206
1187
Cov: 23206 -> 23206
Cov: 23206 -> 23206
1188
Cov: 23206 -> 23206
Cov: 23206 -> 23206
1189
Cov: 23206 -> 23206
Cov: 23206 -> 23206
1190
Cov: 23206 -> 23206
Cov: 23206 -> 23206
1191
Cov: 23206 -> 23206
Cov: 23206 -> 23206
1192
Cov: 23206 -> 23206
Cov: 23206 -> 23206
1193
{"exception": "TypeError", "msg": "roll(): argument 'shifts' (position 1) must be tuple of ints, not Tensor"}
1194
Cov: 23206 -> 23207
Cov: 23207 -> 23207
1195
Cov: 23207 -> 23207
Cov: 23207 -> 23207
1196
Cov: 23207 -> 23207
Cov: 23207 -> 23207
1197
Cov: 23207 -> 23207
Cov: 23207 -> 23207
1198
Cov: 23207 -> 23207
Cov: 23207 -> 23207
1199
Cov: 23207 -> 23210
Cov: 23210 -> 23210
1200
Cov: 23210 -> 23211
Cov: 23211 -> 23211
1201
Cov: 23211 -> 23211
Cov: 23211 -> 23211
1202
Cov: 23211 -> 23212
Cov: 23212 -> 23212
1203
Cov: 23212 -> 23212
Cov: 23212 -> 23212
1204
Cov: 23212 -> 23212
Cov: 23212 -> 23212
1205
Cov: 23212 -> 23212
Cov: 23212 -> 23212
1206
Cov: 23212 -> 23212
Cov: 23212 -> 23212
1207
Cov: 23212 -> 23212
Cov: 23212 -> 23212
1208
Cov: 23212 -> 23212
Cov: 23212 -> 23212
1209
Cov: 23212 -> 23212
Cov: 23212 -> 23212
1210
Cov: 23212 -> 23212
Cov: 23212 -> 23212
1211
Cov: 23212 -> 23213
Cov: 23213 -> 23213
1212
Cov: 23213 -> 23213
Cov: 23213 -> 23213
1213
Cov: 23213 -> 23213
Cov: 23213 -> 23213
1214
Cov: 23213 -> 23213
Cov: 23213 -> 23213
1215
Cov: 23213 -> 23244
Cov: 23244 -> 23244
1216
Cov: 23244 -> 23244
Cov: 23244 -> 23244
1217
Cov: 23244 -> 23244
Cov: 23244 -> 23244
1218
Cov: 23244 -> 23244
Cov: 23244 -> 23244
1219
Cov: 23244 -> 23245
Cov: 23245 -> 23245
1220
Cov: 23245 -> 23245
Cov: 23245 -> 23245
1221
Cov: 23245 -> 23246
Cov: 23246 -> 23246
1222
Cov: 23246 -> 23246
Cov: 23246 -> 23246
1223
Cov: 23246 -> 23247
Cov: 23247 -> 23247
1224
Cov: 23247 -> 23247
Cov: 23247 -> 23247
1225
Cov: 23247 -> 23247
Cov: 23247 -> 23247
1226
Cov: 23247 -> 23248
Cov: 23248 -> 23248
1227
Cov: 23248 -> 23252
Cov: 23252 -> 23252
1228
Cov: 23252 -> 23252
Cov: 23252 -> 23252
1229
Cov: 23252 -> 23252
Cov: 23252 -> 23252
1230
Cov: 23252 -> 23253
Cov: 23253 -> 23253
1231
Cov: 23253 -> 23254
Cov: 23254 -> 23254
1232
Cov: 23254 -> 23259
Cov: 23259 -> 23259
1233
Cov: 23259 -> 23259
Cov: 23259 -> 23259
1234
Cov: 23259 -> 23259
Cov: 23259 -> 23259
1235
Cov: 23259 -> 23266
Cov: 23266 -> 23266
1236
Cov: 23266 -> 23266
Cov: 23266 -> 23266
1237
{"exception": "TypeError", "msg": "'getset_descriptor' object is not callable"}
1238
{"exception": "RuntimeError", "msg": "The expanded size of the tensor (5) must match the existing size (4) at non-singleton dimension 2.  Target sizes: [10, 3, 5].  Tensor sizes: [10, 3, 4]"}
1239
Cov: 23266 -> 23266
Cov: 23266 -> 23266
1240
Cov: 23266 -> 23270
Cov: 23270 -> 23270
1241
Cov: 23270 -> 23270
Cov: 23270 -> 23270
1242
Cov: 23270 -> 23270
Cov: 23270 -> 23270
1243
Cov: 23270 -> 23270
Cov: 23270 -> 23270
1244
Cov: 23270 -> 23270
Cov: 23270 -> 23270
1245
Cov: 23270 -> 23270
Cov: 23270 -> 23270
1246
Cov: 23270 -> 23270
Cov: 23270 -> 23270
1247
Cov: 23270 -> 23270
Cov: 23270 -> 23270
1248
{"exception": "RuntimeError", "msg": "element 0 of tensors does not require grad and does not have a grad_fn"}
1249
Cov: 23270 -> 23283
Cov: 23283 -> 23283
1250
Cov: 23283 -> 23283
Cov: 23283 -> 23283
1251
Cov: 23283 -> 23290
Cov: 23290 -> 23290
1252
Cov: 23290 -> 23290
Cov: 23290 -> 23290
1253
{"exception": "RuntimeError", "msg": "the base given to float_power_ has dtype Float but the operation's result requires dtype Double"}
1254
Cov: 23290 -> 23290
Cov: 23290 -> 23290
1255
Cov: 23290 -> 23290
Cov: 23290 -> 23290
1256
Cov: 23290 -> 23290
Cov: 23290 -> 23290
1257
Cov: 23290 -> 23290
Cov: 23290 -> 23290
1258
Cov: 23290 -> 23291
Cov: 23291 -> 23291
1259
Cov: 23291 -> 23291
Cov: 23291 -> 23291
1260
Cov: 23291 -> 23291
Cov: 23291 -> 23291
1261
Cov: 23291 -> 23291
Cov: 23291 -> 23291
1262
Cov: 23291 -> 23291
Cov: 23291 -> 23291
1263
Cov: 23291 -> 23292
Cov: 23292 -> 23292
1264
Cov: 23292 -> 23293
Cov: 23293 -> 23293
1265
Cov: 23293 -> 23293
Cov: 23293 -> 23293
1266
Cov: 23293 -> 23293
Cov: 23293 -> 23293
1267
Cov: 23293 -> 23293
Cov: 23293 -> 23293
1268
Cov: 23293 -> 23293
Cov: 23293 -> 23293
1269
Cov: 23293 -> 23294
Cov: 23294 -> 23294
1270
{"exception": "TypeError", "msg": "igammac_(): argument 'other' must be Tensor, not float"}
1271
Cov: 23294 -> 23294
Cov: 23294 -> 23294
1272
Cov: 23294 -> 23294
Cov: 23294 -> 23294
1273
Cov: 23294 -> 23294
Cov: 23294 -> 23294
1274
{"exception": "TypeError", "msg": "tensor_split() received an invalid combination of arguments - got (dim=int, indices_or_sections=int, ), but expected one of:\n * (tuple of ints indices, int dim)\n      didn't match because some of the keywords were incorrect: indices_or_sections\n * (Tensor tensor_indices_or_sections, int dim)\n      didn't match because some of the keywords were incorrect: indices_or_sections\n * (int sections, int dim)\n      didn't match because some of the keywords were incorrect: indices_or_sections\n"}
1275
Cov: 23294 -> 23294
Cov: 23294 -> 23294
1276
{"exception": "RuntimeError", "msg": "inconsistent weight size, expected 5 but got [3]"}
1277
Cov: 23294 -> 23299
Cov: 23299 -> 23299
1278
{"exception": "RuntimeError", "msg": "element 0 of tensors does not require grad and does not have a grad_fn"}
1279
Cov: 23299 -> 23299
Cov: 23299 -> 23299
1280
Cov: 23299 -> 23300
Cov: 23300 -> 23300
1281
Cov: 23300 -> 23301
Cov: 23301 -> 23301
1282
Cov: 23301 -> 23301
Cov: 23301 -> 23301
1283
Cov: 23301 -> 23301
Cov: 23301 -> 23301
1284
Cov: 23301 -> 23301
Cov: 23301 -> 23301
1285
Cov: 23301 -> 23301
Cov: 23301 -> 23301
1286
Cov: 23301 -> 23301
Cov: 23301 -> 23301
1287
Cov: 23301 -> 23301
Cov: 23301 -> 23301
1288
{"exception": "TypeError", "msg": "index_put(): argument 'indices' (position 1) must be tuple of Tensors, not Tensor"}
1289
Cov: 23301 -> 23301
Cov: 23301 -> 23301
1290
Cov: 23301 -> 23301
Cov: 23301 -> 23301
1291
{"exception": "RuntimeError", "msg": "outer: Expected 1-D argument self, but got 2-D"}
1292
Cov: 23301 -> 23301
Cov: 23301 -> 23301
1293
Cov: 23301 -> 23302
Cov: 23302 -> 23302
1294
Cov: 23302 -> 23302
Cov: 23302 -> 23302
1295
Cov: 23302 -> 23302
Cov: 23302 -> 23302
1296
Cov: 23302 -> 23358
Cov: 23358 -> 23358
1297
Cov: 23358 -> 23358
Cov: 23358 -> 23358
1298
Cov: 23358 -> 23361
Cov: 23361 -> 23361
1299
Cov: 23361 -> 23361
Cov: 23361 -> 23361
1300
Cov: 23361 -> 23362
Cov: 23362 -> 23362
1301
Cov: 23362 -> 23362
Cov: 23362 -> 23362
1302
Cov: 23362 -> 23362
Cov: 23362 -> 23362
1303
Cov: 23362 -> 23363
Cov: 23363 -> 23363
1304
Cov: 23363 -> 23397
Cov: 23397 -> 23397
1305
Cov: 23397 -> 23397
Cov: 23397 -> 23397
1306
{"exception": "RuntimeError", "msg": "torch.triangular_solve: Expected b to have at least 2 dimensions, but it has 1 dimensions instead"}
1307
Cov: 23397 -> 23397
Cov: 23397 -> 23397
1308
Cov: 23397 -> 23397
Cov: 23397 -> 23397
1309
Cov: 23397 -> 23398
Cov: 23398 -> 23398
1310
{"exception": "TypeError", "msg": "'getset_descriptor' object is not callable"}
1311
Cov: 23398 -> 23398
Cov: 23398 -> 23398
1312
Cov: 23398 -> 23399
Cov: 23399 -> 23399
1313
Cov: 23399 -> 23415
Cov: 23415 -> 23415
1314
Cov: 23415 -> 23415
Cov: 23415 -> 23415
1315
Cov: 23415 -> 23415
Cov: 23415 -> 23415
1316
Cov: 23415 -> 23415
Cov: 23415 -> 23415
1317
Cov: 23415 -> 23415
Cov: 23415 -> 23415
1318
Cov: 23415 -> 23415
Cov: 23415 -> 23415
1319
Cov: 23415 -> 23415
Cov: 23415 -> 23415
1320
Cov: 23415 -> 23415
Cov: 23415 -> 23415
1321
Cov: 23415 -> 23415
Cov: 23415 -> 23415
1322
Cov: 23415 -> 23420
Cov: 23420 -> 23420
1323
{"exception": "NotImplementedError", "msg": "Could not run 'aten::q_per_channel_axis' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::q_per_channel_axis' is only available for these backends: [QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nQuantizedCPU: registered at aten/src/ATen/RegisterQuantizedCPU.cpp:944 [kernel]\nQuantizedCUDA: registered at aten/src/ATen/RegisterQuantizedCUDA.cpp:459 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:153 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:498 [backend fallback]\nFunctionalize: registered at ../aten/src/ATen/FunctionalizeFallbackKernel.cpp:290 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:86 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradMTIA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_4.cpp:13056 [kernel]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:382 [backend fallback]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:249 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:710 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:203 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:161 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:494 [backend fallback]\nPreDispatch: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:165 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:157 [backend fallback]\n"}
1324
Cov: 23420 -> 23420
Cov: 23420 -> 23420
1325
Cov: 23420 -> 23420
Cov: 23420 -> 23420
1326
Cov: 23420 -> 23420
Cov: 23420 -> 23420
1327
Cov: 23420 -> 23421
Cov: 23421 -> 23421
1328
Cov: 23421 -> 23421
Cov: 23421 -> 23421
1329
Cov: 23421 -> 23426
Cov: 23426 -> 23426
1330
Cov: 23426 -> 23426
Cov: 23426 -> 23426
1331
Cov: 23426 -> 23426
Cov: 23426 -> 23426
1332
Cov: 23426 -> 23426
Cov: 23426 -> 23426
1333
Cov: 23426 -> 23426
Cov: 23426 -> 23426
1334
Cov: 23426 -> 23426
Cov: 23426 -> 23426
1335
Cov: 23426 -> 23426
Cov: 23426 -> 23426
1336
Cov: 23426 -> 23426
Cov: 23426 -> 23426
1337
Cov: 23426 -> 23426
Cov: 23426 -> 23426
1338
Cov: 23426 -> 23426
Cov: 23426 -> 23426
1339
Cov: 23426 -> 23426
Cov: 23426 -> 23426
1340
Cov: 23426 -> 23426
Cov: 23426 -> 23426
1341
Cov: 23426 -> 23426
Cov: 23426 -> 23426
1342
Cov: 23426 -> 23426
Cov: 23426 -> 23426
1343
Cov: 23426 -> 23426
Cov: 23426 -> 23426
1344
Cov: 23426 -> 23426
Cov: 23426 -> 23426
1345
Cov: 23426 -> 23426
Cov: 23426 -> 23426
1346
Cov: 23426 -> 23426
Cov: 23426 -> 23426
1347
Cov: 23426 -> 23426
Cov: 23426 -> 23426
1348
Cov: 23426 -> 23426
Cov: 23426 -> 23426
1349
Cov: 23426 -> 23427
Cov: 23427 -> 23427
1350
Cov: 23427 -> 23427
Cov: 23427 -> 23427
1351
Cov: 23427 -> 23427
Cov: 23427 -> 23427
1352
Cov: 23427 -> 23427
Cov: 23427 -> 23427
1353
Cov: 23427 -> 23427
Cov: 23427 -> 23427
1354
Cov: 23427 -> 23427
Cov: 23427 -> 23427
1355
Cov: 23427 -> 23427
Cov: 23427 -> 23427
1356
{"exception": "RuntimeError", "msg": "torch.ormqr: Expected tau to have one dimension less than input, but got tau.ndim equal to 4 and input.ndim is equal to 4"}
1357
Cov: 23427 -> 23427
Cov: 23427 -> 23427
1358
Cov: 23427 -> 23427
Cov: 23427 -> 23427
1359
Cov: 23427 -> 23427
Cov: 23427 -> 23427
1360
Cov: 23427 -> 23427
Cov: 23427 -> 23427
1361
Cov: 23427 -> 23427
Cov: 23427 -> 23427
1362
Cov: 23427 -> 23428
Cov: 23428 -> 23428
1363
Cov: 23428 -> 23430
Cov: 23430 -> 23430
1364
Cov: 23430 -> 23430
Cov: 23430 -> 23430
1365
Cov: 23430 -> 23430
Cov: 23430 -> 23430
1366
Cov: 23430 -> 23430
Cov: 23430 -> 23430
1367
Cov: 23430 -> 23430
Cov: 23430 -> 23430
1368
Cov: 23430 -> 23430
Cov: 23430 -> 23430
1369
{"exception": "RuntimeError", "msg": "The expanded size of the tensor (3) must match the existing size (2) at non-singleton dimension 1.  Target sizes: [2, 3, 3].  Tensor sizes: [2, 3]"}
1370
Cov: 23430 -> 23430
Cov: 23430 -> 23430
1371
Cov: 23430 -> 23430
Cov: 23430 -> 23430
1372
{"exception": "RuntimeError", "msg": "size {[2, 3]} is not expandable to size {[3, 4, 5]}."}
1373
Cov: 23430 -> 23431
Cov: 23431 -> 23431
1374
Cov: 23431 -> 23431
Cov: 23431 -> 23431
1375
Cov: 23431 -> 23431
Cov: 23431 -> 23431
1376
{"exception": "RuntimeError", "msg": "nanmean(): expected input to have floating point or complex dtype but got Long"}
1377
Cov: 23431 -> 23431
Cov: 23431 -> 23431
1378
Cov: 23431 -> 23431
Cov: 23431 -> 23431
1379
Cov: 23431 -> 23431
Cov: 23431 -> 23431
1380
Cov: 23431 -> 23431
Cov: 23431 -> 23431
1381
Cov: 23431 -> 23431
Cov: 23431 -> 23431
1382
Cov: 23431 -> 23432
Cov: 23432 -> 23432
1383
Cov: 23432 -> 23433
Cov: 23433 -> 23433
1384
Cov: 23433 -> 23433
Cov: 23433 -> 23433
1385
Cov: 23433 -> 23434
Cov: 23434 -> 23434
1386
Cov: 23434 -> 23434
Cov: 23434 -> 23434
1387
Cov: 23434 -> 23434
Cov: 23434 -> 23434
1388
Cov: 23434 -> 23434
Cov: 23434 -> 23434
1389
Cov: 23434 -> 23434
Cov: 23434 -> 23434
1390
Cov: 23434 -> 23434
Cov: 23434 -> 23434
1391
Cov: 23434 -> 23434
Cov: 23434 -> 23434
1392
Cov: 23434 -> 23434
Cov: 23434 -> 23434
1393
Cov: 23434 -> 23434
Cov: 23434 -> 23434
1394
Cov: 23434 -> 23434
Cov: 23434 -> 23434
1395
Cov: 23434 -> 23434
Cov: 23434 -> 23434
1396
Cov: 23434 -> 23434
Cov: 23434 -> 23434
1397
Cov: 23434 -> 23435
Cov: 23435 -> 23435
1398
{"exception": "RuntimeError", "msg": "element 0 of tensors does not require grad and does not have a grad_fn"}
1399
Cov: 23435 -> 23441
Cov: 23441 -> 23441
1400
Cov: 23441 -> 23442
Cov: 23442 -> 23442
1401
Cov: 23442 -> 23442
Cov: 23442 -> 23442
1402
{"exception": "RuntimeError", "msg": "element 0 of tensors does not require grad and does not have a grad_fn"}
1403
Cov: 23442 -> 23443
Cov: 23443 -> 23443
1404
Cov: 23443 -> 23443
Cov: 23443 -> 23443
1405
Cov: 23443 -> 23443
Cov: 23443 -> 23443
1406
{"exception": "TypeError", "msg": "gcd_(): argument 'other' (position 1) must be Tensor, not int"}
1407
Cov: 23443 -> 23443
Cov: 23443 -> 23443
1408
Cov: 23443 -> 23443
Cov: 23443 -> 23443
1409
Cov: 23443 -> 23443
Cov: 23443 -> 23443
1410
{"exception": "_LinAlgError", "msg": "torch.linalg.solve: The solver failed because the input matrix is singular."}
1411
Cov: 23443 -> 23443
Cov: 23443 -> 23443
1412
Cov: 23443 -> 23443
Cov: 23443 -> 23443
1413
Cov: 23443 -> 23443
Cov: 23443 -> 23443
1414
Cov: 23443 -> 23443
Cov: 23443 -> 23443
1415
Cov: 23443 -> 23443
Cov: 23443 -> 23443
1416
Cov: 23443 -> 23443
Cov: 23443 -> 23443
1417
Cov: 23443 -> 23443
Cov: 23443 -> 23443
1418
Cov: 23443 -> 23443
Cov: 23443 -> 23443
1419
Cov: 23443 -> 23443
Cov: 23443 -> 23443
1420
Cov: 23443 -> 23443
Cov: 23443 -> 23443
1421
Cov: 23443 -> 23443
Cov: 23443 -> 23443
1422
Cov: 23443 -> 23443
Cov: 23443 -> 23443
1423
Cov: 23443 -> 23443
Cov: 23443 -> 23443
1424
Cov: 23443 -> 23444
Cov: 23444 -> 23444
1425
Cov: 23444 -> 23444
Cov: 23444 -> 23444
1426
Cov: 23444 -> 23444
Cov: 23444 -> 23444
1427
Cov: 23444 -> 23444
Cov: 23444 -> 23444
1428
Cov: 23444 -> 23444
Cov: 23444 -> 23444
1429
Cov: 23444 -> 23444
Cov: 23444 -> 23444
1430
Cov: 23444 -> 23444
Cov: 23444 -> 23444
1431
Cov: 23444 -> 23444
Cov: 23444 -> 23444
1432
Cov: 23444 -> 23444
Cov: 23444 -> 23444
1433
Cov: 23444 -> 23444
Cov: 23444 -> 23444
1434
Cov: 23444 -> 23444
Cov: 23444 -> 23444
1435
Cov: 23444 -> 23446
Cov: 23446 -> 23446
1436
Cov: 23446 -> 23446
Cov: 23446 -> 23446
1437
{"exception": "RuntimeError", "msg": "required rank 4 tensor to use channels_last format"}
1438
Cov: 23446 -> 23446
Cov: 23446 -> 23446
1439
Cov: 23446 -> 23446
Cov: 23446 -> 23446
1440
Cov: 23446 -> 23446
Cov: 23446 -> 23446
1441
Cov: 23446 -> 23452
Cov: 23452 -> 23452
1442
Cov: 23452 -> 23452
Cov: 23452 -> 23452
1443
Cov: 23452 -> 23452
Cov: 23452 -> 23452
1444
Cov: 23452 -> 23461
Cov: 23461 -> 23461
1445
Cov: 23461 -> 23462
Cov: 23462 -> 23462
1446
Cov: 23462 -> 23462
Cov: 23462 -> 23462
1447
Cov: 23462 -> 23462
Cov: 23462 -> 23462
1448
Cov: 23462 -> 23462
Cov: 23462 -> 23462
1449
Cov: 23462 -> 23462
Cov: 23462 -> 23462
1450
Cov: 23462 -> 23462
Cov: 23462 -> 23462
1451
Cov: 23462 -> 23462
Cov: 23462 -> 23462
1452
{"exception": "RuntimeError", "msg": "This function was deprecated since version 1.9 and is now removed. `torch.solve` is deprecated in favor of `torch.linalg.solve`. `torch.linalg.solve` has its arguments reversed and does not return the LU factorization.\n\nTo get the LU factorization see `torch.lu`, which can be used with `torch.lu_solve` or `torch.lu_unpack`.\nX = torch.solve(B, A).solution should be replaced with:\nX = torch.linalg.solve(A, B)"}
1453
Cov: 23462 -> 23463
Cov: 23463 -> 23463
1454
Cov: 23463 -> 23465
Cov: 23465 -> 23465
1455
Cov: 23465 -> 23465
Cov: 23465 -> 23465
1456
Cov: 23465 -> 23465
Cov: 23465 -> 23465
1457
Cov: 23465 -> 23465
Cov: 23465 -> 23465
1458
Cov: 23465 -> 23465
Cov: 23465 -> 23465
1459
{"exception": "RuntimeError", "msg": "This function was deprecated since version 1.9 and is now removed. `torch.lstsq` is deprecated in favor of `torch.linalg.lstsq`.\n`torch.linalg.lstsq` has reversed arguments and does not return the QR decomposition in the returned tuple (although it returns other information about the problem).\n\nTo get the QR decomposition consider using `torch.linalg.qr`.\n\nThe returned solution in `torch.lstsq` stored the residuals of the solution in the last m - n columns of the returned value whenever m > n. In torch.linalg.lstsq, the residuals are in the field 'residuals' of the returned named tuple.\n\nThe unpacking of the solution, as in\nX, _ = torch.lstsq(B, A).solution[:A.size(1)]\nshould be replaced with:\nX = torch.linalg.lstsq(A, B).solution"}
1460
Cov: 23465 -> 23466
Cov: 23466 -> 23466
1461
Cov: 23466 -> 23467
Cov: 23467 -> 23467
1462
Cov: 23467 -> 23467
Cov: 23467 -> 23467
1463
Cov: 23467 -> 23467
Cov: 23467 -> 23467
1464
Cov: 23467 -> 23467
Cov: 23467 -> 23467
1465
Cov: 23467 -> 23467
Cov: 23467 -> 23467
1466
Cov: 23467 -> 23467
Cov: 23467 -> 23467
1467
Cov: 23467 -> 23473
Cov: 23473 -> 23473
1468
{"exception": "_LinAlgError", "msg": "cholesky: The factorization could not be completed because the input is not positive-definite (the leading minor of order 2 is not positive-definite)."}
1469
Cov: 23473 -> 23474
Cov: 23474 -> 23474
1470
Cov: 23474 -> 23474
Cov: 23474 -> 23474
1471
Cov: 23474 -> 23474
Cov: 23474 -> 23474
1472
Cov: 23474 -> 23475
Cov: 23475 -> 23475
1473
{"exception": "RuntimeError", "msg": "masked_scatter_ only supports boolean masks, but got mask with dtype Byte"}
1474
Cov: 23475 -> 23475
Cov: 23475 -> 23475
1475
{"exception": "RuntimeError", "msg": "element 0 of tensors does not require grad and does not have a grad_fn"}
1476
Cov: 23475 -> 23484
Cov: 23484 -> 23484
1477
Cov: 23484 -> 23484
Cov: 23484 -> 23484
1478
Cov: 23484 -> 23485
Cov: 23485 -> 23485
1479
Cov: 23485 -> 23485
Cov: 23485 -> 23485
1480
Cov: 23485 -> 23485
Cov: 23485 -> 23485
1481
Cov: 23485 -> 23487
Cov: 23487 -> 23487
1482
Cov: 23487 -> 23487
Cov: 23487 -> 23487
1483
Cov: 23487 -> 23488
Cov: 23488 -> 23488
1484
Cov: 23488 -> 23488
Cov: 23488 -> 23488
1485
Cov: 23488 -> 23489
Cov: 23489 -> 23489
1486
Cov: 23489 -> 23489
Cov: 23489 -> 23489
1487
Cov: 23489 -> 23489
Cov: 23489 -> 23489
1488
Cov: 23489 -> 23489
Cov: 23489 -> 23489
1489
Cov: 23489 -> 23489
Cov: 23489 -> 23489
1490
Cov: 23489 -> 23489
Cov: 23489 -> 23489
1491
Cov: 23489 -> 23489
Cov: 23489 -> 23489
1492
Cov: 23489 -> 23489
Cov: 23489 -> 23489
1493
Cov: 23489 -> 23489
Cov: 23489 -> 23489
1494
Cov: 23489 -> 23489
Cov: 23489 -> 23489
1495
Cov: 23489 -> 23489
Cov: 23489 -> 23489
1496
{"exception": "RuntimeError", "msg": "element 0 of tensors does not require grad and does not have a grad_fn"}
1497
Cov: 23489 -> 23489
Cov: 23489 -> 23489
1498
Cov: 23489 -> 23490
Cov: 23490 -> 23490
1499
Cov: 23490 -> 23490
Cov: 23490 -> 23490
1500
Cov: 23490 -> 23490
Cov: 23490 -> 23490
1501
Cov: 23490 -> 23490
Cov: 23490 -> 23490
1502
Cov: 23490 -> 23491
Cov: 23491 -> 23491
1503
Cov: 23491 -> 23495
Cov: 23495 -> 23495
1504
Cov: 23495 -> 23495
Cov: 23495 -> 23495
1505
Cov: 23495 -> 23495
Cov: 23495 -> 23495
1506
Cov: 23495 -> 23495
Cov: 23495 -> 23495
1507
Cov: 23495 -> 23495
Cov: 23495 -> 23495
1508
Cov: 23495 -> 23495
Cov: 23495 -> 23495
1509
Cov: 23495 -> 23495
Cov: 23495 -> 23495
1510
Cov: 23495 -> 23495
Cov: 23495 -> 23495
1511
Cov: 23495 -> 23495
Cov: 23495 -> 23495
1512
Cov: 23495 -> 23495
Cov: 23495 -> 23495
1513
{"exception": "_LinAlgError", "msg": "linalg.inv: The diagonal element 3 is zero, the inversion could not be completed because the input matrix is singular."}
1514
Cov: 23495 -> 23495
Cov: 23495 -> 23495
1515
Cov: 23495 -> 23495
Cov: 23495 -> 23495
1516
Cov: 23495 -> 23498
Cov: 23498 -> 23498
1517
Cov: 23498 -> 23498
Cov: 23498 -> 23498
1518
Cov: 23498 -> 23498
Cov: 23498 -> 23498
1519
Cov: 23498 -> 23498
Cov: 23498 -> 23498
1520
Cov: 23498 -> 23588
Cov: 23588 -> 23588
1521
Cov: 23588 -> 23588
Cov: 23588 -> 23588
1522
Cov: 23588 -> 23588
Cov: 23588 -> 23588
1523
Cov: 23588 -> 23588
Cov: 23588 -> 23588
1524
Cov: 23588 -> 23590
Cov: 23590 -> 23590
1525
Cov: 23590 -> 23604
Cov: 23604 -> 23604
1526
Cov: 23604 -> 23604
Cov: 23604 -> 23604
1527
Cov: 23604 -> 23604
Cov: 23604 -> 23604
1528
Cov: 23604 -> 23604
Cov: 23604 -> 23604
1529
Cov: 23604 -> 23605
Cov: 23605 -> 23605
1530
Cov: 23605 -> 23605
Cov: 23605 -> 23605
1531
Cov: 23605 -> 23605
Cov: 23605 -> 23605
1532
Cov: 23605 -> 23605
Cov: 23605 -> 23605
1533
Cov: 23605 -> 23605
Cov: 23605 -> 23605
1534
Cov: 23605 -> 23605
Cov: 23605 -> 23605
1535
Cov: 23605 -> 23605
Cov: 23605 -> 23605
1536
Cov: 23605 -> 23607
Cov: 23607 -> 23607
1537
Cov: 23607 -> 23607
Cov: 23607 -> 23607
1538
Cov: 23607 -> 23608
Cov: 23608 -> 23608
1539
{"exception": "RuntimeError", "msg": "Please look up dimensions by name, got: name = None."}
1540
Cov: 23608 -> 23608
Cov: 23608 -> 23608
1541
Cov: 23608 -> 23608
Cov: 23608 -> 23608
1542
Cov: 23608 -> 23608
Cov: 23608 -> 23608
1543
{"exception": "RuntimeError", "msg": "norm(): input dtype should be either floating point or complex. Got Long instead."}
1544
Cov: 23608 -> 23633
Cov: 23633 -> 23633
1545
Cov: 23633 -> 23633
Cov: 23633 -> 23633
1546
Cov: 23633 -> 23633
Cov: 23633 -> 23633
1547
Cov: 23633 -> 23633
Cov: 23633 -> 23633
1548
Cov: 23633 -> 23633
Cov: 23633 -> 23633
1549
Cov: 23633 -> 23633
Cov: 23633 -> 23633
1550
{"exception": "RuntimeError", "msg": "The expanded size of the tensor (3) must match the existing size (2) at non-singleton dimension 1.  Target sizes: [2, 3, 3].  Tensor sizes: [2, 3]"}
1551
Cov: 23633 -> 23633
Cov: 23633 -> 23633
1552
Cov: 23633 -> 23633
Cov: 23633 -> 23633
1553
Cov: 23633 -> 23634
Cov: 23634 -> 23634
1554
Cov: 23634 -> 23635
Cov: 23635 -> 23635
1555
Cov: 23635 -> 23635
Cov: 23635 -> 23635
1556
{"exception": "RuntimeError", "msg": "torch.ormqr: Expected tau to have one dimension less than input, but got tau.ndim equal to 3 and input.ndim is equal to 3"}
1557
Cov: 23635 -> 23639
Cov: 23639 -> 23639
1558
Cov: 23639 -> 23639
Cov: 23639 -> 23639
1559
Cov: 23639 -> 23639
Cov: 23639 -> 23639
1560
Cov: 23639 -> 23639
Cov: 23639 -> 23639
1561
{"exception": "RuntimeError", "msg": "This function was deprecated since version 1.9 and is now removed. `torch.solve` is deprecated in favor of `torch.linalg.solve`. `torch.linalg.solve` has its arguments reversed and does not return the LU factorization.\n\nTo get the LU factorization see `torch.lu`, which can be used with `torch.lu_solve` or `torch.lu_unpack`.\nX = torch.solve(B, A).solution should be replaced with:\nX = torch.linalg.solve(A, B)"}
1562
Cov: 23639 -> 23642
Cov: 23642 -> 23642
1563
Cov: 23642 -> 23642
Cov: 23642 -> 23642
1564
Cov: 23642 -> 23642
Cov: 23642 -> 23642
1565
Cov: 23642 -> 23642
Cov: 23642 -> 23642
1566
Cov: 23642 -> 23642
Cov: 23642 -> 23642
1567
Cov: 23642 -> 23643
Cov: 23643 -> 23643
1568
Cov: 23643 -> 23643
Cov: 23643 -> 23643
1569
Cov: 23643 -> 23644
Cov: 23644 -> 23644
1570
Cov: 23644 -> 23661
Cov: 23661 -> 23661
1571
Cov: 23661 -> 23661
Cov: 23661 -> 23661
1572
Cov: 23661 -> 23664
Cov: 23664 -> 23664
1573
Cov: 23664 -> 23664
Cov: 23664 -> 23664
1574
{"exception": "TypeError", "msg": "orgqr() missing 1 required positional arguments: \"input2\""}
1575
Cov: 23664 -> 23665
Cov: 23665 -> 23665
1576
Cov: 23665 -> 23665
Cov: 23665 -> 23665
1577
{"exception": "RuntimeError", "msg": "masked_scatter_ only supports boolean masks, but got mask with dtype Byte"}
1578
Cov: 23665 -> 23665
Cov: 23665 -> 23665
1579
Cov: 23665 -> 23666
Cov: 23666 -> 23666
1580
Cov: 23666 -> 23667
Cov: 23667 -> 23667
1581
Cov: 23667 -> 23667
Cov: 23667 -> 23667
1582
Cov: 23667 -> 23672
Cov: 23672 -> 23672
1583
Cov: 23672 -> 23673
Cov: 23673 -> 23673
1584
{"exception": "RuntimeError", "msg": "istft requires a complex-valued input tensor matching the output from stft with return_complex=True."}
1585
Cov: 23673 -> 23674
Cov: 23674 -> 23674
1586
Cov: 23674 -> 23674
Cov: 23674 -> 23674
1587
Cov: 23674 -> 23674
Cov: 23674 -> 23674
1588
Cov: 23674 -> 23674
Cov: 23674 -> 23674
1589
Cov: 23674 -> 23677
Cov: 23677 -> 23677
1590
Cov: 23677 -> 23677
Cov: 23677 -> 23677
1591
Cov: 23677 -> 23677
Cov: 23677 -> 23677
1592
Cov: 23677 -> 23677
Cov: 23677 -> 23677
1593
Cov: 23677 -> 23677
Cov: 23677 -> 23677
1594
Cov: 23677 -> 23677
Cov: 23677 -> 23677
1595
{"exception": "RuntimeError", "msg": "size mismatch, input: [2, 3], v1: [3], v2: [3]"}
1596
Cov: 23677 -> 23677
Cov: 23677 -> 23677
1597
Cov: 23677 -> 23677
Cov: 23677 -> 23677
1598
Cov: 23677 -> 23677
Cov: 23677 -> 23677
1599
Cov: 23677 -> 23677
Cov: 23677 -> 23677
1600
Cov: 23677 -> 23677
Cov: 23677 -> 23677
1601
Cov: 23677 -> 23677
Cov: 23677 -> 23677
1602
Cov: 23677 -> 23678
Cov: 23678 -> 23678
1603
Cov: 23678 -> 23678
Cov: 23678 -> 23678
1604
Cov: 23678 -> 23678
Cov: 23678 -> 23678
1605
Cov: 23678 -> 23678
Cov: 23678 -> 23678
1606
Cov: 23678 -> 23678
Cov: 23678 -> 23678
1607
Cov: 23678 -> 23678
Cov: 23678 -> 23678
1608
Cov: 23678 -> 23678
Cov: 23678 -> 23678
1609
Cov: 23678 -> 23678
Cov: 23678 -> 23678
1610
Cov: 23678 -> 23690
Cov: 23690 -> 23690
1611
Cov: 23690 -> 23690
Cov: 23690 -> 23690
1612
Cov: 23690 -> 23690
Cov: 23690 -> 23690
1613
Cov: 23690 -> 23690
Cov: 23690 -> 23690
1614
{"exception": "RuntimeError", "msg": "masked_fill_ only supports boolean masks, but got mask with dtype unsigned char"}
1615
Cov: 23690 -> 23691
Cov: 23691 -> 23691
1616
Cov: 23691 -> 23691
Cov: 23691 -> 23691
1617
Cov: 23691 -> 23691
Cov: 23691 -> 23691
1618
Cov: 23691 -> 23691
Cov: 23691 -> 23691
1619
Cov: 23691 -> 23691
Cov: 23691 -> 23691
1620
Cov: 23691 -> 23692
Cov: 23692 -> 23692
1621
Cov: 23692 -> 23692
Cov: 23692 -> 23692
1622
Cov: 23692 -> 23692
Cov: 23692 -> 23692
1623
Cov: 23692 -> 23692
Cov: 23692 -> 23692
1624
Cov: 23692 -> 23694
Cov: 23694 -> 23694
1625
Cov: 23694 -> 23694
Cov: 23694 -> 23694
1626
Cov: 23694 -> 23694
Cov: 23694 -> 23694
1627
Cov: 23694 -> 23694
Cov: 23694 -> 23694
1628
Cov: 23694 -> 23694
Cov: 23694 -> 23694
1629
Cov: 23694 -> 23694
Cov: 23694 -> 23694
1630
Cov: 23694 -> 23694
Cov: 23694 -> 23694
1631
Cov: 23694 -> 23694
Cov: 23694 -> 23694
1632
Cov: 23694 -> 23694
Cov: 23694 -> 23694
1633
Cov: 23694 -> 23698
Cov: 23698 -> 23698
1634
Cov: 23698 -> 23698
Cov: 23698 -> 23698
1635
{"exception": "RuntimeError", "msg": "values expected sparse tensor layout but got Strided"}
1636
Cov: 23698 -> 23698
Cov: 23698 -> 23698
1637
Cov: 23698 -> 23698
Cov: 23698 -> 23698
1638
{"exception": "TypeError", "msg": "'getset_descriptor' object is not callable"}
1639
Cov: 23698 -> 23698
Cov: 23698 -> 23698
1640
Cov: 23698 -> 23698
Cov: 23698 -> 23698
1641
Cov: 23698 -> 23698
Cov: 23698 -> 23698
1642
Cov: 23698 -> 23698
Cov: 23698 -> 23698
1643
Cov: 23698 -> 23698
Cov: 23698 -> 23698
1644
Cov: 23698 -> 23698
Cov: 23698 -> 23698
1645
Cov: 23698 -> 23698
Cov: 23698 -> 23698
1646
Cov: 23698 -> 23698
Cov: 23698 -> 23698
1647
Cov: 23698 -> 23698
Cov: 23698 -> 23698
1648
Cov: 23698 -> 23698
Cov: 23698 -> 23698
1649
Cov: 23698 -> 23699
Cov: 23699 -> 23699
1650
Cov: 23699 -> 23699
Cov: 23699 -> 23699
1651
Cov: 23699 -> 23699
Cov: 23699 -> 23699
1652
Cov: 23699 -> 23699
Cov: 23699 -> 23699
1653
Cov: 23699 -> 23700
Cov: 23700 -> 23700
1654
Cov: 23700 -> 23700
Cov: 23700 -> 23700
1655
Cov: 23700 -> 23702
Cov: 23702 -> 23702
1656
Cov: 23702 -> 23703
Cov: 23703 -> 23703
1657
Cov: 23703 -> 23703
Cov: 23703 -> 23703
1658
Cov: 23703 -> 23703
Cov: 23703 -> 23703
1659
Cov: 23703 -> 23703
Cov: 23703 -> 23703
1660
Cov: 23703 -> 23703
Cov: 23703 -> 23703
1661
Cov: 23703 -> 23703
Cov: 23703 -> 23703
1662
Cov: 23703 -> 23704
Cov: 23704 -> 23704
1663
Cov: 23704 -> 23705
Cov: 23705 -> 23705
1664
Cov: 23705 -> 23705
Cov: 23705 -> 23705
1665
Cov: 23705 -> 23705
Cov: 23705 -> 23705
1666
Cov: 23705 -> 23705
Cov: 23705 -> 23705
1667
Cov: 23705 -> 23705
Cov: 23705 -> 23705
1668
Cov: 23705 -> 23705
Cov: 23705 -> 23705
1669
Cov: 23705 -> 23705
Cov: 23705 -> 23705
1670
Cov: 23705 -> 23705
Cov: 23705 -> 23705
1671
Cov: 23705 -> 23705
Cov: 23705 -> 23705
1672
Cov: 23705 -> 23708
Cov: 23708 -> 23708
1673
Cov: 23708 -> 23708
Cov: 23708 -> 23708
1674
Cov: 23708 -> 23709
Cov: 23709 -> 23709
1675
Cov: 23709 -> 23709
Cov: 23709 -> 23709
1676
{"exception": "RuntimeError", "msg": "a Tensor with 27 elements cannot be converted to Scalar"}
1677
Cov: 23709 -> 23709
Cov: 23709 -> 23709
1678
Cov: 23709 -> 23723
Cov: 23723 -> 23723
1679
Cov: 23723 -> 23723
Cov: 23723 -> 23723
1680
Cov: 23723 -> 23723
Cov: 23723 -> 23723
1681
Cov: 23723 -> 23723
Cov: 23723 -> 23723
1682
{"exception": "RuntimeError", "msg": "tensor.sspaddmm(...) can only be called on sparse tensors"}
1683
Cov: 23723 -> 23724
Cov: 23724 -> 23724
1684
Cov: 23724 -> 23724
Cov: 23724 -> 23724
1685
Cov: 23724 -> 23724
Cov: 23724 -> 23724
1686
{"exception": "RuntimeError", "msg": "masked_select: expected BoolTensor for mask"}
1687
Cov: 23724 -> 23724
Cov: 23724 -> 23724
1688
Cov: 23724 -> 23724
Cov: 23724 -> 23724
1689
{"exception": "_LinAlgError", "msg": "cholesky: The factorization could not be completed because the input is not positive-definite (the leading minor of order 3 is not positive-definite)."}
1690
{"exception": "RuntimeError", "msg": "tensor.sspaddmm(...) can only be called on sparse tensors"}
1691
Cov: 23724 -> 23725
Cov: 23725 -> 23725
1692
Cov: 23725 -> 23725
Cov: 23725 -> 23725
1693
Cov: 23725 -> 23725
Cov: 23725 -> 23725
1694
{"exception": "RuntimeError", "msg": "size mismatch, input: [3, 4], v1: [4], v2: [3]"}
1695
Cov: 23725 -> 23726
Cov: 23726 -> 23726
1696
Cov: 23726 -> 23726
Cov: 23726 -> 23726
1697
Cov: 23726 -> 23726
Cov: 23726 -> 23726
1698
Cov: 23726 -> 23729
Cov: 23729 -> 23729
1699
Cov: 23729 -> 23729
Cov: 23729 -> 23729
1700
Cov: 23729 -> 23731
Cov: 23731 -> 23731
1701
Cov: 23731 -> 23732
Cov: 23732 -> 23732
1702
Cov: 23732 -> 23733
Cov: 23733 -> 23733
1703
Cov: 23733 -> 23733
Cov: 23733 -> 23733
1704
Cov: 23733 -> 23733
Cov: 23733 -> 23733
1705
Cov: 23733 -> 23733
Cov: 23733 -> 23733
1706
Cov: 23733 -> 23734
Cov: 23734 -> 23734
1707
Cov: 23734 -> 23734
Cov: 23734 -> 23734
1708
Cov: 23734 -> 23734
Cov: 23734 -> 23734
1709
Cov: 23734 -> 23734
Cov: 23734 -> 23734
1710
Cov: 23734 -> 23735
Cov: 23735 -> 23735
1711
Cov: 23735 -> 23735
Cov: 23735 -> 23735
1712
Cov: 23735 -> 23735
Cov: 23735 -> 23735
1713
Cov: 23735 -> 23735
Cov: 23735 -> 23735
1714
Cov: 23735 -> 23735
Cov: 23735 -> 23735
1715
Cov: 23735 -> 23735
Cov: 23735 -> 23735
1716
Cov: 23735 -> 23735
Cov: 23735 -> 23735
1717
Cov: 23735 -> 23741
Cov: 23741 -> 23741
1718
Cov: 23741 -> 23742
Cov: 23742 -> 23742
1719
Cov: 23742 -> 23743
Cov: 23743 -> 23743
1720
Cov: 23743 -> 23743
Cov: 23743 -> 23743
1721
Cov: 23743 -> 23743
Cov: 23743 -> 23743
1722
Cov: 23743 -> 23743
Cov: 23743 -> 23743
1723
Cov: 23743 -> 23744
Cov: 23744 -> 23744
1724
Cov: 23744 -> 23744
Cov: 23744 -> 23744
1725
Cov: 23744 -> 23745
Cov: 23745 -> 23745
1726
Cov: 23745 -> 23745
Cov: 23745 -> 23745
1727
Cov: 23745 -> 23745
Cov: 23745 -> 23745
1728
Cov: 23745 -> 23746
Cov: 23746 -> 23746
1729
Cov: 23746 -> 23746
Cov: 23746 -> 23746
1730
Cov: 23746 -> 23747
Cov: 23747 -> 23747
1731
Cov: 23747 -> 23747
Cov: 23747 -> 23747
1732
Cov: 23747 -> 23747
Cov: 23747 -> 23747
1733
Cov: 23747 -> 23747
Cov: 23747 -> 23747
1734
Cov: 23747 -> 23748
Cov: 23748 -> 23748
1735
Cov: 23748 -> 23748
Cov: 23748 -> 23748
1736
Cov: 23748 -> 23748
Cov: 23748 -> 23748
1737
Cov: 23748 -> 23748
Cov: 23748 -> 23748
1738
Cov: 23748 -> 23748
Cov: 23748 -> 23748
1739
Cov: 23748 -> 23748
Cov: 23748 -> 23748
1740
Cov: 23748 -> 23754
Cov: 23754 -> 23754
1741
Cov: 23754 -> 23754
Cov: 23754 -> 23754
1742
Cov: 23754 -> 23757
Cov: 23757 -> 23757
1743
Cov: 23757 -> 23867
Cov: 23867 -> 23867
1744
Cov: 23867 -> 23867
Cov: 23867 -> 23867
1745
Cov: 23867 -> 23868
Cov: 23868 -> 23868
1746
Cov: 23868 -> 23874
Cov: 23874 -> 23874
1747
Cov: 23874 -> 23875
Cov: 23875 -> 23875
1748
Cov: 23875 -> 23876
Cov: 23876 -> 23876
1749
Cov: 23876 -> 23876
Cov: 23876 -> 23876
1750
Cov: 23876 -> 23890
Cov: 23890 -> 23890
1751
{"exception": "TypeError", "msg": "'getset_descriptor' object is not callable"}
1752
Cov: 23890 -> 23890
Cov: 23890 -> 23890
1753
Cov: 23890 -> 23891
Cov: 23891 -> 23891
1754
Cov: 23891 -> 23891
Cov: 23891 -> 23891
1755
Cov: 23891 -> 23891
Cov: 23891 -> 23891
1756
Cov: 23891 -> 23892
Cov: 23892 -> 23892
1757
Cov: 23892 -> 23892
Cov: 23892 -> 23892
1758
Cov: 23892 -> 23892
Cov: 23892 -> 23892
1759
{"exception": "TypeError", "msg": "new_full(): argument 'size' (position 1) must be tuple of ints, not int"}
1760
Cov: 23892 -> 23892
Cov: 23892 -> 23892
1761
Cov: 23892 -> 23892
Cov: 23892 -> 23892
1762
Cov: 23892 -> 23892
Cov: 23892 -> 23892
1763
Cov: 23892 -> 23892
Cov: 23892 -> 23892
1764
Cov: 23892 -> 23893
Cov: 23893 -> 23893
1765
Cov: 23893 -> 23894
Cov: 23894 -> 23894
1766
Cov: 23894 -> 23894
Cov: 23894 -> 23894
1767
Cov: 23894 -> 23899
Cov: 23899 -> 23899
1768
Cov: 23899 -> 23899
Cov: 23899 -> 23899
1769
Cov: 23899 -> 23899
Cov: 23899 -> 23899
1770
Cov: 23899 -> 23900
Cov: 23900 -> 23900
1771
Cov: 23900 -> 23900
Cov: 23900 -> 23900
1772
Cov: 23900 -> 23900
Cov: 23900 -> 23900
1773
Cov: 23900 -> 23900
Cov: 23900 -> 23900
1774
Cov: 23900 -> 23900
Cov: 23900 -> 23900
1775
Cov: 23900 -> 23901
Cov: 23901 -> 23901
1776
Cov: 23901 -> 23901
Cov: 23901 -> 23901
1777
Cov: 23901 -> 23901
Cov: 23901 -> 23901
1778
Cov: 23901 -> 23901
Cov: 23901 -> 23901
1779
Cov: 23901 -> 23902
Cov: 23902 -> 23902
1780
Cov: 23902 -> 23902
Cov: 23902 -> 23902
1781
Cov: 23902 -> 23903
Cov: 23903 -> 23903
1782
Cov: 23903 -> 23904
Cov: 23904 -> 23904
1783
Cov: 23904 -> 23904
Cov: 23904 -> 23904
1784
Cov: 23904 -> 23904
Cov: 23904 -> 23904
1785
Cov: 23904 -> 23904
Cov: 23904 -> 23904
1786
Cov: 23904 -> 23904
Cov: 23904 -> 23904
1787
Cov: 23904 -> 23904
Cov: 23904 -> 23904
1788
Cov: 23904 -> 23905
Cov: 23905 -> 23905
1789
Cov: 23905 -> 23918
Cov: 23918 -> 23918
1790
Cov: 23918 -> 23918
Cov: 23918 -> 23918
1791
{"exception": "RuntimeError", "msg": "size {[4, 5]} is not expandable to size {[2, 3]}."}
1792
Cov: 23918 -> 23918
Cov: 23918 -> 23918
1793
Cov: 23918 -> 23918
Cov: 23918 -> 23918
1794
Cov: 23918 -> 23918
Cov: 23918 -> 23918
1795
Cov: 23918 -> 23918
Cov: 23918 -> 23918
1796
Cov: 23918 -> 23918
Cov: 23918 -> 23918
1797
Cov: 23918 -> 23918
Cov: 23918 -> 23918
1798
Cov: 23918 -> 23919
Cov: 23919 -> 23919
1799
Cov: 23919 -> 23919
Cov: 23919 -> 23919
1800
Cov: 23919 -> 23920
Cov: 23920 -> 23920
1801
Cov: 23920 -> 23920
Cov: 23920 -> 23920
1802
Cov: 23920 -> 23924
Cov: 23924 -> 23924
1803
Cov: 23924 -> 23924
Cov: 23924 -> 23924
1804
Cov: 23924 -> 23924
Cov: 23924 -> 23924
1805
Cov: 23924 -> 23924
Cov: 23924 -> 23924
1806
Cov: 23924 -> 23924
Cov: 23924 -> 23924
1807
Cov: 23924 -> 23925
Cov: 23925 -> 23925
1808
Cov: 23925 -> 23925
Cov: 23925 -> 23925
1809
Cov: 23925 -> 23925
Cov: 23925 -> 23925
1810
Cov: 23925 -> 23925
Cov: 23925 -> 23925
1811
Cov: 23925 -> 23925
Cov: 23925 -> 23925
1812
Cov: 23925 -> 23925
Cov: 23925 -> 23925
1813
Cov: 23925 -> 23925
Cov: 23925 -> 23925
1814
Cov: 23925 -> 23925
Cov: 23925 -> 23925
1815
Cov: 23925 -> 23931
Cov: 23931 -> 23931
1816
Cov: 23931 -> 23931
Cov: 23931 -> 23931
1817
Cov: 23931 -> 23931
Cov: 23931 -> 23931
1818
Cov: 23931 -> 23931
Cov: 23931 -> 23931
1819
Cov: 23931 -> 23931
Cov: 23931 -> 23931
1820
Cov: 23931 -> 23931
Cov: 23931 -> 23931
1821
Cov: 23931 -> 23931
Cov: 23931 -> 23931
1822
Cov: 23931 -> 23932
Cov: 23932 -> 23932
1823
Cov: 23932 -> 23932
Cov: 23932 -> 23932
1824
Cov: 23932 -> 23932
Cov: 23932 -> 23932
1825
Cov: 23932 -> 23933
Cov: 23933 -> 23933
1826
{"exception": "RuntimeError", "msg": "div expected rounding_mode to be one of None, 'trunc', or 'floor' but found 'ceil'"}
1827
Cov: 23933 -> 23933
Cov: 23933 -> 23933
1828
Cov: 23933 -> 23933
Cov: 23933 -> 23933
1829
Cov: 23933 -> 23954
Cov: 23954 -> 23954
1830
Cov: 23954 -> 23954
Cov: 23954 -> 23954
1831
Cov: 23954 -> 23954
Cov: 23954 -> 23954
1832
Cov: 23954 -> 23955
Cov: 23955 -> 23955
1833
Cov: 23955 -> 23955
Cov: 23955 -> 23955
1834
Cov: 23955 -> 23956
Cov: 23956 -> 23956
1835
Cov: 23956 -> 23956
Cov: 23956 -> 23956
1836
Cov: 23956 -> 23956
Cov: 23956 -> 23956
1837
{"exception": "RuntimeError", "msg": "This function was deprecated since version 1.9 and is now removed. `torch.linalg.eig` returns complex tensors of dtype `cfloat` or `cdouble` rather than real tensors mimicking complex tensors.\n\nL, _ = torch.eig(A) should be replaced with:\nL_complex = torch.linalg.eigvals(A)\n\nand\n\nL, V = torch.eig(A, eigenvectors=True) should be replaced with:\nL_complex, V_complex = torch.linalg.eig(A)"}
1838
{"exception": "TypeError", "msg": "set_() received an invalid combination of arguments - got (stride=NoneType, size=NoneType, storage_offset=int, source=NoneType, ), but expected one of:\n * ()\n * (torch.Storage source)\n * (torch.Storage source, int storage_offset, tuple of ints size, tuple of ints stride)\n      didn't match because some of the arguments have invalid types: (\u001b[31;1msource=NoneType\u001b[0m, \u001b[32;1mstorage_offset=int\u001b[0m, \u001b[31;1msize=NoneType\u001b[0m, \u001b[31;1mstride=NoneType\u001b[0m, )\n * (Tensor source)\n * (Tensor source, int storage_offset, tuple of ints size, tuple of ints stride)\n      didn't match because some of the arguments have invalid types: (\u001b[31;1msource=NoneType\u001b[0m, \u001b[32;1mstorage_offset=int\u001b[0m, \u001b[31;1msize=NoneType\u001b[0m, \u001b[31;1mstride=NoneType\u001b[0m, )\n"}
1839
Cov: 23956 -> 23957
Cov: 23957 -> 23957
1840
Cov: 23957 -> 23962
Cov: 23962 -> 23962
1841
Cov: 23962 -> 23962
Cov: 23962 -> 23962
1842
Cov: 23962 -> 23962
Cov: 23962 -> 23962
1843
Cov: 23962 -> 23962
Cov: 23962 -> 23962
1844
{"exception": "RuntimeError", "msg": "This function was deprecated since version 1.9 and is now removed. `torch.linalg.eig` returns complex tensors of dtype `cfloat` or `cdouble` rather than real tensors mimicking complex tensors.\n\nL, _ = torch.eig(A) should be replaced with:\nL_complex = torch.linalg.eigvals(A)\n\nand\n\nL, V = torch.eig(A, eigenvectors=True) should be replaced with:\nL_complex, V_complex = torch.linalg.eig(A)"}
1845
Cov: 23962 -> 23963
Cov: 23963 -> 23963
1846
Cov: 23963 -> 23963
Cov: 23963 -> 23963
1847
Cov: 23963 -> 23963
Cov: 23963 -> 23963
1848
Cov: 23963 -> 23963
Cov: 23963 -> 23963
1849
Cov: 23963 -> 23963
Cov: 23963 -> 23963
1850
Cov: 23963 -> 23963
Cov: 23963 -> 23963
1851
Cov: 23963 -> 23963
Cov: 23963 -> 23963
1852
Cov: 23963 -> 23963
Cov: 23963 -> 23963
1853
Cov: 23963 -> 23964
Cov: 23964 -> 23964
1854
{"exception": "TypeError", "msg": "descriptor 'manual_seed' of 'torch._C.Generator' object needs an argument"}
1855
Cov: 23964 -> 23964
Cov: 23964 -> 23964
1856
Cov: 23964 -> 23965
Cov: 23965 -> 23965
1857
Cov: 23965 -> 23965
Cov: 23965 -> 23965
1858
{"exception": "TypeError", "msg": "index_add() received an invalid combination of arguments - got (tensor=Tensor, index=Tensor, dim=int, ), but expected one of:\n * (int dim, Tensor index, Tensor source, *, Number alpha)\n * (name dim, Tensor index, Tensor source, *, Number alpha)\n"}
1859
Cov: 23965 -> 23965
Cov: 23965 -> 23965
1860
Cov: 23965 -> 23965
Cov: 23965 -> 23965
1861
Cov: 23965 -> 23966
Cov: 23966 -> 23966
1862
Cov: 23966 -> 23967
Cov: 23967 -> 23967
1863
Cov: 23967 -> 23968
Cov: 23968 -> 23968
1864
Cov: 23968 -> 23968
Cov: 23968 -> 23968
1865
Cov: 23968 -> 23970
Cov: 23970 -> 23970
1866
Cov: 23970 -> 23971
Cov: 23971 -> 23971
1867
Cov: 23971 -> 23971
Cov: 23971 -> 23971
1868
Cov: 23971 -> 23971
Cov: 23971 -> 23971
1869
Cov: 23971 -> 23971
Cov: 23971 -> 23971
1870
Cov: 23971 -> 23971
Cov: 23971 -> 23971
1871
Cov: 23971 -> 23971
Cov: 23971 -> 23971
1872
Cov: 23971 -> 23971
Cov: 23971 -> 23971
1873
Cov: 23971 -> 23971
Cov: 23971 -> 23971
1874
Cov: 23971 -> 23971
Cov: 23971 -> 23971
1875
Cov: 23971 -> 23972
Cov: 23972 -> 23972
1876
Cov: 23972 -> 23973
Cov: 23973 -> 23973
1877
{"exception": "TypeError", "msg": "'getset_descriptor' object is not callable"}
1878
Cov: 23973 -> 23973
Cov: 23973 -> 23973
1879
Cov: 23973 -> 23973
Cov: 23973 -> 23973
1880
Cov: 23973 -> 23973
Cov: 23973 -> 23973
1881
Cov: 23973 -> 23975
Cov: 23975 -> 23975
1882
Cov: 23975 -> 23975
Cov: 23975 -> 23975
1883
Cov: 23975 -> 23975
Cov: 23975 -> 23975
1884
Cov: 23975 -> 23976
Cov: 23976 -> 23976
1885
Cov: 23976 -> 23976
Cov: 23976 -> 23976
1886
{"exception": "TypeError", "msg": "random_() received an invalid combination of arguments - got (to=int, from_=int, ), but expected one of:\n * (*, torch.Generator generator)\n      didn't match because some of the keywords were incorrect: to, from_\n * (int from, int to, *, torch.Generator generator)\n * (int to, *, torch.Generator generator)\n"}
1887
Cov: 23976 -> 23976
Cov: 23976 -> 23976
1888
Cov: 23976 -> 23976
Cov: 23976 -> 23976
1889
Cov: 23976 -> 23976
Cov: 23976 -> 23976
1890
Cov: 23976 -> 23976
Cov: 23976 -> 23976
1891
Cov: 23976 -> 23980
Cov: 23980 -> 23980
1892
Cov: 23980 -> 23980
Cov: 23980 -> 23980
1893
Cov: 23980 -> 23980
Cov: 23980 -> 23980
1894
Cov: 23980 -> 23986
Cov: 23986 -> 23986
1895
Cov: 23986 -> 23986
Cov: 23986 -> 23986
1896
Cov: 23986 -> 23988
Cov: 23988 -> 23988
1897
Cov: 23988 -> 23988
Cov: 23988 -> 23988
1898
Cov: 23988 -> 23988
Cov: 23988 -> 23988
1899
Cov: 23988 -> 23988
Cov: 23988 -> 23988
1900
Cov: 23988 -> 23988
Cov: 23988 -> 23988
1901
{"exception": "RuntimeError", "msg": "Default process group has not been initialized, please make sure to call init_process_group."}
1902
Cov: 23988 -> 23997
Cov: 23997 -> 23997
1903
Cov: 23997 -> 23998
Cov: 23998 -> 23998
1904
{"exception": "RuntimeError", "msg": "Index tensor must have the same number of dimensions as self tensor"}
1905
Cov: 23998 -> 23999
Cov: 23999 -> 23999
1906
Cov: 23999 -> 23999
Cov: 23999 -> 23999
1907
Cov: 23999 -> 24000
Cov: 24000 -> 24000
1908
Cov: 24000 -> 24000
Cov: 24000 -> 24000
1909
Cov: 24000 -> 24000
Cov: 24000 -> 24000
1910
Cov: 24000 -> 24000
Cov: 24000 -> 24000
1911
Cov: 24000 -> 24000
Cov: 24000 -> 24000
1912
Cov: 24000 -> 24000
Cov: 24000 -> 24000
1913
Cov: 24000 -> 24000
Cov: 24000 -> 24000
1914
Cov: 24000 -> 24004
Cov: 24004 -> 24004
1915
Cov: 24004 -> 24004
Cov: 24004 -> 24004
1916
Cov: 24004 -> 24004
Cov: 24004 -> 24004
1917
Cov: 24004 -> 24005
Cov: 24005 -> 24005
1918
Cov: 24005 -> 24005
Cov: 24005 -> 24005
1919
Cov: 24005 -> 24006
Cov: 24006 -> 24006
1920
Cov: 24006 -> 24006
Cov: 24006 -> 24006
1921
Cov: 24006 -> 24006
Cov: 24006 -> 24006
1922
Cov: 24006 -> 24006
Cov: 24006 -> 24006
1923
Cov: 24006 -> 24007
Cov: 24007 -> 24007
1924
Cov: 24007 -> 24008
Cov: 24008 -> 24008
1925
Cov: 24008 -> 24010
Cov: 24010 -> 24010
1926
{"exception": "RuntimeError", "msg": "The expanded size of the tensor (2) must match the existing size (3) at non-singleton dimension 1.  Target sizes: [3, 2].  Tensor sizes: [2, 3]"}
1927
Cov: 24010 -> 24011
Cov: 24011 -> 24011
1928
Cov: 24011 -> 24011
Cov: 24011 -> 24011
1929
Cov: 24011 -> 24011
Cov: 24011 -> 24011
1930
Cov: 24011 -> 24011
Cov: 24011 -> 24011
1931
Cov: 24011 -> 24011
Cov: 24011 -> 24011
1932
Cov: 24011 -> 24011
Cov: 24011 -> 24011
1933
Cov: 24011 -> 24011
Cov: 24011 -> 24011
1934
Cov: 24011 -> 24011
Cov: 24011 -> 24011
1935
Cov: 24011 -> 24011
Cov: 24011 -> 24011
1936
{"exception": "RuntimeError", "msg": "This function was deprecated since version 1.9 and is now removed. `torch.lstsq` is deprecated in favor of `torch.linalg.lstsq`.\n`torch.linalg.lstsq` has reversed arguments and does not return the QR decomposition in the returned tuple (although it returns other information about the problem).\n\nTo get the QR decomposition consider using `torch.linalg.qr`.\n\nThe returned solution in `torch.lstsq` stored the residuals of the solution in the last m - n columns of the returned value whenever m > n. In torch.linalg.lstsq, the residuals are in the field 'residuals' of the returned named tuple.\n\nThe unpacking of the solution, as in\nX, _ = torch.lstsq(B, A).solution[:A.size(1)]\nshould be replaced with:\nX = torch.linalg.lstsq(A, B).solution"}
1937
Cov: 24011 -> 24012
Cov: 24012 -> 24012
1938
Cov: 24012 -> 24012
Cov: 24012 -> 24012
1939
{"exception": "TypeError", "msg": "to_sparse() received an invalid combination of arguments - got (sparseDims=int, ), but expected one of:\n * (*, torch.layout layout, tuple of ints blocksize, int dense_dim)\n * (int sparse_dim)\n      didn't match because some of the keywords were incorrect: sparseDims\n"}
1940
Cov: 24012 -> 24013
Cov: 24013 -> 24013
1941
Cov: 24013 -> 24014
Cov: 24014 -> 24014
1942
Cov: 24014 -> 24014
Cov: 24014 -> 24014
1943
Cov: 24014 -> 24014
Cov: 24014 -> 24014
1944
Cov: 24014 -> 24014
Cov: 24014 -> 24014
1945
Cov: 24014 -> 24014
Cov: 24014 -> 24014
1946
Cov: 24014 -> 24014
Cov: 24014 -> 24014
1947
{"exception": "RuntimeError", "msg": "the base given to float_power_ has dtype Float but the operation's result requires dtype Double"}
1948
Cov: 24014 -> 24015
Cov: 24015 -> 24015
1949
Cov: 24015 -> 24016
Cov: 24016 -> 24016
1950
{"exception": "TypeError", "msg": "index_add_() missing 1 required positional arguments: \"source\""}
1951
Cov: 24016 -> 24016
Cov: 24016 -> 24016
1952
Cov: 24016 -> 24016
Cov: 24016 -> 24016
1953
Cov: 24016 -> 24016
Cov: 24016 -> 24016
1954
Cov: 24016 -> 24016
Cov: 24016 -> 24016
1955
Cov: 24016 -> 24016
Cov: 24016 -> 24016
1956
{"exception": "RuntimeError", "msg": "vector + matrix @ vector expected, got 2, 2, 1"}
1957
Cov: 24016 -> 24050
Cov: 24050 -> 24050
1958
Cov: 24050 -> 24050
Cov: 24050 -> 24050
1959
{"exception": "RuntimeError", "msg": "This function was deprecated since version 1.9 and is now removed.\nPlease use the `torch.linalg.matrix_rank` function instead. The parameter 'symmetric' was renamed in `torch.linalg.matrix_rank()` to 'hermitian'."}
1960
Cov: 24050 -> 24053
Cov: 24053 -> 24053
1961
Cov: 24053 -> 24053
Cov: 24053 -> 24053
1962
Cov: 24053 -> 24053
Cov: 24053 -> 24053
1963
Cov: 24053 -> 24053
Cov: 24053 -> 24053
1964
Cov: 24053 -> 24053
Cov: 24053 -> 24053
1965
Cov: 24053 -> 24053
Cov: 24053 -> 24053
1966
Cov: 24053 -> 24053
Cov: 24053 -> 24053
1967
Cov: 24053 -> 24053
Cov: 24053 -> 24053
1968
Cov: 24053 -> 24053
Cov: 24053 -> 24053
1969
Cov: 24053 -> 24053
Cov: 24053 -> 24053
1970
Cov: 24053 -> 24053
Cov: 24053 -> 24053
1971
Cov: 24053 -> 24053
Cov: 24053 -> 24053
1972
Cov: 24053 -> 24053
Cov: 24053 -> 24053
1973
{"exception": "RuntimeError", "msg": "The expanded size of the tensor (5) must match the existing size (3) at non-singleton dimension 2.  Target sizes: [2, 3, 5].  Tensor sizes: [2, 3]"}
1974
Cov: 24053 -> 24053
Cov: 24053 -> 24053
1975
Cov: 24053 -> 24053
Cov: 24053 -> 24053
1976
{"exception": "NotImplementedError", "msg": "Could not run 'aten::q_per_channel_axis' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::q_per_channel_axis' is only available for these backends: [QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nQuantizedCPU: registered at aten/src/ATen/RegisterQuantizedCPU.cpp:944 [kernel]\nQuantizedCUDA: registered at aten/src/ATen/RegisterQuantizedCUDA.cpp:459 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:153 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:498 [backend fallback]\nFunctionalize: registered at ../aten/src/ATen/FunctionalizeFallbackKernel.cpp:290 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:86 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradMTIA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_4.cpp:13056 [kernel]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:382 [backend fallback]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:249 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:710 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:203 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:161 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:494 [backend fallback]\nPreDispatch: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:165 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:157 [backend fallback]\n"}
1977
Cov: 24053 -> 24053
Cov: 24053 -> 24053
1978
Cov: 24053 -> 24053
Cov: 24053 -> 24053
1979
Cov: 24053 -> 24053
Cov: 24053 -> 24053
1980
Cov: 24053 -> 24053
Cov: 24053 -> 24053
1981
{"exception": "TypeError", "msg": "igammac(): argument 'other' (position 1) must be Tensor, not int"}
1982
Cov: 24053 -> 24071
Cov: 24071 -> 24071
1983
Cov: 24071 -> 24071
Cov: 24071 -> 24071
1984
Cov: 24071 -> 24071
Cov: 24071 -> 24071
1985
Cov: 24071 -> 24071
Cov: 24071 -> 24071
1986
Cov: 24071 -> 24071
Cov: 24071 -> 24071
1987
Cov: 24071 -> 24071
Cov: 24071 -> 24071
1988
Cov: 24071 -> 24071
Cov: 24071 -> 24071
1989
Cov: 24071 -> 24071
Cov: 24071 -> 24071
1990
Cov: 24071 -> 24073
Cov: 24073 -> 24073
1991
{"exception": "RuntimeError", "msg": "required rank 4 tensor to use channels_last format"}
1992
Cov: 24073 -> 24082
Cov: 24082 -> 24082
1993
Cov: 24082 -> 24082
Cov: 24082 -> 24082
1994
Cov: 24082 -> 24082
Cov: 24082 -> 24082
1995
Cov: 24082 -> 24082
Cov: 24082 -> 24082
1996
Cov: 24082 -> 24082
Cov: 24082 -> 24082
1997
Cov: 24082 -> 24082
Cov: 24082 -> 24082
1998
Cov: 24082 -> 24082
Cov: 24082 -> 24082
1999
Cov: 24082 -> 24082
Cov: 24082 -> 24082
2000
Cov: 24082 -> 24082
Cov: 24082 -> 24082
2001
Cov: 24082 -> 24082
Cov: 24082 -> 24082
2002
Cov: 24082 -> 24082
Cov: 24082 -> 24082
2003
Cov: 24082 -> 24083
Cov: 24083 -> 24083
2004
{"exception": "RuntimeError", "msg": "masked_select: expected BoolTensor for mask"}
2005
Cov: 24083 -> 24083
Cov: 24083 -> 24083
2006
{"exception": "RuntimeError", "msg": "result type Float can't be cast to the desired output type Long"}
2007
{"exception": "TypeError", "msg": "heaviside(): argument 'values' must be Tensor, not tuple"}
2008
Cov: 24083 -> 24083
Cov: 24083 -> 24083
2009
Cov: 24083 -> 24083
Cov: 24083 -> 24083
2010
Cov: 24083 -> 24083
Cov: 24083 -> 24083
2011
Cov: 24083 -> 24083
Cov: 24083 -> 24083
2012
Cov: 24083 -> 24083
Cov: 24083 -> 24083
2013
Cov: 24083 -> 24084
Cov: 24084 -> 24084
2014
Cov: 24084 -> 24084
Cov: 24084 -> 24084
2015
Cov: 24084 -> 24084
Cov: 24084 -> 24084
2016
Cov: 24084 -> 24084
Cov: 24084 -> 24084
2017
{"exception": "TypeError", "msg": "ldexp(): argument 'other' (position 1) must be Tensor, not int"}
2018
Cov: 24084 -> 24084
Cov: 24084 -> 24084
2019
Cov: 24084 -> 24084
Cov: 24084 -> 24084
2020
Cov: 24084 -> 24084
Cov: 24084 -> 24084
2021
Cov: 24084 -> 24084
Cov: 24084 -> 24084
2022
Cov: 24084 -> 24084
Cov: 24084 -> 24084
2023
Cov: 24084 -> 24084
Cov: 24084 -> 24084
2024
Cov: 24084 -> 24084
Cov: 24084 -> 24084
2025
Cov: 24084 -> 24084
Cov: 24084 -> 24084
2026
Cov: 24084 -> 24084
Cov: 24084 -> 24084
2027
Cov: 24084 -> 24084
Cov: 24084 -> 24084
2028
Cov: 24084 -> 24085
Cov: 24085 -> 24085
2029
Cov: 24085 -> 24085
Cov: 24085 -> 24085
2030
Cov: 24085 -> 24085
Cov: 24085 -> 24085
2031
Cov: 24085 -> 24085
Cov: 24085 -> 24085
2032
Cov: 24085 -> 24085
Cov: 24085 -> 24085
2033
Cov: 24085 -> 24085
Cov: 24085 -> 24085
2034
Cov: 24085 -> 24090
Cov: 24090 -> 24090
2035
Cov: 24090 -> 24090
Cov: 24090 -> 24090
2036
Cov: 24090 -> 24090
Cov: 24090 -> 24090
2037
Cov: 24090 -> 24090
Cov: 24090 -> 24090
2038
Cov: 24090 -> 24090
Cov: 24090 -> 24090
2039
Cov: 24090 -> 24090
Cov: 24090 -> 24090
2040
Cov: 24090 -> 24090
Cov: 24090 -> 24090
2041
Cov: 24090 -> 24090
Cov: 24090 -> 24090
2042
Cov: 24090 -> 24099
Cov: 24099 -> 24099
2043
Cov: 24099 -> 24099
Cov: 24099 -> 24099
2044
Cov: 24099 -> 24099
Cov: 24099 -> 24099
2045
Cov: 24099 -> 24099
Cov: 24099 -> 24099
2046
Cov: 24099 -> 24099
Cov: 24099 -> 24099
2047
Cov: 24099 -> 24099
Cov: 24099 -> 24099
2048
Cov: 24099 -> 24099
Cov: 24099 -> 24099
2049
Cov: 24099 -> 24099
Cov: 24099 -> 24099
2050
Cov: 24099 -> 24099
Cov: 24099 -> 24099
2051
Cov: 24099 -> 24099
Cov: 24099 -> 24099
2052
Cov: 24099 -> 24099
Cov: 24099 -> 24099
2053
Cov: 24099 -> 24102
Cov: 24102 -> 24102
2054
Cov: 24102 -> 24102
Cov: 24102 -> 24102
2055
Cov: 24102 -> 24102
Cov: 24102 -> 24102
2056
Cov: 24102 -> 24102
Cov: 24102 -> 24102
2057
Cov: 24102 -> 24102
Cov: 24102 -> 24102
2058
Cov: 24102 -> 24104
Cov: 24104 -> 24104
2059
Cov: 24104 -> 24104
Cov: 24104 -> 24104
2060
Cov: 24104 -> 24104
Cov: 24104 -> 24104
2061
Cov: 24104 -> 24104
Cov: 24104 -> 24104
2062
Cov: 24104 -> 24104
Cov: 24104 -> 24104
2063
Cov: 24104 -> 24104
Cov: 24104 -> 24104
2064
Cov: 24104 -> 24104
Cov: 24104 -> 24104
2065
Cov: 24104 -> 24112
Cov: 24112 -> 24112
2066
Cov: 24112 -> 24112
Cov: 24112 -> 24112
2067
Cov: 24112 -> 24112
Cov: 24112 -> 24112
2068
Cov: 24112 -> 24112
Cov: 24112 -> 24112
2069
Cov: 24112 -> 24112
Cov: 24112 -> 24112
2070
Cov: 24112 -> 24112
Cov: 24112 -> 24112
2071
Cov: 24112 -> 24113
Cov: 24113 -> 24113
2072
Cov: 24113 -> 24113
Cov: 24113 -> 24113
2073
Cov: 24113 -> 24113
Cov: 24113 -> 24113
2074
{"exception": "RuntimeError", "msg": "a Tensor with 150528 elements cannot be converted to Scalar"}
2075
Cov: 24113 -> 24113
Cov: 24113 -> 24113
2076
Cov: 24113 -> 24113
Cov: 24113 -> 24113
2077
{"exception": "RuntimeError", "msg": "torch.dsplit requires a tensor with at least 3 dimension, but got a tensor with 2 dimensions!"}
2078
Cov: 24113 -> 24113
Cov: 24113 -> 24113
2079
Cov: 24113 -> 24113
Cov: 24113 -> 24113
2080
{"exception": "RuntimeError", "msg": "masked_fill_ only supports boolean masks, but got mask with dtype unsigned char"}
2081
{"exception": "NotImplementedError", "msg": "Could not run 'aten::q_scale' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::q_scale' is only available for these backends: [QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nQuantizedCPU: registered at aten/src/ATen/RegisterQuantizedCPU.cpp:944 [kernel]\nQuantizedCUDA: registered at aten/src/ATen/RegisterQuantizedCUDA.cpp:459 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:153 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:498 [backend fallback]\nFunctionalize: registered at ../aten/src/ATen/FunctionalizeFallbackKernel.cpp:290 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:86 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradMTIA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_4.cpp:13056 [kernel]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:382 [backend fallback]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:249 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:710 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:203 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:161 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:494 [backend fallback]\nPreDispatch: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:165 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:157 [backend fallback]\n"}
2082
Cov: 24113 -> 24113
Cov: 24113 -> 24113
2083
Cov: 24113 -> 24113
Cov: 24113 -> 24113
2084
Cov: 24113 -> 24113
Cov: 24113 -> 24113
2085
Cov: 24113 -> 24113
Cov: 24113 -> 24113
2086
Cov: 24113 -> 24113
Cov: 24113 -> 24113
2087
Cov: 24113 -> 24113
Cov: 24113 -> 24113
2088
Cov: 24113 -> 24113
Cov: 24113 -> 24113
2089
Cov: 24113 -> 24115
Cov: 24115 -> 24115
2090
Cov: 24115 -> 24115
Cov: 24115 -> 24115
2091
Cov: 24115 -> 24116
Cov: 24116 -> 24116
2092
Cov: 24116 -> 24116
Cov: 24116 -> 24116
2093
Cov: 24116 -> 24117
Cov: 24117 -> 24117
2094
Cov: 24117 -> 24117
Cov: 24117 -> 24117
2095
Cov: 24117 -> 24117
Cov: 24117 -> 24117
2096
{"exception": "RuntimeError", "msg": "1D tensors expected, but got 2D and 2D tensors"}
2097
Cov: 24117 -> 24117
Cov: 24117 -> 24117
2098
Cov: 24117 -> 24117
Cov: 24117 -> 24117
2099
Cov: 24117 -> 24117
Cov: 24117 -> 24117
2100
Cov: 24117 -> 24117
Cov: 24117 -> 24117
2101
Cov: 24117 -> 24117
Cov: 24117 -> 24117
2102
Cov: 24117 -> 24117
Cov: 24117 -> 24117
2103
Cov: 24117 -> 24118
Cov: 24118 -> 24118
2104
Cov: 24118 -> 24118
Cov: 24118 -> 24118
2105
Cov: 24118 -> 24119
Cov: 24119 -> 24119
2106
Cov: 24119 -> 24119
Cov: 24119 -> 24119
2107
Cov: 24119 -> 24119
Cov: 24119 -> 24119
2108
Cov: 24119 -> 24119
Cov: 24119 -> 24119
2109
{"exception": "RuntimeError", "msg": "the base given to float_power_ has dtype Float but the operation's result requires dtype Double"}
2110
Cov: 24119 -> 24120
Cov: 24120 -> 24120
2111
Cov: 24120 -> 24121
Cov: 24121 -> 24121
2112
Cov: 24121 -> 24122
Cov: 24122 -> 24122
2113
{"exception": "RuntimeError", "msg": "The expanded size of the tensor (5) must match the existing size (4) at non-singleton dimension 2.  Target sizes: [10, 3, 5].  Tensor sizes: [10, 3, 4]"}
2114
Cov: 24122 -> 24123
Cov: 24123 -> 24123
2115
Cov: 24123 -> 24123
Cov: 24123 -> 24123
2116
Cov: 24123 -> 24123
Cov: 24123 -> 24123
2117
Cov: 24123 -> 24123
Cov: 24123 -> 24123
2118
Cov: 24123 -> 24123
Cov: 24123 -> 24123
2119
Cov: 24123 -> 24123
Cov: 24123 -> 24123
2120
Cov: 24123 -> 24123
Cov: 24123 -> 24123
2121
Cov: 24123 -> 24123
Cov: 24123 -> 24123
2122
Cov: 24123 -> 24123
Cov: 24123 -> 24123
2123
Cov: 24123 -> 24124
Cov: 24124 -> 24124
2124
Cov: 24124 -> 24127
Cov: 24127 -> 24127
2125
Cov: 24127 -> 24127
Cov: 24127 -> 24127
2126
{"exception": "RuntimeError", "msg": "The expanded size of the tensor (3) must match the existing size (5) at non-singleton dimension 1.  Target sizes: [5, 3].  Tensor sizes: [3, 5]"}
2127
{"exception": "NameError", "msg": "name 'math' is not defined"}
2128
Cov: 24127 -> 24127
Cov: 24127 -> 24127
2129
Cov: 24127 -> 24127
Cov: 24127 -> 24127
2130
Cov: 24127 -> 24127
Cov: 24127 -> 24127
2131
Cov: 24127 -> 24128
Cov: 24128 -> 24128
2132
Cov: 24128 -> 24129
Cov: 24129 -> 24129
2133
Cov: 24129 -> 24131
Cov: 24131 -> 24131
2134
Cov: 24131 -> 24136
Cov: 24136 -> 24136
2135
Cov: 24136 -> 24136
Cov: 24136 -> 24136
2136
Cov: 24136 -> 24137
Cov: 24137 -> 24137
2137
Cov: 24137 -> 24137
Cov: 24137 -> 24137
2138
Cov: 24137 -> 24137
Cov: 24137 -> 24137
2139
Cov: 24137 -> 24137
Cov: 24137 -> 24137
2140
Cov: 24137 -> 24137
Cov: 24137 -> 24137
2141
Cov: 24137 -> 24137
Cov: 24137 -> 24137
2142
Cov: 24137 -> 24138
Cov: 24138 -> 24138
2143
Cov: 24138 -> 24138
Cov: 24138 -> 24138
2144
Cov: 24138 -> 24138
Cov: 24138 -> 24138
2145
Cov: 24138 -> 24138
Cov: 24138 -> 24138
2146
Cov: 24138 -> 24138
Cov: 24138 -> 24138
2147
Cov: 24138 -> 24144
Cov: 24144 -> 24144
2148
Cov: 24144 -> 24144
Cov: 24144 -> 24144
2149
Cov: 24144 -> 24181
Cov: 24181 -> 24181
2150
Cov: 24181 -> 24181
Cov: 24181 -> 24181
2151
Cov: 24181 -> 24181
Cov: 24181 -> 24181
2152
Cov: 24181 -> 24182
Cov: 24182 -> 24182
2153
Cov: 24182 -> 24182
Cov: 24182 -> 24182
2154
Cov: 24182 -> 24183
Cov: 24183 -> 24183
2155
Cov: 24183 -> 24183
Cov: 24183 -> 24183
2156
Cov: 24183 -> 24183
Cov: 24183 -> 24183
2157
Cov: 24183 -> 24183
Cov: 24183 -> 24183
2158
Cov: 24183 -> 24183
Cov: 24183 -> 24183
2159
Cov: 24183 -> 24183
Cov: 24183 -> 24183
2160
Cov: 24183 -> 24183
Cov: 24183 -> 24183
2161
Cov: 24183 -> 24184
Cov: 24184 -> 24184
2162
Cov: 24184 -> 24184
Cov: 24184 -> 24184
2163
Cov: 24184 -> 24184
Cov: 24184 -> 24184
2164
Cov: 24184 -> 24188
Cov: 24188 -> 24188
2165
Cov: 24188 -> 24189
Cov: 24189 -> 24189
2166
Cov: 24189 -> 24189
Cov: 24189 -> 24189
2167
Cov: 24189 -> 24189
Cov: 24189 -> 24189
2168
Cov: 24189 -> 24189
Cov: 24189 -> 24189
2169
Cov: 24189 -> 24189
Cov: 24189 -> 24189
2170
Cov: 24189 -> 24189
Cov: 24189 -> 24189
2171
Cov: 24189 -> 24189
Cov: 24189 -> 24189
2172
Cov: 24189 -> 24190
Cov: 24190 -> 24190
2173
Cov: 24190 -> 24190
Cov: 24190 -> 24190
2174
{"exception": "NameError", "msg": "name 'from_type' is not defined"}
2175
Cov: 24190 -> 24196
Cov: 24196 -> 24196
2176
Cov: 24196 -> 24196
Cov: 24196 -> 24196
2177
Cov: 24196 -> 24196
Cov: 24196 -> 24196
2178
Cov: 24196 -> 24196
Cov: 24196 -> 24196
2179
{"exception": "TypeError", "msg": "'getset_descriptor' object is not callable"}
2180
Cov: 24196 -> 24202
Cov: 24202 -> 24202
2181
Cov: 24202 -> 24202
Cov: 24202 -> 24202
2182
Cov: 24202 -> 24202
Cov: 24202 -> 24202
2183
Cov: 24202 -> 24202
Cov: 24202 -> 24202
2184
Cov: 24202 -> 24202
Cov: 24202 -> 24202
2185
Cov: 24202 -> 24203
Cov: 24203 -> 24203
2186
{"exception": "RuntimeError", "msg": "This function was deprecated since version 1.9 and is now removed. `torch.linalg.eig` returns complex tensors of dtype `cfloat` or `cdouble` rather than real tensors mimicking complex tensors.\n\nL, _ = torch.eig(A) should be replaced with:\nL_complex = torch.linalg.eigvals(A)\n\nand\n\nL, V = torch.eig(A, eigenvectors=True) should be replaced with:\nL_complex, V_complex = torch.linalg.eig(A)"}
2187
Cov: 24203 -> 24204
Cov: 24204 -> 24204
2188
Cov: 24204 -> 24204
Cov: 24204 -> 24204
2189
Cov: 24204 -> 24204
Cov: 24204 -> 24204
2190
Cov: 24204 -> 24204
Cov: 24204 -> 24204
2191
Cov: 24204 -> 24204
Cov: 24204 -> 24204
2192
Cov: 24204 -> 24204
Cov: 24204 -> 24204
2193
{"exception": "RuntimeError", "msg": "Index tensor must have the same number of dimensions as self tensor"}
2194
{"exception": "TypeError", "msg": "'getset_descriptor' object is not callable"}
2195
Cov: 24204 -> 24210
Cov: 24210 -> 24210
2196
Cov: 24210 -> 24211
Cov: 24211 -> 24211
2197
Cov: 24211 -> 24211
Cov: 24211 -> 24211
2198
Cov: 24211 -> 24211
Cov: 24211 -> 24211
2199
Cov: 24211 -> 24211
Cov: 24211 -> 24211
2200
Cov: 24211 -> 24211
Cov: 24211 -> 24211
2201
Cov: 24211 -> 24216
Cov: 24216 -> 24216
2202
Cov: 24216 -> 24216
Cov: 24216 -> 24216
2203
Cov: 24216 -> 24216
Cov: 24216 -> 24216
2204
Cov: 24216 -> 24219
Cov: 24219 -> 24219
2205
Cov: 24219 -> 24219
Cov: 24219 -> 24219
2206
Cov: 24219 -> 24219
Cov: 24219 -> 24219
2207
Cov: 24219 -> 24219
Cov: 24219 -> 24219
2208
Cov: 24219 -> 24219
Cov: 24219 -> 24219
2209
Cov: 24219 -> 24239
Cov: 24239 -> 24239
2210
Cov: 24239 -> 24239
Cov: 24239 -> 24239
2211
Cov: 24239 -> 24240
Cov: 24240 -> 24240
2212
Cov: 24240 -> 24240
Cov: 24240 -> 24240
2213
Cov: 24240 -> 24240
Cov: 24240 -> 24240
2214
Cov: 24240 -> 24240
Cov: 24240 -> 24240
2215
Cov: 24240 -> 24240
Cov: 24240 -> 24240
2216
Cov: 24240 -> 24240
Cov: 24240 -> 24240
2217
Cov: 24240 -> 24240
Cov: 24240 -> 24240
2218
Cov: 24240 -> 24240
Cov: 24240 -> 24240
2219
{"exception": "RuntimeError", "msg": "1D tensors expected, but got 2D and 2D tensors"}
2220
Cov: 24240 -> 24240
Cov: 24240 -> 24240
2221
Cov: 24240 -> 24240
Cov: 24240 -> 24240
2222
Cov: 24240 -> 24245
Cov: 24245 -> 24245
2223
Cov: 24245 -> 24246
Cov: 24246 -> 24246
2224
Cov: 24246 -> 24247
Cov: 24247 -> 24247
2225
Cov: 24247 -> 24248
Cov: 24248 -> 24248
2226
Cov: 24248 -> 24249
Cov: 24249 -> 24249
2227
Cov: 24249 -> 24249
Cov: 24249 -> 24249
2228
Cov: 24249 -> 24250
Cov: 24250 -> 24250
2229
Cov: 24250 -> 24250
Cov: 24250 -> 24250
2230
Cov: 24250 -> 24251
Cov: 24251 -> 24251
2231
Cov: 24251 -> 24251
Cov: 24251 -> 24251
2232
Cov: 24251 -> 24252
Cov: 24252 -> 24252
2233
Cov: 24252 -> 24252
Cov: 24252 -> 24252
2234
Cov: 24252 -> 24252
Cov: 24252 -> 24252
2235
Cov: 24252 -> 24252
Cov: 24252 -> 24252
2236
Cov: 24252 -> 24252
Cov: 24252 -> 24252
2237
Cov: 24252 -> 24252
Cov: 24252 -> 24252
2238
Cov: 24252 -> 24252
Cov: 24252 -> 24252
2239
Cov: 24252 -> 24252
Cov: 24252 -> 24252
2240
{"exception": "TypeError", "msg": "'getset_descriptor' object is not callable"}
2241
Cov: 24252 -> 24252
Cov: 24252 -> 24252
2242
{"exception": "RuntimeError", "msg": "This function was deprecated since version 1.9 and is now removed.\nPlease use the `torch.linalg.matrix_rank` function instead. The parameter 'symmetric' was renamed in `torch.linalg.matrix_rank()` to 'hermitian'."}
2243
Cov: 24252 -> 24254
Cov: 24254 -> 24254
2244
Cov: 24254 -> 24254
Cov: 24254 -> 24254
2245
Cov: 24254 -> 24254
Cov: 24254 -> 24254
2246
Cov: 24254 -> 24254
Cov: 24254 -> 24254
2247
Cov: 24254 -> 24254
Cov: 24254 -> 24254
2248
Cov: 24254 -> 24254
Cov: 24254 -> 24254
2249
Cov: 24254 -> 24254
Cov: 24254 -> 24254
2250
Cov: 24254 -> 24255
Cov: 24255 -> 24255
2251
Cov: 24255 -> 24255
Cov: 24255 -> 24255
2252
Cov: 24255 -> 24257
Cov: 24257 -> 24257
2253
Cov: 24257 -> 24257
Cov: 24257 -> 24257
2254
Cov: 24257 -> 24257
Cov: 24257 -> 24257
2255
Cov: 24257 -> 24257
Cov: 24257 -> 24257
2256
Cov: 24257 -> 24258
Cov: 24258 -> 24258
2257
Cov: 24258 -> 24258
Cov: 24258 -> 24258
2258
Cov: 24258 -> 24258
Cov: 24258 -> 24258
2259
Cov: 24258 -> 24258
Cov: 24258 -> 24258
2260
Cov: 24258 -> 24258
Cov: 24258 -> 24258
2261
{"exception": "RuntimeError", "msg": "The expanded size of the tensor (5) must match the existing size (4) at non-singleton dimension 2.  Target sizes: [10, 3, 5].  Tensor sizes: [10, 3, 4]"}
2262
Cov: 24258 -> 24258
Cov: 24258 -> 24258
2263
Cov: 24258 -> 24258
Cov: 24258 -> 24258
2264
Cov: 24258 -> 24258
Cov: 24258 -> 24258
2265
Cov: 24258 -> 24258
Cov: 24258 -> 24258
2266
Cov: 24258 -> 24258
Cov: 24258 -> 24258
2267
Cov: 24258 -> 24258
Cov: 24258 -> 24258
2268
Cov: 24258 -> 24259
Cov: 24259 -> 24259
2269
Cov: 24259 -> 24259
Cov: 24259 -> 24259
2270
Cov: 24259 -> 24259
Cov: 24259 -> 24259
2271
Cov: 24259 -> 24259
Cov: 24259 -> 24259
2272
Cov: 24259 -> 24259
Cov: 24259 -> 24259
2273
Cov: 24259 -> 24259
Cov: 24259 -> 24259
2274
Cov: 24259 -> 24259
Cov: 24259 -> 24259
2275
Cov: 24259 -> 24259
Cov: 24259 -> 24259
2276
Cov: 24259 -> 24259
Cov: 24259 -> 24259
2277
Cov: 24259 -> 24261
Cov: 24261 -> 24261
2278
Cov: 24261 -> 24261
Cov: 24261 -> 24261
2279
Cov: 24261 -> 24262
Cov: 24262 -> 24262
2280
Cov: 24262 -> 24262
Cov: 24262 -> 24262
2281
Cov: 24262 -> 24262
Cov: 24262 -> 24262
2282
Cov: 24262 -> 24262
Cov: 24262 -> 24262
2283
{"exception": "IndexError", "msg": "Dimension out of range (expected to be in range of [-1, 0], but got 1)"}
2284
Cov: 24262 -> 24266
Cov: 24266 -> 24266
2285
Cov: 24266 -> 24302
Cov: 24302 -> 24302
2286
Cov: 24302 -> 24302
Cov: 24302 -> 24302
2287
Cov: 24302 -> 24303
Cov: 24303 -> 24303
2288
Cov: 24303 -> 24306
Cov: 24306 -> 24306
2289
Cov: 24306 -> 24306
Cov: 24306 -> 24306
2290
Cov: 24306 -> 24306
Cov: 24306 -> 24306
2291
Cov: 24306 -> 24306
Cov: 24306 -> 24306
2292
Cov: 24306 -> 24317
Cov: 24317 -> 24317
2293
Cov: 24317 -> 24317
Cov: 24317 -> 24317
2294
Cov: 24317 -> 24317
Cov: 24317 -> 24317
2295
Cov: 24317 -> 24317
Cov: 24317 -> 24317
2296
Cov: 24317 -> 24317
Cov: 24317 -> 24317
2297
Cov: 24317 -> 24318
Cov: 24318 -> 24318
2298
Cov: 24318 -> 24318
Cov: 24318 -> 24318
2299
Cov: 24318 -> 24319
Cov: 24319 -> 24319
2300
{"exception": "RuntimeError", "msg": "indices expected sparse coordinate tensor layout but got Strided"}
2301
Cov: 24319 -> 24325
Cov: 24325 -> 24325
2302
Cov: 24325 -> 24325
Cov: 24325 -> 24325
2303
Cov: 24325 -> 24325
Cov: 24325 -> 24325
2304
Cov: 24325 -> 24325
Cov: 24325 -> 24325
2305
Cov: 24325 -> 24325
Cov: 24325 -> 24325
2306
Cov: 24325 -> 24329
Cov: 24329 -> 24329
2307
Cov: 24329 -> 24329
Cov: 24329 -> 24329
2308
Cov: 24329 -> 24329
Cov: 24329 -> 24329
2309
Cov: 24329 -> 24330
Cov: 24330 -> 24330
2310
Cov: 24330 -> 24331
Cov: 24331 -> 24331
2311
Cov: 24331 -> 24337
Cov: 24337 -> 24337
2312
Cov: 24337 -> 24337
Cov: 24337 -> 24337
2313
Cov: 24337 -> 24337
Cov: 24337 -> 24337
2314
Cov: 24337 -> 24337
Cov: 24337 -> 24337
2315
Cov: 24337 -> 24337
Cov: 24337 -> 24337
2316
Cov: 24337 -> 24337
Cov: 24337 -> 24337
2317
Cov: 24337 -> 24337
Cov: 24337 -> 24337
2318
Cov: 24337 -> 24337
Cov: 24337 -> 24337
2319
Cov: 24337 -> 24339
Cov: 24339 -> 24339
2320
Cov: 24339 -> 24339
Cov: 24339 -> 24339
2321
Cov: 24339 -> 24339
Cov: 24339 -> 24339
2322
Cov: 24339 -> 24339
Cov: 24339 -> 24339
2323
Cov: 24339 -> 24339
Cov: 24339 -> 24339
2324
Cov: 24339 -> 24339
Cov: 24339 -> 24339
2325
Cov: 24339 -> 24340
Cov: 24340 -> 24340
2326
Cov: 24340 -> 24340
Cov: 24340 -> 24340
2327
Cov: 24340 -> 24340
Cov: 24340 -> 24340
2328
Cov: 24340 -> 24340
Cov: 24340 -> 24340
2329
Cov: 24340 -> 24341
Cov: 24341 -> 24341
2330
Cov: 24341 -> 24341
Cov: 24341 -> 24341
2331
{"exception": "RuntimeError", "msg": "a leaf Variable that requires grad is being used in an in-place operation."}
2332
Cov: 24341 -> 24341
Cov: 24341 -> 24341
2333
Cov: 24341 -> 24341
Cov: 24341 -> 24341
2334
Cov: 24341 -> 24342
Cov: 24342 -> 24342
2335
Cov: 24342 -> 24343
Cov: 24343 -> 24343
2336
{"exception": "RuntimeError", "msg": "This function was deprecated since version 1.9 and is now removed. The default behavior has changed from using the upper triangular portion of the matrix by default to using the lower triangular portion.\n\nL, _ = torch.symeig(A, upper=upper) should be replaced with:\nL = torch.linalg.eigvalsh(A, UPLO='U' if upper else 'L')\n\nand\n\nL, V = torch.symeig(A, eigenvectors=True) should be replaced with:\nL, V = torch.linalg.eigh(A, UPLO='U' if upper else 'L')"}
2337
Cov: 24343 -> 24347
Cov: 24347 -> 24347
2338
{"exception": "RuntimeError", "msg": "The expanded size of the tensor (4) must match the existing size (3) at non-singleton dimension 1.  Target sizes: [2, 4].  Tensor sizes: [2, 3]"}
2339
Cov: 24347 -> 24347
Cov: 24347 -> 24347
2340
Cov: 24347 -> 24347
Cov: 24347 -> 24347
2341
{"exception": "RuntimeError", "msg": "result type Float can't be cast to the desired output type Long"}
2342
Cov: 24347 -> 24347
Cov: 24347 -> 24347
2343
Cov: 24347 -> 24348
Cov: 24348 -> 24348
2344
Cov: 24348 -> 24348
Cov: 24348 -> 24348
2345
Cov: 24348 -> 24350
Cov: 24350 -> 24350
2346
Cov: 24350 -> 24350
Cov: 24350 -> 24350
2347
Cov: 24350 -> 24350
Cov: 24350 -> 24350
2348
Cov: 24350 -> 24352
Cov: 24352 -> 24352
2349
Cov: 24352 -> 24352
Cov: 24352 -> 24352
2350
Cov: 24352 -> 24352
Cov: 24352 -> 24352
2351
Cov: 24352 -> 24352
Cov: 24352 -> 24352
2352
Cov: 24352 -> 24352
Cov: 24352 -> 24352
2353
Cov: 24352 -> 24352
Cov: 24352 -> 24352
2354
{"exception": "NotImplementedError", "msg": "Could not run 'aten::sparse_mask' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::sparse_mask' is only available for these backends: [SparseCPU, SparseCUDA, SparseCsrCPU, SparseCsrCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nSparseCPU: registered at aten/src/ATen/RegisterSparseCPU.cpp:1387 [kernel]\nSparseCUDA: registered at aten/src/ATen/RegisterSparseCUDA.cpp:1573 [kernel]\nSparseCsrCPU: registered at aten/src/ATen/RegisterSparseCsrCPU.cpp:1135 [kernel]\nSparseCsrCUDA: registered at aten/src/ATen/RegisterSparseCsrCUDA.cpp:1276 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:153 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:498 [backend fallback]\nFunctionalize: registered at ../aten/src/ATen/FunctionalizeFallbackKernel.cpp:290 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:86 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradMTIA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_0.cpp:16725 [kernel]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:382 [backend fallback]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:249 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:710 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:203 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:161 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:494 [backend fallback]\nPreDispatch: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:165 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:157 [backend fallback]\n"}
2355
Cov: 24352 -> 24352
Cov: 24352 -> 24352
2356
Cov: 24352 -> 24352
Cov: 24352 -> 24352
2357
{"exception": "RuntimeError", "msg": "This function was deprecated since version 1.9 and is now removed. The default behavior has changed from using the upper triangular portion of the matrix by default to using the lower triangular portion.\n\nL, _ = torch.symeig(A, upper=upper) should be replaced with:\nL = torch.linalg.eigvalsh(A, UPLO='U' if upper else 'L')\n\nand\n\nL, V = torch.symeig(A, eigenvectors=True) should be replaced with:\nL, V = torch.linalg.eigh(A, UPLO='U' if upper else 'L')"}
2358
Cov: 24352 -> 24355
Cov: 24355 -> 24355
2359
Cov: 24355 -> 24355
Cov: 24355 -> 24355
2360
{"exception": "TypeError", "msg": "random_() received an invalid combination of arguments - got (to=int, from_=int, ), but expected one of:\n * (*, torch.Generator generator)\n      didn't match because some of the keywords were incorrect: to, from_\n * (int from, int to, *, torch.Generator generator)\n * (int to, *, torch.Generator generator)\n"}
2361
Cov: 24355 -> 24355
Cov: 24355 -> 24355
2362
Cov: 24355 -> 24355
Cov: 24355 -> 24355
2363
Cov: 24355 -> 24355
Cov: 24355 -> 24355
2364
Cov: 24355 -> 24355
Cov: 24355 -> 24355
2365
Cov: 24355 -> 24355
Cov: 24355 -> 24355
2366
Cov: 24355 -> 24355
Cov: 24355 -> 24355
2367
Cov: 24355 -> 24355
Cov: 24355 -> 24355
2368
Cov: 24355 -> 24355
Cov: 24355 -> 24355
2369
Cov: 24355 -> 24356
Cov: 24356 -> 24356
2370
Cov: 24356 -> 24357
Cov: 24357 -> 24357
2371
Cov: 24357 -> 24357
Cov: 24357 -> 24357
2372
Cov: 24357 -> 24357
Cov: 24357 -> 24357
2373
Cov: 24357 -> 24357
Cov: 24357 -> 24357
2374
Cov: 24357 -> 24358
Cov: 24358 -> 24358
2375
Cov: 24358 -> 24366
Cov: 24366 -> 24366
2376
Cov: 24366 -> 24366
Cov: 24366 -> 24366
2377
Cov: 24366 -> 24366
Cov: 24366 -> 24366
2378
Cov: 24366 -> 24366
Cov: 24366 -> 24366
2379
Cov: 24366 -> 24366
Cov: 24366 -> 24366
2380
Cov: 24366 -> 24367
Cov: 24367 -> 24367
2381
Cov: 24367 -> 24369
Cov: 24369 -> 24369
2382
Cov: 24369 -> 24369
Cov: 24369 -> 24369
2383
Cov: 24369 -> 24369
Cov: 24369 -> 24369
2384
{"exception": "TypeError", "msg": "'getset_descriptor' object is not callable"}
2385
Cov: 24369 -> 24369
Cov: 24369 -> 24369
2386
{"exception": "TypeError", "msg": "'getset_descriptor' object is not callable"}
2387
Cov: 24369 -> 24369
Cov: 24369 -> 24369
2388
Cov: 24369 -> 24371
Cov: 24371 -> 24371
2389
Cov: 24371 -> 24375
Cov: 24375 -> 24375
2390
Cov: 24375 -> 24376
Cov: 24376 -> 24376
2391
Cov: 24376 -> 24376
Cov: 24376 -> 24376
2392
Cov: 24376 -> 24378
Cov: 24378 -> 24378
2393
Cov: 24378 -> 24382
Cov: 24382 -> 24382
2394
Cov: 24382 -> 24382
Cov: 24382 -> 24382
2395
Cov: 24382 -> 24382
Cov: 24382 -> 24382
2396
Cov: 24382 -> 24387
Cov: 24387 -> 24387
2397
Cov: 24387 -> 24387
Cov: 24387 -> 24387
2398
Cov: 24387 -> 24387
Cov: 24387 -> 24387
2399
Cov: 24387 -> 24387
Cov: 24387 -> 24387
2400
Cov: 24387 -> 24388
Cov: 24388 -> 24388
2401
Cov: 24388 -> 24389
Cov: 24389 -> 24389
2402
Cov: 24389 -> 24390
Cov: 24390 -> 24390
2403
Cov: 24390 -> 24390
Cov: 24390 -> 24390
2404
Cov: 24390 -> 24390
Cov: 24390 -> 24390
2405
Cov: 24390 -> 24390
Cov: 24390 -> 24390
2406
Cov: 24390 -> 24390
Cov: 24390 -> 24390
2407
Cov: 24390 -> 24390
Cov: 24390 -> 24390
2408
Cov: 24390 -> 24391
Cov: 24391 -> 24391
2409
Cov: 24391 -> 24391
Cov: 24391 -> 24391
2410
{"exception": "NotImplementedError", "msg": "Could not run 'aten::sparse_mask' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::sparse_mask' is only available for these backends: [SparseCPU, SparseCUDA, SparseCsrCPU, SparseCsrCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nSparseCPU: registered at aten/src/ATen/RegisterSparseCPU.cpp:1387 [kernel]\nSparseCUDA: registered at aten/src/ATen/RegisterSparseCUDA.cpp:1573 [kernel]\nSparseCsrCPU: registered at aten/src/ATen/RegisterSparseCsrCPU.cpp:1135 [kernel]\nSparseCsrCUDA: registered at aten/src/ATen/RegisterSparseCsrCUDA.cpp:1276 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:153 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:498 [backend fallback]\nFunctionalize: registered at ../aten/src/ATen/FunctionalizeFallbackKernel.cpp:290 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:86 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradMTIA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_0.cpp:16725 [kernel]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:382 [backend fallback]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:249 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:710 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:203 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:161 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:494 [backend fallback]\nPreDispatch: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:165 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:157 [backend fallback]\n"}
2411
Cov: 24391 -> 24391
Cov: 24391 -> 24391
2412
Cov: 24391 -> 24392
Cov: 24392 -> 24392
2413
Cov: 24392 -> 24392
Cov: 24392 -> 24392
2414
Cov: 24392 -> 24392
Cov: 24392 -> 24392
2415
Cov: 24392 -> 24392
Cov: 24392 -> 24392
2416
Cov: 24392 -> 24393
Cov: 24393 -> 24393
2417
Cov: 24393 -> 24394
Cov: 24394 -> 24394
2418
Cov: 24394 -> 24394
Cov: 24394 -> 24394
2419
Cov: 24394 -> 24394
Cov: 24394 -> 24394
2420
Cov: 24394 -> 24394
Cov: 24394 -> 24394
2421
Cov: 24394 -> 24394
Cov: 24394 -> 24394
2422
Cov: 24394 -> 24409
Cov: 24409 -> 24409
2423
Cov: 24409 -> 24410
Cov: 24410 -> 24410
2424
Cov: 24410 -> 24410
Cov: 24410 -> 24410
2425
Cov: 24410 -> 24410
Cov: 24410 -> 24410
2426
{"exception": "NameError", "msg": "name 'Tensor' is not defined"}
2427
Cov: 24410 -> 24410
Cov: 24410 -> 24410
2428
Cov: 24410 -> 24410
Cov: 24410 -> 24410
2429
Cov: 24410 -> 24410
Cov: 24410 -> 24410
2430
Cov: 24410 -> 24411
Cov: 24411 -> 24411
2431
Cov: 24411 -> 24414
Cov: 24414 -> 24414
2432
Cov: 24414 -> 24414
Cov: 24414 -> 24414
2433
Cov: 24414 -> 24414
Cov: 24414 -> 24414
2434
Cov: 24414 -> 24415
Cov: 24415 -> 24415
2435
Cov: 24415 -> 24415
Cov: 24415 -> 24415
2436
Cov: 24415 -> 24415
Cov: 24415 -> 24415
2437
{"exception": "NotImplementedError", "msg": "Could not run 'aten::q_scale' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::q_scale' is only available for these backends: [QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nQuantizedCPU: registered at aten/src/ATen/RegisterQuantizedCPU.cpp:944 [kernel]\nQuantizedCUDA: registered at aten/src/ATen/RegisterQuantizedCUDA.cpp:459 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:153 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:498 [backend fallback]\nFunctionalize: registered at ../aten/src/ATen/FunctionalizeFallbackKernel.cpp:290 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:86 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradMTIA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_4.cpp:13056 [kernel]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:382 [backend fallback]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:249 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:710 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:203 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:161 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:494 [backend fallback]\nPreDispatch: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:165 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:157 [backend fallback]\n"}
2438
{"exception": "TypeError", "msg": "'getset_descriptor' object is not callable"}
2439
Cov: 24415 -> 24416
Cov: 24416 -> 24416
2440
Cov: 24416 -> 24416
Cov: 24416 -> 24416
2441
Cov: 24416 -> 24417
Cov: 24417 -> 24417
2442
Cov: 24417 -> 24417
Cov: 24417 -> 24417
2443
Cov: 24417 -> 24418
Cov: 24418 -> 24418
2444
Cov: 24418 -> 24418
Cov: 24418 -> 24418
2445
{"exception": "TypeError", "msg": "tile(): argument 'dims' must be tuple of ints, not int"}
2446
Cov: 24418 -> 24418
Cov: 24418 -> 24418
2447
Cov: 24418 -> 24418
Cov: 24418 -> 24418
2448
Cov: 24418 -> 24418
Cov: 24418 -> 24418
2449
Cov: 24418 -> 24418
Cov: 24418 -> 24418
2450
Cov: 24418 -> 24419
Cov: 24419 -> 24419
2451
Cov: 24419 -> 24419
Cov: 24419 -> 24419
2452
Cov: 24419 -> 24419
Cov: 24419 -> 24419
2453
Cov: 24419 -> 24420
Cov: 24420 -> 24420
2454
Cov: 24420 -> 24421
Cov: 24421 -> 24421
2455
Cov: 24421 -> 24423
Cov: 24423 -> 24423
2456
Cov: 24423 -> 24423
Cov: 24423 -> 24423
2457
Cov: 24423 -> 24423
Cov: 24423 -> 24423
2458
Cov: 24423 -> 24423
Cov: 24423 -> 24423
2459
{"exception": "RuntimeError", "msg": "outer: Expected 1-D argument self, but got 2-D"}
2460
Cov: 24423 -> 24423
Cov: 24423 -> 24423
2461
Cov: 24423 -> 24423
Cov: 24423 -> 24423
2462
Cov: 24423 -> 24424
Cov: 24424 -> 24424
2463
Cov: 24424 -> 24424
Cov: 24424 -> 24424
2464
Cov: 24424 -> 24427
Cov: 24427 -> 24427
2465
Cov: 24427 -> 24427
Cov: 24427 -> 24427
2466
Cov: 24427 -> 24427
Cov: 24427 -> 24427
2467
Cov: 24427 -> 24427
Cov: 24427 -> 24427
2468
Cov: 24427 -> 24427
Cov: 24427 -> 24427
2469
Cov: 24427 -> 24427
Cov: 24427 -> 24427
2470
Cov: 24427 -> 24427
Cov: 24427 -> 24427
2471
Cov: 24427 -> 24428
Cov: 24428 -> 24428
2472
Cov: 24428 -> 24428
Cov: 24428 -> 24428
2473
Cov: 24428 -> 24428
Cov: 24428 -> 24428
2474
Cov: 24428 -> 24428
Cov: 24428 -> 24428
2475
{"exception": "TypeError", "msg": "new_full(): argument 'size' (position 1) must be tuple of ints, not int"}
2476
Cov: 24428 -> 24428
Cov: 24428 -> 24428
2477
Cov: 24428 -> 24428
Cov: 24428 -> 24428
2478
Cov: 24428 -> 24428
Cov: 24428 -> 24428
2479
Cov: 24428 -> 24428
Cov: 24428 -> 24428
2480
Cov: 24428 -> 24428
Cov: 24428 -> 24428
2481
Cov: 24428 -> 24429
Cov: 24429 -> 24429
2482
Cov: 24429 -> 24429
Cov: 24429 -> 24429
2483
Cov: 24429 -> 24429
Cov: 24429 -> 24429
2484
Cov: 24429 -> 24431
Cov: 24431 -> 24431
2485
Cov: 24431 -> 24431
Cov: 24431 -> 24431
2486
{"exception": "RuntimeError", "msg": "1D tensors expected, but got 2D and 2D tensors"}
2487
Cov: 24431 -> 24431
Cov: 24431 -> 24431
2488
Cov: 24431 -> 24431
Cov: 24431 -> 24431
2489
Cov: 24431 -> 24431
Cov: 24431 -> 24431
2490
Cov: 24431 -> 24431
Cov: 24431 -> 24431
2491
{"exception": "RuntimeError", "msg": "masked_scatter_ only supports boolean masks, but got mask with dtype Byte"}
2492
Cov: 24431 -> 24432
Cov: 24432 -> 24432
2493
Cov: 24432 -> 24433
Cov: 24433 -> 24433
2494
Cov: 24433 -> 24433
Cov: 24433 -> 24433
2495
Cov: 24433 -> 24433
Cov: 24433 -> 24433
2496
{"exception": "TypeError", "msg": "index_add_() missing 1 required positional arguments: \"source\""}
2497
Cov: 24433 -> 24433
Cov: 24433 -> 24433
2498
Cov: 24433 -> 24433
Cov: 24433 -> 24433
2499
Cov: 24433 -> 24433
Cov: 24433 -> 24433
2500
Cov: 24433 -> 24435
Cov: 24435 -> 24435
2501
Cov: 24435 -> 24436
Cov: 24436 -> 24436
2502
Cov: 24436 -> 24437
Cov: 24437 -> 24437
2503
{"exception": "RuntimeError", "msg": "result type Float can't be cast to the desired output type Long"}
2504
Cov: 24437 -> 24437
Cov: 24437 -> 24437
2505
Cov: 24437 -> 24437
Cov: 24437 -> 24437
2506
Cov: 24437 -> 24438
Cov: 24438 -> 24438
2507
Cov: 24438 -> 24438
Cov: 24438 -> 24438
2508
Cov: 24438 -> 24438
Cov: 24438 -> 24438
2509
Cov: 24438 -> 24438
Cov: 24438 -> 24438
2510
Cov: 24438 -> 24438
Cov: 24438 -> 24438
2511
Cov: 24438 -> 24438
Cov: 24438 -> 24438
2512
Cov: 24438 -> 24438
Cov: 24438 -> 24438
2513
Cov: 24438 -> 24438
Cov: 24438 -> 24438
2514
Cov: 24438 -> 24438
Cov: 24438 -> 24438
2515
Cov: 24438 -> 24438
Cov: 24438 -> 24438
2516
Cov: 24438 -> 24438
Cov: 24438 -> 24438
2517
Cov: 24438 -> 24438
Cov: 24438 -> 24438
2518
Cov: 24438 -> 24438
Cov: 24438 -> 24438
2519
Cov: 24438 -> 24438
Cov: 24438 -> 24438
2520
Cov: 24438 -> 24439
Cov: 24439 -> 24439
2521
Cov: 24439 -> 24439
Cov: 24439 -> 24439
2522
Cov: 24439 -> 24439
Cov: 24439 -> 24439
2523
Cov: 24439 -> 24439
Cov: 24439 -> 24439
2524
Cov: 24439 -> 24439
Cov: 24439 -> 24439
2525
Cov: 24439 -> 24626
Cov: 24626 -> 24626
2526
Cov: 24626 -> 24626
Cov: 24626 -> 24626
2527
{"exception": "RuntimeError", "msg": "nanmean(): expected input to have floating point or complex dtype but got Long"}
2528
{"exception": "RuntimeError", "msg": "masked_scatter_ only supports boolean masks, but got mask with dtype Byte"}
2529
Cov: 24626 -> 24626
Cov: 24626 -> 24626
2530
Cov: 24626 -> 24626
Cov: 24626 -> 24626
2531
Cov: 24626 -> 24626
Cov: 24626 -> 24626
2532
Cov: 24626 -> 24626
Cov: 24626 -> 24626
2533
Cov: 24626 -> 24656
Cov: 24656 -> 24656
2534
Cov: 24656 -> 24673
Cov: 24673 -> 24673
2535
Cov: 24673 -> 24673
Cov: 24673 -> 24673
2536
Cov: 24673 -> 24673
Cov: 24673 -> 24673
2537
Cov: 24673 -> 24673
Cov: 24673 -> 24673
2538
Cov: 24673 -> 24676
Cov: 24676 -> 24676
2539
Cov: 24676 -> 24678
Cov: 24678 -> 24678
2540
Cov: 24678 -> 24678
Cov: 24678 -> 24678
2541
Cov: 24678 -> 24678
Cov: 24678 -> 24678
2542
Cov: 24678 -> 24678
Cov: 24678 -> 24678
2543
Cov: 24678 -> 24678
Cov: 24678 -> 24678
2544
Cov: 24678 -> 24678
Cov: 24678 -> 24678
2545
Cov: 24678 -> 24678
Cov: 24678 -> 24678
2546
Cov: 24678 -> 24679
Cov: 24679 -> 24679
2547
Cov: 24679 -> 24680
Cov: 24680 -> 24680
2548
Cov: 24680 -> 24680
Cov: 24680 -> 24680
2549
Cov: 24680 -> 24683
Cov: 24683 -> 24683
2550
Cov: 24683 -> 24683
Cov: 24683 -> 24683
2551
{"exception": "RuntimeError", "msg": "vector + matrix @ vector expected, got 2, 2, 1"}
2552
Cov: 24683 -> 24683
Cov: 24683 -> 24683
2553
Cov: 24683 -> 24683
Cov: 24683 -> 24683
2554
Cov: 24683 -> 24683
Cov: 24683 -> 24683
2555
Cov: 24683 -> 24683
Cov: 24683 -> 24683
2556
Cov: 24683 -> 24683
Cov: 24683 -> 24683
2557
Cov: 24683 -> 24684
Cov: 24684 -> 24684
2558
Cov: 24684 -> 24684
Cov: 24684 -> 24684
2559
Cov: 24684 -> 24686
Cov: 24686 -> 24686
2560
Cov: 24686 -> 24686
Cov: 24686 -> 24686
2561
Cov: 24686 -> 24687
Cov: 24687 -> 24687
2562
Cov: 24687 -> 24687
Cov: 24687 -> 24687
2563
Cov: 24687 -> 24687
Cov: 24687 -> 24687
2564
Cov: 24687 -> 24687
Cov: 24687 -> 24687
2565
Cov: 24687 -> 24687
Cov: 24687 -> 24687
2566
Cov: 24687 -> 24687
Cov: 24687 -> 24687
2567
Cov: 24687 -> 24688
Cov: 24688 -> 24688
2568
Cov: 24688 -> 24688
Cov: 24688 -> 24688
2569
Cov: 24688 -> 24706
Cov: 24706 -> 24706
2570
Cov: 24706 -> 24707
Cov: 24707 -> 24707
2571
Cov: 24707 -> 24707
Cov: 24707 -> 24707
2572
Cov: 24707 -> 24708
Cov: 24708 -> 24708
2573
Cov: 24708 -> 24708
Cov: 24708 -> 24708
2574
Cov: 24708 -> 24708
Cov: 24708 -> 24708
2575
Cov: 24708 -> 24708
Cov: 24708 -> 24708
2576
Cov: 24708 -> 24708
Cov: 24708 -> 24708
2577
Cov: 24708 -> 24708
Cov: 24708 -> 24708
2578
Cov: 24708 -> 24708
Cov: 24708 -> 24708
2579
Cov: 24708 -> 24708
Cov: 24708 -> 24708
2580
Cov: 24708 -> 24709
Cov: 24709 -> 24709
2581
Cov: 24709 -> 24709
Cov: 24709 -> 24709
2582
Cov: 24709 -> 24710
Cov: 24710 -> 24710
2583
Cov: 24710 -> 24710
Cov: 24710 -> 24710
2584
Cov: 24710 -> 24711
Cov: 24711 -> 24711
2585
Cov: 24711 -> 24711
Cov: 24711 -> 24711
2586
Cov: 24711 -> 24711
Cov: 24711 -> 24711
2587
Cov: 24711 -> 24711
Cov: 24711 -> 24711
2588
Cov: 24711 -> 24711
Cov: 24711 -> 24711
2589
Cov: 24711 -> 24713
Cov: 24713 -> 24713
2590
Cov: 24713 -> 24713
Cov: 24713 -> 24713
2591
Cov: 24713 -> 24713
Cov: 24713 -> 24713
2592
Cov: 24713 -> 24713
Cov: 24713 -> 24713
2593
Cov: 24713 -> 24713
Cov: 24713 -> 24713
2594
Cov: 24713 -> 24714
Cov: 24714 -> 24714
2595
Cov: 24714 -> 24714
Cov: 24714 -> 24714
2596
Cov: 24714 -> 24714
Cov: 24714 -> 24714
2597
Cov: 24714 -> 24714
Cov: 24714 -> 24714
2598
Cov: 24714 -> 24714
Cov: 24714 -> 24714
2599
Cov: 24714 -> 24714
Cov: 24714 -> 24714
2600
Cov: 24714 -> 24715
Cov: 24715 -> 24715
2601
Cov: 24715 -> 24715
Cov: 24715 -> 24715
2602
Cov: 24715 -> 24715
Cov: 24715 -> 24715
2603
Cov: 24715 -> 24715
Cov: 24715 -> 24715
2604
Cov: 24715 -> 24715
Cov: 24715 -> 24715
2605
Cov: 24715 -> 24715
Cov: 24715 -> 24715
2606
Cov: 24715 -> 24715
Cov: 24715 -> 24715
2607
Cov: 24715 -> 24716
Cov: 24716 -> 24716
2608
Cov: 24716 -> 24716
Cov: 24716 -> 24716
2609
Cov: 24716 -> 24716
Cov: 24716 -> 24716
2610
{"exception": "RuntimeError", "msg": "any: You passed a dimname (string) to this op in place of a dimension index but it does not yet support this behavior. Please pass a dimension index to work around this."}
2611
Cov: 24716 -> 24716
Cov: 24716 -> 24716
2612
Cov: 24716 -> 24717
Cov: 24717 -> 24717
2613
Cov: 24717 -> 24717
Cov: 24717 -> 24717
2614
{"exception": "RuntimeError", "msg": "Index tensor must have the same number of dimensions as self tensor"}
2615
Cov: 24717 -> 24717
Cov: 24717 -> 24717
2616
Cov: 24717 -> 24717
Cov: 24717 -> 24717
2617
Cov: 24717 -> 24717
Cov: 24717 -> 24717
2618
Cov: 24717 -> 24719
Cov: 24719 -> 24719
2619
Cov: 24719 -> 24719
Cov: 24719 -> 24719
2620
Cov: 24719 -> 24719
Cov: 24719 -> 24719
2621
Cov: 24719 -> 24719
Cov: 24719 -> 24719
2622
Cov: 24719 -> 24719
Cov: 24719 -> 24719
2623
Cov: 24719 -> 24719
Cov: 24719 -> 24719
2624
Cov: 24719 -> 24719
Cov: 24719 -> 24719
2625
Cov: 24719 -> 24719
Cov: 24719 -> 24719
2626
Cov: 24719 -> 24719
Cov: 24719 -> 24719
2627
Cov: 24719 -> 24719
Cov: 24719 -> 24719
2628
{"exception": "NameError", "msg": "name 'random' is not defined"}
2629
Cov: 24719 -> 24719
Cov: 24719 -> 24719
2630
Cov: 24719 -> 24719
Cov: 24719 -> 24719
2631
Cov: 24719 -> 24719
Cov: 24719 -> 24719
2632
Cov: 24719 -> 24719
Cov: 24719 -> 24719
2633
Cov: 24719 -> 24719
Cov: 24719 -> 24719
2634
Cov: 24719 -> 24720
Cov: 24720 -> 24720
2635
Cov: 24720 -> 24720
Cov: 24720 -> 24720
2636
Cov: 24720 -> 24720
Cov: 24720 -> 24720
2637
Cov: 24720 -> 24720
Cov: 24720 -> 24720
2638
Cov: 24720 -> 24720
Cov: 24720 -> 24720
2639
Cov: 24720 -> 24720
Cov: 24720 -> 24720
2640
{"exception": "RuntimeError", "msg": "Expected nested_tensorlist[0].size() > 0 to be true, but got false.  (Could this error message be improved?  If so, please report an enhancement request to PyTorch.)"}
2641
Cov: 24720 -> 24722
Cov: 24722 -> 24722
2642
Cov: 24722 -> 24722
Cov: 24722 -> 24722
2643
Cov: 24722 -> 24722
Cov: 24722 -> 24722
2644
Cov: 24722 -> 24723
Cov: 24723 -> 24723
2645
Cov: 24723 -> 24723
Cov: 24723 -> 24723
2646
Cov: 24723 -> 24723
Cov: 24723 -> 24723
2647
Cov: 24723 -> 24808
Cov: 24808 -> 24808
2648
Cov: 24808 -> 24808
Cov: 24808 -> 24808
2649
{"exception": "RuntimeError", "msg": "\"gcd_cpu\" not implemented for 'Double'"}
2650
Cov: 24808 -> 24808
Cov: 24808 -> 24808
2651
Cov: 24808 -> 24809
Cov: 24809 -> 24809
2652
Cov: 24809 -> 24809
Cov: 24809 -> 24809
2653
Cov: 24809 -> 24811
Cov: 24811 -> 24811
2654
Cov: 24811 -> 24811
Cov: 24811 -> 24811
2655
Cov: 24811 -> 24812
Cov: 24812 -> 24812
2656
Cov: 24812 -> 24814
Cov: 24814 -> 24814
2657
Cov: 24814 -> 24814
Cov: 24814 -> 24814
2658
Cov: 24814 -> 24814
Cov: 24814 -> 24814
2659
Cov: 24814 -> 24814
Cov: 24814 -> 24814
2660
{"exception": "RuntimeError", "msg": "torch.ormqr: Expected other to have the same number of dimensions as input, but got other.ndim equal to 2 and input.ndim is equal to 3"}
2661
Cov: 24814 -> 24815
Cov: 24815 -> 24815
2662
Cov: 24815 -> 24815
Cov: 24815 -> 24815
2663
Cov: 24815 -> 24816
Cov: 24816 -> 24816
2664
Cov: 24816 -> 24816
Cov: 24816 -> 24816
2665
Cov: 24816 -> 24816
Cov: 24816 -> 24816
2666
Cov: 24816 -> 24816
Cov: 24816 -> 24816
2667
Cov: 24816 -> 24817
Cov: 24817 -> 24817
2668
Cov: 24817 -> 24817
Cov: 24817 -> 24817
2669
Cov: 24817 -> 24817
Cov: 24817 -> 24817
2670
Cov: 24817 -> 24818
Cov: 24818 -> 24818
2671
Cov: 24818 -> 24818
Cov: 24818 -> 24818
2672
Cov: 24818 -> 24818
Cov: 24818 -> 24818
2673
Cov: 24818 -> 24819
Cov: 24819 -> 24819
2674
Cov: 24819 -> 24819
Cov: 24819 -> 24819
2675
Cov: 24819 -> 24820
Cov: 24820 -> 24820
2676
Cov: 24820 -> 24820
Cov: 24820 -> 24820
2677
Cov: 24820 -> 24820
Cov: 24820 -> 24820
2678
Cov: 24820 -> 24820
Cov: 24820 -> 24820
2679
Cov: 24820 -> 24820
Cov: 24820 -> 24820
2680
Cov: 24820 -> 24820
Cov: 24820 -> 24820
2681
Cov: 24820 -> 24820
Cov: 24820 -> 24820
2682
{"exception": "RuntimeError", "msg": "Index tensor must have the same number of dimensions as input tensor"}
2683
Cov: 24820 -> 24820
Cov: 24820 -> 24820
2684
Cov: 24820 -> 24820
Cov: 24820 -> 24820
2685
Cov: 24820 -> 24820
Cov: 24820 -> 24820
2686
Cov: 24820 -> 24821
Cov: 24821 -> 24821
2687
Cov: 24821 -> 24822
Cov: 24822 -> 24822
2688
Cov: 24822 -> 24823
Cov: 24823 -> 24823
2689
Cov: 24823 -> 24823
Cov: 24823 -> 24823
2690
Cov: 24823 -> 24824
Cov: 24824 -> 24824
2691
Cov: 24824 -> 24824
Cov: 24824 -> 24824
2692
Cov: 24824 -> 24824
Cov: 24824 -> 24824
2693
Cov: 24824 -> 24824
Cov: 24824 -> 24824
2694
Cov: 24824 -> 24824
Cov: 24824 -> 24824
2695
Cov: 24824 -> 24824
Cov: 24824 -> 24824
2696
Cov: 24824 -> 24824
Cov: 24824 -> 24824
2697
Cov: 24824 -> 24824
Cov: 24824 -> 24824
2698
Cov: 24824 -> 24825
Cov: 24825 -> 24825
2699
Cov: 24825 -> 24825
Cov: 24825 -> 24825
2700
{"exception": "RuntimeError", "msg": "Bad in-place call: input tensor size [10, 20] and output tensor size [20, 40] should match"}
2701
{"exception": "TypeError", "msg": "'getset_descriptor' object is not callable"}
2702
Cov: 24825 -> 24825
Cov: 24825 -> 24825
2703
Cov: 24825 -> 24825
Cov: 24825 -> 24825
2704
Cov: 24825 -> 24825
Cov: 24825 -> 24825
2705
Cov: 24825 -> 24825
Cov: 24825 -> 24825
2706
Cov: 24825 -> 24825
Cov: 24825 -> 24825
2707
Cov: 24825 -> 24826
Cov: 24826 -> 24826
2708
Cov: 24826 -> 24826
Cov: 24826 -> 24826
2709
Cov: 24826 -> 24826
Cov: 24826 -> 24826
2710
Cov: 24826 -> 24826
Cov: 24826 -> 24826
2711
Cov: 24826 -> 24828
Cov: 24828 -> 24828
2712
Cov: 24828 -> 24828
Cov: 24828 -> 24828
2713
Cov: 24828 -> 24828
Cov: 24828 -> 24828
2714
Cov: 24828 -> 24833
Cov: 24833 -> 24833
2715
Cov: 24833 -> 24833
Cov: 24833 -> 24833
2716
Cov: 24833 -> 24833
Cov: 24833 -> 24833
2717
Cov: 24833 -> 24833
Cov: 24833 -> 24833
2718
Cov: 24833 -> 24834
Cov: 24834 -> 24834
2719
Cov: 24834 -> 24834
Cov: 24834 -> 24834
2720
Cov: 24834 -> 24834
Cov: 24834 -> 24834
2721
Cov: 24834 -> 24834
Cov: 24834 -> 24834
2722
Cov: 24834 -> 24834
Cov: 24834 -> 24834
2723
Cov: 24834 -> 24834
Cov: 24834 -> 24834
2724
Cov: 24834 -> 24834
Cov: 24834 -> 24834
2725
Cov: 24834 -> 24834
Cov: 24834 -> 24834
2726
Cov: 24834 -> 24834
Cov: 24834 -> 24834
2727
Cov: 24834 -> 24834
Cov: 24834 -> 24834
2728
Cov: 24834 -> 24834
Cov: 24834 -> 24834
2729
Cov: 24834 -> 24835
Cov: 24835 -> 24835
2730
{"exception": "RuntimeError", "msg": "source tensor shape must match self tensor shape, excluding the specified dimension. Got self.shape = [3, 3] source.shape = [2, 2]"}
2731
Cov: 24835 -> 24836
Cov: 24836 -> 24836
2732
Cov: 24836 -> 24836
Cov: 24836 -> 24836
2733
Cov: 24836 -> 24841
Cov: 24841 -> 24841
2734
Cov: 24841 -> 24841
Cov: 24841 -> 24841
2735
Cov: 24841 -> 24841
Cov: 24841 -> 24841
2736
Cov: 24841 -> 24841
Cov: 24841 -> 24841
2737
Cov: 24841 -> 24842
Cov: 24842 -> 24842
2738
Cov: 24842 -> 24842
Cov: 24842 -> 24842
2739
Cov: 24842 -> 24842
Cov: 24842 -> 24842
2740
Cov: 24842 -> 24842
Cov: 24842 -> 24842
2741
Cov: 24842 -> 24843
Cov: 24843 -> 24843
2742
Cov: 24843 -> 24843
Cov: 24843 -> 24843
2743
Cov: 24843 -> 24844
Cov: 24844 -> 24844
2744
Cov: 24844 -> 24844
Cov: 24844 -> 24844
2745
Cov: 24844 -> 24844
Cov: 24844 -> 24844
2746
Cov: 24844 -> 24844
Cov: 24844 -> 24844
2747
Cov: 24844 -> 24849
Cov: 24849 -> 24849
2748
Cov: 24849 -> 24849
Cov: 24849 -> 24849
2749
Cov: 24849 -> 24849
Cov: 24849 -> 24849
2750
{"exception": "TypeError", "msg": "map_(): argument 'other' (position 1) must be Tensor, not function"}
2751
Cov: 24849 -> 24852
Cov: 24852 -> 24852
2752
Cov: 24852 -> 24852
Cov: 24852 -> 24852
2753
Cov: 24852 -> 24852
Cov: 24852 -> 24852
2754
Cov: 24852 -> 24852
Cov: 24852 -> 24852
2755
Cov: 24852 -> 24852
Cov: 24852 -> 24852
2756
Cov: 24852 -> 24852
Cov: 24852 -> 24852
2757
Cov: 24852 -> 24852
Cov: 24852 -> 24852
2758
Cov: 24852 -> 24853
Cov: 24853 -> 24853
2759
Cov: 24853 -> 24853
Cov: 24853 -> 24853
2760
Cov: 24853 -> 24853
Cov: 24853 -> 24853
2761
Cov: 24853 -> 24855
Cov: 24855 -> 24855
2762
Cov: 24855 -> 24855
Cov: 24855 -> 24855
2763
{"exception": "TypeError", "msg": "<lambda>() takes 1 positional argument but 2 were given"}
2764
Cov: 24855 -> 24856
Cov: 24856 -> 24856
2765
Cov: 24856 -> 24856
Cov: 24856 -> 24856
2766
Cov: 24856 -> 24856
Cov: 24856 -> 24856
2767
Cov: 24856 -> 24856
Cov: 24856 -> 24856
2768
Cov: 24856 -> 24856
Cov: 24856 -> 24856
2769
Cov: 24856 -> 24856
Cov: 24856 -> 24856
2770
{"exception": "RuntimeError", "msg": "source tensor shape must match self tensor shape, excluding the specified dimension. Got self.shape = [3, 3] source.shape = [2]"}
2771
Cov: 24856 -> 24856
Cov: 24856 -> 24856
2772
Cov: 24856 -> 24856
Cov: 24856 -> 24856
2773
Cov: 24856 -> 24856
Cov: 24856 -> 24856
2774
Cov: 24856 -> 24856
Cov: 24856 -> 24856
2775
Cov: 24856 -> 24856
Cov: 24856 -> 24856
2776
Cov: 24856 -> 24856
Cov: 24856 -> 24856
2777
Cov: 24856 -> 24862
Cov: 24862 -> 24862
2778
Cov: 24862 -> 24862
Cov: 24862 -> 24862
2779
Cov: 24862 -> 24862
Cov: 24862 -> 24862
2780
Cov: 24862 -> 24863
Cov: 24863 -> 24863
2781
Cov: 24863 -> 24863
Cov: 24863 -> 24863
2782
Cov: 24863 -> 24863
Cov: 24863 -> 24863
2783
Cov: 24863 -> 24863
Cov: 24863 -> 24863
2784
{"exception": "NotImplementedError", "msg": "Could not run 'aten::int_repr' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::int_repr' is only available for these backends: [QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nQuantizedCPU: registered at aten/src/ATen/RegisterQuantizedCPU.cpp:944 [kernel]\nQuantizedCUDA: registered at aten/src/ATen/RegisterQuantizedCUDA.cpp:459 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:153 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:498 [backend fallback]\nFunctionalize: registered at ../aten/src/ATen/FunctionalizeFallbackKernel.cpp:290 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:86 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradMTIA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_0.cpp:16725 [kernel]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:382 [backend fallback]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:249 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:710 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:203 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:161 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:494 [backend fallback]\nPreDispatch: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:165 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:157 [backend fallback]\n"}
2785
Cov: 24863 -> 24863
Cov: 24863 -> 24863
2786
{"exception": "RuntimeError", "msg": "\"lerp_kernel_scalar\" not implemented for 'Long'"}
2787
Cov: 24863 -> 24864
Cov: 24864 -> 24864
2788
{"exception": "TypeError", "msg": "index_put_(): argument 'indices' (position 1) must be tuple of Tensors, not Tensor"}
2789
Cov: 24864 -> 24865
Cov: 24865 -> 24865
2790
Cov: 24865 -> 24865
Cov: 24865 -> 24865
2791
Cov: 24865 -> 24865
Cov: 24865 -> 24865
2792
Cov: 24865 -> 24865
Cov: 24865 -> 24865
2793
Cov: 24865 -> 24865
Cov: 24865 -> 24865
2794
Cov: 24865 -> 24865
Cov: 24865 -> 24865
2795
Cov: 24865 -> 24865
Cov: 24865 -> 24865
2796
Cov: 24865 -> 24865
Cov: 24865 -> 24865
2797
Cov: 24865 -> 24866
Cov: 24866 -> 24866
2798
Cov: 24866 -> 24866
Cov: 24866 -> 24866
2799
Cov: 24866 -> 24866
Cov: 24866 -> 24866
2800
Cov: 24866 -> 24866
Cov: 24866 -> 24866
2801
Cov: 24866 -> 24866
Cov: 24866 -> 24866
2802
Cov: 24866 -> 24872
Cov: 24872 -> 24872
2803
Cov: 24872 -> 24872
Cov: 24872 -> 24872
2804
Cov: 24872 -> 24872
Cov: 24872 -> 24872
2805
{"exception": "RuntimeError", "msg": "This function was deprecated since version 1.9 and is now removed. `torch.linalg.eig` returns complex tensors of dtype `cfloat` or `cdouble` rather than real tensors mimicking complex tensors.\n\nL, _ = torch.eig(A) should be replaced with:\nL_complex = torch.linalg.eigvals(A)\n\nand\n\nL, V = torch.eig(A, eigenvectors=True) should be replaced with:\nL_complex, V_complex = torch.linalg.eig(A)"}
2806
Cov: 24872 -> 24874
Cov: 24874 -> 24874
2807
Cov: 24874 -> 24874
Cov: 24874 -> 24874
2808
Cov: 24874 -> 24874
Cov: 24874 -> 24874
2809
Cov: 24874 -> 24875
Cov: 24875 -> 24875
2810
Cov: 24875 -> 24875
Cov: 24875 -> 24875
2811
Cov: 24875 -> 24875
Cov: 24875 -> 24875
2812
Cov: 24875 -> 24875
Cov: 24875 -> 24875
2813
Cov: 24875 -> 24875
Cov: 24875 -> 24875
2814
{"exception": "RuntimeError", "msg": "The expanded size of the tensor (3) must match the existing size (2) at non-singleton dimension 0.  Target sizes: [3, 3].  Tensor sizes: [2, 3]"}
2815
Cov: 24875 -> 24876
Cov: 24876 -> 24876
2816
Cov: 24876 -> 24876
Cov: 24876 -> 24876
2817
Cov: 24876 -> 24877
Cov: 24877 -> 24877
2818
Cov: 24877 -> 24889
Cov: 24889 -> 24889
2819
Cov: 24889 -> 24889
Cov: 24889 -> 24889
2820
Cov: 24889 -> 24889
Cov: 24889 -> 24889
2821
Cov: 24889 -> 24889
Cov: 24889 -> 24889
2822
Cov: 24889 -> 24889
Cov: 24889 -> 24889
2823
Cov: 24889 -> 24889
Cov: 24889 -> 24889
2824
Cov: 24889 -> 24889
Cov: 24889 -> 24889
2825
Cov: 24889 -> 24889
Cov: 24889 -> 24889
2826
Cov: 24889 -> 24889
Cov: 24889 -> 24889
2827
Cov: 24889 -> 24889
Cov: 24889 -> 24889
2828
Cov: 24889 -> 24889
Cov: 24889 -> 24889
2829
Cov: 24889 -> 24889
Cov: 24889 -> 24889
2830
Cov: 24889 -> 24889
Cov: 24889 -> 24889
2831
{"exception": "TypeError", "msg": "arctanh_() takes no arguments (1 given)"}
2832
Cov: 24889 -> 24889
Cov: 24889 -> 24889
2833
{"exception": "NotImplementedError", "msg": "Could not run 'aten::q_scale' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::q_scale' is only available for these backends: [QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nQuantizedCPU: registered at aten/src/ATen/RegisterQuantizedCPU.cpp:944 [kernel]\nQuantizedCUDA: registered at aten/src/ATen/RegisterQuantizedCUDA.cpp:459 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:153 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:498 [backend fallback]\nFunctionalize: registered at ../aten/src/ATen/FunctionalizeFallbackKernel.cpp:290 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:86 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradMTIA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_4.cpp:13056 [kernel]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:382 [backend fallback]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:249 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:710 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:203 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:161 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:494 [backend fallback]\nPreDispatch: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:165 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:157 [backend fallback]\n"}
2834
Cov: 24889 -> 24894
Cov: 24894 -> 24894
2835
Cov: 24894 -> 24894
Cov: 24894 -> 24894
2836
Cov: 24894 -> 24894
Cov: 24894 -> 24894
2837
Cov: 24894 -> 24894
Cov: 24894 -> 24894
2838
Cov: 24894 -> 24894
Cov: 24894 -> 24894
2839
Cov: 24894 -> 24894
Cov: 24894 -> 24894
2840
Cov: 24894 -> 24894
Cov: 24894 -> 24894
2841
Cov: 24894 -> 24919
Cov: 24919 -> 24919
2842
Cov: 24919 -> 24919
Cov: 24919 -> 24919
2843
Cov: 24919 -> 24920
Cov: 24920 -> 24920
2844
Cov: 24920 -> 24920
Cov: 24920 -> 24920
2845
Cov: 24920 -> 24920
Cov: 24920 -> 24920
2846
Cov: 24920 -> 24920
Cov: 24920 -> 24920
2847
Cov: 24920 -> 24920
Cov: 24920 -> 24920
2848
Cov: 24920 -> 24920
Cov: 24920 -> 24920
2849
Cov: 24920 -> 24920
Cov: 24920 -> 24920
2850
Cov: 24920 -> 24920
Cov: 24920 -> 24920
2851
Cov: 24920 -> 24920
Cov: 24920 -> 24920
2852
Cov: 24920 -> 24921
Cov: 24921 -> 24921
2853
Cov: 24921 -> 24921
Cov: 24921 -> 24921
2854
Cov: 24921 -> 24922
Cov: 24922 -> 24922
2855
Cov: 24922 -> 24922
Cov: 24922 -> 24922
2856
Cov: 24922 -> 24922
Cov: 24922 -> 24922
2857
Cov: 24922 -> 24922
Cov: 24922 -> 24922
2858
Cov: 24922 -> 24922
Cov: 24922 -> 24922
2859
Cov: 24922 -> 24922
Cov: 24922 -> 24922
2860
Cov: 24922 -> 24922
Cov: 24922 -> 24922
2861
Cov: 24922 -> 24922
Cov: 24922 -> 24922
2862
Cov: 24922 -> 24922
Cov: 24922 -> 24922
2863
Cov: 24922 -> 24923
Cov: 24923 -> 24923
2864
Cov: 24923 -> 24923
Cov: 24923 -> 24923
2865
{"exception": "TypeError", "msg": "'getset_descriptor' object is not callable"}
2866
Cov: 24923 -> 24923
Cov: 24923 -> 24923
2867
Cov: 24923 -> 24923
Cov: 24923 -> 24923
2868
Cov: 24923 -> 24923
Cov: 24923 -> 24923
2869
Cov: 24923 -> 24923
Cov: 24923 -> 24923
2870
Cov: 24923 -> 24923
Cov: 24923 -> 24923
2871
Cov: 24923 -> 24923
Cov: 24923 -> 24923
2872
Cov: 24923 -> 24923
Cov: 24923 -> 24923
2873
Cov: 24923 -> 24923
Cov: 24923 -> 24923
2874
Cov: 24923 -> 24923
Cov: 24923 -> 24923
2875
Cov: 24923 -> 24923
Cov: 24923 -> 24923
2876
Cov: 24923 -> 24923
Cov: 24923 -> 24923
2877
Cov: 24923 -> 24923
Cov: 24923 -> 24923
2878
Cov: 24923 -> 24924
Cov: 24924 -> 24924
2879
Cov: 24924 -> 24924
Cov: 24924 -> 24924
2880
Cov: 24924 -> 24924
Cov: 24924 -> 24924
2881
Cov: 24924 -> 24924
Cov: 24924 -> 24924
2882
Cov: 24924 -> 24924
Cov: 24924 -> 24924
2883
Cov: 24924 -> 24924
Cov: 24924 -> 24924
2884
Cov: 24924 -> 24925
Cov: 24925 -> 24925
2885
{"exception": "RuntimeError", "msg": "1D tensors expected, but got 2D and 2D tensors"}
2886
{"exception": "RuntimeError", "msg": "\"lshift_cpu\" not implemented for 'Float'"}
2887
Cov: 24925 -> 24926
Cov: 24926 -> 24926
2888
Cov: 24926 -> 24926
Cov: 24926 -> 24926
2889
Cov: 24926 -> 24926
Cov: 24926 -> 24926
2890
Cov: 24926 -> 24926
Cov: 24926 -> 24926
2891
Cov: 24926 -> 24926
Cov: 24926 -> 24926
2892
Cov: 24926 -> 24929
Cov: 24929 -> 24929
2893
Cov: 24929 -> 24929
Cov: 24929 -> 24929
2894
Cov: 24929 -> 24931
Cov: 24931 -> 24931
2895
Cov: 24931 -> 24932
Cov: 24932 -> 24932
2896
Cov: 24932 -> 24933
Cov: 24933 -> 24933
2897
Cov: 24933 -> 24934
Cov: 24934 -> 24934
2898
Cov: 24934 -> 24934
Cov: 24934 -> 24934
2899
Cov: 24934 -> 24934
Cov: 24934 -> 24934
2900
Cov: 24934 -> 24935
Cov: 24935 -> 24935
2901
Cov: 24935 -> 24935
Cov: 24935 -> 24935
2902
Cov: 24935 -> 24935
Cov: 24935 -> 24935
2903
Cov: 24935 -> 24935
Cov: 24935 -> 24935
2904
Cov: 24935 -> 24935
Cov: 24935 -> 24935
2905
Cov: 24935 -> 24935
Cov: 24935 -> 24935
2906
Cov: 24935 -> 24935
Cov: 24935 -> 24935
2907
Cov: 24935 -> 24936
Cov: 24936 -> 24936
2908
Cov: 24936 -> 24963
Cov: 24963 -> 24963
2909
Cov: 24963 -> 24964
Cov: 24964 -> 24964
2910
Cov: 24964 -> 24965
Cov: 24965 -> 24965
2911
Cov: 24965 -> 24965
Cov: 24965 -> 24965
2912
Cov: 24965 -> 24965
Cov: 24965 -> 24965
2913
Cov: 24965 -> 24966
Cov: 24966 -> 24966
2914
Cov: 24966 -> 24966
Cov: 24966 -> 24966
2915
Cov: 24966 -> 24966
Cov: 24966 -> 24966
2916
Cov: 24966 -> 24967
Cov: 24967 -> 24967
2917
Cov: 24967 -> 24967
Cov: 24967 -> 24967
2918
Cov: 24967 -> 24968
Cov: 24968 -> 24968
2919
Cov: 24968 -> 24968
Cov: 24968 -> 24968
2920
Cov: 24968 -> 24968
Cov: 24968 -> 24968
2921
Cov: 24968 -> 24969
Cov: 24969 -> 24969
2922
Cov: 24969 -> 24969
Cov: 24969 -> 24969
2923
Cov: 24969 -> 24969
Cov: 24969 -> 24969
2924
Cov: 24969 -> 24973
Cov: 24973 -> 24973
2925
Cov: 24973 -> 24973
Cov: 24973 -> 24973
2926
Cov: 24973 -> 24973
Cov: 24973 -> 24973
2927
Cov: 24973 -> 24974
Cov: 24974 -> 24974
2928
Cov: 24974 -> 24974
Cov: 24974 -> 24974
2929
Cov: 24974 -> 24974
Cov: 24974 -> 24974
2930
{"exception": "RuntimeError", "msg": "Expected nested_tensorlist[0].size() > 0 to be true, but got false.  (Could this error message be improved?  If so, please report an enhancement request to PyTorch.)"}
2931
Cov: 24974 -> 24975
Cov: 24975 -> 24975
2932
Cov: 24975 -> 24976
Cov: 24976 -> 24976
2933
Cov: 24976 -> 24980
Cov: 24980 -> 24980
2934
Cov: 24980 -> 24980
Cov: 24980 -> 24980
2935
Cov: 24980 -> 24980
Cov: 24980 -> 24980
2936
Cov: 24980 -> 24980
Cov: 24980 -> 24980
2937
Cov: 24980 -> 24980
Cov: 24980 -> 24980
2938
Cov: 24980 -> 24980
Cov: 24980 -> 24980
2939
Cov: 24980 -> 24980
Cov: 24980 -> 24980
2940
Cov: 24980 -> 24980
Cov: 24980 -> 24980
2941
{"exception": "RuntimeError", "msg": "Index tensor must have the same number of dimensions as self tensor"}
2942
Cov: 24980 -> 24980
Cov: 24980 -> 24980
2943
Cov: 24980 -> 24980
Cov: 24980 -> 24980
2944
Cov: 24980 -> 24980
Cov: 24980 -> 24980
2945
Cov: 24980 -> 24980
Cov: 24980 -> 24980
2946
Cov: 24980 -> 24980
Cov: 24980 -> 24980
2947
Cov: 24980 -> 24980
Cov: 24980 -> 24980
2948
Cov: 24980 -> 24980
Cov: 24980 -> 24980
2949
{"exception": "RuntimeError", "msg": "vector + matrix @ vector expected, got 3, 2, 1"}
2950
Cov: 24980 -> 24980
Cov: 24980 -> 24980
2951
Cov: 24980 -> 24980
Cov: 24980 -> 24980
2952
Cov: 24980 -> 24980
Cov: 24980 -> 24980
2953
Cov: 24980 -> 24980
Cov: 24980 -> 24980
2954
Cov: 24980 -> 24980
Cov: 24980 -> 24980
2955
Cov: 24980 -> 24982
Cov: 24982 -> 24982
2956
Cov: 24982 -> 24982
Cov: 24982 -> 24982
2957
Cov: 24982 -> 24983
Cov: 24983 -> 24983
2958
Cov: 24983 -> 24984
Cov: 24984 -> 24984
2959
Cov: 24984 -> 24984
Cov: 24984 -> 24984
2960
Cov: 24984 -> 24985
Cov: 24985 -> 24985
2961
Cov: 24985 -> 24992
Cov: 24992 -> 24992
2962
Cov: 24992 -> 24992
Cov: 24992 -> 24992
2963
Cov: 24992 -> 24992
Cov: 24992 -> 24992
2964
Cov: 24992 -> 24992
Cov: 24992 -> 24992
2965
Cov: 24992 -> 24992
Cov: 24992 -> 24992
2966
Cov: 24992 -> 24993
Cov: 24993 -> 24993
2967
Cov: 24993 -> 24993
Cov: 24993 -> 24993
2968
Cov: 24993 -> 24993
Cov: 24993 -> 24993
2969
Cov: 24993 -> 24993
Cov: 24993 -> 24993
2970
Cov: 24993 -> 24993
Cov: 24993 -> 24993
2971
Cov: 24993 -> 24993
Cov: 24993 -> 24993
2972
Cov: 24993 -> 24993
Cov: 24993 -> 24993
2973
Cov: 24993 -> 24993
Cov: 24993 -> 24993
2974
Cov: 24993 -> 24993
Cov: 24993 -> 24993
2975
Cov: 24993 -> 24993
Cov: 24993 -> 24993
2976
Cov: 24993 -> 24993
Cov: 24993 -> 24993
2977
Cov: 24993 -> 24993
Cov: 24993 -> 24993
2978
Cov: 24993 -> 24994
Cov: 24994 -> 24994
2979
Cov: 24994 -> 24994
Cov: 24994 -> 24994
2980
Cov: 24994 -> 24994
Cov: 24994 -> 24994
2981
Cov: 24994 -> 24994
Cov: 24994 -> 24994
2982
Cov: 24994 -> 24994
Cov: 24994 -> 24994
2983
Cov: 24994 -> 24994
Cov: 24994 -> 24994
2984
Cov: 24994 -> 24994
Cov: 24994 -> 24994
2985
Cov: 24994 -> 24996
Cov: 24996 -> 24996
2986
Cov: 24996 -> 24996
Cov: 24996 -> 24996
2987
Cov: 24996 -> 24996
Cov: 24996 -> 24996
2988
Cov: 24996 -> 24996
Cov: 24996 -> 24996
2989
Cov: 24996 -> 24996
Cov: 24996 -> 24996
2990
Cov: 24996 -> 25002
Cov: 25002 -> 25002
2991
Cov: 25002 -> 25003
Cov: 25003 -> 25003
2992
Cov: 25003 -> 25003
Cov: 25003 -> 25003
2993
{"exception": "NameError", "msg": "name 'math' is not defined"}
2994
{"exception": "RuntimeError", "msg": "torch.dsplit requires a tensor with at least 3 dimension, but got a tensor with 2 dimensions!"}
2995
Cov: 25003 -> 25004
Cov: 25004 -> 25004
2996
Cov: 25004 -> 25004
Cov: 25004 -> 25004
2997
Cov: 25004 -> 25004
Cov: 25004 -> 25004
2998
{"exception": "RuntimeError", "msg": "inner() the last dimension must match on both input tensors but got shapes [4, 3] and [3, 4]"}
2999
Cov: 25004 -> 25004
Cov: 25004 -> 25004
3000
Cov: 25004 -> 25005
Cov: 25005 -> 25005
3001
Cov: 25005 -> 25005
Cov: 25005 -> 25005
3002
Cov: 25005 -> 25006
Cov: 25006 -> 25006
3003
Cov: 25006 -> 25006
Cov: 25006 -> 25006
3004
Cov: 25006 -> 25006
Cov: 25006 -> 25006
3005
Cov: 25006 -> 25006
Cov: 25006 -> 25006
3006
Cov: 25006 -> 25007
Cov: 25007 -> 25007
3007
{"exception": "RuntimeError", "msg": "any: You passed a dimname (string) to this op in place of a dimension index but it does not yet support this behavior. Please pass a dimension index to work around this."}
3008
Cov: 25007 -> 25008
Cov: 25008 -> 25008
3009
Cov: 25008 -> 25009
Cov: 25009 -> 25009
3010
Cov: 25009 -> 25009
Cov: 25009 -> 25009
3011
{"exception": "TypeError", "msg": "descriptor 'int' for 'torch._C._TensorBase' objects doesn't apply to a 'list' object"}
3012
Cov: 25009 -> 25009
Cov: 25009 -> 25009
3013
Cov: 25009 -> 25009
Cov: 25009 -> 25009
3014
Cov: 25009 -> 25009
Cov: 25009 -> 25009
3015
Cov: 25009 -> 25009
Cov: 25009 -> 25009
3016
{"exception": "TypeError", "msg": "arctanh_() takes no arguments (1 given)"}
3017
Cov: 25009 -> 25009
Cov: 25009 -> 25009
3018
Cov: 25009 -> 25010
Cov: 25010 -> 25010
3019
Cov: 25010 -> 25010
Cov: 25010 -> 25010
3020
Cov: 25010 -> 25014
Cov: 25014 -> 25014
3021
Cov: 25014 -> 25014
Cov: 25014 -> 25014
3022
Cov: 25014 -> 25014
Cov: 25014 -> 25014
3023
Cov: 25014 -> 25014
Cov: 25014 -> 25014
3024
Cov: 25014 -> 25014
Cov: 25014 -> 25014
3025
Cov: 25014 -> 25014
Cov: 25014 -> 25014
3026
Cov: 25014 -> 25014
Cov: 25014 -> 25014
3027
Cov: 25014 -> 25014
Cov: 25014 -> 25014
3028
Cov: 25014 -> 25014
Cov: 25014 -> 25014
3029
Cov: 25014 -> 25014
Cov: 25014 -> 25014
3030
Cov: 25014 -> 25014
Cov: 25014 -> 25014
3031
{"exception": "RuntimeError", "msg": "This function was deprecated since version 1.9 and is now removed. `torch.lstsq` is deprecated in favor of `torch.linalg.lstsq`.\n`torch.linalg.lstsq` has reversed arguments and does not return the QR decomposition in the returned tuple (although it returns other information about the problem).\n\nTo get the QR decomposition consider using `torch.linalg.qr`.\n\nThe returned solution in `torch.lstsq` stored the residuals of the solution in the last m - n columns of the returned value whenever m > n. In torch.linalg.lstsq, the residuals are in the field 'residuals' of the returned named tuple.\n\nThe unpacking of the solution, as in\nX, _ = torch.lstsq(B, A).solution[:A.size(1)]\nshould be replaced with:\nX = torch.linalg.lstsq(A, B).solution"}
3032
Cov: 25014 -> 25015
Cov: 25015 -> 25015
3033
Cov: 25015 -> 25017
Cov: 25017 -> 25017
3034
Cov: 25017 -> 25017
Cov: 25017 -> 25017
3035
Cov: 25017 -> 25017
Cov: 25017 -> 25017
3036
Cov: 25017 -> 25017
Cov: 25017 -> 25017
3037
Cov: 25017 -> 25017
Cov: 25017 -> 25017
3038
Cov: 25017 -> 25017
Cov: 25017 -> 25017
3039
Cov: 25017 -> 25017
Cov: 25017 -> 25017
3040
{"exception": "RuntimeError", "msg": "vector + matrix @ vector expected, got 2, 2, 1"}
3041
Cov: 25017 -> 25017
Cov: 25017 -> 25017
3042
Cov: 25017 -> 25017
Cov: 25017 -> 25017
3043
Cov: 25017 -> 25017
Cov: 25017 -> 25017
3044
Cov: 25017 -> 25017
Cov: 25017 -> 25017
3045
Cov: 25017 -> 25017
Cov: 25017 -> 25017
3046
Cov: 25017 -> 25017
Cov: 25017 -> 25017
3047
Cov: 25017 -> 25017
Cov: 25017 -> 25017
3048
Cov: 25017 -> 25017
Cov: 25017 -> 25017
3049
Cov: 25017 -> 25017
Cov: 25017 -> 25017
3050
Cov: 25017 -> 25017
Cov: 25017 -> 25017
3051
Cov: 25017 -> 25017
Cov: 25017 -> 25017
3052
Cov: 25017 -> 25017
Cov: 25017 -> 25017
3053
Cov: 25017 -> 25017
Cov: 25017 -> 25017
3054
Cov: 25017 -> 25017
Cov: 25017 -> 25017
3055
Cov: 25017 -> 25017
Cov: 25017 -> 25017
3056
Cov: 25017 -> 25017
Cov: 25017 -> 25017
3057
Cov: 25017 -> 25017
Cov: 25017 -> 25017
3058
Cov: 25017 -> 25018
Cov: 25018 -> 25018
3059
Cov: 25018 -> 25018
Cov: 25018 -> 25018
3060
Cov: 25018 -> 25018
Cov: 25018 -> 25018
3061
Cov: 25018 -> 25019
Cov: 25019 -> 25019
3062
Cov: 25019 -> 25019
Cov: 25019 -> 25019
3063
Cov: 25019 -> 25019
Cov: 25019 -> 25019
3064
Cov: 25019 -> 25019
Cov: 25019 -> 25019
3065
Cov: 25019 -> 25020
Cov: 25020 -> 25020
3066
Cov: 25020 -> 25020
Cov: 25020 -> 25020
3067
Cov: 25020 -> 25020
Cov: 25020 -> 25020
3068
Cov: 25020 -> 25020
Cov: 25020 -> 25020
3069
Cov: 25020 -> 25020
Cov: 25020 -> 25020
3070
Cov: 25020 -> 25020
Cov: 25020 -> 25020
3071
Cov: 25020 -> 25020
Cov: 25020 -> 25020
3072
Cov: 25020 -> 25020
Cov: 25020 -> 25020
3073
Cov: 25020 -> 25020
Cov: 25020 -> 25020
3074
Cov: 25020 -> 25020
Cov: 25020 -> 25020
3075
{"exception": "TypeError", "msg": "'getset_descriptor' object is not callable"}
3076
Cov: 25020 -> 25020
Cov: 25020 -> 25020
3077
Cov: 25020 -> 25020
Cov: 25020 -> 25020
3078
Cov: 25020 -> 25020
Cov: 25020 -> 25020
3079
Cov: 25020 -> 25020
Cov: 25020 -> 25020
3080
Cov: 25020 -> 25020
Cov: 25020 -> 25020
3081
Cov: 25020 -> 25020
Cov: 25020 -> 25020
3082
Cov: 25020 -> 25020
Cov: 25020 -> 25020
3083
Cov: 25020 -> 25020
Cov: 25020 -> 25020
3084
{"exception": "TypeError", "msg": "arctanh_() takes no arguments (1 given)"}
3085
Cov: 25020 -> 25020
Cov: 25020 -> 25020
3086
Cov: 25020 -> 25020
Cov: 25020 -> 25020
3087
Cov: 25020 -> 25020
Cov: 25020 -> 25020
3088
{"exception": "TypeError", "msg": "'getset_descriptor' object is not callable"}
3089
Cov: 25020 -> 25020
Cov: 25020 -> 25020
3090
Cov: 25020 -> 25021
Cov: 25021 -> 25021
3091
Cov: 25021 -> 25021
Cov: 25021 -> 25021
3092
Cov: 25021 -> 25021
Cov: 25021 -> 25021
3093
Cov: 25021 -> 25022
Cov: 25022 -> 25022
3094
Cov: 25022 -> 25022
Cov: 25022 -> 25022
3095
Cov: 25022 -> 25022
Cov: 25022 -> 25022
3096
Cov: 25022 -> 25022
Cov: 25022 -> 25022
3097
Cov: 25022 -> 25022
Cov: 25022 -> 25022
3098
Cov: 25022 -> 25022
Cov: 25022 -> 25022
3099
Cov: 25022 -> 25023
Cov: 25023 -> 25023
3100
Cov: 25023 -> 25024
Cov: 25024 -> 25024
3101
Cov: 25024 -> 25024
Cov: 25024 -> 25024
3102
Cov: 25024 -> 25024
Cov: 25024 -> 25024
3103
Cov: 25024 -> 25024
Cov: 25024 -> 25024
3104
Cov: 25024 -> 25024
Cov: 25024 -> 25024
3105
Cov: 25024 -> 25024
Cov: 25024 -> 25024
3106
Cov: 25024 -> 25025
Cov: 25025 -> 25025
3107
Cov: 25025 -> 25025
Cov: 25025 -> 25025
3108
Cov: 25025 -> 25026
Cov: 25026 -> 25026
3109
Cov: 25026 -> 25027
Cov: 25027 -> 25027
3110
Cov: 25027 -> 25027
Cov: 25027 -> 25027
3111
Cov: 25027 -> 25027
Cov: 25027 -> 25027
3112
Cov: 25027 -> 25027
Cov: 25027 -> 25027
3113
Cov: 25027 -> 25027
Cov: 25027 -> 25027
3114
Cov: 25027 -> 25055
Cov: 25055 -> 25055
3115
{"exception": "RuntimeError", "msg": "Please look up dimensions by name, got: name = None."}
3116
{"exception": "AttributeError", "msg": "module 'torch.utils' has no attribute 'model_zoo'"}
3117
Cov: 25055 -> 25056
Cov: 25056 -> 25056
3118
Cov: 25056 -> 25056
Cov: 25056 -> 25056
3119
Cov: 25056 -> 25056
Cov: 25056 -> 25056
3120
Cov: 25056 -> 25057
Cov: 25057 -> 25057
3121
Cov: 25057 -> 25059
Cov: 25059 -> 25059
3122
Cov: 25059 -> 25059
Cov: 25059 -> 25059
3123
Cov: 25059 -> 25060
Cov: 25060 -> 25060
3124
Cov: 25060 -> 25060
Cov: 25060 -> 25060
3125
{"exception": "RuntimeError", "msg": "Please look up dimensions by name, got: name = None."}
3126
Cov: 25060 -> 25061
Cov: 25061 -> 25061
3127
Cov: 25061 -> 25061
Cov: 25061 -> 25061
3128
Cov: 25061 -> 25061
Cov: 25061 -> 25061
3129
Cov: 25061 -> 25061
Cov: 25061 -> 25061
3130
Cov: 25061 -> 25061
Cov: 25061 -> 25061
3131
Cov: 25061 -> 25061
Cov: 25061 -> 25061
3132
Cov: 25061 -> 25062
Cov: 25062 -> 25062
3133
Cov: 25062 -> 25063
Cov: 25063 -> 25063
3134
Cov: 25063 -> 25063
Cov: 25063 -> 25063
3135
Cov: 25063 -> 25064
Cov: 25064 -> 25064
3136
{"exception": "RuntimeError", "msg": "Cauchy distribution is a continuous probability distribution. dtype must be a floating point but you specified long int"}
3137
Cov: 25064 -> 25064
Cov: 25064 -> 25064
3138
{"exception": "RuntimeError", "msg": "The size of tensor a (3) must match the size of tensor b (2) at non-singleton dimension 1"}
3139
Cov: 25064 -> 25064
Cov: 25064 -> 25064
3140
Cov: 25064 -> 25064
Cov: 25064 -> 25064
3141
Cov: 25064 -> 25064
Cov: 25064 -> 25064
3142
Cov: 25064 -> 25065
Cov: 25065 -> 25065
3143
Cov: 25065 -> 25066
Cov: 25066 -> 25066
3144
Cov: 25066 -> 25066
Cov: 25066 -> 25066
3145
Cov: 25066 -> 25066
Cov: 25066 -> 25066
3146
Cov: 25066 -> 25066
Cov: 25066 -> 25066
3147
Cov: 25066 -> 25067
Cov: 25067 -> 25067
3148
Cov: 25067 -> 25067
Cov: 25067 -> 25067
3149
Cov: 25067 -> 25067
Cov: 25067 -> 25067
3150
Cov: 25067 -> 25067
Cov: 25067 -> 25067
3151
{"exception": "RuntimeError", "msg": "expand(torch.FloatTensor{[2, 2, 3, 4, 5]}, size=[2, 2, 3]): the number of sizes provided (3) must be greater or equal to the number of dimensions in the tensor (5)"}
3152
{"exception": "TypeError", "msg": "msort() takes no keyword arguments"}
3153
Cov: 25067 -> 25068
Cov: 25068 -> 25068
3154
Cov: 25068 -> 25072
Cov: 25072 -> 25072
3155
Cov: 25072 -> 25073
Cov: 25073 -> 25073
3156
Cov: 25073 -> 25073
Cov: 25073 -> 25073
3157
Cov: 25073 -> 25073
Cov: 25073 -> 25073
3158
{"exception": "TypeError", "msg": "descriptor 'exponential_' for 'torch._C._TensorBase' objects doesn't apply to a 'numpy.ndarray' object"}
3159
Cov: 25073 -> 25073
Cov: 25073 -> 25073
3160
Cov: 25073 -> 25073
Cov: 25073 -> 25073
3161
Cov: 25073 -> 25074
Cov: 25074 -> 25074
3162
Cov: 25074 -> 25074
Cov: 25074 -> 25074
3163
Cov: 25074 -> 25075
Cov: 25075 -> 25075
3164
Cov: 25075 -> 25076
Cov: 25076 -> 25076
3165
Cov: 25076 -> 25080
Cov: 25080 -> 25080
3166
Cov: 25080 -> 25080
Cov: 25080 -> 25080
3167
{"exception": "TypeError", "msg": "'getset_descriptor' object is not callable"}
3168
Cov: 25080 -> 25080
Cov: 25080 -> 25080
3169
Cov: 25080 -> 25080
Cov: 25080 -> 25080
3170
Cov: 25080 -> 25080
Cov: 25080 -> 25080
3171
Cov: 25080 -> 25090
Cov: 25090 -> 25090
3172
{"exception": "TypeError", "msg": "uniform_() got an unexpected keyword argument 'from_'"}
3173
Cov: 25090 -> 25090
Cov: 25090 -> 25090
3174
Cov: 25090 -> 25090
Cov: 25090 -> 25090
3175
Cov: 25090 -> 25090
Cov: 25090 -> 25090
3176
{"exception": "IndexError", "msg": "Dimension out of range (expected to be in range of [-1, 0], but got 1)"}
3177
Cov: 25090 -> 25091
Cov: 25091 -> 25091
3178
Cov: 25091 -> 25091
Cov: 25091 -> 25091
3179
Cov: 25091 -> 25092
Cov: 25092 -> 25092
3180
Cov: 25092 -> 25092
Cov: 25092 -> 25092
3181
Cov: 25092 -> 25092
Cov: 25092 -> 25092
3182
Cov: 25092 -> 25092
Cov: 25092 -> 25092
3183
Cov: 25092 -> 25092
Cov: 25092 -> 25092
3184
Cov: 25092 -> 25092
Cov: 25092 -> 25092
3185
Cov: 25092 -> 25092
Cov: 25092 -> 25092
3186
Cov: 25092 -> 25092
Cov: 25092 -> 25092
3187
Cov: 25092 -> 25095
Cov: 25095 -> 25095
3188
Cov: 25095 -> 25095
Cov: 25095 -> 25095
3189
Cov: 25095 -> 25095
Cov: 25095 -> 25095
3190
Cov: 25095 -> 25095
Cov: 25095 -> 25095
3191
Cov: 25095 -> 25095
Cov: 25095 -> 25095
3192
{"exception": "TypeError", "msg": "copy_() missing 1 required positional arguments: \"other\""}
3193
Cov: 25095 -> 25095
Cov: 25095 -> 25095
3194
Cov: 25095 -> 25096
Cov: 25096 -> 25096
3195
Cov: 25096 -> 25097
Cov: 25097 -> 25097
3196
Cov: 25097 -> 25098
Cov: 25098 -> 25098
3197
Cov: 25098 -> 25099
Cov: 25099 -> 25099
3198
Cov: 25099 -> 25099
Cov: 25099 -> 25099
3199
Cov: 25099 -> 25099
Cov: 25099 -> 25099
3200
Cov: 25099 -> 25100
Cov: 25100 -> 25100
3201
Cov: 25100 -> 25100
Cov: 25100 -> 25100
3202
Cov: 25100 -> 25100
Cov: 25100 -> 25100
3203
Cov: 25100 -> 25100
Cov: 25100 -> 25100
3204
Cov: 25100 -> 25101
Cov: 25101 -> 25101
3205
Cov: 25101 -> 25102
Cov: 25102 -> 25102
3206
Cov: 25102 -> 25102
Cov: 25102 -> 25102
3207
Cov: 25102 -> 25102
Cov: 25102 -> 25102
3208
{"exception": "RuntimeError", "msg": "\"gcd_cpu\" not implemented for 'Float'"}
3209
Cov: 25102 -> 25102
Cov: 25102 -> 25102
3210
Cov: 25102 -> 25102
Cov: 25102 -> 25102
3211
Cov: 25102 -> 25102
Cov: 25102 -> 25102
3212
Cov: 25102 -> 25102
Cov: 25102 -> 25102
3213
Cov: 25102 -> 25102
Cov: 25102 -> 25102
3214
Cov: 25102 -> 25108
Cov: 25108 -> 25108
3215
Cov: 25108 -> 25108
Cov: 25108 -> 25108
3216
Cov: 25108 -> 25108
Cov: 25108 -> 25108
3217
Cov: 25108 -> 25108
Cov: 25108 -> 25108
3218
Cov: 25108 -> 25108
Cov: 25108 -> 25108
3219
Cov: 25108 -> 25108
Cov: 25108 -> 25108
3220
Cov: 25108 -> 25108
Cov: 25108 -> 25108
3221
{"exception": "TypeError", "msg": "dequantize() takes no keyword arguments"}
3222
Cov: 25108 -> 25108
Cov: 25108 -> 25108
3223
Cov: 25108 -> 25108
Cov: 25108 -> 25108
3224
Cov: 25108 -> 25109
Cov: 25109 -> 25109
3225
Cov: 25109 -> 25110
Cov: 25110 -> 25110
3226
Cov: 25110 -> 25110
Cov: 25110 -> 25110
3227
Cov: 25110 -> 25111
Cov: 25111 -> 25111
3228
Cov: 25111 -> 25111
Cov: 25111 -> 25111
3229
Cov: 25111 -> 25111
Cov: 25111 -> 25111
3230
Cov: 25111 -> 25111
Cov: 25111 -> 25111
3231
Cov: 25111 -> 25111
Cov: 25111 -> 25111
3232
Cov: 25111 -> 25112
Cov: 25112 -> 25112
3233
Cov: 25112 -> 25112
Cov: 25112 -> 25112
3234
Cov: 25112 -> 25112
Cov: 25112 -> 25112
3235
{"exception": "TypeError", "msg": "baddbmm() missing 1 required positional arguments: \"batch2\""}
3236
{"exception": "TypeError", "msg": "map_(): argument 'other' (position 1) must be Tensor, not function"}
3237
Cov: 25112 -> 25112
Cov: 25112 -> 25112
3238
Cov: 25112 -> 25114
Cov: 25114 -> 25114
3239
Cov: 25114 -> 25114
Cov: 25114 -> 25114
3240
Cov: 25114 -> 25115
Cov: 25115 -> 25115
3241
Cov: 25115 -> 25115
Cov: 25115 -> 25115
3242
Cov: 25115 -> 25115
Cov: 25115 -> 25115
3243
Cov: 25115 -> 25115
Cov: 25115 -> 25115
3244
Cov: 25115 -> 25115
Cov: 25115 -> 25115
3245
Cov: 25115 -> 25115
Cov: 25115 -> 25115
3246
Cov: 25115 -> 25115
Cov: 25115 -> 25115
3247
Cov: 25115 -> 25115
Cov: 25115 -> 25115
3248
Cov: 25115 -> 25115
Cov: 25115 -> 25115
3249
Cov: 25115 -> 25115
Cov: 25115 -> 25115
3250
Cov: 25115 -> 25116
Cov: 25116 -> 25116
3251
Cov: 25116 -> 25117
Cov: 25117 -> 25117
3252
Cov: 25117 -> 25117
Cov: 25117 -> 25117
3253
Cov: 25117 -> 25118
Cov: 25118 -> 25118
3254
Cov: 25118 -> 25118
Cov: 25118 -> 25118
3255
Cov: 25118 -> 25118
Cov: 25118 -> 25118
3256
Cov: 25118 -> 25119
Cov: 25119 -> 25119
3257
Cov: 25119 -> 25119
Cov: 25119 -> 25119
3258
Cov: 25119 -> 25119
Cov: 25119 -> 25119
3259
Cov: 25119 -> 25128
Cov: 25128 -> 25128
3260
Cov: 25128 -> 25128
Cov: 25128 -> 25128
3261
Cov: 25128 -> 25255
Cov: 25255 -> 25255
3262
{"exception": "TypeError", "msg": "'getset_descriptor' object is not callable"}
3263
Cov: 25255 -> 25260
Cov: 25260 -> 25260
3264
Cov: 25260 -> 25260
Cov: 25260 -> 25260
3265
Cov: 25260 -> 25261
Cov: 25261 -> 25261
3266
Cov: 25261 -> 25261
Cov: 25261 -> 25261
3267
{"exception": "NotImplementedError", "msg": "Could not run 'aten::sparse_mask' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::sparse_mask' is only available for these backends: [SparseCPU, SparseCUDA, SparseCsrCPU, SparseCsrCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nSparseCPU: registered at aten/src/ATen/RegisterSparseCPU.cpp:1387 [kernel]\nSparseCUDA: registered at aten/src/ATen/RegisterSparseCUDA.cpp:1573 [kernel]\nSparseCsrCPU: registered at aten/src/ATen/RegisterSparseCsrCPU.cpp:1135 [kernel]\nSparseCsrCUDA: registered at aten/src/ATen/RegisterSparseCsrCUDA.cpp:1276 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:153 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:498 [backend fallback]\nFunctionalize: registered at ../aten/src/ATen/FunctionalizeFallbackKernel.cpp:290 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:86 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradMTIA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_0.cpp:16725 [kernel]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:382 [backend fallback]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:249 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:710 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:203 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:161 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:494 [backend fallback]\nPreDispatch: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:165 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:157 [backend fallback]\n"}
3268
Cov: 25261 -> 25262
Cov: 25262 -> 25262
3269
Cov: 25262 -> 25262
Cov: 25262 -> 25262
3270
Cov: 25262 -> 25262
Cov: 25262 -> 25262
3271
Cov: 25262 -> 25262
Cov: 25262 -> 25262
3272
Cov: 25262 -> 25262
Cov: 25262 -> 25262
3273
Cov: 25262 -> 25262
Cov: 25262 -> 25262
3274
Cov: 25262 -> 25262
Cov: 25262 -> 25262
3275
Cov: 25262 -> 25264
Cov: 25264 -> 25264
3276
Cov: 25264 -> 25264
Cov: 25264 -> 25264
3277
Cov: 25264 -> 25264
Cov: 25264 -> 25264
3278
Cov: 25264 -> 25264
Cov: 25264 -> 25264
3279
Cov: 25264 -> 25264
Cov: 25264 -> 25264
3280
{"exception": "TypeError", "msg": "roll(): argument 'shifts' (position 1) must be tuple of ints, not Tensor"}
3281
Cov: 25264 -> 25264
Cov: 25264 -> 25264
3282
Cov: 25264 -> 25264
Cov: 25264 -> 25264
3283
Cov: 25264 -> 25264
Cov: 25264 -> 25264
3284
Cov: 25264 -> 25518
Cov: 25518 -> 25518
3285
Cov: 25518 -> 25518
Cov: 25518 -> 25518
3286
Cov: 25518 -> 25518
Cov: 25518 -> 25518
3287
Cov: 25518 -> 25518
Cov: 25518 -> 25518
3288
Cov: 25518 -> 25518
Cov: 25518 -> 25518
3289
Cov: 25518 -> 25518
Cov: 25518 -> 25518
3290
Cov: 25518 -> 25518
Cov: 25518 -> 25518
3291
Cov: 25518 -> 25518
Cov: 25518 -> 25518
3292
Cov: 25518 -> 25518
Cov: 25518 -> 25518
3293
Cov: 25518 -> 25518
Cov: 25518 -> 25518
3294
Cov: 25518 -> 25518
Cov: 25518 -> 25518
3295
Cov: 25518 -> 25518
Cov: 25518 -> 25518
3296
Cov: 25518 -> 25519
Cov: 25519 -> 25519
3297
Cov: 25519 -> 25519
Cov: 25519 -> 25519
3298
Cov: 25519 -> 25519
Cov: 25519 -> 25519
3299
Cov: 25519 -> 25519
Cov: 25519 -> 25519
3300
Cov: 25519 -> 25519
Cov: 25519 -> 25519
3301
Cov: 25519 -> 25519
Cov: 25519 -> 25519
3302
Cov: 25519 -> 25519
Cov: 25519 -> 25519
3303
Cov: 25519 -> 25525
Cov: 25525 -> 25525
3304
Cov: 25525 -> 25526
Cov: 25526 -> 25526
3305
Cov: 25526 -> 25526
Cov: 25526 -> 25526
3306
Cov: 25526 -> 25526
Cov: 25526 -> 25526
3307
{"exception": "NameError", "msg": "name 'torchvision' is not defined"}
3308
Cov: 25526 -> 25540
Cov: 25540 -> 25540
3309
{"exception": "RuntimeError", "msg": "1D tensors expected, but got 2D and 2D tensors"}
3310
Cov: 25540 -> 25540
Cov: 25540 -> 25540
3311
Cov: 25540 -> 25541
Cov: 25541 -> 25541
3312
Cov: 25541 -> 25541
Cov: 25541 -> 25541
3313
Cov: 25541 -> 25541
Cov: 25541 -> 25541
3314
Cov: 25541 -> 25541
Cov: 25541 -> 25541
3315
Cov: 25541 -> 25541
Cov: 25541 -> 25541
3316
Cov: 25541 -> 25541
Cov: 25541 -> 25541
3317
{"exception": "RuntimeError", "msg": "The expanded size of the tensor (3) must match the existing size (4) at non-singleton dimension 1.  Target sizes: [4, 3].  Tensor sizes: [3, 4]"}
3318
Cov: 25541 -> 25541
Cov: 25541 -> 25541
3319
Cov: 25541 -> 25541
Cov: 25541 -> 25541
3320
Cov: 25541 -> 25541
Cov: 25541 -> 25541
3321
Cov: 25541 -> 25542
Cov: 25542 -> 25542
3322
Cov: 25542 -> 25542
Cov: 25542 -> 25542
3323
{"exception": "RuntimeError", "msg": "expand(torch.FloatTensor{[2, 3, 4]}, size=[2, 4]): the number of sizes provided (2) must be greater or equal to the number of dimensions in the tensor (3)"}
3324
Cov: 25542 -> 25544
Cov: 25544 -> 25544
3325
Cov: 25544 -> 25544
Cov: 25544 -> 25544
3326
Cov: 25544 -> 25544
Cov: 25544 -> 25544
3327
Cov: 25544 -> 25544
Cov: 25544 -> 25544
3328
Cov: 25544 -> 25544
Cov: 25544 -> 25544
3329
Cov: 25544 -> 25545
Cov: 25545 -> 25545
3330
Cov: 25545 -> 25546
Cov: 25546 -> 25546
3331
Cov: 25546 -> 25546
Cov: 25546 -> 25546
3332
Cov: 25546 -> 25546
Cov: 25546 -> 25546
3333
Cov: 25546 -> 25546
Cov: 25546 -> 25546
3334
Cov: 25546 -> 25546
Cov: 25546 -> 25546
3335
Cov: 25546 -> 25546
Cov: 25546 -> 25546
3336
Cov: 25546 -> 25546
Cov: 25546 -> 25546
3337
Cov: 25546 -> 25550
Cov: 25550 -> 25550
3338
Cov: 25550 -> 25550
Cov: 25550 -> 25550
3339
Cov: 25550 -> 25550
Cov: 25550 -> 25550
3340
Cov: 25550 -> 25550
Cov: 25550 -> 25550
3341
Cov: 25550 -> 25550
Cov: 25550 -> 25550
3342
Cov: 25550 -> 25550
Cov: 25550 -> 25550
3343
{"exception": "RuntimeError", "msg": "\"lshift_cpu\" not implemented for 'Float'"}
3344
Cov: 25550 -> 25551
Cov: 25551 -> 25551
3345
Cov: 25551 -> 25551
Cov: 25551 -> 25551
3346
Cov: 25551 -> 25551
Cov: 25551 -> 25551
3347
Cov: 25551 -> 25551
Cov: 25551 -> 25551
3348
Cov: 25551 -> 25556
Cov: 25556 -> 25556
3349
Cov: 25556 -> 25556
Cov: 25556 -> 25556
3350
Cov: 25556 -> 25556
Cov: 25556 -> 25556
3351
Cov: 25556 -> 25556
Cov: 25556 -> 25556
3352
Cov: 25556 -> 25556
Cov: 25556 -> 25556
3353
{"exception": "RuntimeError", "msg": "Expected nested_tensorlist[0].size() > 0 to be true, but got false.  (Could this error message be improved?  If so, please report an enhancement request to PyTorch.)"}
3354
Cov: 25556 -> 25557
Cov: 25557 -> 25557
3355
Cov: 25557 -> 25557
Cov: 25557 -> 25557
3356
Cov: 25557 -> 25557
Cov: 25557 -> 25557
3357
Cov: 25557 -> 25557
Cov: 25557 -> 25557
3358
{"exception": "TypeError", "msg": "baddbmm() missing 1 required positional arguments: \"batch2\""}
3359
Cov: 25557 -> 25557
Cov: 25557 -> 25557
3360
{"exception": "RuntimeError", "msg": "This function was deprecated since version 1.9 and is now removed. `torch.linalg.eig` returns complex tensors of dtype `cfloat` or `cdouble` rather than real tensors mimicking complex tensors.\n\nL, _ = torch.eig(A) should be replaced with:\nL_complex = torch.linalg.eigvals(A)\n\nand\n\nL, V = torch.eig(A, eigenvectors=True) should be replaced with:\nL_complex, V_complex = torch.linalg.eig(A)"}
3361
Cov: 25557 -> 25558
Cov: 25558 -> 25558
3362
Cov: 25558 -> 25558
Cov: 25558 -> 25558
3363
Cov: 25558 -> 25558
Cov: 25558 -> 25558
3364
Cov: 25558 -> 25558
Cov: 25558 -> 25558
3365
Cov: 25558 -> 25558
Cov: 25558 -> 25558
3366
Cov: 25558 -> 25559
Cov: 25559 -> 25559
3367
Cov: 25559 -> 25559
Cov: 25559 -> 25559
3368
Cov: 25559 -> 25559
Cov: 25559 -> 25559
3369
Cov: 25559 -> 25559
Cov: 25559 -> 25559
3370
Cov: 25559 -> 25559
Cov: 25559 -> 25559
3371
Cov: 25559 -> 25560
Cov: 25560 -> 25560
3372
Cov: 25560 -> 25560
Cov: 25560 -> 25560
3373
Cov: 25560 -> 25561
Cov: 25561 -> 25561
3374
Cov: 25561 -> 25561
Cov: 25561 -> 25561
3375
Cov: 25561 -> 25561
Cov: 25561 -> 25561
3376
Cov: 25561 -> 25561
Cov: 25561 -> 25561
3377
Cov: 25561 -> 25561
Cov: 25561 -> 25561
3378
Cov: 25561 -> 25561
Cov: 25561 -> 25561
3379
{"exception": "RuntimeError", "msg": "linalg.cross: inputs dimension -1 must have length 3. Got 4 and 4"}
3380
Cov: 25561 -> 25561
Cov: 25561 -> 25561
3381
Cov: 25561 -> 25561
Cov: 25561 -> 25561
3382
Cov: 25561 -> 25562
Cov: 25562 -> 25562
3383
Cov: 25562 -> 25563
Cov: 25563 -> 25563
3384
Cov: 25563 -> 25564
Cov: 25564 -> 25564
3385
Cov: 25564 -> 25564
Cov: 25564 -> 25564
3386
Cov: 25564 -> 25565
Cov: 25565 -> 25565
3387
Cov: 25565 -> 25565
Cov: 25565 -> 25565
3388
Cov: 25565 -> 25565
Cov: 25565 -> 25565
3389
Cov: 25565 -> 25565
Cov: 25565 -> 25565
3390
Cov: 25565 -> 25566
Cov: 25566 -> 25566
3391
Cov: 25566 -> 25566
Cov: 25566 -> 25566
3392
Cov: 25566 -> 25566
Cov: 25566 -> 25566
3393
Cov: 25566 -> 25566
Cov: 25566 -> 25566
3394
Cov: 25566 -> 25576
Cov: 25576 -> 25576
3395
Cov: 25576 -> 25576
Cov: 25576 -> 25576
3396
Cov: 25576 -> 25576
Cov: 25576 -> 25576
3397
Cov: 25576 -> 25577
Cov: 25577 -> 25577
3398
Cov: 25577 -> 25577
Cov: 25577 -> 25577
3399
Cov: 25577 -> 25577
Cov: 25577 -> 25577
3400
Cov: 25577 -> 25577
Cov: 25577 -> 25577
3401
Cov: 25577 -> 25577
Cov: 25577 -> 25577
3402
Cov: 25577 -> 25577
Cov: 25577 -> 25577
3403
Cov: 25577 -> 25577
Cov: 25577 -> 25577
3404
Cov: 25577 -> 25577
Cov: 25577 -> 25577
3405
Cov: 25577 -> 25577
Cov: 25577 -> 25577
3406
Cov: 25577 -> 25577
Cov: 25577 -> 25577
3407
Cov: 25577 -> 25577
Cov: 25577 -> 25577
3408
Cov: 25577 -> 25577
Cov: 25577 -> 25577
3409
Cov: 25577 -> 25577
Cov: 25577 -> 25577
3410
Cov: 25577 -> 25579
Cov: 25579 -> 25579
3411
Cov: 25579 -> 25579
Cov: 25579 -> 25579
3412
Cov: 25579 -> 25579
Cov: 25579 -> 25579
3413
Cov: 25579 -> 25580
Cov: 25580 -> 25580
3414
Cov: 25580 -> 25622
Cov: 25622 -> 25622
3415
Cov: 25622 -> 25623
Cov: 25623 -> 25623
3416
Cov: 25623 -> 25623
Cov: 25623 -> 25623
3417
Cov: 25623 -> 25624
Cov: 25624 -> 25624
3418
Cov: 25624 -> 25624
Cov: 25624 -> 25624
3419
Cov: 25624 -> 25625
Cov: 25625 -> 25625
3420
Cov: 25625 -> 25625
Cov: 25625 -> 25625
3421
Cov: 25625 -> 25625
Cov: 25625 -> 25625
3422
Cov: 25625 -> 25625
Cov: 25625 -> 25625
3423
Cov: 25625 -> 25625
Cov: 25625 -> 25625
3424
Cov: 25625 -> 25625
Cov: 25625 -> 25625
3425
Cov: 25625 -> 25625
Cov: 25625 -> 25625
3426
Cov: 25625 -> 25626
Cov: 25626 -> 25626
3427
Cov: 25626 -> 25626
Cov: 25626 -> 25626
3428
Cov: 25626 -> 25626
Cov: 25626 -> 25626
3429
Cov: 25626 -> 25628
Cov: 25628 -> 25628
3430
Cov: 25628 -> 25630
Cov: 25630 -> 25630
3431
Cov: 25630 -> 25631
Cov: 25631 -> 25631
3432
Cov: 25631 -> 25631
Cov: 25631 -> 25631
3433
Cov: 25631 -> 25632
Cov: 25632 -> 25632
3434
Cov: 25632 -> 25633
Cov: 25633 -> 25633
3435
{"exception": "RuntimeError", "msg": "required rank 4 tensor to use channels_last format"}
3436
Cov: 25633 -> 25634
Cov: 25634 -> 25634
3437
Cov: 25634 -> 25634
Cov: 25634 -> 25634
3438
Cov: 25634 -> 25636
Cov: 25636 -> 25636
3439
Cov: 25636 -> 25636
Cov: 25636 -> 25636
3440
Cov: 25636 -> 25636
Cov: 25636 -> 25636
3441
Cov: 25636 -> 25642
Cov: 25642 -> 25642
3442
Cov: 25642 -> 25642
Cov: 25642 -> 25642
3443
Cov: 25642 -> 25648
Cov: 25648 -> 25648
3444
Cov: 25648 -> 25648
Cov: 25648 -> 25648
3445
Cov: 25648 -> 25648
Cov: 25648 -> 25648
3446
Cov: 25648 -> 25648
Cov: 25648 -> 25648
3447
Cov: 25648 -> 25649
Cov: 25649 -> 25649
3448
Cov: 25649 -> 25650
Cov: 25650 -> 25650
3449
Cov: 25650 -> 25650
Cov: 25650 -> 25650
3450
Cov: 25650 -> 25651
Cov: 25651 -> 25651
3451
Cov: 25651 -> 25652
Cov: 25652 -> 25652
3452
Cov: 25652 -> 25652
Cov: 25652 -> 25652
3453
Cov: 25652 -> 25652
Cov: 25652 -> 25652
3454
Cov: 25652 -> 25653
Cov: 25653 -> 25653
3455
Cov: 25653 -> 25653
Cov: 25653 -> 25653
3456
Cov: 25653 -> 25654
Cov: 25654 -> 25654
3457
Cov: 25654 -> 25654
Cov: 25654 -> 25654
3458
Cov: 25654 -> 25654
Cov: 25654 -> 25654
3459
Cov: 25654 -> 25654
Cov: 25654 -> 25654
3460
Cov: 25654 -> 25654
Cov: 25654 -> 25654
3461
Cov: 25654 -> 25655
Cov: 25655 -> 25655
3462
Cov: 25655 -> 25655
Cov: 25655 -> 25655
3463
Cov: 25655 -> 25655
Cov: 25655 -> 25655
3464
Cov: 25655 -> 25655
Cov: 25655 -> 25655
3465
Cov: 25655 -> 25663
Cov: 25663 -> 25663
3466
Cov: 25663 -> 25663
Cov: 25663 -> 25663
3467
Cov: 25663 -> 25664
Cov: 25664 -> 25664
3468
Cov: 25664 -> 25664
Cov: 25664 -> 25664
3469
{"exception": "RuntimeError", "msg": "torch.dsplit attempted to split along dimension 2, but the size of the dimension 5 is not divisible by the split_size 2!"}
3470
Cov: 25664 -> 25664
Cov: 25664 -> 25664
3471
Cov: 25664 -> 25664
Cov: 25664 -> 25664
3472
Cov: 25664 -> 25664
Cov: 25664 -> 25664
3473
Cov: 25664 -> 25664
Cov: 25664 -> 25664
3474
Cov: 25664 -> 25664
Cov: 25664 -> 25664
3475
Cov: 25664 -> 25666
Cov: 25666 -> 25666
3476
Cov: 25666 -> 25666
Cov: 25666 -> 25666
3477
Cov: 25666 -> 25666
Cov: 25666 -> 25666
3478
Cov: 25666 -> 25666
Cov: 25666 -> 25666
3479
Cov: 25666 -> 25666
Cov: 25666 -> 25666
3480
{"exception": "RuntimeError", "msg": "torch.linalg.householder_product: input.shape[-1] must be greater than or equal to tau.shape[-1]"}
3481
Cov: 25666 -> 25666
Cov: 25666 -> 25666
3482
Cov: 25666 -> 25666
Cov: 25666 -> 25666
3483
Cov: 25666 -> 25667
Cov: 25667 -> 25667
3484
Cov: 25667 -> 25667
Cov: 25667 -> 25667
3485
Cov: 25667 -> 25667
Cov: 25667 -> 25667
3486
Cov: 25667 -> 25667
Cov: 25667 -> 25667
3487
Cov: 25667 -> 25667
Cov: 25667 -> 25667
3488
Cov: 25667 -> 25669
Cov: 25669 -> 25669
3489
Cov: 25669 -> 25669
Cov: 25669 -> 25669
3490
Cov: 25669 -> 25669
Cov: 25669 -> 25669
3491
Cov: 25669 -> 25670
Cov: 25670 -> 25670
3492
Cov: 25670 -> 25671
Cov: 25671 -> 25671
3493
Cov: 25671 -> 25672
Cov: 25672 -> 25672
3494
Cov: 25672 -> 25672
Cov: 25672 -> 25672
3495
Cov: 25672 -> 25673
Cov: 25673 -> 25673
3496
Cov: 25673 -> 25673
Cov: 25673 -> 25673
3497
Cov: 25673 -> 25673
Cov: 25673 -> 25673
3498
Cov: 25673 -> 25674
Cov: 25674 -> 25674
3499
Cov: 25674 -> 25674
Cov: 25674 -> 25674
3500
Cov: 25674 -> 25674
Cov: 25674 -> 25674
3501
Cov: 25674 -> 25674
Cov: 25674 -> 25674
3502
Cov: 25674 -> 25677
Cov: 25677 -> 25677
3503
Cov: 25677 -> 25677
Cov: 25677 -> 25677
3504
Cov: 25677 -> 25677
Cov: 25677 -> 25677
3505
Cov: 25677 -> 25677
Cov: 25677 -> 25677
3506
Cov: 25677 -> 25677
Cov: 25677 -> 25677
3507
Cov: 25677 -> 25677
Cov: 25677 -> 25677
3508
Cov: 25677 -> 25677
Cov: 25677 -> 25677
3509
Cov: 25677 -> 25677
Cov: 25677 -> 25677
3510
Cov: 25677 -> 25677
Cov: 25677 -> 25677
3511
Cov: 25677 -> 25677
Cov: 25677 -> 25677
3512
Cov: 25677 -> 25678
Cov: 25678 -> 25678
3513
Cov: 25678 -> 25678
Cov: 25678 -> 25678
3514
{"exception": "TypeError", "msg": "'getset_descriptor' object is not callable"}
3515
Cov: 25678 -> 25678
Cov: 25678 -> 25678
3516
Cov: 25678 -> 25679
Cov: 25679 -> 25679
3517
Cov: 25679 -> 25680
Cov: 25680 -> 25680
3518
Cov: 25680 -> 25681
Cov: 25681 -> 25681
3519
Cov: 25681 -> 25682
Cov: 25682 -> 25682
3520
Cov: 25682 -> 25682
Cov: 25682 -> 25682
3521
Cov: 25682 -> 25682
Cov: 25682 -> 25682
3522
Cov: 25682 -> 25682
Cov: 25682 -> 25682
3523
Cov: 25682 -> 25682
Cov: 25682 -> 25682
3524
{"exception": "NotImplementedError", "msg": "Could not run 'aten::sparse_mask' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::sparse_mask' is only available for these backends: [SparseCPU, SparseCUDA, SparseCsrCPU, SparseCsrCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nSparseCPU: registered at aten/src/ATen/RegisterSparseCPU.cpp:1387 [kernel]\nSparseCUDA: registered at aten/src/ATen/RegisterSparseCUDA.cpp:1573 [kernel]\nSparseCsrCPU: registered at aten/src/ATen/RegisterSparseCsrCPU.cpp:1135 [kernel]\nSparseCsrCUDA: registered at aten/src/ATen/RegisterSparseCsrCUDA.cpp:1276 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:153 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:498 [backend fallback]\nFunctionalize: registered at ../aten/src/ATen/FunctionalizeFallbackKernel.cpp:290 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:86 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradMTIA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_0.cpp:16725 [kernel]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:382 [backend fallback]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:249 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:710 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:203 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:161 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:494 [backend fallback]\nPreDispatch: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:165 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:157 [backend fallback]\n"}
3525
Cov: 25682 -> 25682
Cov: 25682 -> 25682
3526
Cov: 25682 -> 25683
Cov: 25683 -> 25683
3527
Cov: 25683 -> 25684
Cov: 25684 -> 25684
3528
Cov: 25684 -> 25684
Cov: 25684 -> 25684
3529
{"exception": "RuntimeError", "msg": "The expanded size of the tensor (3) must match the existing size (2) at non-singleton dimension 0.  Target sizes: [3, 3].  Tensor sizes: [2, 3]"}
3530
{"exception": "RuntimeError", "msg": "masked_scatter_ only supports boolean masks, but got mask with dtype Byte"}
3531
Cov: 25684 -> 25684
Cov: 25684 -> 25684
3532
Cov: 25684 -> 25684
Cov: 25684 -> 25684
3533
Cov: 25684 -> 25684
Cov: 25684 -> 25684
3534
Cov: 25684 -> 25684
Cov: 25684 -> 25684
3535
Cov: 25684 -> 25684
Cov: 25684 -> 25684
3536
Cov: 25684 -> 25684
Cov: 25684 -> 25684
3537
{"exception": "NotImplementedError", "msg": "Could not run 'aten::qscheme' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::qscheme' is only available for these backends: [QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nQuantizedCPU: registered at aten/src/ATen/RegisterQuantizedCPU.cpp:944 [kernel]\nQuantizedCUDA: registered at aten/src/ATen/RegisterQuantizedCUDA.cpp:459 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:153 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:498 [backend fallback]\nFunctionalize: registered at ../aten/src/ATen/FunctionalizeFallbackKernel.cpp:290 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:86 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradMTIA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_0.cpp:16725 [kernel]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:382 [backend fallback]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:249 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:710 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:203 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:161 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:494 [backend fallback]\nPreDispatch: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:165 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:157 [backend fallback]\n"}
3538
Cov: 25684 -> 25684
Cov: 25684 -> 25684
3539
Cov: 25684 -> 25686
Cov: 25686 -> 25686
3540
Cov: 25686 -> 25686
Cov: 25686 -> 25686
3541
Cov: 25686 -> 25686
Cov: 25686 -> 25686
3542
Cov: 25686 -> 25686
Cov: 25686 -> 25686
3543
Cov: 25686 -> 25687
Cov: 25687 -> 25687
3544
Cov: 25687 -> 25687
Cov: 25687 -> 25687
3545
Cov: 25687 -> 25687
Cov: 25687 -> 25687
3546
Cov: 25687 -> 25687
Cov: 25687 -> 25687
3547
Cov: 25687 -> 25688
Cov: 25688 -> 25688
3548
Cov: 25688 -> 25689
Cov: 25689 -> 25689
3549
Cov: 25689 -> 25689
Cov: 25689 -> 25689
3550
Cov: 25689 -> 25690
Cov: 25690 -> 25690
3551
Cov: 25690 -> 25690
Cov: 25690 -> 25690
3552
{"exception": "TypeError", "msg": "scatter_() received an invalid combination of arguments - got (int, Tensor, Tensor, NoneType), but expected one of:\n * (int dim, Tensor index, Tensor src)\n * (int dim, Tensor index, Tensor src, *, str reduce)\n * (int dim, Tensor index, Number value)\n * (int dim, Tensor index, Number value, *, str reduce)\n"}
3553
Cov: 25690 -> 25691
Cov: 25691 -> 25691
3554
Cov: 25691 -> 25691
Cov: 25691 -> 25691
3555
Cov: 25691 -> 25692
Cov: 25692 -> 25692
3556
Cov: 25692 -> 25692
Cov: 25692 -> 25692
3557
Cov: 25692 -> 25693
Cov: 25693 -> 25693
3558
Cov: 25693 -> 25693
Cov: 25693 -> 25693
3559
Cov: 25693 -> 25693
Cov: 25693 -> 25693
3560
Cov: 25693 -> 25693
Cov: 25693 -> 25693
3561
Cov: 25693 -> 25693
Cov: 25693 -> 25693
3562
Cov: 25693 -> 25693
Cov: 25693 -> 25693
3563
Cov: 25693 -> 25693
Cov: 25693 -> 25693
3564
Cov: 25693 -> 25693
Cov: 25693 -> 25693
3565
Cov: 25693 -> 25693
Cov: 25693 -> 25693
3566
Cov: 25693 -> 25693
Cov: 25693 -> 25693
3567
Cov: 25693 -> 25693
Cov: 25693 -> 25693
3568
{"exception": "TypeError", "msg": "heaviside(): argument 'values' must be Tensor, not int"}
3569
{"exception": "RuntimeError", "msg": "Expected size for first two dimensions of batch2 tensor to be: [2, 4] but got: [2, 3]."}
3570
Cov: 25693 -> 25694
Cov: 25694 -> 25694
3571
Cov: 25694 -> 25694
Cov: 25694 -> 25694
3572
Cov: 25694 -> 25694
Cov: 25694 -> 25694
3573
Cov: 25694 -> 25694
Cov: 25694 -> 25694
3574
Cov: 25694 -> 25694
Cov: 25694 -> 25694
3575
Cov: 25694 -> 25694
Cov: 25694 -> 25694
3576
Cov: 25694 -> 25714
Cov: 25714 -> 25714
3577
Cov: 25714 -> 25717
Cov: 25717 -> 25717
3578
Cov: 25717 -> 25717
Cov: 25717 -> 25717
3579
Cov: 25717 -> 25718
Cov: 25718 -> 25718
3580
Cov: 25718 -> 25719
Cov: 25719 -> 25719
3581
Cov: 25719 -> 25719
Cov: 25719 -> 25719
3582
Cov: 25719 -> 25719
Cov: 25719 -> 25719
3583
Cov: 25719 -> 25720
Cov: 25720 -> 25720
3584
Cov: 25720 -> 25721
Cov: 25721 -> 25721
3585
Cov: 25721 -> 25721
Cov: 25721 -> 25721
3586
Cov: 25721 -> 25721
Cov: 25721 -> 25721
3587
Cov: 25721 -> 25722
Cov: 25722 -> 25722
3588
Cov: 25722 -> 25722
Cov: 25722 -> 25722
3589
Cov: 25722 -> 25797
Cov: 25797 -> 25797
3590
Cov: 25797 -> 25797
Cov: 25797 -> 25797
3591
Cov: 25797 -> 25798
Cov: 25798 -> 25798
3592
Cov: 25798 -> 25799
Cov: 25799 -> 25799
3593
Cov: 25799 -> 25799
Cov: 25799 -> 25799
3594
Cov: 25799 -> 25799
Cov: 25799 -> 25799
3595
Cov: 25799 -> 25800
Cov: 25800 -> 25800
3596
Cov: 25800 -> 25800
Cov: 25800 -> 25800
3597
Cov: 25800 -> 25801
Cov: 25801 -> 25801
3598
Cov: 25801 -> 25803
Cov: 25803 -> 25803
3599
Cov: 25803 -> 25803
Cov: 25803 -> 25803
3600
Cov: 25803 -> 25803
Cov: 25803 -> 25803
3601
Cov: 25803 -> 25803
Cov: 25803 -> 25803
3602
Cov: 25803 -> 25803
Cov: 25803 -> 25803
3603
{"exception": "TypeError", "msg": "'getset_descriptor' object is not callable"}
3604
Cov: 25803 -> 25803
Cov: 25803 -> 25803
3605
Cov: 25803 -> 25803
Cov: 25803 -> 25803
3606
Cov: 25803 -> 25804
Cov: 25804 -> 25804
3607
Cov: 25804 -> 25819
Cov: 25819 -> 25819
3608
Cov: 25819 -> 25819
Cov: 25819 -> 25819
3609
Cov: 25819 -> 25820
Cov: 25820 -> 25820
3610
Cov: 25820 -> 25842
Cov: 25842 -> 25842
3611
Cov: 25842 -> 25842
Cov: 25842 -> 25842
3612
Cov: 25842 -> 25842
Cov: 25842 -> 25842
3613
Cov: 25842 -> 25842
Cov: 25842 -> 25842
3614
Cov: 25842 -> 25842
Cov: 25842 -> 25842
3615
Cov: 25842 -> 25842
Cov: 25842 -> 25842
3616
Cov: 25842 -> 25842
Cov: 25842 -> 25842
3617
Cov: 25842 -> 25842
Cov: 25842 -> 25842
3618
Cov: 25842 -> 25842
Cov: 25842 -> 25842
3619
Cov: 25842 -> 25842
Cov: 25842 -> 25842
3620
Cov: 25842 -> 25842
Cov: 25842 -> 25842
3621
Cov: 25842 -> 25844
Cov: 25844 -> 25844
3622
Cov: 25844 -> 25844
Cov: 25844 -> 25844
3623
Cov: 25844 -> 25844
Cov: 25844 -> 25844
3624
Cov: 25844 -> 25844
Cov: 25844 -> 25844
3625
Cov: 25844 -> 25844
Cov: 25844 -> 25844
3626
Cov: 25844 -> 25844
Cov: 25844 -> 25844
3627
Cov: 25844 -> 25844
Cov: 25844 -> 25844
3628
Cov: 25844 -> 25844
Cov: 25844 -> 25844
3629
Cov: 25844 -> 25844
Cov: 25844 -> 25844
3630
Cov: 25844 -> 25845
Cov: 25845 -> 25845
3631
Cov: 25845 -> 25845
Cov: 25845 -> 25845
3632
Cov: 25845 -> 25846
Cov: 25846 -> 25846
3633
Cov: 25846 -> 25847
Cov: 25847 -> 25847
3634
Cov: 25847 -> 25848
Cov: 25848 -> 25848
3635
Cov: 25848 -> 25848
Cov: 25848 -> 25848
3636
Cov: 25848 -> 25849
Cov: 25849 -> 25849
3637
Cov: 25849 -> 25849
Cov: 25849 -> 25849
3638
Cov: 25849 -> 25850
Cov: 25850 -> 25850
3639
Cov: 25850 -> 25850
Cov: 25850 -> 25850
3640
Cov: 25850 -> 25850
Cov: 25850 -> 25850
3641
Cov: 25850 -> 25850
Cov: 25850 -> 25850
3642
Cov: 25850 -> 25850
Cov: 25850 -> 25850
3643
Cov: 25850 -> 25851
Cov: 25851 -> 25851
3644
Cov: 25851 -> 25851
Cov: 25851 -> 25851
3645
Cov: 25851 -> 25851
Cov: 25851 -> 25851
3646
Cov: 25851 -> 25851
Cov: 25851 -> 25851
3647
Cov: 25851 -> 25851
Cov: 25851 -> 25851
3648
Cov: 25851 -> 25851
Cov: 25851 -> 25851
3649
Cov: 25851 -> 25851
Cov: 25851 -> 25851
3650
Cov: 25851 -> 25851
Cov: 25851 -> 25851
3651
Cov: 25851 -> 25851
Cov: 25851 -> 25851
3652
Cov: 25851 -> 25851
Cov: 25851 -> 25851
3653
Cov: 25851 -> 25851
Cov: 25851 -> 25851
3654
Cov: 25851 -> 25852
Cov: 25852 -> 25852
3655
Cov: 25852 -> 25852
Cov: 25852 -> 25852
3656
{"exception": "TypeError", "msg": "map_(): argument 'other' (position 1) must be Tensor, not function"}
3657
Cov: 25852 -> 25852
Cov: 25852 -> 25852
3658
Cov: 25852 -> 25852
Cov: 25852 -> 25852
3659
Cov: 25852 -> 25852
Cov: 25852 -> 25852
3660
Cov: 25852 -> 25867
Cov: 25867 -> 25867
3661
Cov: 25867 -> 25867
Cov: 25867 -> 25867
3662
Cov: 25867 -> 25867
Cov: 25867 -> 25867
3663
Cov: 25867 -> 25867
Cov: 25867 -> 25867
3664
Cov: 25867 -> 25867
Cov: 25867 -> 25867
3665
Cov: 25867 -> 25867
Cov: 25867 -> 25867
3666
Cov: 25867 -> 25867
Cov: 25867 -> 25867
3667
Cov: 25867 -> 25867
Cov: 25867 -> 25867
3668
Cov: 25867 -> 25867
Cov: 25867 -> 25867
3669
Cov: 25867 -> 25868
Cov: 25868 -> 25868
3670
{"exception": "TypeError", "msg": "ldexp_(): argument 'other' (position 1) must be Tensor, not int"}
3671
Cov: 25868 -> 25868
Cov: 25868 -> 25868
3672
Cov: 25868 -> 25868
Cov: 25868 -> 25868
3673
Cov: 25868 -> 25868
Cov: 25868 -> 25868
3674
Cov: 25868 -> 25868
Cov: 25868 -> 25868
3675
Cov: 25868 -> 25869
Cov: 25869 -> 25869
3676
{"exception": "RuntimeError", "msg": "Expected nested_tensorlist[0].size() > 0 to be true, but got false.  (Could this error message be improved?  If so, please report an enhancement request to PyTorch.)"}
3677
Cov: 25869 -> 25870
Cov: 25870 -> 25870
3678
Cov: 25870 -> 25872
Cov: 25872 -> 25872
3679
Cov: 25872 -> 25873
Cov: 25873 -> 25873
3680
Cov: 25873 -> 25874
Cov: 25874 -> 25874
3681
{"exception": "RuntimeError", "msg": "istft requires a complex-valued input tensor matching the output from stft with return_complex=True."}
3682
Cov: 25874 -> 25875
Cov: 25875 -> 25875
3683
Cov: 25875 -> 25875
Cov: 25875 -> 25875
3684
Cov: 25875 -> 25875
Cov: 25875 -> 25875
3685
Cov: 25875 -> 25875
Cov: 25875 -> 25875
3686
Cov: 25875 -> 25876
Cov: 25876 -> 25876
3687
Cov: 25876 -> 25876
Cov: 25876 -> 25876
3688
Cov: 25876 -> 25876
Cov: 25876 -> 25876
3689
Cov: 25876 -> 25876
Cov: 25876 -> 25876
3690
{"exception": "RuntimeError", "msg": "torch.ormqr: Expected other to have the same number of dimensions as input, but got other.ndim equal to 2 and input.ndim is equal to 3"}
3691
Cov: 25876 -> 25877
Cov: 25877 -> 25877
3692
Cov: 25877 -> 25877
Cov: 25877 -> 25877
3693
Cov: 25877 -> 25877
Cov: 25877 -> 25877
3694
Cov: 25877 -> 25877
Cov: 25877 -> 25877
3695
Cov: 25877 -> 25878
Cov: 25878 -> 25878
3696
Cov: 25878 -> 25878
Cov: 25878 -> 25878
3697
Cov: 25878 -> 25878
Cov: 25878 -> 25878
3698
{"exception": "RuntimeError", "msg": "masked_fill_ only supports boolean masks, but got mask with dtype unsigned char"}
3699
Cov: 25878 -> 25878
Cov: 25878 -> 25878
3700
{"exception": "NotImplementedError", "msg": "Could not run 'aten::q_per_channel_zero_points' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::q_per_channel_zero_points' is only available for these backends: [QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nQuantizedCPU: registered at aten/src/ATen/RegisterQuantizedCPU.cpp:944 [kernel]\nQuantizedCUDA: registered at aten/src/ATen/RegisterQuantizedCUDA.cpp:459 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:153 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:498 [backend fallback]\nFunctionalize: registered at ../aten/src/ATen/FunctionalizeFallbackKernel.cpp:290 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:86 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradMTIA: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_1.cpp:16002 [kernel]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:382 [backend fallback]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:249 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:710 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:203 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:161 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:494 [backend fallback]\nPreDispatch: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:165 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:157 [backend fallback]\n"}
3701
Cov: 25878 -> 25878
Cov: 25878 -> 25878
3702
Cov: 25878 -> 25878
Cov: 25878 -> 25878
3703
Cov: 25878 -> 25878
Cov: 25878 -> 25878
3704
Cov: 25878 -> 25878
Cov: 25878 -> 25878
3705
Cov: 25878 -> 25878
Cov: 25878 -> 25878
3706
Cov: 25878 -> 25878
Cov: 25878 -> 25878
3707
Cov: 25878 -> 25878
Cov: 25878 -> 25878
3708
Cov: 25878 -> 25878
Cov: 25878 -> 25878
3709
Cov: 25878 -> 25879
Cov: 25879 -> 25879
3710
{"exception": "RuntimeError", "msg": "This function was deprecated since version 1.9 and is now removed. `torch.linalg.eig` returns complex tensors of dtype `cfloat` or `cdouble` rather than real tensors mimicking complex tensors.\n\nL, _ = torch.eig(A) should be replaced with:\nL_complex = torch.linalg.eigvals(A)\n\nand\n\nL, V = torch.eig(A, eigenvectors=True) should be replaced with:\nL_complex, V_complex = torch.linalg.eig(A)"}
3711
Cov: 25879 -> 25881
Cov: 25881 -> 25881
3712
Cov: 25881 -> 25881
Cov: 25881 -> 25881
3713
Cov: 25881 -> 25881
Cov: 25881 -> 25881
3714
Cov: 25881 -> 25881
Cov: 25881 -> 25881
3715
Cov: 25881 -> 25882
Cov: 25882 -> 25882
3716
Cov: 25882 -> 25882
Cov: 25882 -> 25882
3717
Cov: 25882 -> 25882
Cov: 25882 -> 25882
3718
Cov: 25882 -> 25882
Cov: 25882 -> 25882
3719
Cov: 25882 -> 25882
Cov: 25882 -> 25882
3720
Cov: 25882 -> 25883
Cov: 25883 -> 25883
3721
Cov: 25883 -> 25883
Cov: 25883 -> 25883
3722
Cov: 25883 -> 25883
Cov: 25883 -> 25883
3723
Cov: 25883 -> 25884
Cov: 25884 -> 25884
3724
Cov: 25884 -> 25884
Cov: 25884 -> 25884
3725
Cov: 25884 -> 25884
Cov: 25884 -> 25884
3726
Cov: 25884 -> 25884
Cov: 25884 -> 25884
3727
Cov: 25884 -> 25884
Cov: 25884 -> 25884
3728
Cov: 25884 -> 25885
Cov: 25885 -> 25885
3729
Cov: 25885 -> 25885
Cov: 25885 -> 25885
3730
Cov: 25885 -> 25885
Cov: 25885 -> 25885
3731
Cov: 25885 -> 25885
Cov: 25885 -> 25885
3732
Cov: 25885 -> 25886
Cov: 25886 -> 25886
3733
Cov: 25886 -> 25886
Cov: 25886 -> 25886
3734
Cov: 25886 -> 25886
Cov: 25886 -> 25886
3735
Cov: 25886 -> 25886
Cov: 25886 -> 25886
3736
Cov: 25886 -> 25886
Cov: 25886 -> 25886
3737
Cov: 25886 -> 25886
Cov: 25886 -> 25886
3738
Cov: 25886 -> 25886
Cov: 25886 -> 25886
3739
Cov: 25886 -> 25889
Cov: 25889 -> 25889
3740
Cov: 25889 -> 25889
Cov: 25889 -> 25889
3741
{"exception": "TypeError", "msg": "'getset_descriptor' object is not callable"}
3742
Cov: 25889 -> 25893
Cov: 25893 -> 25893
3743
Cov: 25893 -> 25893
Cov: 25893 -> 25893
3744
Cov: 25893 -> 25893
Cov: 25893 -> 25893
3745
Cov: 25893 -> 25893
Cov: 25893 -> 25893
3746
Cov: 25893 -> 25893
Cov: 25893 -> 25893
3747
Cov: 25893 -> 25893
Cov: 25893 -> 25893
3748
Cov: 25893 -> 25893
Cov: 25893 -> 25893
3749
Cov: 25893 -> 25893
Cov: 25893 -> 25893
3750
Cov: 25893 -> 25893
Cov: 25893 -> 25893
3751
Cov: 25893 -> 25893
Cov: 25893 -> 25893
3752
Cov: 25893 -> 25894
Cov: 25894 -> 25894
3753
Cov: 25894 -> 25894
Cov: 25894 -> 25894
3754
Cov: 25894 -> 25894
Cov: 25894 -> 25894
3755
Cov: 25894 -> 25894
Cov: 25894 -> 25894
3756
Cov: 25894 -> 25894
Cov: 25894 -> 25894
3757
Cov: 25894 -> 25894
Cov: 25894 -> 25894
3758
Cov: 25894 -> 25896
Cov: 25896 -> 25896
3759
Cov: 25896 -> 25897
Cov: 25897 -> 25897
3760
Cov: 25897 -> 25898
Cov: 25898 -> 25898
3761
Cov: 25898 -> 25898
Cov: 25898 -> 25898
3762
Cov: 25898 -> 25899
Cov: 25899 -> 25899
3763
Cov: 25899 -> 25899
Cov: 25899 -> 25899
3764
Cov: 25899 -> 25900
Cov: 25900 -> 25900
3765
Cov: 25900 -> 25900
Cov: 25900 -> 25900
3766
Cov: 25900 -> 25900
Cov: 25900 -> 25900
3767
Cov: 25900 -> 25900
Cov: 25900 -> 25900
3768
Cov: 25900 -> 25900
Cov: 25900 -> 25900
3769
Cov: 25900 -> 25900
Cov: 25900 -> 25900
3770
Cov: 25900 -> 25900
Cov: 25900 -> 25900
3771
Cov: 25900 -> 25900
Cov: 25900 -> 25900
3772
{"exception": "TypeError", "msg": "heaviside(): argument 'values' must be Tensor, not float"}
3773
Cov: 25900 -> 25900
Cov: 25900 -> 25900
3774
Cov: 25900 -> 25901
Cov: 25901 -> 25901
3775
Cov: 25901 -> 25901
Cov: 25901 -> 25901
3776
Cov: 25901 -> 25902
Cov: 25902 -> 25902
3777
Cov: 25902 -> 25903
Cov: 25903 -> 25903
3778
Cov: 25903 -> 25903
Cov: 25903 -> 25903
3779
Cov: 25903 -> 25903
Cov: 25903 -> 25903
3780
Cov: 25903 -> 25904
Cov: 25904 -> 25904
3781
Cov: 25904 -> 25904
Cov: 25904 -> 25904
3782
Cov: 25904 -> 25904
Cov: 25904 -> 25904
3783
Cov: 25904 -> 25904
Cov: 25904 -> 25904
3784
Cov: 25904 -> 25904
Cov: 25904 -> 25904
3785
Cov: 25904 -> 25904
Cov: 25904 -> 25904
3786
Cov: 25904 -> 25905
Cov: 25905 -> 25905
3787
Cov: 25905 -> 25905
Cov: 25905 -> 25905
3788
Cov: 25905 -> 25905
Cov: 25905 -> 25905
3789
Cov: 25905 -> 25905
Cov: 25905 -> 25905
3790
Cov: 25905 -> 25906
Cov: 25906 -> 25906
3791
Cov: 25906 -> 25906
Cov: 25906 -> 25906
3792
Cov: 25906 -> 25909
Cov: 25909 -> 25909
3793
Cov: 25909 -> 25909
Cov: 25909 -> 25909
3794
Cov: 25909 -> 25909
Cov: 25909 -> 25909
3795
Cov: 25909 -> 25909
Cov: 25909 -> 25909
3796
Cov: 25909 -> 25910
Cov: 25910 -> 25910
3797
Cov: 25910 -> 25911
Cov: 25911 -> 25911
3798
{"exception": "RuntimeError", "msg": "invalid Poisson rate, expected rate to be non-negative"}
3799
Cov: 25911 -> 25911
Cov: 25911 -> 25911
3800
Cov: 25911 -> 25911
Cov: 25911 -> 25911
3801
Cov: 25911 -> 25912
Cov: 25912 -> 25912
3802
Cov: 25912 -> 25913
Cov: 25913 -> 25913
3803
Cov: 25913 -> 25913
Cov: 25913 -> 25913
3804
Cov: 25913 -> 25913
Cov: 25913 -> 25913
3805
Cov: 25913 -> 25913
Cov: 25913 -> 25913
3806
Cov: 25913 -> 25913
Cov: 25913 -> 25913
3807
Cov: 25913 -> 25913
Cov: 25913 -> 25913
3808
Cov: 25913 -> 25913
Cov: 25913 -> 25913
3809
Cov: 25913 -> 25914
Cov: 25914 -> 25914
3810
Cov: 25914 -> 25915
Cov: 25915 -> 25915
3811
Cov: 25915 -> 25915
Cov: 25915 -> 25915
3812
{"exception": "RuntimeError", "msg": "tensor.sspaddmm(...) can only be called on sparse tensors"}
3813
Cov: 25915 -> 25915
Cov: 25915 -> 25915
3814
Cov: 25915 -> 25915
Cov: 25915 -> 25915
3815
Cov: 25915 -> 25916
Cov: 25916 -> 25916
3816
{"exception": "RuntimeError", "msg": "mat1 and mat2 shapes cannot be multiplied (2x3 and 2x3)"}
3817
Cov: 25916 -> 25916
Cov: 25916 -> 25916
3818
Cov: 25916 -> 25917
Cov: 25917 -> 25917
3819
Cov: 25917 -> 25917
Cov: 25917 -> 25917
3820
Cov: 25917 -> 25917
Cov: 25917 -> 25917
3821
Cov: 25917 -> 25917
Cov: 25917 -> 25917
3822
Cov: 25917 -> 25917
Cov: 25917 -> 25917
3823
Cov: 25917 -> 25917
Cov: 25917 -> 25917
3824
Cov: 25917 -> 25917
Cov: 25917 -> 25917
3825
Cov: 25917 -> 25917
Cov: 25917 -> 25917
3826
Cov: 25917 -> 25917
Cov: 25917 -> 25917
3827
Cov: 25917 -> 25917
Cov: 25917 -> 25917
3828
Cov: 25917 -> 25917
Cov: 25917 -> 25917
3829
Cov: 25917 -> 25918
Cov: 25918 -> 25918
3830
Cov: 25918 -> 25918
Cov: 25918 -> 25918
3831
Cov: 25918 -> 25918
Cov: 25918 -> 25918
3832
Cov: 25918 -> 25918
Cov: 25918 -> 25918
3833
Cov: 25918 -> 25919
Cov: 25919 -> 25919
3834
Cov: 25919 -> 25919
Cov: 25919 -> 25919
3835
Cov: 25919 -> 25919
Cov: 25919 -> 25919
3836
{"exception": "TypeError", "msg": "logical_xor(): argument 'other' must be Tensor, not bool"}
3837
Cov: 25919 -> 25919
Cov: 25919 -> 25919
3838
Cov: 25919 -> 25919
Cov: 25919 -> 25919
3839
Cov: 25919 -> 25919
Cov: 25919 -> 25919
3840
Cov: 25919 -> 25919
Cov: 25919 -> 25919
3841
Cov: 25919 -> 25919
Cov: 25919 -> 25919
3842
Cov: 25919 -> 25919
Cov: 25919 -> 25919
3843
Cov: 25919 -> 25920
Cov: 25920 -> 25920
3844
Cov: 25920 -> 25920
Cov: 25920 -> 25920
3845
Cov: 25920 -> 25920
Cov: 25920 -> 25920
3846
Cov: 25920 -> 25920
Cov: 25920 -> 25920
3847
Cov: 25920 -> 25920
Cov: 25920 -> 25920
3848
Cov: 25920 -> 25920
Cov: 25920 -> 25920
3849
Cov: 25920 -> 25920
Cov: 25920 -> 25920
3850
Cov: 25920 -> 25921
Cov: 25921 -> 25921
3851
{"exception": "RuntimeError", "msg": "Index tensor must have the same number of dimensions as input tensor"}
3852
Cov: 25921 -> 25921
Cov: 25921 -> 25921
3853
Cov: 25921 -> 25921
Cov: 25921 -> 25921
3854
Cov: 25921 -> 25921
Cov: 25921 -> 25921
3855
Cov: 25921 -> 25922
Cov: 25922 -> 25922
3856
Cov: 25922 -> 25922
Cov: 25922 -> 25922
3857
Cov: 25922 -> 25922
Cov: 25922 -> 25922
3858
Cov: 25922 -> 25922
Cov: 25922 -> 25922
3859
Cov: 25922 -> 25923
Cov: 25923 -> 25923
3860
Cov: 25923 -> 25924
Cov: 25924 -> 25924
3861
Cov: 25924 -> 25924
Cov: 25924 -> 25924
3862
Cov: 25924 -> 25924
Cov: 25924 -> 25924
3863
Cov: 25924 -> 25924
Cov: 25924 -> 25924
3864
Cov: 25924 -> 25924
Cov: 25924 -> 25924
3865
Cov: 25924 -> 25924
Cov: 25924 -> 25924
3866
Cov: 25924 -> 25925
Cov: 25925 -> 25925
3867
Cov: 25925 -> 25926
Cov: 25926 -> 25926
3868
Cov: 25926 -> 25927
Cov: 25927 -> 25927
3869
{"exception": "RuntimeError", "msg": "1D tensors expected, but got 2D and 2D tensors"}
3870
Cov: 25927 -> 25927
Cov: 25927 -> 25927
3871
Cov: 25927 -> 25927
Cov: 25927 -> 25927
3872
Cov: 25927 -> 25927
Cov: 25927 -> 25927
3873
{"exception": "TypeError", "msg": "random_() received an invalid combination of arguments - got (to=int, from_=int, ), but expected one of:\n * (*, torch.Generator generator)\n      didn't match because some of the keywords were incorrect: to, from_\n * (int from, int to, *, torch.Generator generator)\n * (int to, *, torch.Generator generator)\n"}
3874
Cov: 25927 -> 25927
Cov: 25927 -> 25927
3875
Cov: 25927 -> 25928
Cov: 25928 -> 25928
3876
Cov: 25928 -> 25928
Cov: 25928 -> 25928
3877
Cov: 25928 -> 25928
Cov: 25928 -> 25928
3878
Cov: 25928 -> 25928
Cov: 25928 -> 25928
3879
Cov: 25928 -> 25928
Cov: 25928 -> 25928
3880
Cov: 25928 -> 25928
Cov: 25928 -> 25928
3881
Cov: 25928 -> 25928
Cov: 25928 -> 25928
3882
Cov: 25928 -> 25928
Cov: 25928 -> 25928
3883
Cov: 25928 -> 25929
Cov: 25929 -> 25929
3884
Cov: 25929 -> 25929
Cov: 25929 -> 25929
3885
Cov: 25929 -> 25929
Cov: 25929 -> 25929
3886
Cov: 25929 -> 25929
Cov: 25929 -> 25929
3887
{"exception": "NotImplementedError", "msg": "Could not run 'aten::q_per_channel_zero_points' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::q_per_channel_zero_points' is only available for these backends: [QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nQuantizedCPU: registered at aten/src/ATen/RegisterQuantizedCPU.cpp:944 [kernel]\nQuantizedCUDA: registered at aten/src/ATen/RegisterQuantizedCUDA.cpp:459 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:153 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:498 [backend fallback]\nFunctionalize: registered at ../aten/src/ATen/FunctionalizeFallbackKernel.cpp:290 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:86 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradMTIA: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_1.cpp:16002 [kernel]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:382 [backend fallback]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:249 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:710 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:203 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:161 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:494 [backend fallback]\nPreDispatch: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:165 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:157 [backend fallback]\n"}
3888
Cov: 25929 -> 25929
Cov: 25929 -> 25929
3889
Cov: 25929 -> 25929
Cov: 25929 -> 25929
3890
Cov: 25929 -> 25930
Cov: 25930 -> 25930
3891
Cov: 25930 -> 25930
Cov: 25930 -> 25930
3892
Cov: 25930 -> 25931
Cov: 25931 -> 25931
3893
{"exception": "NameError", "msg": "name 'other' is not defined"}
3894
Cov: 25931 -> 25932
Cov: 25932 -> 25932
3895
Cov: 25932 -> 25932
Cov: 25932 -> 25932
3896
Cov: 25932 -> 25932
Cov: 25932 -> 25932
3897
Cov: 25932 -> 25932
Cov: 25932 -> 25932
3898
Cov: 25932 -> 25932
Cov: 25932 -> 25932
3899
Cov: 25932 -> 25932
Cov: 25932 -> 25932
3900
Cov: 25932 -> 25932
Cov: 25932 -> 25932
3901
Cov: 25932 -> 25941
Cov: 25941 -> 25941
3902
Cov: 25941 -> 25956
Cov: 25956 -> 25956
3903
Cov: 25956 -> 25956
Cov: 25956 -> 25956
3904
Cov: 25956 -> 25956
Cov: 25956 -> 25956
3905
Cov: 25956 -> 25956
Cov: 25956 -> 25956
3906
Cov: 25956 -> 25956
Cov: 25956 -> 25956
3907
Cov: 25956 -> 25957
Cov: 25957 -> 25957
3908
Cov: 25957 -> 25957
Cov: 25957 -> 25957
3909
Cov: 25957 -> 25957
Cov: 25957 -> 25957
3910
Cov: 25957 -> 25957
Cov: 25957 -> 25957
3911
Cov: 25957 -> 25958
Cov: 25958 -> 25958
3912
Cov: 25958 -> 25958
Cov: 25958 -> 25958
3913
Cov: 25958 -> 25958
Cov: 25958 -> 25958
3914
{"exception": "NotImplementedError", "msg": "Could not run 'aten::q_per_channel_axis' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::q_per_channel_axis' is only available for these backends: [QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nQuantizedCPU: registered at aten/src/ATen/RegisterQuantizedCPU.cpp:944 [kernel]\nQuantizedCUDA: registered at aten/src/ATen/RegisterQuantizedCUDA.cpp:459 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:153 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:498 [backend fallback]\nFunctionalize: registered at ../aten/src/ATen/FunctionalizeFallbackKernel.cpp:290 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:86 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradMTIA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_4.cpp:13056 [kernel]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:382 [backend fallback]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:249 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:710 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:203 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:161 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:494 [backend fallback]\nPreDispatch: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:165 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:157 [backend fallback]\n"}
3915
Cov: 25958 -> 25961
Cov: 25961 -> 25961
3916
Cov: 25961 -> 25961
Cov: 25961 -> 25961
3917
Cov: 25961 -> 25962
Cov: 25962 -> 25962
3918
Cov: 25962 -> 25962
Cov: 25962 -> 25962
3919
Cov: 25962 -> 25962
Cov: 25962 -> 25962
3920
Cov: 25962 -> 25962
Cov: 25962 -> 25962
3921
Cov: 25962 -> 25962
Cov: 25962 -> 25962
3922
Cov: 25962 -> 25962
Cov: 25962 -> 25962
3923
Cov: 25962 -> 25962
Cov: 25962 -> 25962
3924
Cov: 25962 -> 25963
Cov: 25963 -> 25963
3925
Cov: 25963 -> 25963
Cov: 25963 -> 25963
3926
Cov: 25963 -> 25964
Cov: 25964 -> 25964
3927
Cov: 25964 -> 25964
Cov: 25964 -> 25964
3928
Cov: 25964 -> 25964
Cov: 25964 -> 25964
3929
Cov: 25964 -> 25964
Cov: 25964 -> 25964
3930
Cov: 25964 -> 25964
Cov: 25964 -> 25964
3931
Cov: 25964 -> 25964
Cov: 25964 -> 25964
3932
Cov: 25964 -> 25964
Cov: 25964 -> 25964
3933
Cov: 25964 -> 25964
Cov: 25964 -> 25964
3934
Cov: 25964 -> 25964
Cov: 25964 -> 25964
3935
Cov: 25964 -> 25964
Cov: 25964 -> 25964
3936
{"exception": "RuntimeError", "msg": "masked_fill_ only supports a 0-dimensional value tensor, but got tensor with 1 dimension(s)."}
3937
Cov: 25964 -> 25965
Cov: 25965 -> 25965
3938
Cov: 25965 -> 25965
Cov: 25965 -> 25965
3939
Cov: 25965 -> 25965
Cov: 25965 -> 25965
3940
Cov: 25965 -> 25966
Cov: 25966 -> 25966
3941
Cov: 25966 -> 25967
Cov: 25967 -> 25967
3942
Cov: 25967 -> 25967
Cov: 25967 -> 25967
3943
Cov: 25967 -> 25968
Cov: 25968 -> 25968
3944
Cov: 25968 -> 25968
Cov: 25968 -> 25968
3945
Cov: 25968 -> 25968
Cov: 25968 -> 25968
3946
Cov: 25968 -> 25968
Cov: 25968 -> 25968
3947
{"exception": "RuntimeError", "msg": "scatter(): Expected self.dtype to be equal to src.dtype"}
3948
Cov: 25968 -> 25969
Cov: 25969 -> 25969
3949
Cov: 25969 -> 25969
Cov: 25969 -> 25969
3950
Cov: 25969 -> 25969
Cov: 25969 -> 25969
3951
Cov: 25969 -> 25969
Cov: 25969 -> 25969
3952
{"exception": "TypeError", "msg": "atanh_() takes no arguments (1 given)"}
3953
Cov: 25969 -> 25969
Cov: 25969 -> 25969
3954
Cov: 25969 -> 25969
Cov: 25969 -> 25969
3955
Cov: 25969 -> 25975
Cov: 25975 -> 25975
3956
Cov: 25975 -> 25975
Cov: 25975 -> 25975
3957
Cov: 25975 -> 25975
Cov: 25975 -> 25975
3958
Cov: 25975 -> 25975
Cov: 25975 -> 25975
3959
Cov: 25975 -> 25975
Cov: 25975 -> 25975
3960
Cov: 25975 -> 25975
Cov: 25975 -> 25975
3961
{"exception": "RuntimeError", "msg": "size {[4, 4]} is not expandable to size {[2, 3, 4]}."}
3962
Cov: 25975 -> 25975
Cov: 25975 -> 25975
3963
Cov: 25975 -> 25975
Cov: 25975 -> 25975
3964
Cov: 25975 -> 25975
Cov: 25975 -> 25975
3965
Cov: 25975 -> 25975
Cov: 25975 -> 25975
3966
Cov: 25975 -> 25975
Cov: 25975 -> 25975
3967
Cov: 25975 -> 25975
Cov: 25975 -> 25975
3968
Cov: 25975 -> 25975
Cov: 25975 -> 25975
3969
Cov: 25975 -> 25976
Cov: 25976 -> 25976
3970
Cov: 25976 -> 25976
Cov: 25976 -> 25976
3971
Cov: 25976 -> 25977
Cov: 25977 -> 25977
3972
Cov: 25977 -> 25977
Cov: 25977 -> 25977
3973
Cov: 25977 -> 25977
Cov: 25977 -> 25977
3974
Cov: 25977 -> 25977
Cov: 25977 -> 25977
3975
Cov: 25977 -> 25977
Cov: 25977 -> 25977
3976
Cov: 25977 -> 26002
Cov: 26002 -> 26002
3977
Cov: 26002 -> 26002
Cov: 26002 -> 26002
3978
Cov: 26002 -> 26002
Cov: 26002 -> 26002
3979
Cov: 26002 -> 26002
Cov: 26002 -> 26002
3980
Cov: 26002 -> 26003
Cov: 26003 -> 26003
3981
Cov: 26003 -> 26003
Cov: 26003 -> 26003
3982
Cov: 26003 -> 26004
Cov: 26004 -> 26004
3983
Cov: 26004 -> 26004
Cov: 26004 -> 26004
3984
Cov: 26004 -> 26004
Cov: 26004 -> 26004
3985
Cov: 26004 -> 26005
Cov: 26005 -> 26005
3986
Cov: 26005 -> 26006
Cov: 26006 -> 26006
3987
Cov: 26006 -> 26006
Cov: 26006 -> 26006
3988
Cov: 26006 -> 26006
Cov: 26006 -> 26006
3989
Cov: 26006 -> 26006
Cov: 26006 -> 26006
3990
{"exception": "RuntimeError", "msg": "masked_scatter: expected self and source to have same dtypes but gotLong and Int"}
3991
Cov: 26006 -> 26006
Cov: 26006 -> 26006
3992
Cov: 26006 -> 26006
Cov: 26006 -> 26006
3993
Cov: 26006 -> 26006
Cov: 26006 -> 26006
3994
Cov: 26006 -> 26006
Cov: 26006 -> 26006
3995
Cov: 26006 -> 26006
Cov: 26006 -> 26006
3996
Cov: 26006 -> 26006
Cov: 26006 -> 26006
3997
Cov: 26006 -> 26007
Cov: 26007 -> 26007
3998
Cov: 26007 -> 26007
Cov: 26007 -> 26007
3999
Cov: 26007 -> 26007
Cov: 26007 -> 26007
4000
Cov: 26007 -> 26007
Cov: 26007 -> 26007
4001
Cov: 26007 -> 26007
Cov: 26007 -> 26007
4002
Cov: 26007 -> 26007
Cov: 26007 -> 26007
4003
Cov: 26007 -> 26009
Cov: 26009 -> 26009
4004
Cov: 26009 -> 26009
Cov: 26009 -> 26009
4005
Cov: 26009 -> 26009
Cov: 26009 -> 26009
4006
Cov: 26009 -> 26010
Cov: 26010 -> 26010
4007
Cov: 26010 -> 26010
Cov: 26010 -> 26010
4008
{"exception": "NotImplementedError", "msg": "Could not run 'aten::int_repr' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::int_repr' is only available for these backends: [QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nQuantizedCPU: registered at aten/src/ATen/RegisterQuantizedCPU.cpp:944 [kernel]\nQuantizedCUDA: registered at aten/src/ATen/RegisterQuantizedCUDA.cpp:459 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:153 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:498 [backend fallback]\nFunctionalize: registered at ../aten/src/ATen/FunctionalizeFallbackKernel.cpp:290 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:86 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradMTIA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_0.cpp:16725 [kernel]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:382 [backend fallback]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:249 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:710 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:203 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:161 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:494 [backend fallback]\nPreDispatch: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:165 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:157 [backend fallback]\n"}
4009
Cov: 26010 -> 26010
Cov: 26010 -> 26010
4010
Cov: 26010 -> 26010
Cov: 26010 -> 26010
4011
Cov: 26010 -> 26012
Cov: 26012 -> 26012
4012
Cov: 26012 -> 26013
Cov: 26013 -> 26013
4013
Cov: 26013 -> 26013
Cov: 26013 -> 26013
4014
Cov: 26013 -> 26037
Cov: 26037 -> 26037
4015
Cov: 26037 -> 26037
Cov: 26037 -> 26037
4016
Cov: 26037 -> 26037
Cov: 26037 -> 26037
4017
Cov: 26037 -> 26037
Cov: 26037 -> 26037
4018
Cov: 26037 -> 26038
Cov: 26038 -> 26038
4019
{"exception": "NotImplementedError", "msg": "Could not run 'aten::q_scale' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::q_scale' is only available for these backends: [QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nQuantizedCPU: registered at aten/src/ATen/RegisterQuantizedCPU.cpp:944 [kernel]\nQuantizedCUDA: registered at aten/src/ATen/RegisterQuantizedCUDA.cpp:459 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:153 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:498 [backend fallback]\nFunctionalize: registered at ../aten/src/ATen/FunctionalizeFallbackKernel.cpp:290 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:86 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradMTIA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_4.cpp:13056 [kernel]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:382 [backend fallback]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:249 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:710 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:203 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:161 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:494 [backend fallback]\nPreDispatch: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:165 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:157 [backend fallback]\n"}
4020
Cov: 26038 -> 26039
Cov: 26039 -> 26039
4021
{"exception": "RuntimeError", "msg": "all: You passed a dimname (string) to this op in place of a dimension index but it does not yet support this behavior. Please pass a dimension index to work around this."}
4022
Cov: 26039 -> 26039
Cov: 26039 -> 26039
4023
{"exception": "RuntimeError", "msg": "This function was deprecated since version 1.9 and is now removed. The default behavior has changed from using the upper triangular portion of the matrix by default to using the lower triangular portion.\n\nL, _ = torch.symeig(A, upper=upper) should be replaced with:\nL = torch.linalg.eigvalsh(A, UPLO='U' if upper else 'L')\n\nand\n\nL, V = torch.symeig(A, eigenvectors=True) should be replaced with:\nL, V = torch.linalg.eigh(A, UPLO='U' if upper else 'L')"}
4024
Cov: 26039 -> 26040
Cov: 26040 -> 26040
4025
Cov: 26040 -> 26041
Cov: 26041 -> 26041
4026
Cov: 26041 -> 26041
Cov: 26041 -> 26041
4027
{"exception": "RuntimeError", "msg": "linalg.det: A must be batches of square matrices, but they are 3 by 4 matrices"}
4028
Cov: 26041 -> 26042
Cov: 26042 -> 26042
4029
Cov: 26042 -> 26042
Cov: 26042 -> 26042
4030
Cov: 26042 -> 26042
Cov: 26042 -> 26042
4031
Cov: 26042 -> 26042
Cov: 26042 -> 26042
4032
Cov: 26042 -> 26042
Cov: 26042 -> 26042
4033
Cov: 26042 -> 26042
Cov: 26042 -> 26042
4034
Cov: 26042 -> 26042
Cov: 26042 -> 26042
4035
Cov: 26042 -> 26042
Cov: 26042 -> 26042
4036
Cov: 26042 -> 26043
Cov: 26043 -> 26043
4037
Cov: 26043 -> 26043
Cov: 26043 -> 26043
4038
Cov: 26043 -> 26043
Cov: 26043 -> 26043
4039
Cov: 26043 -> 26043
Cov: 26043 -> 26043
4040
Cov: 26043 -> 26043
Cov: 26043 -> 26043
4041
Cov: 26043 -> 26043
Cov: 26043 -> 26043
4042
Cov: 26043 -> 26043
Cov: 26043 -> 26043
4043
Cov: 26043 -> 26044
Cov: 26044 -> 26044
4044
Cov: 26044 -> 26044
Cov: 26044 -> 26044
4045
Cov: 26044 -> 26044
Cov: 26044 -> 26044
4046
Cov: 26044 -> 26045
Cov: 26045 -> 26045
4047
Cov: 26045 -> 26045
Cov: 26045 -> 26045
4048
Cov: 26045 -> 26045
Cov: 26045 -> 26045
4049
Cov: 26045 -> 26045
Cov: 26045 -> 26045
4050
Cov: 26045 -> 26045
Cov: 26045 -> 26045
4051
Cov: 26045 -> 26045
Cov: 26045 -> 26045
4052
{"exception": "_LinAlgError", "msg": "torch.linalg.solve: The solver failed because the input matrix is singular."}
4053
{"exception": "RuntimeError", "msg": "This function was deprecated since version 1.9 and is now removed. The default behavior has changed from using the upper triangular portion of the matrix by default to using the lower triangular portion.\n\nL, _ = torch.symeig(A, upper=upper) should be replaced with:\nL = torch.linalg.eigvalsh(A, UPLO='U' if upper else 'L')\n\nand\n\nL, V = torch.symeig(A, eigenvectors=True) should be replaced with:\nL, V = torch.linalg.eigh(A, UPLO='U' if upper else 'L')"}
4054
Cov: 26045 -> 26047
Cov: 26047 -> 26047
4055
Cov: 26047 -> 26047
Cov: 26047 -> 26047
4056
Cov: 26047 -> 26047
Cov: 26047 -> 26047
4057
Cov: 26047 -> 26071
Cov: 26071 -> 26071
4058
Cov: 26071 -> 26071
Cov: 26071 -> 26071
4059
Cov: 26071 -> 26072
Cov: 26072 -> 26072
4060
Cov: 26072 -> 26072
Cov: 26072 -> 26072
4061
Cov: 26072 -> 26072
Cov: 26072 -> 26072
4062
Cov: 26072 -> 26072
Cov: 26072 -> 26072
4063
Cov: 26072 -> 26072
Cov: 26072 -> 26072
4064
Cov: 26072 -> 26072
Cov: 26072 -> 26072
4065
Cov: 26072 -> 26072
Cov: 26072 -> 26072
4066
{"exception": "AttributeError", "msg": "type object 'torch.DoubleStorage' has no attribute 'from_list'"}
4067
{"exception": "NotImplementedError", "msg": "Could not run 'aten::int_repr' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::int_repr' is only available for these backends: [QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nQuantizedCPU: registered at aten/src/ATen/RegisterQuantizedCPU.cpp:944 [kernel]\nQuantizedCUDA: registered at aten/src/ATen/RegisterQuantizedCUDA.cpp:459 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:153 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:498 [backend fallback]\nFunctionalize: registered at ../aten/src/ATen/FunctionalizeFallbackKernel.cpp:290 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:86 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradMTIA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_0.cpp:16725 [kernel]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:382 [backend fallback]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:249 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:710 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:203 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:161 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:494 [backend fallback]\nPreDispatch: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:165 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:157 [backend fallback]\n"}
4068
Cov: 26072 -> 26072
Cov: 26072 -> 26072
4069
Cov: 26072 -> 26072
Cov: 26072 -> 26072
4070
Cov: 26072 -> 26072
Cov: 26072 -> 26072
4071
Cov: 26072 -> 26072
Cov: 26072 -> 26072
4072
Cov: 26072 -> 26072
Cov: 26072 -> 26072
4073
Cov: 26072 -> 26072
Cov: 26072 -> 26072
4074
Cov: 26072 -> 26072
Cov: 26072 -> 26072
4075
Cov: 26072 -> 26072
Cov: 26072 -> 26072
4076
Cov: 26072 -> 26072
Cov: 26072 -> 26072
4077
Cov: 26072 -> 26072
Cov: 26072 -> 26072
4078
Cov: 26072 -> 26072
Cov: 26072 -> 26072
4079
Cov: 26072 -> 26072
Cov: 26072 -> 26072
4080
Cov: 26072 -> 26072
Cov: 26072 -> 26072
4081
{"exception": "TypeError", "msg": "index_copy_() received an invalid combination of arguments - got (tensor=Tensor, index=Tensor, dim=int, ), but expected one of:\n * (int dim, Tensor index, Tensor source)\n      didn't match because some of the keywords were incorrect: tensor\n * (name dim, Tensor index, Tensor source)\n      didn't match because some of the keywords were incorrect: tensor\n"}
4082
Cov: 26072 -> 26073
Cov: 26073 -> 26073
4083
{"exception": "RuntimeError", "msg": "masked_scatter_ only supports boolean masks, but got mask with dtype Long"}
4084
Cov: 26073 -> 26073
Cov: 26073 -> 26073
4085
Cov: 26073 -> 26073
Cov: 26073 -> 26073
4086
Cov: 26073 -> 26073
Cov: 26073 -> 26073
4087
Cov: 26073 -> 26073
Cov: 26073 -> 26073
4088
Cov: 26073 -> 26073
Cov: 26073 -> 26073
4089
Cov: 26073 -> 26073
Cov: 26073 -> 26073
4090
Cov: 26073 -> 26074
Cov: 26074 -> 26074
4091
Cov: 26074 -> 26074
Cov: 26074 -> 26074
4092
Cov: 26074 -> 26074
Cov: 26074 -> 26074
4093
Cov: 26074 -> 26074
Cov: 26074 -> 26074
4094
Cov: 26074 -> 26074
Cov: 26074 -> 26074
4095
Cov: 26074 -> 26074
Cov: 26074 -> 26074
4096
Cov: 26074 -> 26074
Cov: 26074 -> 26074
4097
Cov: 26074 -> 26075
Cov: 26075 -> 26075
4098
Cov: 26075 -> 26075
Cov: 26075 -> 26075
4099
Cov: 26075 -> 26075
Cov: 26075 -> 26075
4100
{"exception": "RuntimeError", "msg": "addr: Expected 1-D argument vec1, but got 2-D"}
4101
Cov: 26075 -> 26075
Cov: 26075 -> 26075
4102
Cov: 26075 -> 26075
Cov: 26075 -> 26075
4103
Cov: 26075 -> 26075
Cov: 26075 -> 26075
4104
Cov: 26075 -> 26075
Cov: 26075 -> 26075
4105
Cov: 26075 -> 26075
Cov: 26075 -> 26075
4106
Cov: 26075 -> 26076
Cov: 26076 -> 26076
4107
Cov: 26076 -> 26076
Cov: 26076 -> 26076
4108
Cov: 26076 -> 26076
Cov: 26076 -> 26076
4109
Cov: 26076 -> 26077
Cov: 26077 -> 26077
4110
Cov: 26077 -> 26078
Cov: 26078 -> 26078
4111
{"exception": "RuntimeError", "msg": "This function was deprecated since version 1.9 and is now removed. `torch.linalg.eig` returns complex tensors of dtype `cfloat` or `cdouble` rather than real tensors mimicking complex tensors.\n\nL, _ = torch.eig(A) should be replaced with:\nL_complex = torch.linalg.eigvals(A)\n\nand\n\nL, V = torch.eig(A, eigenvectors=True) should be replaced with:\nL_complex, V_complex = torch.linalg.eig(A)"}
4112
Cov: 26078 -> 26079
Cov: 26079 -> 26079
4113
Cov: 26079 -> 26080
Cov: 26080 -> 26080
4114
Cov: 26080 -> 26080
Cov: 26080 -> 26080
4115
Cov: 26080 -> 26080
Cov: 26080 -> 26080
4116
Cov: 26080 -> 26080
Cov: 26080 -> 26080
4117
Cov: 26080 -> 26080
Cov: 26080 -> 26080
4118
Cov: 26080 -> 26081
Cov: 26081 -> 26081
4119
Cov: 26081 -> 26081
Cov: 26081 -> 26081
4120
{"exception": "RuntimeError", "msg": "This function was deprecated since version 1.9 and is now removed. The default behavior has changed from using the upper triangular portion of the matrix by default to using the lower triangular portion.\n\nL, _ = torch.symeig(A, upper=upper) should be replaced with:\nL = torch.linalg.eigvalsh(A, UPLO='U' if upper else 'L')\n\nand\n\nL, V = torch.symeig(A, eigenvectors=True) should be replaced with:\nL, V = torch.linalg.eigh(A, UPLO='U' if upper else 'L')"}
4121
Cov: 26081 -> 26082
Cov: 26082 -> 26082
4122
Cov: 26082 -> 26082
Cov: 26082 -> 26082
4123
Cov: 26082 -> 26082
Cov: 26082 -> 26082
4124
Cov: 26082 -> 26082
Cov: 26082 -> 26082
4125
Cov: 26082 -> 26082
Cov: 26082 -> 26082
4126
Cov: 26082 -> 26082
Cov: 26082 -> 26082
4127
Cov: 26082 -> 26082
Cov: 26082 -> 26082
4128
{"exception": "RuntimeError", "msg": "\"hypot_cpu\" not implemented for 'Long'"}
4129
Cov: 26082 -> 26083
Cov: 26083 -> 26083
4130
Cov: 26083 -> 26084
Cov: 26084 -> 26084
4131
Cov: 26084 -> 26084
Cov: 26084 -> 26084
4132
Cov: 26084 -> 26084
Cov: 26084 -> 26084
4133
Cov: 26084 -> 26084
Cov: 26084 -> 26084
4134
Cov: 26084 -> 26084
Cov: 26084 -> 26084
4135
Cov: 26084 -> 26084
Cov: 26084 -> 26084
4136
{"exception": "TypeError", "msg": "type_as(): argument 'other' (position 1) must be Tensor, not torch.tensortype"}
4137
Cov: 26084 -> 26087
Cov: 26087 -> 26087
4138
Cov: 26087 -> 26087
Cov: 26087 -> 26087
4139
Cov: 26087 -> 26087
Cov: 26087 -> 26087
4140
Cov: 26087 -> 26087
Cov: 26087 -> 26087
4141
Cov: 26087 -> 26087
Cov: 26087 -> 26087
4142
Cov: 26087 -> 26087
Cov: 26087 -> 26087
4143
Cov: 26087 -> 26087
Cov: 26087 -> 26087
4144
Cov: 26087 -> 26088
Cov: 26088 -> 26088
4145
Cov: 26088 -> 26088
Cov: 26088 -> 26088
4146
Cov: 26088 -> 26089
Cov: 26089 -> 26089
4147
Cov: 26089 -> 26090
Cov: 26090 -> 26090
4148
Cov: 26090 -> 26090
Cov: 26090 -> 26090
4149
Cov: 26090 -> 26090
Cov: 26090 -> 26090
4150
Cov: 26090 -> 26090
Cov: 26090 -> 26090
4151
Cov: 26090 -> 26090
Cov: 26090 -> 26090
4152
Cov: 26090 -> 26090
Cov: 26090 -> 26090
4153
Cov: 26090 -> 26090
Cov: 26090 -> 26090
4154
Cov: 26090 -> 26090
Cov: 26090 -> 26090
4155
Cov: 26090 -> 26090
Cov: 26090 -> 26090
4156
Cov: 26090 -> 26092
Cov: 26092 -> 26092
4157
Cov: 26092 -> 26103
Cov: 26103 -> 26103
4158
Cov: 26103 -> 26103
Cov: 26103 -> 26103
4159
Cov: 26103 -> 26103
Cov: 26103 -> 26103
4160
Cov: 26103 -> 26103
Cov: 26103 -> 26103
4161
Cov: 26103 -> 26103
Cov: 26103 -> 26103
4162
Cov: 26103 -> 26103
Cov: 26103 -> 26103
4163
Cov: 26103 -> 26103
Cov: 26103 -> 26103
4164
Cov: 26103 -> 26103
Cov: 26103 -> 26103
4165
{"exception": "RuntimeError", "msg": "The expanded size of the tensor (2) must match the existing size (3) at non-singleton dimension 1.  Target sizes: [2, 2].  Tensor sizes: [2, 3]"}
4166
Cov: 26103 -> 26103
Cov: 26103 -> 26103
4167
Cov: 26103 -> 26103
Cov: 26103 -> 26103
4168
Cov: 26103 -> 26104
Cov: 26104 -> 26104
4169
Cov: 26104 -> 26104
Cov: 26104 -> 26104
4170
{"exception": "NotImplementedError", "msg": "Could not run 'aten::q_zero_point' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::q_zero_point' is only available for these backends: [QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nQuantizedCPU: registered at aten/src/ATen/RegisterQuantizedCPU.cpp:944 [kernel]\nQuantizedCUDA: registered at aten/src/ATen/RegisterQuantizedCUDA.cpp:459 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:153 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:498 [backend fallback]\nFunctionalize: registered at ../aten/src/ATen/FunctionalizeFallbackKernel.cpp:290 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:86 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradMTIA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_3.cpp:14610 [kernel]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:382 [backend fallback]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:249 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:710 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:203 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:161 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:494 [backend fallback]\nPreDispatch: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:165 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:157 [backend fallback]\n"}
4171
{"exception": "_LinAlgError", "msg": "linalg.inv: The diagonal element 3 is zero, the inversion could not be completed because the input matrix is singular."}
4172
Cov: 26104 -> 26107
Cov: 26107 -> 26107
4173
Cov: 26107 -> 26107
Cov: 26107 -> 26107
4174
Cov: 26107 -> 26108
Cov: 26108 -> 26108
4175
Cov: 26108 -> 26108
Cov: 26108 -> 26108
4176
Cov: 26108 -> 26108
Cov: 26108 -> 26108
4177
{"exception": "RuntimeError", "msg": "scatter(): Expected self.dtype to be equal to src.dtype"}
4178
Cov: 26108 -> 26109
Cov: 26109 -> 26109
4179
{"exception": "RuntimeError", "msg": "1D tensors expected, but got 2D and 2D tensors"}
4180
Cov: 26109 -> 26109
Cov: 26109 -> 26109
4181
Cov: 26109 -> 26110
Cov: 26110 -> 26110
4182
Cov: 26110 -> 26110
Cov: 26110 -> 26110
4183
Cov: 26110 -> 26110
Cov: 26110 -> 26110
4184
Cov: 26110 -> 26111
Cov: 26111 -> 26111
4185
Cov: 26111 -> 26111
Cov: 26111 -> 26111
4186
Cov: 26111 -> 26111
Cov: 26111 -> 26111
4187
Cov: 26111 -> 26111
Cov: 26111 -> 26111
4188
Cov: 26111 -> 26111
Cov: 26111 -> 26111
4189
Cov: 26111 -> 26111
Cov: 26111 -> 26111
4190
Cov: 26111 -> 26111
Cov: 26111 -> 26111
4191
Cov: 26111 -> 26111
Cov: 26111 -> 26111
4192
Cov: 26111 -> 26112
Cov: 26112 -> 26112
4193
Cov: 26112 -> 26112
Cov: 26112 -> 26112
4194
Cov: 26112 -> 26112
Cov: 26112 -> 26112
4195
Cov: 26112 -> 26112
Cov: 26112 -> 26112
4196
{"exception": "_LinAlgError", "msg": "cholesky: The factorization could not be completed because the input is not positive-definite (the leading minor of order 1 is not positive-definite)."}
4197
Cov: 26112 -> 26112
Cov: 26112 -> 26112
4198
Cov: 26112 -> 26112
Cov: 26112 -> 26112
4199
Cov: 26112 -> 26112
Cov: 26112 -> 26112
4200
Cov: 26112 -> 26114
Cov: 26114 -> 26114
4201
Cov: 26114 -> 26114
Cov: 26114 -> 26114
4202
Cov: 26114 -> 26115
Cov: 26115 -> 26115
4203
Cov: 26115 -> 26115
Cov: 26115 -> 26115
4204
Cov: 26115 -> 26115
Cov: 26115 -> 26115
4205
{"exception": "NameError", "msg": "name 'F' is not defined"}
4206
Cov: 26115 -> 26115
Cov: 26115 -> 26115
4207
{"exception": "RuntimeError", "msg": "istft requires a complex-valued input tensor matching the output from stft with return_complex=True."}
4208
Cov: 26115 -> 26116
Cov: 26116 -> 26116
4209
Cov: 26116 -> 26121
Cov: 26121 -> 26121
4210
Cov: 26121 -> 26121
Cov: 26121 -> 26121
4211
Cov: 26121 -> 26121
Cov: 26121 -> 26121
4212
Cov: 26121 -> 26121
Cov: 26121 -> 26121
4213
{"exception": "RuntimeError", "msg": "torch.ormqr: Expected tau to have one dimension less than input, but got tau.ndim equal to 2 and input.ndim is equal to 2"}
4214
Cov: 26121 -> 26121
Cov: 26121 -> 26121
4215
Cov: 26121 -> 26121
Cov: 26121 -> 26121
4216
{"exception": "RuntimeError", "msg": "\"triangular_solve_cpu\" not implemented for 'Long'"}
4217
Cov: 26121 -> 26121
Cov: 26121 -> 26121
4218
Cov: 26121 -> 26121
Cov: 26121 -> 26121
4219
Cov: 26121 -> 26121
Cov: 26121 -> 26121
4220
Cov: 26121 -> 26121
Cov: 26121 -> 26121
4221
Cov: 26121 -> 26122
Cov: 26122 -> 26122
4222
Cov: 26122 -> 26123
Cov: 26123 -> 26123
4223
Cov: 26123 -> 26123
Cov: 26123 -> 26123
4224
{"exception": "RuntimeError", "msg": "This function was deprecated since version 1.9 and is now removed. `torch.lstsq` is deprecated in favor of `torch.linalg.lstsq`.\n`torch.linalg.lstsq` has reversed arguments and does not return the QR decomposition in the returned tuple (although it returns other information about the problem).\n\nTo get the QR decomposition consider using `torch.linalg.qr`.\n\nThe returned solution in `torch.lstsq` stored the residuals of the solution in the last m - n columns of the returned value whenever m > n. In torch.linalg.lstsq, the residuals are in the field 'residuals' of the returned named tuple.\n\nThe unpacking of the solution, as in\nX, _ = torch.lstsq(B, A).solution[:A.size(1)]\nshould be replaced with:\nX = torch.linalg.lstsq(A, B).solution"}
4225
Cov: 26123 -> 26123
Cov: 26123 -> 26123
4226
Cov: 26123 -> 26123
Cov: 26123 -> 26123
4227
Cov: 26123 -> 26125
Cov: 26125 -> 26125
4228
Cov: 26125 -> 26125
Cov: 26125 -> 26125
4229
Cov: 26125 -> 26126
Cov: 26126 -> 26126
4230
Cov: 26126 -> 26126
Cov: 26126 -> 26126
4231
Cov: 26126 -> 26126
Cov: 26126 -> 26126
4232
Cov: 26126 -> 26126
Cov: 26126 -> 26126
4233
{"exception": "TypeError", "msg": "type_as(): argument 'other' (position 1) must be Tensor, not torch.dtype"}
4234
Cov: 26126 -> 26126
Cov: 26126 -> 26126
4235
Cov: 26126 -> 26126
Cov: 26126 -> 26126
4236
Cov: 26126 -> 26127
Cov: 26127 -> 26127
4237
Cov: 26127 -> 26234
Cov: 26234 -> 26234
4238
Cov: 26234 -> 26234
Cov: 26234 -> 26234
4239
Cov: 26234 -> 26234
Cov: 26234 -> 26234
4240
{"exception": "RuntimeError", "msg": "tensor.sspaddmm(...) can only be called on sparse tensors"}
4241
Cov: 26234 -> 26236
Cov: 26236 -> 26236
4242
Cov: 26236 -> 26236
Cov: 26236 -> 26236
4243
Cov: 26236 -> 26236
Cov: 26236 -> 26236
4244
Cov: 26236 -> 26236
Cov: 26236 -> 26236
4245
Cov: 26236 -> 26236
Cov: 26236 -> 26236
4246
Cov: 26236 -> 26237
Cov: 26237 -> 26237
4247
Cov: 26237 -> 26238
Cov: 26238 -> 26238
4248
Cov: 26238 -> 26238
Cov: 26238 -> 26238
4249
Cov: 26238 -> 26238
Cov: 26238 -> 26238
4250
Cov: 26238 -> 26238
Cov: 26238 -> 26238
4251
Cov: 26238 -> 26238
Cov: 26238 -> 26238
4252
Cov: 26238 -> 26239
Cov: 26239 -> 26239
4253
Cov: 26239 -> 26239
Cov: 26239 -> 26239
4254
Cov: 26239 -> 26240
Cov: 26240 -> 26240
4255
Cov: 26240 -> 26240
Cov: 26240 -> 26240
4256
{"exception": "RuntimeError", "msg": "Creating a Tensor subclass from a class that does not inherit from Tensor is not possible. Make sure your class inherits from Tensor."}
4257
Cov: 26240 -> 26240
Cov: 26240 -> 26240
4258
Cov: 26240 -> 26240
Cov: 26240 -> 26240
4259
Cov: 26240 -> 26240
Cov: 26240 -> 26240
4260
Cov: 26240 -> 26241
Cov: 26241 -> 26241
4261
Cov: 26241 -> 26242
Cov: 26242 -> 26242
4262
Cov: 26242 -> 26242
Cov: 26242 -> 26242
4263
Cov: 26242 -> 26242
Cov: 26242 -> 26242
4264
Cov: 26242 -> 26242
Cov: 26242 -> 26242
4265
Cov: 26242 -> 26242
Cov: 26242 -> 26242
4266
Cov: 26242 -> 26243
Cov: 26243 -> 26243
4267
Cov: 26243 -> 26244
Cov: 26244 -> 26244
4268
{"exception": "RuntimeError", "msg": "masked_scatter_ only supports boolean masks, but got mask with dtype Long"}
4269
Cov: 26244 -> 26244
Cov: 26244 -> 26244
4270
{"exception": "RuntimeError", "msg": "Index tensor must have the same number of dimensions as self tensor"}
4271
{"exception": "NotImplementedError", "msg": "Could not run 'aten::q_scale' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::q_scale' is only available for these backends: [QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nQuantizedCPU: registered at aten/src/ATen/RegisterQuantizedCPU.cpp:944 [kernel]\nQuantizedCUDA: registered at aten/src/ATen/RegisterQuantizedCUDA.cpp:459 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:153 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:498 [backend fallback]\nFunctionalize: registered at ../aten/src/ATen/FunctionalizeFallbackKernel.cpp:290 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:86 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradMTIA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_4.cpp:13056 [kernel]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:382 [backend fallback]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:249 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:710 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:203 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:161 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:494 [backend fallback]\nPreDispatch: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:165 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:157 [backend fallback]\n"}
4272
{"exception": "RuntimeError", "msg": "result type Double can't be cast to the desired output type Long"}
4273
Cov: 26244 -> 26245
Cov: 26245 -> 26245
4274
Cov: 26245 -> 26245
Cov: 26245 -> 26245
4275
Cov: 26245 -> 26245
Cov: 26245 -> 26245
4276
Cov: 26245 -> 26245
Cov: 26245 -> 26245
4277
Cov: 26245 -> 26245
Cov: 26245 -> 26245
4278
Cov: 26245 -> 26245
Cov: 26245 -> 26245
4279
Cov: 26245 -> 26245
Cov: 26245 -> 26245
4280
Cov: 26245 -> 26245
Cov: 26245 -> 26245
4281
Cov: 26245 -> 26245
Cov: 26245 -> 26245
4282
Cov: 26245 -> 26245
Cov: 26245 -> 26245
4283
Cov: 26245 -> 26245
Cov: 26245 -> 26245
4284
Cov: 26245 -> 26245
Cov: 26245 -> 26245
4285
Cov: 26245 -> 26246
Cov: 26246 -> 26246
4286
Cov: 26246 -> 26246
Cov: 26246 -> 26246
4287
Cov: 26246 -> 26246
Cov: 26246 -> 26246
4288
Cov: 26246 -> 26246
Cov: 26246 -> 26246
4289
Cov: 26246 -> 26246
Cov: 26246 -> 26246
4290
Cov: 26246 -> 26246
Cov: 26246 -> 26246
4291
Cov: 26246 -> 26246
Cov: 26246 -> 26246
4292
Cov: 26246 -> 26246
Cov: 26246 -> 26246
4293
Cov: 26246 -> 26246
Cov: 26246 -> 26246
4294
{"exception": "RuntimeError", "msg": "stft requires the return_complex parameter be given for real inputs, and will further require that return_complex=True in a future PyTorch release."}
4295
Cov: 26246 -> 26258
Cov: 26258 -> 26258
4296
Cov: 26258 -> 26258
Cov: 26258 -> 26258
4297
Cov: 26258 -> 26258
Cov: 26258 -> 26258
4298
Cov: 26258 -> 26258
Cov: 26258 -> 26258
4299
Cov: 26258 -> 26258
Cov: 26258 -> 26258
4300
Cov: 26258 -> 26258
Cov: 26258 -> 26258
4301
Cov: 26258 -> 26258
Cov: 26258 -> 26258
4302
Cov: 26258 -> 26258
Cov: 26258 -> 26258
4303
Cov: 26258 -> 26258
Cov: 26258 -> 26258
4304
{"exception": "RuntimeError", "msg": "The expanded size of the tensor (4) must match the existing size (3) at non-singleton dimension 2.  Target sizes: [2, 3, 4].  Tensor sizes: [2, 3]"}
4305
Cov: 26258 -> 26258
Cov: 26258 -> 26258
4306
{"exception": "RuntimeError", "msg": "expand(torch.DoubleTensor{[4, 3, 2]}, size=[3, 3]): the number of sizes provided (2) must be greater or equal to the number of dimensions in the tensor (3)"}
4307
Cov: 26258 -> 26258
Cov: 26258 -> 26258
4308
Cov: 26258 -> 26258
Cov: 26258 -> 26258
4309
Cov: 26258 -> 26258
Cov: 26258 -> 26258
4310
Cov: 26258 -> 26258
Cov: 26258 -> 26258
4311
Cov: 26258 -> 26258
Cov: 26258 -> 26258
4312
Cov: 26258 -> 26259
Cov: 26259 -> 26259
4313
{"exception": "RuntimeError", "msg": "masked_scatter: expected self and source to have same dtypes but gotDouble and Long"}
4314
Cov: 26259 -> 26259
Cov: 26259 -> 26259
4315
Cov: 26259 -> 26259
Cov: 26259 -> 26259
4316
Cov: 26259 -> 26259
Cov: 26259 -> 26259
4317
Cov: 26259 -> 26259
Cov: 26259 -> 26259
4318
Cov: 26259 -> 26259
Cov: 26259 -> 26259
4319
Cov: 26259 -> 26259
Cov: 26259 -> 26259
4320
Cov: 26259 -> 26259
Cov: 26259 -> 26259
4321
{"exception": "NotImplementedError", "msg": "Could not run 'aten::qscheme' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::qscheme' is only available for these backends: [QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nQuantizedCPU: registered at aten/src/ATen/RegisterQuantizedCPU.cpp:944 [kernel]\nQuantizedCUDA: registered at aten/src/ATen/RegisterQuantizedCUDA.cpp:459 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:153 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:498 [backend fallback]\nFunctionalize: registered at ../aten/src/ATen/FunctionalizeFallbackKernel.cpp:290 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:86 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradMTIA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_0.cpp:16725 [kernel]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:382 [backend fallback]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:249 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:710 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:203 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:161 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:494 [backend fallback]\nPreDispatch: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:165 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:157 [backend fallback]\n"}
4322
Cov: 26259 -> 26259
Cov: 26259 -> 26259
4323
Cov: 26259 -> 26259
Cov: 26259 -> 26259
4324
Cov: 26259 -> 26259
Cov: 26259 -> 26259
4325
Cov: 26259 -> 26260
Cov: 26260 -> 26260
4326
Cov: 26260 -> 26260
Cov: 26260 -> 26260
4327
Cov: 26260 -> 26260
Cov: 26260 -> 26260
4328
Cov: 26260 -> 26260
Cov: 26260 -> 26260
4329
Cov: 26260 -> 26260
Cov: 26260 -> 26260
4330
Cov: 26260 -> 26260
Cov: 26260 -> 26260
4331
Cov: 26260 -> 26260
Cov: 26260 -> 26260
4332
Cov: 26260 -> 26262
Cov: 26262 -> 26262
4333
Cov: 26262 -> 26262
Cov: 26262 -> 26262
4334
Cov: 26262 -> 26262
Cov: 26262 -> 26262
4335
Cov: 26262 -> 26262
Cov: 26262 -> 26262
4336
Cov: 26262 -> 26262
Cov: 26262 -> 26262
4337
Cov: 26262 -> 26262
Cov: 26262 -> 26262
4338
Cov: 26262 -> 26262
Cov: 26262 -> 26262
4339
Cov: 26262 -> 26262
Cov: 26262 -> 26262
4340
Cov: 26262 -> 26262
Cov: 26262 -> 26262
4341
Cov: 26262 -> 26262
Cov: 26262 -> 26262
4342
Cov: 26262 -> 26262
Cov: 26262 -> 26262
4343
Cov: 26262 -> 26262
Cov: 26262 -> 26262
4344
Cov: 26262 -> 26263
Cov: 26263 -> 26263
4345
Cov: 26263 -> 26263
Cov: 26263 -> 26263
4346
Cov: 26263 -> 26264
Cov: 26264 -> 26264
4347
Cov: 26264 -> 26264
Cov: 26264 -> 26264
4348
Cov: 26264 -> 26264
Cov: 26264 -> 26264
4349
Cov: 26264 -> 26264
Cov: 26264 -> 26264
4350
Cov: 26264 -> 26264
Cov: 26264 -> 26264
4351
Cov: 26264 -> 26264
Cov: 26264 -> 26264
4352
Cov: 26264 -> 26264
Cov: 26264 -> 26264
4353
Cov: 26264 -> 26264
Cov: 26264 -> 26264
4354
Cov: 26264 -> 26264
Cov: 26264 -> 26264
4355
Cov: 26264 -> 26265
Cov: 26265 -> 26265
4356
Cov: 26265 -> 26266
Cov: 26266 -> 26266
4357
Cov: 26266 -> 26278
Cov: 26278 -> 26278
4358
Cov: 26278 -> 26278
Cov: 26278 -> 26278
4359
Cov: 26278 -> 26279
Cov: 26279 -> 26279
4360
Cov: 26279 -> 26279
Cov: 26279 -> 26279
4361
Cov: 26279 -> 26280
Cov: 26280 -> 26280
4362
Cov: 26280 -> 26280
Cov: 26280 -> 26280
4363
Cov: 26280 -> 26280
Cov: 26280 -> 26280
4364
Cov: 26280 -> 26280
Cov: 26280 -> 26280
4365
Cov: 26280 -> 26280
Cov: 26280 -> 26280
4366
Cov: 26280 -> 26280
Cov: 26280 -> 26280
4367
Cov: 26280 -> 26280
Cov: 26280 -> 26280
4368
Cov: 26280 -> 26288
Cov: 26288 -> 26288
4369
{"exception": "RuntimeError", "msg": "vector + matrix @ vector expected, got 2, 2, 1"}
4370
Cov: 26288 -> 26288
Cov: 26288 -> 26288
4371
Cov: 26288 -> 26289
Cov: 26289 -> 26289
4372
Cov: 26289 -> 26289
Cov: 26289 -> 26289
4373
Cov: 26289 -> 26289
Cov: 26289 -> 26289
4374
Cov: 26289 -> 26289
Cov: 26289 -> 26289
4375
Cov: 26289 -> 26289
Cov: 26289 -> 26289
4376
Cov: 26289 -> 26289
Cov: 26289 -> 26289
4377
Cov: 26289 -> 26291
Cov: 26291 -> 26291
4378
Cov: 26291 -> 26291
Cov: 26291 -> 26291
4379
Cov: 26291 -> 26291
Cov: 26291 -> 26291
4380
Cov: 26291 -> 26291
Cov: 26291 -> 26291
4381
Cov: 26291 -> 26292
Cov: 26292 -> 26292
4382
Cov: 26292 -> 26292
Cov: 26292 -> 26292
4383
Cov: 26292 -> 26292
Cov: 26292 -> 26292
4384
Cov: 26292 -> 26292
Cov: 26292 -> 26292
4385
Cov: 26292 -> 26292
Cov: 26292 -> 26292
4386
Cov: 26292 -> 26292
Cov: 26292 -> 26292
4387
Cov: 26292 -> 26292
Cov: 26292 -> 26292
4388
Cov: 26292 -> 26292
Cov: 26292 -> 26292
4389
Cov: 26292 -> 26292
Cov: 26292 -> 26292
4390
Cov: 26292 -> 26293
Cov: 26293 -> 26293
4391
Cov: 26293 -> 26293
Cov: 26293 -> 26293
4392
Cov: 26293 -> 26293
Cov: 26293 -> 26293
4393
Cov: 26293 -> 26294
Cov: 26294 -> 26294
4394
Cov: 26294 -> 26294
Cov: 26294 -> 26294
4395
Cov: 26294 -> 26295
Cov: 26295 -> 26295
4396
Cov: 26295 -> 26295
Cov: 26295 -> 26295
4397
Cov: 26295 -> 26295
Cov: 26295 -> 26295
4398
Cov: 26295 -> 26295
Cov: 26295 -> 26295
4399
Cov: 26295 -> 26295
Cov: 26295 -> 26295
4400
Cov: 26295 -> 26295
Cov: 26295 -> 26295
4401
Cov: 26295 -> 26296
Cov: 26296 -> 26296
4402
Cov: 26296 -> 26296
Cov: 26296 -> 26296
4403
Cov: 26296 -> 26296
Cov: 26296 -> 26296
4404
Cov: 26296 -> 26296
Cov: 26296 -> 26296
4405
Cov: 26296 -> 26297
Cov: 26297 -> 26297
4406
Cov: 26297 -> 26297
Cov: 26297 -> 26297
4407
Cov: 26297 -> 26298
Cov: 26298 -> 26298
4408
Cov: 26298 -> 26300
Cov: 26300 -> 26300
4409
Cov: 26300 -> 26300
Cov: 26300 -> 26300
4410
Cov: 26300 -> 26301
Cov: 26301 -> 26301
4411
Cov: 26301 -> 26301
Cov: 26301 -> 26301
4412
Cov: 26301 -> 26301
Cov: 26301 -> 26301
4413
Cov: 26301 -> 26302
Cov: 26302 -> 26302
4414
Cov: 26302 -> 26302
Cov: 26302 -> 26302
4415
Cov: 26302 -> 26303
Cov: 26303 -> 26303
4416
{"exception": "RuntimeError", "msg": "element 0 of tensors does not require grad and does not have a grad_fn"}
4417
Cov: 26303 -> 26305
Cov: 26305 -> 26305
4418
Cov: 26305 -> 26305
Cov: 26305 -> 26305
4419
Cov: 26305 -> 26305
Cov: 26305 -> 26305
4420
Cov: 26305 -> 26306
Cov: 26306 -> 26306
4421
Cov: 26306 -> 26306
Cov: 26306 -> 26306
4422
Cov: 26306 -> 26307
Cov: 26307 -> 26307
4423
Cov: 26307 -> 26308
Cov: 26308 -> 26308
4424
{"exception": "RuntimeError", "msg": "expand(torch.FloatTensor{[10, 3, 4]}, size=[3, 5]): the number of sizes provided (2) must be greater or equal to the number of dimensions in the tensor (3)"}
4425
Cov: 26308 -> 26309
Cov: 26309 -> 26309
4426
{"exception": "AssertionError", "msg": "d must require grad"}
4427
Cov: 26309 -> 26310
Cov: 26310 -> 26310
4428
Cov: 26310 -> 26310
Cov: 26310 -> 26310
4429
Cov: 26310 -> 26310
Cov: 26310 -> 26310
4430
Cov: 26310 -> 26310
Cov: 26310 -> 26310
4431
Cov: 26310 -> 26310
Cov: 26310 -> 26310
4432
Cov: 26310 -> 26310
Cov: 26310 -> 26310
4433
Cov: 26310 -> 26310
Cov: 26310 -> 26310
4434
Cov: 26310 -> 26315
Cov: 26315 -> 26315
4435
Cov: 26315 -> 26315
Cov: 26315 -> 26315
4436
Cov: 26315 -> 26315
Cov: 26315 -> 26315
4437
Cov: 26315 -> 26315
Cov: 26315 -> 26315
4438
Cov: 26315 -> 26315
Cov: 26315 -> 26315
4439
Cov: 26315 -> 26315
Cov: 26315 -> 26315
4440
Cov: 26315 -> 26316
Cov: 26316 -> 26316
4441
{"exception": "RuntimeError", "msg": "addr: Expected 1-D argument vec1, but got 2-D"}
4442
Cov: 26316 -> 26316
Cov: 26316 -> 26316
4443
Cov: 26316 -> 26316
Cov: 26316 -> 26316
4444
Cov: 26316 -> 26330
Cov: 26330 -> 26330
4445
Cov: 26330 -> 26330
Cov: 26330 -> 26330
4446
{"exception": "RuntimeError", "msg": "expand(torch.FloatTensor{[10, 3, 4]}, size=[3, 5]): the number of sizes provided (2) must be greater or equal to the number of dimensions in the tensor (3)"}
4447
Cov: 26330 -> 26331
Cov: 26331 -> 26331
4448
Cov: 26331 -> 26331
Cov: 26331 -> 26331
4449
Cov: 26331 -> 26331
Cov: 26331 -> 26331
4450
Cov: 26331 -> 26331
Cov: 26331 -> 26331
4451
Cov: 26331 -> 26331
Cov: 26331 -> 26331
4452
Cov: 26331 -> 26331
Cov: 26331 -> 26331
4453
Cov: 26331 -> 26331
Cov: 26331 -> 26331
4454
Cov: 26331 -> 26333
Cov: 26333 -> 26333
4455
Cov: 26333 -> 26334
Cov: 26334 -> 26334
4456
Cov: 26334 -> 26334
Cov: 26334 -> 26334
4457
Cov: 26334 -> 26334
Cov: 26334 -> 26334
4458
{"exception": "RuntimeError", "msg": "This function was deprecated since version 1.9 and is now removed. `torch.linalg.eig` returns complex tensors of dtype `cfloat` or `cdouble` rather than real tensors mimicking complex tensors.\n\nL, _ = torch.eig(A) should be replaced with:\nL_complex = torch.linalg.eigvals(A)\n\nand\n\nL, V = torch.eig(A, eigenvectors=True) should be replaced with:\nL_complex, V_complex = torch.linalg.eig(A)"}
4459
Cov: 26334 -> 26335
Cov: 26335 -> 26335
4460
Cov: 26335 -> 26335
Cov: 26335 -> 26335
4461
Cov: 26335 -> 26335
Cov: 26335 -> 26335
4462
Cov: 26335 -> 26335
Cov: 26335 -> 26335
4463
Cov: 26335 -> 26335
Cov: 26335 -> 26335
4464
Cov: 26335 -> 26335
Cov: 26335 -> 26335
4465
Cov: 26335 -> 26336
Cov: 26336 -> 26336
4466
Cov: 26336 -> 26336
Cov: 26336 -> 26336
4467
Cov: 26336 -> 26336
Cov: 26336 -> 26336
4468
Cov: 26336 -> 26336
Cov: 26336 -> 26336
4469
Cov: 26336 -> 26336
Cov: 26336 -> 26336
4470
Cov: 26336 -> 26337
Cov: 26337 -> 26337
4471
Cov: 26337 -> 26337
Cov: 26337 -> 26337
4472
Cov: 26337 -> 26337
Cov: 26337 -> 26337
4473
Cov: 26337 -> 26337
Cov: 26337 -> 26337
4474
Cov: 26337 -> 26342
Cov: 26342 -> 26342
4475
Cov: 26342 -> 26342
Cov: 26342 -> 26342
4476
Cov: 26342 -> 26342
Cov: 26342 -> 26342
4477
Cov: 26342 -> 26342
Cov: 26342 -> 26342
4478
Cov: 26342 -> 26343
Cov: 26343 -> 26343
4479
Cov: 26343 -> 26344
Cov: 26344 -> 26344
4480
Cov: 26344 -> 26344
Cov: 26344 -> 26344
4481
Cov: 26344 -> 26344
Cov: 26344 -> 26344
4482
Cov: 26344 -> 26344
Cov: 26344 -> 26344
4483
Cov: 26344 -> 26344
Cov: 26344 -> 26344
4484
Cov: 26344 -> 26344
Cov: 26344 -> 26344
4485
{"exception": "RuntimeError", "msg": "\"hypot_cpu\" not implemented for 'Long'"}
4486
Cov: 26344 -> 26344
Cov: 26344 -> 26344
4487
Cov: 26344 -> 26344
Cov: 26344 -> 26344
4488
Cov: 26344 -> 26344
Cov: 26344 -> 26344
4489
Cov: 26344 -> 26344
Cov: 26344 -> 26344
4490
Cov: 26344 -> 26347
Cov: 26347 -> 26347
4491
Cov: 26347 -> 26354
Cov: 26354 -> 26354
4492
Cov: 26354 -> 26354
Cov: 26354 -> 26354
4493
Cov: 26354 -> 26354
Cov: 26354 -> 26354
4494
Cov: 26354 -> 26354
Cov: 26354 -> 26354
4495
Cov: 26354 -> 26354
Cov: 26354 -> 26354
4496
{"exception": "RuntimeError", "msg": "This function was deprecated since version 1.9 and is now removed. The default behavior has changed from using the upper triangular portion of the matrix by default to using the lower triangular portion.\n\nL, _ = torch.symeig(A, upper=upper) should be replaced with:\nL = torch.linalg.eigvalsh(A, UPLO='U' if upper else 'L')\n\nand\n\nL, V = torch.symeig(A, eigenvectors=True) should be replaced with:\nL, V = torch.linalg.eigh(A, UPLO='U' if upper else 'L')"}
4497
Cov: 26354 -> 26654
Cov: 26654 -> 26654
4498
Cov: 26654 -> 26654
Cov: 26654 -> 26654
4499
Cov: 26654 -> 26654
Cov: 26654 -> 26654
4500
Cov: 26654 -> 26654
Cov: 26654 -> 26654
4501
{"exception": "RuntimeError", "msg": "any: You passed a dimname (string) to this op in place of a dimension index but it does not yet support this behavior. Please pass a dimension index to work around this."}
4502
Cov: 26654 -> 26654
Cov: 26654 -> 26654
4503
Cov: 26654 -> 26655
Cov: 26655 -> 26655
4504
Cov: 26655 -> 26655
Cov: 26655 -> 26655
4505
Cov: 26655 -> 26655
Cov: 26655 -> 26655
4506
Cov: 26655 -> 26655
Cov: 26655 -> 26655
4507
Cov: 26655 -> 26655
Cov: 26655 -> 26655
4508
Cov: 26655 -> 26656
Cov: 26656 -> 26656
4509
Cov: 26656 -> 26656
Cov: 26656 -> 26656
4510
Cov: 26656 -> 26656
Cov: 26656 -> 26656
4511
Cov: 26656 -> 26657
Cov: 26657 -> 26657
4512
Cov: 26657 -> 26657
Cov: 26657 -> 26657
4513
Cov: 26657 -> 26658
Cov: 26658 -> 26658
4514
Cov: 26658 -> 26658
Cov: 26658 -> 26658
4515
Cov: 26658 -> 26658
Cov: 26658 -> 26658
4516
Cov: 26658 -> 26658
Cov: 26658 -> 26658
4517
{"exception": "RuntimeError", "msg": "\"log_normal_cpu\" not implemented for 'Long'"}
4518
Cov: 26658 -> 26658
Cov: 26658 -> 26658
4519
{"exception": "RuntimeError", "msg": "element 0 of tensors does not require grad and does not have a grad_fn"}
4520
Cov: 26658 -> 26674
Cov: 26674 -> 26674
4521
Cov: 26674 -> 26674
Cov: 26674 -> 26674
4522
Cov: 26674 -> 26674
Cov: 26674 -> 26674
4523
Cov: 26674 -> 26675
Cov: 26675 -> 26675
4524
Cov: 26675 -> 26675
Cov: 26675 -> 26675
4525
Cov: 26675 -> 26676
Cov: 26676 -> 26676
4526
Cov: 26676 -> 26676
Cov: 26676 -> 26676
4527
Cov: 26676 -> 26677
Cov: 26677 -> 26677
4528
Cov: 26677 -> 26677
Cov: 26677 -> 26677
4529
Cov: 26677 -> 26677
Cov: 26677 -> 26677
4530
Cov: 26677 -> 26677
Cov: 26677 -> 26677
4531
Cov: 26677 -> 26678
Cov: 26678 -> 26678
4532
Cov: 26678 -> 26678
Cov: 26678 -> 26678
4533
Cov: 26678 -> 26678
Cov: 26678 -> 26678
4534
{"exception": "TypeError", "msg": "'getset_descriptor' object is not callable"}
4535
{"exception": "RuntimeError", "msg": "element 0 of tensors does not require grad and does not have a grad_fn"}
4536
Cov: 26678 -> 26679
Cov: 26679 -> 26679
4537
Cov: 26679 -> 26679
Cov: 26679 -> 26679
4538
Cov: 26679 -> 26679
Cov: 26679 -> 26679
4539
Cov: 26679 -> 26679
Cov: 26679 -> 26679
4540
Cov: 26679 -> 26679
Cov: 26679 -> 26679
4541
Cov: 26679 -> 26680
Cov: 26680 -> 26680
4542
Cov: 26680 -> 26682
Cov: 26682 -> 26682
4543
Cov: 26682 -> 26682
Cov: 26682 -> 26682
4544
{"exception": "TypeError", "msg": "descriptor 'new_tensor' for 'torch._C._TensorBase' objects doesn't apply to a 'list' object"}
4545
Cov: 26682 -> 26683
Cov: 26683 -> 26683
4546
Cov: 26683 -> 26683
Cov: 26683 -> 26683
4547
Cov: 26683 -> 26683
Cov: 26683 -> 26683
4548
Cov: 26683 -> 26683
Cov: 26683 -> 26683
4549
Cov: 26683 -> 26683
Cov: 26683 -> 26683
4550
Cov: 26683 -> 26683
Cov: 26683 -> 26683
4551
Cov: 26683 -> 26684
Cov: 26684 -> 26684
4552
Cov: 26684 -> 26684
Cov: 26684 -> 26684
4553
Cov: 26684 -> 26684
Cov: 26684 -> 26684
4554
=================================================================
timeout reached. testcase took: 10
[Error] ...
Hangs during coverage collection.
Had to restart coverage executor...
timeout
4555
Cov: 26684 -> 0
Cov: 0 -> 0
4556
Cov: 0 -> 0
Cov: 0 -> 0
4557
Cov: 0 -> 0
Cov: 0 -> 0
4558
Cov: 0 -> 0
Cov: 0 -> 0
4559
Cov: 0 -> 0
Cov: 0 -> 0
4560
Cov: 0 -> 0
Cov: 0 -> 0
4561
Cov: 0 -> 17
Cov: 17 -> 17
4562
Cov: 17 -> 17
Cov: 17 -> 17
4563
Cov: 17 -> 17
Cov: 17 -> 17
4564
Cov: 17 -> 17
Cov: 17 -> 17
4565
Cov: 17 -> 17
Cov: 17 -> 17
4566
Cov: 17 -> 17
Cov: 17 -> 17
4567
Cov: 17 -> 17
Cov: 17 -> 17
4568
Cov: 17 -> 17
Cov: 17 -> 17
4569
Cov: 17 -> 17
Cov: 17 -> 17
4570
Cov: 17 -> 17
Cov: 17 -> 17
4571
Cov: 17 -> 17
Cov: 17 -> 17
4572
Cov: 17 -> 17
Cov: 17 -> 17
4573
{"exception": "TypeError", "msg": "narrow_copy() missing 3 required positional argument: \"dim\", \"start\", \"length\""}
4574
Cov: 17 -> 17
Cov: 17 -> 17
4575
Cov: 17 -> 22
Cov: 22 -> 22
4576
Cov: 22 -> 73
Cov: 73 -> 73
4577
Cov: 73 -> 73
Cov: 73 -> 73
4578
{"exception": "RuntimeError", "msg": "\"bincount_cpu\" not implemented for 'Float'"}
4579
Cov: 73 -> 73
Cov: 73 -> 73
4580
Cov: 73 -> 73
Cov: 73 -> 73
4581
Cov: 73 -> 133
Cov: 133 -> 133
4582
Cov: 133 -> 133
Cov: 133 -> 133
4583
Cov: 133 -> 133
Cov: 133 -> 133
4584
Cov: 133 -> 133
Cov: 133 -> 133
4585
Cov: 133 -> 133
Cov: 133 -> 133
4586
Cov: 133 -> 139
Cov: 139 -> 139
4587
Cov: 139 -> 141
Cov: 141 -> 141
4588
Cov: 141 -> 141
Cov: 141 -> 141
4589
Cov: 141 -> 805
Cov: 805 -> 805
4590
Cov: 805 -> 805
Cov: 805 -> 805
4591
Cov: 805 -> 805
Cov: 805 -> 805
4592
Cov: 805 -> 805
Cov: 805 -> 805
4593
Cov: 805 -> 805
Cov: 805 -> 805
4594
Cov: 805 -> 955
Cov: 955 -> 955
4595
Cov: 955 -> 955
Cov: 955 -> 955
4596
Cov: 955 -> 955
Cov: 955 -> 955
4597
Cov: 955 -> 955
Cov: 955 -> 955
4598
Cov: 955 -> 958
Cov: 958 -> 958
4599
Cov: 958 -> 969
Cov: 969 -> 969
4600
Cov: 969 -> 978
Cov: 978 -> 978
4601
Cov: 978 -> 983
Cov: 983 -> 983
4602
Cov: 983 -> 983
Cov: 983 -> 983
4603
Cov: 983 -> 983
Cov: 983 -> 983
4604
Cov: 983 -> 983
Cov: 983 -> 983
4605
Cov: 983 -> 983
Cov: 983 -> 983
4606
Cov: 983 -> 983
Cov: 983 -> 983
4607
Cov: 983 -> 990
Cov: 990 -> 990
4608
Cov: 990 -> 990
Cov: 990 -> 990
4609
Cov: 990 -> 990
Cov: 990 -> 990
4610
Cov: 990 -> 990
Cov: 990 -> 990
4611
Cov: 990 -> 990
Cov: 990 -> 990
4612
Cov: 990 -> 990
Cov: 990 -> 990
4613
Cov: 990 -> 1000
Cov: 1000 -> 1000
4614
Cov: 1000 -> 1028
Cov: 1028 -> 1028
4615
Cov: 1028 -> 1028
Cov: 1028 -> 1028
4616
Cov: 1028 -> 1028
Cov: 1028 -> 1028
4617
Cov: 1028 -> 1032
Cov: 1032 -> 1032
4618
Cov: 1032 -> 1032
Cov: 1032 -> 1032
4619
Cov: 1032 -> 1032
Cov: 1032 -> 1032
4620
Cov: 1032 -> 1032
Cov: 1032 -> 1032
4621
Cov: 1032 -> 1041
Cov: 1041 -> 1041
4622
Cov: 1041 -> 1041
Cov: 1041 -> 1041
4623
Cov: 1041 -> 1041
Cov: 1041 -> 1041
4624
Cov: 1041 -> 1041
Cov: 1041 -> 1041
4625
Cov: 1041 -> 1041
Cov: 1041 -> 1041
4626
Cov: 1041 -> 1044
Cov: 1044 -> 1044
4627
Cov: 1044 -> 1044
Cov: 1044 -> 1044
4628
Cov: 1044 -> 1044
Cov: 1044 -> 1044
4629
{"exception": "TypeError", "msg": "addmv_() missing 1 required positional arguments: \"vec\""}
4630
Cov: 1044 -> 1044
Cov: 1044 -> 1044
4631
Cov: 1044 -> 1044
Cov: 1044 -> 1044
4632
Cov: 1044 -> 1044
Cov: 1044 -> 1044
4633
Cov: 1044 -> 1044
Cov: 1044 -> 1044
4634
Cov: 1044 -> 1044
Cov: 1044 -> 1044
4635
Cov: 1044 -> 1117
Cov: 1117 -> 1117
4636
Cov: 1117 -> 1117
Cov: 1117 -> 1117
4637
{"exception": "TypeError", "msg": "index_add_() missing 1 required positional arguments: \"source\""}
4638
Cov: 1117 -> 1117
Cov: 1117 -> 1117
4639
Cov: 1117 -> 1156
Cov: 1156 -> 1156
4640
Cov: 1156 -> 1174
Cov: 1174 -> 1174
4641
Cov: 1174 -> 1174
Cov: 1174 -> 1174
4642
Cov: 1174 -> 1174
Cov: 1174 -> 1174
4643
Cov: 1174 -> 1174
Cov: 1174 -> 1174
4644
Cov: 1174 -> 1196
Cov: 1196 -> 1196
4645
Cov: 1196 -> 1196
Cov: 1196 -> 1196
4646
Cov: 1196 -> 1196
Cov: 1196 -> 1196
4647
Cov: 1196 -> 1196
Cov: 1196 -> 1196
4648
Cov: 1196 -> 1196
Cov: 1196 -> 1196
4649
Cov: 1196 -> 1198
Cov: 1198 -> 1198
4650
Cov: 1198 -> 1198
Cov: 1198 -> 1198
4651
Cov: 1198 -> 1198
Cov: 1198 -> 1198
4652
Cov: 1198 -> 1198
Cov: 1198 -> 1198
4653
Cov: 1198 -> 1212
Cov: 1212 -> 1212
4654
Cov: 1212 -> 1212
Cov: 1212 -> 1212
4655
{"exception": "RuntimeError", "msg": "Bad in-place call: input tensor size [2, 3] and output tensor size [3, 5] should match"}
4656
Cov: 1212 -> 1212
Cov: 1212 -> 1212
4657
Cov: 1212 -> 1212
Cov: 1212 -> 1212
4658
{"exception": "RuntimeError", "msg": "istft requires a complex-valued input tensor matching the output from stft with return_complex=True."}
4659
Cov: 1212 -> 1225
Cov: 1225 -> 1225
4660
Cov: 1225 -> 1225
Cov: 1225 -> 1225
4661
Cov: 1225 -> 1225
Cov: 1225 -> 1225
4662
Cov: 1225 -> 1225
Cov: 1225 -> 1225
4663
Cov: 1225 -> 1225
Cov: 1225 -> 1225
4664
Cov: 1225 -> 1225
Cov: 1225 -> 1225
4665
Cov: 1225 -> 1236
Cov: 1236 -> 1236
4666
Cov: 1236 -> 1236
Cov: 1236 -> 1236
4667
Cov: 1236 -> 1236
Cov: 1236 -> 1236
4668
Cov: 1236 -> 1236
Cov: 1236 -> 1236
4669
Cov: 1236 -> 1236
Cov: 1236 -> 1236
4670
{"exception": "TypeError", "msg": "must be real number, not NoneType"}
4671
Cov: 1236 -> 1236
Cov: 1236 -> 1236
4672
{"exception": "TypeError", "msg": "'getset_descriptor' object is not callable"}
4673
Cov: 1236 -> 1236
Cov: 1236 -> 1236
4674
Cov: 1236 -> 1236
Cov: 1236 -> 1236
4675
Cov: 1236 -> 1236
Cov: 1236 -> 1236
4676
Cov: 1236 -> 1237
Cov: 1237 -> 1237
4677
Cov: 1237 -> 1237
Cov: 1237 -> 1237
4678
Cov: 1237 -> 1245
Cov: 1245 -> 1245
4679
Cov: 1245 -> 1245
Cov: 1245 -> 1245
4680
Cov: 1245 -> 1245
Cov: 1245 -> 1245
4681
{"exception": "TypeError", "msg": "symeig() got an unexpected keyword argument 'upper'"}
4682
Cov: 1245 -> 1245
Cov: 1245 -> 1245
4683
Cov: 1245 -> 1245
Cov: 1245 -> 1245
4684
Cov: 1245 -> 1245
Cov: 1245 -> 1245
4685
Cov: 1245 -> 1245
Cov: 1245 -> 1245
4686
Cov: 1245 -> 1245
Cov: 1245 -> 1245
4687
Cov: 1245 -> 1245
Cov: 1245 -> 1245
4688
Cov: 1245 -> 1245
Cov: 1245 -> 1245
4689
Cov: 1245 -> 1255
Cov: 1255 -> 1255
4690
Cov: 1255 -> 1255
Cov: 1255 -> 1255
4691
Cov: 1255 -> 1281
Cov: 1281 -> 1281
4692
Cov: 1281 -> 1281
Cov: 1281 -> 1281
4693
Cov: 1281 -> 1306
Cov: 1306 -> 1306
4694
Cov: 1306 -> 1306
Cov: 1306 -> 1306
4695
Cov: 1306 -> 1306
Cov: 1306 -> 1306
4696
{"exception": "TypeError", "msg": "'getset_descriptor' object is not callable"}
4697
Cov: 1306 -> 1311
Cov: 1311 -> 1311
4698
Cov: 1311 -> 1312
Cov: 1312 -> 1312
4699
Cov: 1312 -> 1353
Cov: 1353 -> 1353
4700
Cov: 1353 -> 1353
Cov: 1353 -> 1353
4701
Cov: 1353 -> 1353
Cov: 1353 -> 1353
4702
Cov: 1353 -> 1353
Cov: 1353 -> 1353
4703
Cov: 1353 -> 1354
Cov: 1354 -> 1354
4704
Cov: 1354 -> 1354
Cov: 1354 -> 1354
4705
Cov: 1354 -> 1354
Cov: 1354 -> 1354
4706
Cov: 1354 -> 1354
Cov: 1354 -> 1354
4707
Cov: 1354 -> 1354
Cov: 1354 -> 1354
4708
Cov: 1354 -> 1354
Cov: 1354 -> 1354
4709
{"exception": "NotImplementedError", "msg": "Could not run 'aten::q_per_channel_scales' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::q_per_channel_scales' is only available for these backends: [QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nQuantizedCPU: registered at aten/src/ATen/RegisterQuantizedCPU.cpp:944 [kernel]\nQuantizedCUDA: registered at aten/src/ATen/RegisterQuantizedCUDA.cpp:459 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:153 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:498 [backend fallback]\nFunctionalize: registered at ../aten/src/ATen/FunctionalizeFallbackKernel.cpp:290 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:86 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradMTIA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_3.cpp:14610 [kernel]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:382 [backend fallback]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:249 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:710 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:203 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:161 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:494 [backend fallback]\nPreDispatch: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:165 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:157 [backend fallback]\n"}
4710
Cov: 1354 -> 1354
Cov: 1354 -> 1354
4711
Cov: 1354 -> 1354
Cov: 1354 -> 1354
4712
Cov: 1354 -> 1354
Cov: 1354 -> 1354
4713
Cov: 1354 -> 1354
Cov: 1354 -> 1354
4714
Cov: 1354 -> 1354
Cov: 1354 -> 1354
4715
Cov: 1354 -> 1354
Cov: 1354 -> 1354
4716
Cov: 1354 -> 1354
Cov: 1354 -> 1354
4717
Cov: 1354 -> 1354
Cov: 1354 -> 1354
4718
Cov: 1354 -> 1354
Cov: 1354 -> 1354
4719
Cov: 1354 -> 1354
Cov: 1354 -> 1354
4720
Cov: 1354 -> 1390
Cov: 1390 -> 1390
4721
Cov: 1390 -> 1390
Cov: 1390 -> 1390
4722
{"exception": "RuntimeError", "msg": "\"rshift_cpu\" not implemented for 'Float'"}
4723
Cov: 1390 -> 1390
Cov: 1390 -> 1390
4724
Cov: 1390 -> 1422
Cov: 1422 -> 1422
4725
Cov: 1422 -> 1422
Cov: 1422 -> 1422
4726
Cov: 1422 -> 1422
Cov: 1422 -> 1422
4727
{"exception": "RuntimeError", "msg": "Please look up dimensions by name, got: name = None."}
4728
Cov: 1422 -> 1422
Cov: 1422 -> 1422
4729
{"exception": "RuntimeError", "msg": "The expanded size of the tensor (2) must match the existing size (3) at non-singleton dimension 1.  Target sizes: [3, 2].  Tensor sizes: [2, 3]"}
4730
Cov: 1422 -> 1422
Cov: 1422 -> 1422
4731
Cov: 1422 -> 1422
Cov: 1422 -> 1422
4732
Cov: 1422 -> 1422
Cov: 1422 -> 1422
4733
Cov: 1422 -> 1427
Cov: 1427 -> 1427
4734
Cov: 1427 -> 1477
Cov: 1477 -> 1477
4735
Cov: 1477 -> 1477
Cov: 1477 -> 1477
4736
Cov: 1477 -> 1477
Cov: 1477 -> 1477
4737
Cov: 1477 -> 1477
Cov: 1477 -> 1477
4738
Cov: 1477 -> 1477
Cov: 1477 -> 1477
4739
Cov: 1477 -> 1477
Cov: 1477 -> 1477
4740
Cov: 1477 -> 1477
Cov: 1477 -> 1477
4741
Cov: 1477 -> 1480
Cov: 1480 -> 1480
4742
Cov: 1480 -> 1480
Cov: 1480 -> 1480
4743
Cov: 1480 -> 1481
Cov: 1481 -> 1481
4744
Cov: 1481 -> 1481
Cov: 1481 -> 1481
4745
Cov: 1481 -> 1481
Cov: 1481 -> 1481
4746
Cov: 1481 -> 1483
Cov: 1483 -> 1483
4747
Cov: 1483 -> 1483
Cov: 1483 -> 1483
4748
Cov: 1483 -> 1483
Cov: 1483 -> 1483
4749
Cov: 1483 -> 1517
Cov: 1517 -> 1517
4750
Cov: 1517 -> 1527
Cov: 1527 -> 1527
4751
Cov: 1527 -> 1527
Cov: 1527 -> 1527
4752
Cov: 1527 -> 1527
Cov: 1527 -> 1527
4753
Cov: 1527 -> 1575
Cov: 1575 -> 1575
4754
Cov: 1575 -> 1575
Cov: 1575 -> 1575
4755
Cov: 1575 -> 1575
Cov: 1575 -> 1575
4756
Cov: 1575 -> 1575
Cov: 1575 -> 1575
4757
Cov: 1575 -> 1575
Cov: 1575 -> 1575
4758
Cov: 1575 -> 1575
Cov: 1575 -> 1575
4759
Cov: 1575 -> 1585
Cov: 1585 -> 1585
4760
Cov: 1585 -> 1594
Cov: 1594 -> 1594
4761
Cov: 1594 -> 1594
Cov: 1594 -> 1594
4762
Cov: 1594 -> 1594
Cov: 1594 -> 1594
4763
Cov: 1594 -> 1594
Cov: 1594 -> 1594
4764
Cov: 1594 -> 1594
Cov: 1594 -> 1594
4765
Cov: 1594 -> 1594
Cov: 1594 -> 1594
4766
Cov: 1594 -> 1594
Cov: 1594 -> 1594
4767
Cov: 1594 -> 1596
Cov: 1596 -> 1596
4768
{"exception": "TypeError", "msg": "geometric_(): argument 'p' (position 1) must be float, not Tensor"}
4769
Cov: 1596 -> 1596
Cov: 1596 -> 1596
4770
Cov: 1596 -> 1596
Cov: 1596 -> 1596
4771
Cov: 1596 -> 1604
Cov: 1604 -> 1604
4772
Cov: 1604 -> 1604
Cov: 1604 -> 1604
4773
Cov: 1604 -> 1608
Cov: 1608 -> 1608
4774
Cov: 1608 -> 1608
Cov: 1608 -> 1608
4775
Cov: 1608 -> 1608
Cov: 1608 -> 1608
4776
{"exception": "TypeError", "msg": "random_() missing 2 required positional argument: \"from\", \"to\""}
4777
Cov: 1608 -> 1609
Cov: 1609 -> 1609
4778
Cov: 1609 -> 1658
Cov: 1658 -> 1658
4779
Cov: 1658 -> 1658
Cov: 1658 -> 1658
4780
Cov: 1658 -> 1659
Cov: 1659 -> 1659
4781
Cov: 1659 -> 1659
Cov: 1659 -> 1659
4782
Cov: 1659 -> 1659
Cov: 1659 -> 1659
4783
Cov: 1659 -> 1659
Cov: 1659 -> 1659
4784
Cov: 1659 -> 1659
Cov: 1659 -> 1659
4785
Cov: 1659 -> 1659
Cov: 1659 -> 1659
4786
{"exception": "RuntimeError", "msg": "self and mat2 must have the same dtype, but got Float and Double"}
4787
Cov: 1659 -> 1659
Cov: 1659 -> 1659
4788
Cov: 1659 -> 1662
Cov: 1662 -> 1662
4789
Cov: 1662 -> 1671
Cov: 1671 -> 1671
4790
Cov: 1671 -> 1671
Cov: 1671 -> 1671
4791
Cov: 1671 -> 1671
Cov: 1671 -> 1671
4792
Cov: 1671 -> 1671
Cov: 1671 -> 1671
4793
Cov: 1671 -> 1671
Cov: 1671 -> 1671
4794
{"exception": "RuntimeError", "msg": "a leaf Variable that requires grad is being used in an in-place operation."}
4795
Cov: 1671 -> 1682
Cov: 1682 -> 1682
4796
Cov: 1682 -> 1682
Cov: 1682 -> 1682
4797
Cov: 1682 -> 1682
Cov: 1682 -> 1682
4798
Cov: 1682 -> 1684
Cov: 1684 -> 1684
4799
Cov: 1684 -> 1684
Cov: 1684 -> 1684
4800
Cov: 1684 -> 1699
Cov: 1699 -> 1699
4801
Cov: 1699 -> 1699
Cov: 1699 -> 1699
4802
Cov: 1699 -> 1699
Cov: 1699 -> 1699
4803
Cov: 1699 -> 1706
Cov: 1706 -> 1706
4804
Cov: 1706 -> 1709
Cov: 1709 -> 1709
4805
Cov: 1709 -> 1709
Cov: 1709 -> 1709
4806
Cov: 1709 -> 1714
Cov: 1714 -> 1714
4807
Cov: 1714 -> 1723
Cov: 1723 -> 1723
4808
Cov: 1723 -> 1723
Cov: 1723 -> 1723
4809
Cov: 1723 -> 1743
Cov: 1743 -> 1743
4810
Cov: 1743 -> 1748
Cov: 1748 -> 1748
4811
Cov: 1748 -> 1748
Cov: 1748 -> 1748
4812
Cov: 1748 -> 1748
Cov: 1748 -> 1748
4813
{"exception": "RuntimeError", "msg": "masked_scatter_ only supports boolean masks, but got mask with dtype Byte"}
4814
Cov: 1748 -> 1748
Cov: 1748 -> 1748
4815
Cov: 1748 -> 1753
Cov: 1753 -> 1753
4816
Cov: 1753 -> 1753
Cov: 1753 -> 1753
4817
Cov: 1753 -> 1753
Cov: 1753 -> 1753
4818
Cov: 1753 -> 1753
Cov: 1753 -> 1753
4819
Cov: 1753 -> 1753
Cov: 1753 -> 1753
4820
Cov: 1753 -> 1753
Cov: 1753 -> 1753
4821
Cov: 1753 -> 1753
Cov: 1753 -> 1753
4822
Cov: 1753 -> 1753
Cov: 1753 -> 1753
4823
{"exception": "RuntimeError", "msg": "istft requires a complex-valued input tensor matching the output from stft with return_complex=True."}
4824
Cov: 1753 -> 1754
Cov: 1754 -> 1754
4825
Cov: 1754 -> 1754
Cov: 1754 -> 1754
4826
Cov: 1754 -> 1754
Cov: 1754 -> 1754
4827
Cov: 1754 -> 1754
Cov: 1754 -> 1754
4828
Cov: 1754 -> 1760
Cov: 1760 -> 1760
4829
Cov: 1760 -> 1760
Cov: 1760 -> 1760
4830
Cov: 1760 -> 1760
Cov: 1760 -> 1760
4831
Cov: 1760 -> 1760
Cov: 1760 -> 1760
4832
Cov: 1760 -> 1763
Cov: 1763 -> 1763
4833
Cov: 1763 -> 1774
Cov: 1774 -> 1774
4834
Cov: 1774 -> 1774
Cov: 1774 -> 1774
4835
Cov: 1774 -> 1774
Cov: 1774 -> 1774
4836
{"exception": "RuntimeError", "msg": "mat1 and mat2 must have the same dtype, but got Float and Double"}
4837
Cov: 1774 -> 18006
Cov: 18006 -> 18006
4838
Cov: 18006 -> 18006
Cov: 18006 -> 18006
4839
Cov: 18006 -> 18006
Cov: 18006 -> 18006
4840
Cov: 18006 -> 18006
Cov: 18006 -> 18006
4841
Cov: 18006 -> 18006
Cov: 18006 -> 18006
4842
Cov: 18006 -> 18006
Cov: 18006 -> 18006
4843
Cov: 18006 -> 18019
Cov: 18019 -> 18019
4844
Cov: 18019 -> 18019
Cov: 18019 -> 18019
4845
Cov: 18019 -> 18019
Cov: 18019 -> 18019
4846
Cov: 18019 -> 18019
Cov: 18019 -> 18019
4847
Cov: 18019 -> 18019
Cov: 18019 -> 18019
4848
Cov: 18019 -> 18019
Cov: 18019 -> 18019
4849
Cov: 18019 -> 18019
Cov: 18019 -> 18019
4850
Cov: 18019 -> 18019
Cov: 18019 -> 18019
4851
Cov: 18019 -> 18019
Cov: 18019 -> 18019
4852
Cov: 18019 -> 18053
Cov: 18053 -> 18053
4853
Cov: 18053 -> 18053
Cov: 18053 -> 18053
4854
Cov: 18053 -> 18053
Cov: 18053 -> 18053
4855
Cov: 18053 -> 18053
Cov: 18053 -> 18053
4856
Cov: 18053 -> 18053
Cov: 18053 -> 18053
4857
Cov: 18053 -> 18054
Cov: 18054 -> 18054
4858
Cov: 18054 -> 18054
Cov: 18054 -> 18054
4859
Cov: 18054 -> 18054
Cov: 18054 -> 18054
4860
{"exception": "NotImplementedError", "msg": "Could not run 'aten::int_repr' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::int_repr' is only available for these backends: [QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nQuantizedCPU: registered at aten/src/ATen/RegisterQuantizedCPU.cpp:944 [kernel]\nQuantizedCUDA: registered at aten/src/ATen/RegisterQuantizedCUDA.cpp:459 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:153 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:498 [backend fallback]\nFunctionalize: registered at ../aten/src/ATen/FunctionalizeFallbackKernel.cpp:290 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:86 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradMTIA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_0.cpp:16725 [kernel]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:382 [backend fallback]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:249 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:710 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:203 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:161 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:494 [backend fallback]\nPreDispatch: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:165 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:157 [backend fallback]\n"}
4861
Cov: 18054 -> 18070
Cov: 18070 -> 18070
4862
Cov: 18070 -> 18074
Cov: 18074 -> 18074
4863
Cov: 18074 -> 18084
Cov: 18084 -> 18084
4864
Cov: 18084 -> 18084
Cov: 18084 -> 18084
4865
Cov: 18084 -> 18084
Cov: 18084 -> 18084
4866
Cov: 18084 -> 18084
Cov: 18084 -> 18084
4867
Cov: 18084 -> 18090
Cov: 18090 -> 18090
4868
Cov: 18090 -> 18094
Cov: 18094 -> 18094
4869
Cov: 18094 -> 18094
Cov: 18094 -> 18094
4870
Cov: 18094 -> 18106
Cov: 18106 -> 18106
4871
Cov: 18106 -> 18107
Cov: 18107 -> 18107
4872
Cov: 18107 -> 18107
Cov: 18107 -> 18107
4873
Cov: 18107 -> 18107
Cov: 18107 -> 18107
4874
Cov: 18107 -> 18107
Cov: 18107 -> 18107
4875
Cov: 18107 -> 18107
Cov: 18107 -> 18107
4876
Cov: 18107 -> 18107
Cov: 18107 -> 18107
4877
Cov: 18107 -> 18108
Cov: 18108 -> 18108
4878
Cov: 18108 -> 18108
Cov: 18108 -> 18108
4879
Cov: 18108 -> 18108
Cov: 18108 -> 18108
4880
Cov: 18108 -> 18108
Cov: 18108 -> 18108
4881
Cov: 18108 -> 18108
Cov: 18108 -> 18108
4882
Cov: 18108 -> 18108
Cov: 18108 -> 18108
4883
Cov: 18108 -> 18108
Cov: 18108 -> 18108
4884
Cov: 18108 -> 18127
Cov: 18127 -> 18127
4885
Cov: 18127 -> 18129
Cov: 18129 -> 18129
4886
Cov: 18129 -> 18138
Cov: 18138 -> 18138
4887
Cov: 18138 -> 18138
Cov: 18138 -> 18138
4888
Cov: 18138 -> 18141
Cov: 18141 -> 18141
4889
Cov: 18141 -> 18141
Cov: 18141 -> 18141
4890
Cov: 18141 -> 18141
Cov: 18141 -> 18141
4891
Cov: 18141 -> 18141
Cov: 18141 -> 18141
4892
Cov: 18141 -> 18141
Cov: 18141 -> 18141
4893
Cov: 18141 -> 18141
Cov: 18141 -> 18141
4894
Cov: 18141 -> 18141
Cov: 18141 -> 18141
4895
Cov: 18141 -> 18141
Cov: 18141 -> 18141
4896
Cov: 18141 -> 18141
Cov: 18141 -> 18141
4897
Cov: 18141 -> 18141
Cov: 18141 -> 18141
4898
Cov: 18141 -> 18141
Cov: 18141 -> 18141
4899
Cov: 18141 -> 18141
Cov: 18141 -> 18141
4900
Cov: 18141 -> 18405
Cov: 18405 -> 18405
4901
{"exception": "TypeError", "msg": "remainder_() received an invalid combination of arguments - got (divisor=int, ), but expected one of:\n * (Tensor other)\n      didn't match because some of the keywords were incorrect: divisor\n * (Number other)\n      didn't match because some of the keywords were incorrect: divisor\n"}
4902
Cov: 18405 -> 18405
Cov: 18405 -> 18405
4903
Cov: 18405 -> 18405
Cov: 18405 -> 18405
4904
Cov: 18405 -> 18409
Cov: 18409 -> 18409
4905
{"exception": "RuntimeError", "msg": "This function was deprecated since version 1.9 and is now removed. `torch.solve` is deprecated in favor of `torch.linalg.solve`. `torch.linalg.solve` has its arguments reversed and does not return the LU factorization.\n\nTo get the LU factorization see `torch.lu`, which can be used with `torch.lu_solve` or `torch.lu_unpack`.\nX = torch.solve(B, A).solution should be replaced with:\nX = torch.linalg.solve(A, B)"}
4906
{"exception": "RuntimeError", "msg": "a leaf Variable that requires grad is being used in an in-place operation."}
4907
Cov: 18409 -> 18415
Cov: 18415 -> 18415
4908
Cov: 18415 -> 18415
Cov: 18415 -> 18415
4909
Cov: 18415 -> 18415
Cov: 18415 -> 18415
4910
Cov: 18415 -> 18415
Cov: 18415 -> 18415
4911
Cov: 18415 -> 18416
Cov: 18416 -> 18416
4912
Cov: 18416 -> 18416
Cov: 18416 -> 18416
4913
Cov: 18416 -> 18416
Cov: 18416 -> 18416
4914
Cov: 18416 -> 18444
Cov: 18444 -> 18444
4915
Cov: 18444 -> 18488
Cov: 18488 -> 18488
4916
Cov: 18488 -> 18488
Cov: 18488 -> 18488
4917
Cov: 18488 -> 18488
Cov: 18488 -> 18488
4918
Cov: 18488 -> 18488
Cov: 18488 -> 18488
4919
{"exception": "TypeError", "msg": "'getset_descriptor' object is not callable"}
4920
Cov: 18488 -> 18489
Cov: 18489 -> 18489
4921
{"exception": "RuntimeError", "msg": "Index tensor must have the same number of dimensions as input tensor"}
4922
Cov: 18489 -> 18489
Cov: 18489 -> 18489
4923
Cov: 18489 -> 18490
Cov: 18490 -> 18490
4924
Cov: 18490 -> 18490
Cov: 18490 -> 18490
4925
Cov: 18490 -> 18490
Cov: 18490 -> 18490
4926
Cov: 18490 -> 18490
Cov: 18490 -> 18490
4927
Cov: 18490 -> 18493
Cov: 18493 -> 18493
4928
Cov: 18493 -> 18493
Cov: 18493 -> 18493
4929
Cov: 18493 -> 18493
Cov: 18493 -> 18493
4930
Cov: 18493 -> 18493
Cov: 18493 -> 18493
4931
Cov: 18493 -> 18493
Cov: 18493 -> 18493
4932
Cov: 18493 -> 18493
Cov: 18493 -> 18493
4933
Cov: 18493 -> 18493
Cov: 18493 -> 18493
4934
Cov: 18493 -> 18494
Cov: 18494 -> 18494
4935
Cov: 18494 -> 18497
Cov: 18497 -> 18497
4936
Cov: 18497 -> 18497
Cov: 18497 -> 18497
4937
Cov: 18497 -> 18497
Cov: 18497 -> 18497
4938
Cov: 18497 -> 18497
Cov: 18497 -> 18497
4939
Cov: 18497 -> 18497
Cov: 18497 -> 18497
4940
Cov: 18497 -> 18501
Cov: 18501 -> 18501
4941
Cov: 18501 -> 18501
Cov: 18501 -> 18501
4942
Cov: 18501 -> 18501
Cov: 18501 -> 18501
4943
Cov: 18501 -> 18501
Cov: 18501 -> 18501
4944
{"exception": "RuntimeError", "msg": "masked_scatter: expected self and source to have same dtypes but gotDouble and Long"}
4945
Cov: 18501 -> 18501
Cov: 18501 -> 18501
4946
Cov: 18501 -> 18501
Cov: 18501 -> 18501
4947
Cov: 18501 -> 18501
Cov: 18501 -> 18501
4948
Cov: 18501 -> 18501
Cov: 18501 -> 18501
4949
Cov: 18501 -> 18501
Cov: 18501 -> 18501
4950
Cov: 18501 -> 18501
Cov: 18501 -> 18501
4951
Cov: 18501 -> 18501
Cov: 18501 -> 18501
4952
{"exception": "TypeError", "msg": "'getset_descriptor' object is not callable"}
4953
Cov: 18501 -> 18502
Cov: 18502 -> 18502
4954
{"exception": "RuntimeError", "msg": "Bad in-place call: input tensor size [3, 2] and output tensor size [2, 3] should match"}
4955
Cov: 18502 -> 18502
Cov: 18502 -> 18502
4956
Cov: 18502 -> 18513
Cov: 18513 -> 18513
4957
Cov: 18513 -> 18513
Cov: 18513 -> 18513
4958
Cov: 18513 -> 18513
Cov: 18513 -> 18513
4959
Cov: 18513 -> 18525
Cov: 18525 -> 18525
4960
Cov: 18525 -> 18525
Cov: 18525 -> 18525
4961
Cov: 18525 -> 18526
Cov: 18526 -> 18526
4962
Cov: 18526 -> 18526
Cov: 18526 -> 18526
4963
{"exception": "RuntimeError", "msg": "masked_select: expected BoolTensor for mask"}
4964
Cov: 18526 -> 18526
Cov: 18526 -> 18526
4965
Cov: 18526 -> 18526
Cov: 18526 -> 18526
4966
Cov: 18526 -> 18526
Cov: 18526 -> 18526
4967
Cov: 18526 -> 18526
Cov: 18526 -> 18526
4968
Cov: 18526 -> 18526
Cov: 18526 -> 18526
4969
Cov: 18526 -> 18526
Cov: 18526 -> 18526
4970
{"exception": "RuntimeError", "msg": "the base given to float_power_ has dtype Float but the operation's result requires dtype Double"}
4971
Cov: 18526 -> 18526
Cov: 18526 -> 18526
4972
Cov: 18526 -> 18526
Cov: 18526 -> 18526
4973
Cov: 18526 -> 18526
Cov: 18526 -> 18526
4974
Cov: 18526 -> 18526
Cov: 18526 -> 18526
4975
Cov: 18526 -> 18526
Cov: 18526 -> 18526
4976
Cov: 18526 -> 18526
Cov: 18526 -> 18526
4977
Cov: 18526 -> 18526
Cov: 18526 -> 18526
4978
Cov: 18526 -> 18526
Cov: 18526 -> 18526
4979
Cov: 18526 -> 18526
Cov: 18526 -> 18526
4980
Cov: 18526 -> 18526
Cov: 18526 -> 18526
4981
Cov: 18526 -> 18526
Cov: 18526 -> 18526
4982
Cov: 18526 -> 18542
Cov: 18542 -> 18542
4983
Cov: 18542 -> 18542
Cov: 18542 -> 18542
4984
Cov: 18542 -> 18542
Cov: 18542 -> 18542
4985
{"exception": "TypeError", "msg": "'getset_descriptor' object is not callable"}
4986
Cov: 18542 -> 18542
Cov: 18542 -> 18542
4987
Cov: 18542 -> 18542
Cov: 18542 -> 18542
4988
Cov: 18542 -> 18716
Cov: 18716 -> 18716
4989
Cov: 18716 -> 18728
Cov: 18728 -> 18728
4990
Cov: 18728 -> 18728
Cov: 18728 -> 18728
4991
Cov: 18728 -> 18728
Cov: 18728 -> 18728
4992
Cov: 18728 -> 18728
Cov: 18728 -> 18728
4993
Cov: 18728 -> 18728
Cov: 18728 -> 18728
4994
Cov: 18728 -> 18728
Cov: 18728 -> 18728
4995
Cov: 18728 -> 18728
Cov: 18728 -> 18728
4996
Cov: 18728 -> 18728
Cov: 18728 -> 18728
4997
Cov: 18728 -> 18728
Cov: 18728 -> 18728
4998
Cov: 18728 -> 18729
Cov: 18729 -> 18729
4999
Cov: 18729 -> 18729
Cov: 18729 -> 18729
5000
Cov: 18729 -> 18729
Cov: 18729 -> 18729
test_torch_cov passed

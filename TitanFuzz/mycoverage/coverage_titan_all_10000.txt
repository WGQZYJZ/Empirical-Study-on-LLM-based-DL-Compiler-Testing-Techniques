Initizalize torch cov
1
Cov: 0 -> 0
Cov: 0 -> 0
2
Cov: 0 -> 0
Cov: 0 -> 0
3
Cov: 0 -> 58
Cov: 58 -> 58
4
Cov: 58 -> 58
Cov: 58 -> 58
5
Cov: 58 -> 58
Cov: 58 -> 58
6
Cov: 58 -> 68
Cov: 68 -> 68
7
Cov: 68 -> 84
Cov: 84 -> 84
8
Cov: 84 -> 84
Cov: 84 -> 84
9
Cov: 84 -> 92
Cov: 92 -> 92
10
{"exception": "RuntimeError", "msg": "scatter(): Expected self.dtype to be equal to src.dtype"}
11
{"exception": "NameError", "msg": "name 'math' is not defined"}
12
Cov: 92 -> 92
Cov: 92 -> 92
13
Cov: 92 -> 92
Cov: 92 -> 92
14
Cov: 92 -> 92
Cov: 92 -> 92
15
Cov: 92 -> 92
Cov: 92 -> 92
16
Cov: 92 -> 92
Cov: 92 -> 92
17
{"exception": "RuntimeError", "msg": "\"hypot_cpu\" not implemented for 'Long'"}
18
Cov: 92 -> 92
Cov: 92 -> 92
19
Cov: 92 -> 92
Cov: 92 -> 92
20
Cov: 92 -> 92
Cov: 92 -> 92
21
Cov: 92 -> 160
Cov: 160 -> 160
22
Cov: 160 -> 160
Cov: 160 -> 160
23
Cov: 160 -> 160
Cov: 160 -> 160
24
Cov: 160 -> 160
Cov: 160 -> 160
25
Cov: 160 -> 160
Cov: 160 -> 160
26
Cov: 160 -> 160
Cov: 160 -> 160
27
Cov: 160 -> 164
Cov: 164 -> 164
28
Cov: 164 -> 164
Cov: 164 -> 164
29
Cov: 164 -> 164
Cov: 164 -> 164
30
Cov: 164 -> 181
Cov: 181 -> 181
31
Cov: 181 -> 181
Cov: 181 -> 181
32
Cov: 181 -> 205
Cov: 205 -> 205
33
Cov: 205 -> 382
Cov: 382 -> 382
34
Cov: 382 -> 382
Cov: 382 -> 382
35
Cov: 382 -> 477
Cov: 477 -> 477
36
{"exception": "NameError", "msg": "name 'torchvision' is not defined"}
37
Cov: 477 -> 491
Cov: 491 -> 491
38
Cov: 491 -> 496
Cov: 496 -> 496
39
Cov: 496 -> 496
Cov: 496 -> 496
40
{"exception": "TypeError", "msg": "new_full(): argument 'size' (position 1) must be tuple of ints, not int"}
41
Cov: 496 -> 496
Cov: 496 -> 496
42
Cov: 496 -> 496
Cov: 496 -> 496
43
Cov: 496 -> 496
Cov: 496 -> 496
44
Cov: 496 -> 496
Cov: 496 -> 496
45
Cov: 496 -> 496
Cov: 496 -> 496
46
Cov: 496 -> 496
Cov: 496 -> 496
47
Cov: 496 -> 501
Cov: 501 -> 501
48
{"exception": "RuntimeError", "msg": "setStorage: sizes [3, 3], strides [4, 1], storage offset 0, and itemsize 8 requiring a storage size of 88 are out of bounds for storage of size 80"}
49
Cov: 501 -> 501
Cov: 501 -> 501
50
Cov: 501 -> 501
Cov: 501 -> 501
51
Cov: 501 -> 501
Cov: 501 -> 501
52
Cov: 501 -> 502
Cov: 502 -> 502
53
Cov: 502 -> 507
Cov: 507 -> 507
54
Cov: 507 -> 507
Cov: 507 -> 507
55
Cov: 507 -> 507
Cov: 507 -> 507
56
Cov: 507 -> 507
Cov: 507 -> 507
57
Cov: 507 -> 507
Cov: 507 -> 507
58
Cov: 507 -> 507
Cov: 507 -> 507
59
Cov: 507 -> 507
Cov: 507 -> 507
60
Cov: 507 -> 507
Cov: 507 -> 507
61
Cov: 507 -> 507
Cov: 507 -> 507
62
Cov: 507 -> 507
Cov: 507 -> 507
63
Cov: 507 -> 507
Cov: 507 -> 507
64
Cov: 507 -> 507
Cov: 507 -> 507
65
Cov: 507 -> 507
Cov: 507 -> 507
66
Cov: 507 -> 510
Cov: 510 -> 510
67
Cov: 510 -> 514
Cov: 514 -> 514
68
Cov: 514 -> 514
Cov: 514 -> 514
69
Cov: 514 -> 514
Cov: 514 -> 514
70
Cov: 514 -> 514
Cov: 514 -> 514
71
Cov: 514 -> 517
Cov: 517 -> 517
72
Cov: 517 -> 517
Cov: 517 -> 517
73
Cov: 517 -> 525
Cov: 525 -> 525
74
{"exception": "TypeError", "msg": "baddbmm() missing 1 required positional arguments: \"batch2\""}
75
Cov: 525 -> 529
Cov: 529 -> 529
76
Cov: 529 -> 529
Cov: 529 -> 529
77
Cov: 529 -> 532
Cov: 532 -> 532
78
Cov: 532 -> 532
Cov: 532 -> 532
79
Cov: 532 -> 532
Cov: 532 -> 532
80
Cov: 532 -> 532
Cov: 532 -> 532
81
Cov: 532 -> 550
Cov: 550 -> 550
82
Cov: 550 -> 550
Cov: 550 -> 550
83
Cov: 550 -> 550
Cov: 550 -> 550
84
Cov: 550 -> 550
Cov: 550 -> 550
85
Cov: 550 -> 550
Cov: 550 -> 550
86
{"exception": "NotImplementedError", "msg": "Could not run 'aten::qscheme' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::qscheme' is only available for these backends: [QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nQuantizedCPU: registered at aten/src/ATen/RegisterQuantizedCPU.cpp:944 [kernel]\nQuantizedCUDA: registered at aten/src/ATen/RegisterQuantizedCUDA.cpp:459 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:153 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:498 [backend fallback]\nFunctionalize: registered at ../aten/src/ATen/FunctionalizeFallbackKernel.cpp:290 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:86 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradMTIA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_0.cpp:16725 [kernel]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:382 [backend fallback]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:249 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:710 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:203 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:161 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:494 [backend fallback]\nPreDispatch: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:165 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:157 [backend fallback]\n"}
87
Cov: 550 -> 550
Cov: 550 -> 550
88
{"exception": "RuntimeError", "msg": "size mismatch, input: [2, 3], v1: [3], v2: [3]"}
89
{"exception": "ModuleNotFoundError", "msg": "No module named 'PIL'"}
90
Cov: 550 -> 11701
Cov: 11701 -> 11701
91
Cov: 11701 -> 11705
Cov: 11705 -> 11705
92
Cov: 11705 -> 11705
Cov: 11705 -> 11705
93
Cov: 11705 -> 11705
Cov: 11705 -> 11705
94
Cov: 11705 -> 11705
Cov: 11705 -> 11705
95
Cov: 11705 -> 11705
Cov: 11705 -> 11705
96
Cov: 11705 -> 11705
Cov: 11705 -> 11705
97
Cov: 11705 -> 11710
Cov: 11710 -> 11710
98
Cov: 11710 -> 11710
Cov: 11710 -> 11710
99
Cov: 11710 -> 11710
Cov: 11710 -> 11710
100
Cov: 11710 -> 11710
Cov: 11710 -> 11710
101
{"exception": "RuntimeError", "msg": "Bad in-place call: input tensor size [2, 3] and output tensor size [2, 4] should match"}
102
{"exception": "RuntimeError", "msg": "Please look up dimensions by name, got: name = None."}
103
Cov: 11710 -> 11710
Cov: 11710 -> 11710
104
Cov: 11710 -> 11714
Cov: 11714 -> 11714
105
Cov: 11714 -> 11714
Cov: 11714 -> 11714
106
{"exception": "NameError", "msg": "name 'MNIST' is not defined"}
107
Cov: 11714 -> 11715
Cov: 11715 -> 11715
108
Cov: 11715 -> 11715
Cov: 11715 -> 11715
109
Cov: 11715 -> 11716
Cov: 11716 -> 11716
110
Cov: 11716 -> 11716
Cov: 11716 -> 11716
111
Cov: 11716 -> 11737
Cov: 11737 -> 11737
112
{"exception": "TypeError", "msg": "'getset_descriptor' object is not callable"}
113
Cov: 11737 -> 11761
Cov: 11761 -> 11761
114
Cov: 11761 -> 11762
Cov: 11762 -> 11762
115
Cov: 11762 -> 11762
Cov: 11762 -> 11762
116
Cov: 11762 -> 11762
Cov: 11762 -> 11762
117
Cov: 11762 -> 11762
Cov: 11762 -> 11762
118
Cov: 11762 -> 11762
Cov: 11762 -> 11762
119
Cov: 11762 -> 11762
Cov: 11762 -> 11762
120
Cov: 11762 -> 11787
Cov: 11787 -> 11787
121
Cov: 11787 -> 11800
Cov: 11800 -> 11800
122
Cov: 11800 -> 11800
Cov: 11800 -> 11800
123
Cov: 11800 -> 11800
Cov: 11800 -> 11800
124
Cov: 11800 -> 11804
Cov: 11804 -> 11804
125
Cov: 11804 -> 11810
Cov: 11810 -> 11810
126
Cov: 11810 -> 11810
Cov: 11810 -> 11810
127
Cov: 11810 -> 11810
Cov: 11810 -> 11810
128
Cov: 11810 -> 11810
Cov: 11810 -> 11810
129
Cov: 11810 -> 11810
Cov: 11810 -> 11810
130
Cov: 11810 -> 11836
Cov: 11836 -> 11836
131
Cov: 11836 -> 11836
Cov: 11836 -> 11836
132
Cov: 11836 -> 11836
Cov: 11836 -> 11836
133
Cov: 11836 -> 11836
Cov: 11836 -> 11836
134
Cov: 11836 -> 11836
Cov: 11836 -> 11836
135
Cov: 11836 -> 11900
Cov: 11900 -> 11900
136
Cov: 11900 -> 11900
Cov: 11900 -> 11900
137
Cov: 11900 -> 11900
Cov: 11900 -> 11900
138
Cov: 11900 -> 11900
Cov: 11900 -> 11900
139
Cov: 11900 -> 11900
Cov: 11900 -> 11900
140
Cov: 11900 -> 11900
Cov: 11900 -> 11900
141
Cov: 11900 -> 11900
Cov: 11900 -> 11900
142
Cov: 11900 -> 11901
Cov: 11901 -> 11901
143
Cov: 11901 -> 11901
Cov: 11901 -> 11901
144
Cov: 11901 -> 11901
Cov: 11901 -> 11901
145
Cov: 11901 -> 11901
Cov: 11901 -> 11901
146
Cov: 11901 -> 11901
Cov: 11901 -> 11901
147
Cov: 11901 -> 11901
Cov: 11901 -> 11901
148
Cov: 11901 -> 11901
Cov: 11901 -> 11901
149
Cov: 11901 -> 11925
Cov: 11925 -> 11925
150
Cov: 11925 -> 11925
Cov: 11925 -> 11925
151
Cov: 11925 -> 11925
Cov: 11925 -> 11925
152
Cov: 11925 -> 11925
Cov: 11925 -> 11925
153
Cov: 11925 -> 11925
Cov: 11925 -> 11925
154
Cov: 11925 -> 11925
Cov: 11925 -> 11925
155
Cov: 11925 -> 11943
Cov: 11943 -> 11943
156
Cov: 11943 -> 11943
Cov: 11943 -> 11943
157
Cov: 11943 -> 11943
Cov: 11943 -> 11943
158
Cov: 11943 -> 11943
Cov: 11943 -> 11943
159
Cov: 11943 -> 11943
Cov: 11943 -> 11943
160
Cov: 11943 -> 11943
Cov: 11943 -> 11943
161
Cov: 11943 -> 11943
Cov: 11943 -> 11943
162
Cov: 11943 -> 11943
Cov: 11943 -> 11943
163
Cov: 11943 -> 11943
Cov: 11943 -> 11943
164
Cov: 11943 -> 11943
Cov: 11943 -> 11943
165
{"exception": "TypeError", "msg": "'getset_descriptor' object is not callable"}
166
Cov: 11943 -> 11943
Cov: 11943 -> 11943
167
Cov: 11943 -> 11943
Cov: 11943 -> 11943
168
Cov: 11943 -> 11943
Cov: 11943 -> 11943
169
Cov: 11943 -> 11943
Cov: 11943 -> 11943
170
Cov: 11943 -> 11943
Cov: 11943 -> 11943
171
Cov: 11943 -> 11943
Cov: 11943 -> 11943
172
Cov: 11943 -> 11943
Cov: 11943 -> 11943
173
Cov: 11943 -> 11943
Cov: 11943 -> 11943
174
Cov: 11943 -> 11943
Cov: 11943 -> 11943
175
Cov: 11943 -> 11943
Cov: 11943 -> 11943
176
Cov: 11943 -> 11952
Cov: 11952 -> 11952
177
Cov: 11952 -> 11952
Cov: 11952 -> 11952
178
Cov: 11952 -> 11952
Cov: 11952 -> 11952
179
Cov: 11952 -> 11952
Cov: 11952 -> 11952
180
Cov: 11952 -> 11952
Cov: 11952 -> 11952
181
Cov: 11952 -> 11955
Cov: 11955 -> 11955
182
Cov: 11955 -> 11955
Cov: 11955 -> 11955
183
Cov: 11955 -> 11955
Cov: 11955 -> 11955
184
Cov: 11955 -> 11955
Cov: 11955 -> 11955
185
Cov: 11955 -> 11955
Cov: 11955 -> 11955
186
{"exception": "TypeError", "msg": "put_(): argument 'index' (position 1) must be Tensor, not list"}
187
Cov: 11955 -> 11960
Cov: 11960 -> 11960
188
Cov: 11960 -> 11960
Cov: 11960 -> 11960
189
Cov: 11960 -> 11960
Cov: 11960 -> 11960
190
Cov: 11960 -> 11960
Cov: 11960 -> 11960
191
Cov: 11960 -> 11960
Cov: 11960 -> 11960
192
Cov: 11960 -> 11961
Cov: 11961 -> 11961
193
Cov: 11961 -> 11961
Cov: 11961 -> 11961
194
Cov: 11961 -> 11963
Cov: 11963 -> 11963
195
Cov: 11963 -> 11968
Cov: 11968 -> 11968
196
Cov: 11968 -> 11968
Cov: 11968 -> 11968
197
{"exception": "NotImplementedError", "msg": "Could not run 'aten::sparse_mask' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::sparse_mask' is only available for these backends: [SparseCPU, SparseCUDA, SparseCsrCPU, SparseCsrCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nSparseCPU: registered at aten/src/ATen/RegisterSparseCPU.cpp:1387 [kernel]\nSparseCUDA: registered at aten/src/ATen/RegisterSparseCUDA.cpp:1573 [kernel]\nSparseCsrCPU: registered at aten/src/ATen/RegisterSparseCsrCPU.cpp:1135 [kernel]\nSparseCsrCUDA: registered at aten/src/ATen/RegisterSparseCsrCUDA.cpp:1276 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:153 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:498 [backend fallback]\nFunctionalize: registered at ../aten/src/ATen/FunctionalizeFallbackKernel.cpp:290 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:86 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradMTIA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_0.cpp:16725 [kernel]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:382 [backend fallback]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:249 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:710 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:203 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:161 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:494 [backend fallback]\nPreDispatch: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:165 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:157 [backend fallback]\n"}
198
Cov: 11968 -> 11968
Cov: 11968 -> 11968
199
Cov: 11968 -> 11968
Cov: 11968 -> 11968
200
Cov: 11968 -> 11977
Cov: 11977 -> 11977
201
Cov: 11977 -> 11977
Cov: 11977 -> 11977
202
Cov: 11977 -> 11977
Cov: 11977 -> 11977
203
Cov: 11977 -> 11977
Cov: 11977 -> 11977
204
Cov: 11977 -> 11977
Cov: 11977 -> 11977
205
Cov: 11977 -> 11977
Cov: 11977 -> 11977
206
Cov: 11977 -> 11978
Cov: 11978 -> 11978
207
Cov: 11978 -> 11978
Cov: 11978 -> 11978
208
Cov: 11978 -> 11979
Cov: 11979 -> 11979
209
{"exception": "TypeError", "msg": "uniform_() got an unexpected keyword argument 'from_'"}
210
Cov: 11979 -> 11979
Cov: 11979 -> 11979
211
Cov: 11979 -> 11991
Cov: 11991 -> 11991
212
Cov: 11991 -> 11991
Cov: 11991 -> 11991
213
Cov: 11991 -> 11991
Cov: 11991 -> 11991
214
Cov: 11991 -> 11991
Cov: 11991 -> 11991
215
Cov: 11991 -> 11991
Cov: 11991 -> 11991
216
Cov: 11991 -> 11991
Cov: 11991 -> 11991
217
Cov: 11991 -> 12008
Cov: 12008 -> 12008
218
Cov: 12008 -> 12008
Cov: 12008 -> 12008
219
Cov: 12008 -> 12015
Cov: 12015 -> 12015
220
Cov: 12015 -> 12015
Cov: 12015 -> 12015
221
Cov: 12015 -> 12015
Cov: 12015 -> 12015
222
Cov: 12015 -> 12015
Cov: 12015 -> 12015
223
Cov: 12015 -> 12015
Cov: 12015 -> 12015
224
Cov: 12015 -> 12015
Cov: 12015 -> 12015
225
{"exception": "TypeError", "msg": "'getset_descriptor' object is not callable"}
226
Cov: 12015 -> 12015
Cov: 12015 -> 12015
227
Cov: 12015 -> 12015
Cov: 12015 -> 12015
228
Cov: 12015 -> 12031
Cov: 12031 -> 12031
229
Cov: 12031 -> 12031
Cov: 12031 -> 12031
230
Cov: 12031 -> 12033
Cov: 12033 -> 12033
231
Cov: 12033 -> 12035
Cov: 12035 -> 12035
232
Cov: 12035 -> 12035
Cov: 12035 -> 12035
233
Cov: 12035 -> 12035
Cov: 12035 -> 12035
234
Cov: 12035 -> 12035
Cov: 12035 -> 12035
235
Cov: 12035 -> 12041
Cov: 12041 -> 12041
236
Cov: 12041 -> 12089
Cov: 12089 -> 12089
237
Cov: 12089 -> 12089
Cov: 12089 -> 12089
238
Cov: 12089 -> 12089
Cov: 12089 -> 12089
239
Cov: 12089 -> 12281
Cov: 12281 -> 12281
240
Cov: 12281 -> 12281
Cov: 12281 -> 12281
241
Cov: 12281 -> 12283
Cov: 12283 -> 12283
242
Cov: 12283 -> 12288
Cov: 12288 -> 12288
243
Cov: 12288 -> 12288
Cov: 12288 -> 12288
244
Cov: 12288 -> 12288
Cov: 12288 -> 12288
245
Cov: 12288 -> 12299
Cov: 12299 -> 12299
246
Cov: 12299 -> 12299
Cov: 12299 -> 12299
247
Cov: 12299 -> 12301
Cov: 12301 -> 12301
248
Cov: 12301 -> 12301
Cov: 12301 -> 12301
249
{"exception": "RuntimeError", "msg": "This function was deprecated since version 1.9 and is now removed. The default behavior has changed from using the upper triangular portion of the matrix by default to using the lower triangular portion.\n\nL, _ = torch.symeig(A, upper=upper) should be replaced with:\nL = torch.linalg.eigvalsh(A, UPLO='U' if upper else 'L')\n\nand\n\nL, V = torch.symeig(A, eigenvectors=True) should be replaced with:\nL, V = torch.linalg.eigh(A, UPLO='U' if upper else 'L')"}
250
Cov: 12301 -> 12304
Cov: 12304 -> 12304
251
{"exception": "RuntimeError", "msg": "torch.ormqr: other.shape[-2] must be greater than or equal to tau.shape[-1]"}
252
Cov: 12304 -> 12314
Cov: 12314 -> 12314
253
Cov: 12314 -> 12369
Cov: 12369 -> 12369
254
Cov: 12369 -> 12369
Cov: 12369 -> 12369
255
Cov: 12369 -> 12369
Cov: 12369 -> 12369
256
Cov: 12369 -> 12370
Cov: 12370 -> 12370
257
Cov: 12370 -> 12370
Cov: 12370 -> 12370
258
Cov: 12370 -> 12371
Cov: 12371 -> 12371
259
Cov: 12371 -> 12371
Cov: 12371 -> 12371
260
Cov: 12371 -> 12377
Cov: 12377 -> 12377
261
Cov: 12377 -> 12377
Cov: 12377 -> 12377
262
Cov: 12377 -> 12377
Cov: 12377 -> 12377
263
Cov: 12377 -> 12377
Cov: 12377 -> 12377
264
Cov: 12377 -> 12377
Cov: 12377 -> 12377
265
Cov: 12377 -> 12398
Cov: 12398 -> 12398
266
Cov: 12398 -> 12431
Cov: 12431 -> 12431
267
Cov: 12431 -> 12431
Cov: 12431 -> 12431
268
Cov: 12431 -> 12431
Cov: 12431 -> 12431
269
Cov: 12431 -> 12431
Cov: 12431 -> 12431
270
Cov: 12431 -> 12431
Cov: 12431 -> 12431
271
Cov: 12431 -> 12431
Cov: 12431 -> 12431
272
Cov: 12431 -> 12431
Cov: 12431 -> 12431
273
Cov: 12431 -> 12431
Cov: 12431 -> 12431
274
Cov: 12431 -> 12431
Cov: 12431 -> 12431
275
Cov: 12431 -> 12431
Cov: 12431 -> 12431
276
Cov: 12431 -> 12432
Cov: 12432 -> 12432
277
Cov: 12432 -> 12432
Cov: 12432 -> 12432
278
Cov: 12432 -> 12432
Cov: 12432 -> 12432
279
Cov: 12432 -> 12470
Cov: 12470 -> 12470
280
Cov: 12470 -> 12478
Cov: 12478 -> 12478
281
Cov: 12478 -> 12478
Cov: 12478 -> 12478
282
Cov: 12478 -> 12478
Cov: 12478 -> 12478
283
Cov: 12478 -> 12478
Cov: 12478 -> 12478
284
Cov: 12478 -> 12478
Cov: 12478 -> 12478
285
Cov: 12478 -> 12478
Cov: 12478 -> 12478
286
Cov: 12478 -> 12488
Cov: 12488 -> 12488
287
Cov: 12488 -> 12488
Cov: 12488 -> 12488
288
Cov: 12488 -> 12488
Cov: 12488 -> 12488
289
{"exception": "TypeError", "msg": "msort() takes no keyword arguments"}
290
{"exception": "RuntimeError", "msg": "Bad in-place call: input tensor size [2, 3] and output tensor size [3, 5] should match"}
291
Cov: 12488 -> 12488
Cov: 12488 -> 12488
292
Cov: 12488 -> 12502
Cov: 12502 -> 12502
293
Cov: 12502 -> 12502
Cov: 12502 -> 12502
294
Cov: 12502 -> 12502
Cov: 12502 -> 12502
295
Cov: 12502 -> 12502
Cov: 12502 -> 12502
296
Cov: 12502 -> 12502
Cov: 12502 -> 12502
297
Cov: 12502 -> 12502
Cov: 12502 -> 12502
298
Cov: 12502 -> 12502
Cov: 12502 -> 12502
299
Cov: 12502 -> 12503
Cov: 12503 -> 12503
300
Cov: 12503 -> 12504
Cov: 12504 -> 12504
301
Cov: 12504 -> 12504
Cov: 12504 -> 12504
302
Cov: 12504 -> 12505
Cov: 12505 -> 12505
303
Cov: 12505 -> 12505
Cov: 12505 -> 12505
304
Cov: 12505 -> 12505
Cov: 12505 -> 12505
305
Cov: 12505 -> 12506
Cov: 12506 -> 12506
306
Cov: 12506 -> 12506
Cov: 12506 -> 12506
307
Cov: 12506 -> 12522
Cov: 12522 -> 12522
308
Cov: 12522 -> 12522
Cov: 12522 -> 12522
309
Cov: 12522 -> 12522
Cov: 12522 -> 12522
310
Cov: 12522 -> 12522
Cov: 12522 -> 12522
311
Cov: 12522 -> 12522
Cov: 12522 -> 12522
312
Cov: 12522 -> 12522
Cov: 12522 -> 12522
313
Cov: 12522 -> 12522
Cov: 12522 -> 12522
314
Cov: 12522 -> 12522
Cov: 12522 -> 12522
315
Cov: 12522 -> 12522
Cov: 12522 -> 12522
316
Cov: 12522 -> 12522
Cov: 12522 -> 12522
317
Cov: 12522 -> 12522
Cov: 12522 -> 12522
318
Cov: 12522 -> 12522
Cov: 12522 -> 12522
319
Cov: 12522 -> 12522
Cov: 12522 -> 12522
320
Cov: 12522 -> 12523
Cov: 12523 -> 12523
321
{"exception": "NotImplementedError", "msg": "Could not run 'aten::q_scale' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::q_scale' is only available for these backends: [QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nQuantizedCPU: registered at aten/src/ATen/RegisterQuantizedCPU.cpp:944 [kernel]\nQuantizedCUDA: registered at aten/src/ATen/RegisterQuantizedCUDA.cpp:459 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:153 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:498 [backend fallback]\nFunctionalize: registered at ../aten/src/ATen/FunctionalizeFallbackKernel.cpp:290 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:86 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradMTIA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_4.cpp:13056 [kernel]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:382 [backend fallback]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:249 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:710 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:203 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:161 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:494 [backend fallback]\nPreDispatch: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:165 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:157 [backend fallback]\n"}
322
Cov: 12523 -> 12523
Cov: 12523 -> 12523
323
Cov: 12523 -> 12523
Cov: 12523 -> 12523
324
Cov: 12523 -> 12523
Cov: 12523 -> 12523
325
Cov: 12523 -> 12523
Cov: 12523 -> 12523
326
Cov: 12523 -> 12523
Cov: 12523 -> 12523
327
Cov: 12523 -> 12523
Cov: 12523 -> 12523
328
Cov: 12523 -> 12529
Cov: 12529 -> 12529
329
Cov: 12529 -> 12533
Cov: 12533 -> 12533
330
Cov: 12533 -> 12533
Cov: 12533 -> 12533
331
Cov: 12533 -> 12533
Cov: 12533 -> 12533
332
Cov: 12533 -> 12534
Cov: 12534 -> 12534
333
Cov: 12534 -> 12538
Cov: 12538 -> 12538
334
Cov: 12538 -> 12538
Cov: 12538 -> 12538
335
Cov: 12538 -> 12611
Cov: 12611 -> 12611
336
Cov: 12611 -> 12611
Cov: 12611 -> 12611
337
Cov: 12611 -> 12611
Cov: 12611 -> 12611
338
Cov: 12611 -> 12611
Cov: 12611 -> 12611
339
{"exception": "TypeError", "msg": "'getset_descriptor' object is not callable"}
340
Cov: 12611 -> 12611
Cov: 12611 -> 12611
341
Cov: 12611 -> 12625
Cov: 12625 -> 12625
342
Cov: 12625 -> 12625
Cov: 12625 -> 12625
343
Cov: 12625 -> 12625
Cov: 12625 -> 12625
344
Cov: 12625 -> 12626
Cov: 12626 -> 12626
345
Cov: 12626 -> 12626
Cov: 12626 -> 12626
346
{"exception": "RuntimeError", "msg": "The expanded size of the tensor (3) must match the existing size (2) at non-singleton dimension 0.  Target sizes: [3, 3].  Tensor sizes: [2, 3]"}
347
Cov: 12626 -> 12627
Cov: 12627 -> 12627
348
Cov: 12627 -> 12627
Cov: 12627 -> 12627
349
Cov: 12627 -> 12627
Cov: 12627 -> 12627
350
Cov: 12627 -> 12627
Cov: 12627 -> 12627
351
Cov: 12627 -> 12627
Cov: 12627 -> 12627
352
Cov: 12627 -> 12627
Cov: 12627 -> 12627
353
Cov: 12627 -> 12627
Cov: 12627 -> 12627
354
Cov: 12627 -> 12627
Cov: 12627 -> 12627
355
Cov: 12627 -> 12627
Cov: 12627 -> 12627
356
Cov: 12627 -> 12630
Cov: 12630 -> 12630
357
Cov: 12630 -> 12638
Cov: 12638 -> 12638
358
{"exception": "RuntimeError", "msg": "This function was deprecated since version 1.9 and is now removed. `torch.lstsq` is deprecated in favor of `torch.linalg.lstsq`.\n`torch.linalg.lstsq` has reversed arguments and does not return the QR decomposition in the returned tuple (although it returns other information about the problem).\n\nTo get the QR decomposition consider using `torch.linalg.qr`.\n\nThe returned solution in `torch.lstsq` stored the residuals of the solution in the last m - n columns of the returned value whenever m > n. In torch.linalg.lstsq, the residuals are in the field 'residuals' of the returned named tuple.\n\nThe unpacking of the solution, as in\nX, _ = torch.lstsq(B, A).solution[:A.size(1)]\nshould be replaced with:\nX = torch.linalg.lstsq(A, B).solution"}
359
Cov: 12638 -> 12644
Cov: 12644 -> 12644
360
Cov: 12644 -> 12653
Cov: 12653 -> 12653
361
Cov: 12653 -> 12653
Cov: 12653 -> 12653
362
Cov: 12653 -> 12653
Cov: 12653 -> 12653
363
Cov: 12653 -> 12653
Cov: 12653 -> 12653
364
Cov: 12653 -> 12653
Cov: 12653 -> 12653
365
Cov: 12653 -> 12653
Cov: 12653 -> 12653
366
Cov: 12653 -> 12653
Cov: 12653 -> 12653
367
Cov: 12653 -> 12653
Cov: 12653 -> 12653
368
Cov: 12653 -> 12653
Cov: 12653 -> 12653
369
{"exception": "RuntimeError", "msg": "The expanded size of the tensor (4) must match the existing size (2) at non-singleton dimension 1.  Target sizes: [2, 4].  Tensor sizes: [2, 2]"}
370
Cov: 12653 -> 12653
Cov: 12653 -> 12653
371
Cov: 12653 -> 12653
Cov: 12653 -> 12653
372
Cov: 12653 -> 12653
Cov: 12653 -> 12653
373
Cov: 12653 -> 12653
Cov: 12653 -> 12653
374
Cov: 12653 -> 12653
Cov: 12653 -> 12653
375
Cov: 12653 -> 12653
Cov: 12653 -> 12653
376
Cov: 12653 -> 12653
Cov: 12653 -> 12653
377
{"exception": "RuntimeError", "msg": "index_copy_(): self and source expected to have the same dtype, but got (self) Double and (source) Long"}
378
Cov: 12653 -> 12653
Cov: 12653 -> 12653
379
{"exception": "RuntimeError", "msg": "1D tensors expected, but got 2D and 2D tensors"}
380
Cov: 12653 -> 12653
Cov: 12653 -> 12653
381
Cov: 12653 -> 12653
Cov: 12653 -> 12653
382
Cov: 12653 -> 12653
Cov: 12653 -> 12653
383
Cov: 12653 -> 12653
Cov: 12653 -> 12653
384
Cov: 12653 -> 12653
Cov: 12653 -> 12653
385
Cov: 12653 -> 20019
Cov: 20019 -> 20019
386
Cov: 20019 -> 20019
Cov: 20019 -> 20019
387
Cov: 20019 -> 20019
Cov: 20019 -> 20019
388
Cov: 20019 -> 20019
Cov: 20019 -> 20019
389
Cov: 20019 -> 20019
Cov: 20019 -> 20019
390
Cov: 20019 -> 20019
Cov: 20019 -> 20019
391
Cov: 20019 -> 20023
Cov: 20023 -> 20023
392
Cov: 20023 -> 20023
Cov: 20023 -> 20023
393
{"exception": "RuntimeError", "msg": "\"histogramdd\" not implemented for 'Long'"}
394
Cov: 20023 -> 20023
Cov: 20023 -> 20023
395
{"exception": "RuntimeError", "msg": "Index tensor must have the same number of dimensions as self tensor"}
396
Cov: 20023 -> 20023
Cov: 20023 -> 20023
397
Cov: 20023 -> 20024
Cov: 20024 -> 20024
398
Cov: 20024 -> 20024
Cov: 20024 -> 20024
399
Cov: 20024 -> 20024
Cov: 20024 -> 20024
400
Cov: 20024 -> 20034
Cov: 20034 -> 20034
401
Cov: 20034 -> 20034
Cov: 20034 -> 20034
402
Cov: 20034 -> 20034
Cov: 20034 -> 20034
403
{"exception": "RuntimeError", "msg": "source tensor shape must match self tensor shape, excluding the specified dimension. Got self.shape = [3, 3] source.shape = [2]"}
404
{"exception": "RuntimeError", "msg": "linalg.slogdet: A must be batches of square matrices, but they are 4 by 5 matrices"}
405
Cov: 20034 -> 20034
Cov: 20034 -> 20034
406
Cov: 20034 -> 20034
Cov: 20034 -> 20034
407
Cov: 20034 -> 20034
Cov: 20034 -> 20034
408
{"exception": "TypeError", "msg": "ldexp(): argument 'other' (position 1) must be Tensor, not int"}
409
Cov: 20034 -> 20034
Cov: 20034 -> 20034
410
{"exception": "RuntimeError", "msg": "This function was deprecated since version 1.9 and is now removed. The default behavior has changed from using the upper triangular portion of the matrix by default to using the lower triangular portion.\n\nL, _ = torch.symeig(A, upper=upper) should be replaced with:\nL = torch.linalg.eigvalsh(A, UPLO='U' if upper else 'L')\n\nand\n\nL, V = torch.symeig(A, eigenvectors=True) should be replaced with:\nL, V = torch.linalg.eigh(A, UPLO='U' if upper else 'L')"}
411
Cov: 20034 -> 20035
Cov: 20035 -> 20035
412
Cov: 20035 -> 20035
Cov: 20035 -> 20035
413
Cov: 20035 -> 20040
Cov: 20040 -> 20040
414
Cov: 20040 -> 20040
Cov: 20040 -> 20040
415
Cov: 20040 -> 20040
Cov: 20040 -> 20040
416
{"exception": "RuntimeError", "msg": "result type Double can't be cast to the desired output type Long"}
417
Cov: 20040 -> 20040
Cov: 20040 -> 20040
418
Cov: 20040 -> 20040
Cov: 20040 -> 20040
419
Cov: 20040 -> 20040
Cov: 20040 -> 20040
420
Cov: 20040 -> 20041
Cov: 20041 -> 20041
421
Cov: 20041 -> 20041
Cov: 20041 -> 20041
422
Cov: 20041 -> 20041
Cov: 20041 -> 20041
423
{"exception": "TypeError", "msg": "normal_(): argument 'mean' must be float, not Tensor"}
424
Cov: 20041 -> 20041
Cov: 20041 -> 20041
425
Cov: 20041 -> 20042
Cov: 20042 -> 20042
426
Cov: 20042 -> 20047
Cov: 20047 -> 20047
427
Cov: 20047 -> 20047
Cov: 20047 -> 20047
428
Cov: 20047 -> 20047
Cov: 20047 -> 20047
429
Cov: 20047 -> 20047
Cov: 20047 -> 20047
430
Cov: 20047 -> 20047
Cov: 20047 -> 20047
431
Cov: 20047 -> 20047
Cov: 20047 -> 20047
432
Cov: 20047 -> 20047
Cov: 20047 -> 20047
433
Cov: 20047 -> 20059
Cov: 20059 -> 20059
434
Cov: 20059 -> 20059
Cov: 20059 -> 20059
435
Cov: 20059 -> 20059
Cov: 20059 -> 20059
436
Cov: 20059 -> 20059
Cov: 20059 -> 20059
437
Cov: 20059 -> 20059
Cov: 20059 -> 20059
438
{"exception": "RuntimeError", "msg": "This function was deprecated since version 1.9 and is now removed. `torch.linalg.eig` returns complex tensors of dtype `cfloat` or `cdouble` rather than real tensors mimicking complex tensors.\n\nL, _ = torch.eig(A) should be replaced with:\nL_complex = torch.linalg.eigvals(A)\n\nand\n\nL, V = torch.eig(A, eigenvectors=True) should be replaced with:\nL_complex, V_complex = torch.linalg.eig(A)"}
439
Cov: 20059 -> 20062
Cov: 20062 -> 20062
440
Cov: 20062 -> 20062
Cov: 20062 -> 20062
441
Cov: 20062 -> 20062
Cov: 20062 -> 20062
442
Cov: 20062 -> 20062
Cov: 20062 -> 20062
443
Cov: 20062 -> 20062
Cov: 20062 -> 20062
444
{"exception": "RuntimeError", "msg": "a leaf Variable that requires grad is being used in an in-place operation."}
445
{"exception": "NotImplementedError", "msg": "Could not run 'aten::q_scale' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::q_scale' is only available for these backends: [QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nQuantizedCPU: registered at aten/src/ATen/RegisterQuantizedCPU.cpp:944 [kernel]\nQuantizedCUDA: registered at aten/src/ATen/RegisterQuantizedCUDA.cpp:459 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:153 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:498 [backend fallback]\nFunctionalize: registered at ../aten/src/ATen/FunctionalizeFallbackKernel.cpp:290 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:86 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradMTIA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_4.cpp:13056 [kernel]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:382 [backend fallback]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:249 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:710 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:203 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:161 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:494 [backend fallback]\nPreDispatch: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:165 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:157 [backend fallback]\n"}
446
Cov: 20062 -> 20066
Cov: 20066 -> 20066
447
Cov: 20066 -> 20078
Cov: 20078 -> 20078
448
{"exception": "RuntimeError", "msg": "Quantize only works on Float Tensor, got Double"}
449
Cov: 20078 -> 20081
Cov: 20081 -> 20081
450
Cov: 20081 -> 20081
Cov: 20081 -> 20081
451
Cov: 20081 -> 20081
Cov: 20081 -> 20081
452
Cov: 20081 -> 20081
Cov: 20081 -> 20081
453
Cov: 20081 -> 20081
Cov: 20081 -> 20081
454
Cov: 20081 -> 20082
Cov: 20082 -> 20082
455
Cov: 20082 -> 20083
Cov: 20083 -> 20083
456
Cov: 20083 -> 20086
Cov: 20086 -> 20086
457
Cov: 20086 -> 20086
Cov: 20086 -> 20086
458
Cov: 20086 -> 20086
Cov: 20086 -> 20086
459
Cov: 20086 -> 20086
Cov: 20086 -> 20086
460
Cov: 20086 -> 20086
Cov: 20086 -> 20086
461
Cov: 20086 -> 20086
Cov: 20086 -> 20086
462
Cov: 20086 -> 20086
Cov: 20086 -> 20086
463
Cov: 20086 -> 20086
Cov: 20086 -> 20086
464
Cov: 20086 -> 20086
Cov: 20086 -> 20086
465
Cov: 20086 -> 20086
Cov: 20086 -> 20086
466
Cov: 20086 -> 20086
Cov: 20086 -> 20086
467
Cov: 20086 -> 20086
Cov: 20086 -> 20086
468
Cov: 20086 -> 20086
Cov: 20086 -> 20086
469
=================================================================
timeout reached. testcase took: 10
[Error] ...
Hangs during coverage collection.
Had to restart coverage executor...
timeout
470
Cov: 20086 -> 0
Cov: 0 -> 0
471
Cov: 0 -> 68
Cov: 68 -> 68
472
Cov: 68 -> 87
Cov: 87 -> 87
473
Cov: 87 -> 144
Cov: 144 -> 144
474
Cov: 144 -> 144
Cov: 144 -> 144
475
Cov: 144 -> 144
Cov: 144 -> 144
476
Cov: 144 -> 144
Cov: 144 -> 144
477
Cov: 144 -> 148
Cov: 148 -> 148
478
Cov: 148 -> 148
Cov: 148 -> 148
479
Cov: 148 -> 148
Cov: 148 -> 148
480
Cov: 148 -> 157
Cov: 157 -> 157
481
Cov: 157 -> 157
Cov: 157 -> 157
482
Cov: 157 -> 157
Cov: 157 -> 157
483
Cov: 157 -> 181
Cov: 181 -> 181
484
Cov: 181 -> 485
Cov: 485 -> 485
485
Cov: 485 -> 521
Cov: 521 -> 521
486
Cov: 521 -> 521
Cov: 521 -> 521
487
Cov: 521 -> 521
Cov: 521 -> 521
488
Cov: 521 -> 521
Cov: 521 -> 521
489
Cov: 521 -> 537
Cov: 537 -> 537
490
Cov: 537 -> 537
Cov: 537 -> 537
491
Cov: 537 -> 540
Cov: 540 -> 540
492
Cov: 540 -> 550
Cov: 550 -> 550
493
Cov: 550 -> 550
Cov: 550 -> 550
494
Cov: 550 -> 551
Cov: 551 -> 551
495
{"exception": "RuntimeError", "msg": "This function was deprecated since version 1.9 and is now removed. The default behavior has changed from using the upper triangular portion of the matrix by default to using the lower triangular portion.\n\nL, _ = torch.symeig(A, upper=upper) should be replaced with:\nL = torch.linalg.eigvalsh(A, UPLO='U' if upper else 'L')\n\nand\n\nL, V = torch.symeig(A, eigenvectors=True) should be replaced with:\nL, V = torch.linalg.eigh(A, UPLO='U' if upper else 'L')"}
496
Cov: 551 -> 16932
Cov: 16932 -> 16932
497
Cov: 16932 -> 16932
Cov: 16932 -> 16932
498
{"exception": "NameError", "msg": "name 'torchvision' is not defined"}
499
Cov: 16932 -> 16932
Cov: 16932 -> 16932
500
Cov: 16932 -> 16932
Cov: 16932 -> 16932
501
Cov: 16932 -> 16932
Cov: 16932 -> 16932
502
Cov: 16932 -> 16932
Cov: 16932 -> 16932
503
Cov: 16932 -> 16960
Cov: 16960 -> 16960
504
Cov: 16960 -> 16960
Cov: 16960 -> 16960
505
Cov: 16960 -> 16960
Cov: 16960 -> 16960
506
Cov: 16960 -> 16960
Cov: 16960 -> 16960
507
Cov: 16960 -> 16971
Cov: 16971 -> 16971
508
{"exception": "RuntimeError", "msg": "torch.ormqr: Expected tau to have one dimension less than input, but got tau.ndim equal to 4 and input.ndim is equal to 4"}
509
Cov: 16971 -> 16971
Cov: 16971 -> 16971
510
Cov: 16971 -> 16971
Cov: 16971 -> 16971
511
Cov: 16971 -> 16971
Cov: 16971 -> 16971
512
Cov: 16971 -> 16973
Cov: 16973 -> 16973
513
Cov: 16973 -> 16973
Cov: 16973 -> 16973
514
Cov: 16973 -> 16978
Cov: 16978 -> 16978
515
Cov: 16978 -> 16978
Cov: 16978 -> 16978
516
Cov: 16978 -> 16978
Cov: 16978 -> 16978
517
Cov: 16978 -> 16978
Cov: 16978 -> 16978
518
Cov: 16978 -> 16978
Cov: 16978 -> 16978
519
Cov: 16978 -> 16978
Cov: 16978 -> 16978
520
{"exception": "NotImplementedError", "msg": "Could not run 'aten::q_per_channel_axis' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::q_per_channel_axis' is only available for these backends: [QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nQuantizedCPU: registered at aten/src/ATen/RegisterQuantizedCPU.cpp:944 [kernel]\nQuantizedCUDA: registered at aten/src/ATen/RegisterQuantizedCUDA.cpp:459 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:153 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:498 [backend fallback]\nFunctionalize: registered at ../aten/src/ATen/FunctionalizeFallbackKernel.cpp:290 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:86 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradMTIA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_4.cpp:13056 [kernel]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:382 [backend fallback]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:249 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:710 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:203 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:161 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:494 [backend fallback]\nPreDispatch: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:165 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:157 [backend fallback]\n"}
521
Cov: 16978 -> 16978
Cov: 16978 -> 16978
522
Cov: 16978 -> 16978
Cov: 16978 -> 16978
523
Cov: 16978 -> 16978
Cov: 16978 -> 16978
524
{"exception": "TypeError", "msg": "'getset_descriptor' object is not callable"}
525
Cov: 16978 -> 16984
Cov: 16984 -> 16984
526
{"exception": "RuntimeError", "msg": "all: You passed a dimname (string) to this op in place of a dimension index but it does not yet support this behavior. Please pass a dimension index to work around this."}
527
Cov: 16984 -> 16984
Cov: 16984 -> 16984
528
{"exception": "TypeError", "msg": "heaviside(): argument 'values' (position 1) must be Tensor, not float"}
529
Cov: 16984 -> 16984
Cov: 16984 -> 16984
530
Cov: 16984 -> 16984
Cov: 16984 -> 16984
531
Cov: 16984 -> 16984
Cov: 16984 -> 16984
532
Cov: 16984 -> 16984
Cov: 16984 -> 16984
533
Cov: 16984 -> 16984
Cov: 16984 -> 16984
534
Cov: 16984 -> 16984
Cov: 16984 -> 16984
535
Cov: 16984 -> 16984
Cov: 16984 -> 16984
536
Cov: 16984 -> 16984
Cov: 16984 -> 16984
537
Cov: 16984 -> 17038
Cov: 17038 -> 17038
538
Cov: 17038 -> 17038
Cov: 17038 -> 17038
539
Cov: 17038 -> 17038
Cov: 17038 -> 17038
540
Cov: 17038 -> 17038
Cov: 17038 -> 17038
541
Cov: 17038 -> 17039
Cov: 17039 -> 17039
542
Cov: 17039 -> 17039
Cov: 17039 -> 17039
543
Cov: 17039 -> 17039
Cov: 17039 -> 17039
544
Cov: 17039 -> 17048
Cov: 17048 -> 17048
545
Cov: 17048 -> 17048
Cov: 17048 -> 17048
546
Cov: 17048 -> 17048
Cov: 17048 -> 17048
547
Cov: 17048 -> 17049
Cov: 17049 -> 17049
548
Cov: 17049 -> 17049
Cov: 17049 -> 17049
549
Cov: 17049 -> 17049
Cov: 17049 -> 17049
550
Cov: 17049 -> 17049
Cov: 17049 -> 17049
551
Cov: 17049 -> 17049
Cov: 17049 -> 17049
552
Cov: 17049 -> 17081
Cov: 17081 -> 17081
553
Cov: 17081 -> 17081
Cov: 17081 -> 17081
554
Cov: 17081 -> 17128
Cov: 17128 -> 17128
555
Cov: 17128 -> 17135
Cov: 17135 -> 17135
556
Cov: 17135 -> 17136
Cov: 17136 -> 17136
557
{"exception": "RuntimeError", "msg": "all: You passed a dimname (string) to this op in place of a dimension index but it does not yet support this behavior. Please pass a dimension index to work around this."}
558
Cov: 17136 -> 17136
Cov: 17136 -> 17136
559
Cov: 17136 -> 17136
Cov: 17136 -> 17136
560
Cov: 17136 -> 17136
Cov: 17136 -> 17136
561
Cov: 17136 -> 17136
Cov: 17136 -> 17136
562
Cov: 17136 -> 17136
Cov: 17136 -> 17136
563
Cov: 17136 -> 17136
Cov: 17136 -> 17136
564
Cov: 17136 -> 17140
Cov: 17140 -> 17140
565
Cov: 17140 -> 17140
Cov: 17140 -> 17140
566
Cov: 17140 -> 17140
Cov: 17140 -> 17140
567
Cov: 17140 -> 17140
Cov: 17140 -> 17140
568
Cov: 17140 -> 17140
Cov: 17140 -> 17140
569
Cov: 17140 -> 17140
Cov: 17140 -> 17140
570
Cov: 17140 -> 17149
Cov: 17149 -> 17149
571
Cov: 17149 -> 17149
Cov: 17149 -> 17149
572
Cov: 17149 -> 17149
Cov: 17149 -> 17149
573
Cov: 17149 -> 17149
Cov: 17149 -> 17149
574
Cov: 17149 -> 17149
Cov: 17149 -> 17149
575
Cov: 17149 -> 17153
Cov: 17153 -> 17153
576
Cov: 17153 -> 17153
Cov: 17153 -> 17153
577
Cov: 17153 -> 17155
Cov: 17155 -> 17155
578
Cov: 17155 -> 17156
Cov: 17156 -> 17156
579
Cov: 17156 -> 17158
Cov: 17158 -> 17158
580
Cov: 17158 -> 17336
Cov: 17336 -> 17336
581
Cov: 17336 -> 17336
Cov: 17336 -> 17336
582
Cov: 17336 -> 17336
Cov: 17336 -> 17336
583
Cov: 17336 -> 17336
Cov: 17336 -> 17336
584
Cov: 17336 -> 17336
Cov: 17336 -> 17336
585
Cov: 17336 -> 17336
Cov: 17336 -> 17336
586
Cov: 17336 -> 17336
Cov: 17336 -> 17336
587
Cov: 17336 -> 17336
Cov: 17336 -> 17336
588
Cov: 17336 -> 17345
Cov: 17345 -> 17345
589
Cov: 17345 -> 17345
Cov: 17345 -> 17345
590
Cov: 17345 -> 17345
Cov: 17345 -> 17345
591
Cov: 17345 -> 17346
Cov: 17346 -> 17346
592
Cov: 17346 -> 17350
Cov: 17350 -> 17350
593
Cov: 17350 -> 17406
Cov: 17406 -> 17406
594
Cov: 17406 -> 17406
Cov: 17406 -> 17406
595
Cov: 17406 -> 17406
Cov: 17406 -> 17406
596
Cov: 17406 -> 17406
Cov: 17406 -> 17406
597
Cov: 17406 -> 17406
Cov: 17406 -> 17406
598
Cov: 17406 -> 17424
Cov: 17424 -> 17424
599
Cov: 17424 -> 17424
Cov: 17424 -> 17424
600
Cov: 17424 -> 17424
Cov: 17424 -> 17424
601
Cov: 17424 -> 17424
Cov: 17424 -> 17424
602
Cov: 17424 -> 17427
Cov: 17427 -> 17427
603
Cov: 17427 -> 17443
Cov: 17443 -> 17443
604
{"exception": "NotImplementedError", "msg": "Could not run 'aten::int_repr' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::int_repr' is only available for these backends: [QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nQuantizedCPU: registered at aten/src/ATen/RegisterQuantizedCPU.cpp:944 [kernel]\nQuantizedCUDA: registered at aten/src/ATen/RegisterQuantizedCUDA.cpp:459 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:153 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:498 [backend fallback]\nFunctionalize: registered at ../aten/src/ATen/FunctionalizeFallbackKernel.cpp:290 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:86 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradMTIA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_0.cpp:16725 [kernel]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:382 [backend fallback]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:249 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:710 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:203 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:161 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:494 [backend fallback]\nPreDispatch: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:165 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:157 [backend fallback]\n"}
605
Cov: 17443 -> 17763
Cov: 17763 -> 17763
606
{"exception": "RuntimeError", "msg": "source tensor shape must match self tensor shape, excluding the specified dimension. Got self.shape = [3, 3] source.shape = [2, 2]"}
607
Cov: 17763 -> 17763
Cov: 17763 -> 17763
608
Cov: 17763 -> 17763
Cov: 17763 -> 17763
609
Cov: 17763 -> 17763
Cov: 17763 -> 17763
610
Cov: 17763 -> 17780
Cov: 17780 -> 17780
611
Cov: 17780 -> 17780
Cov: 17780 -> 17780
612
Cov: 17780 -> 17780
Cov: 17780 -> 17780
613
Cov: 17780 -> 17780
Cov: 17780 -> 17780
614
Cov: 17780 -> 17780
Cov: 17780 -> 17780
615
Cov: 17780 -> 17780
Cov: 17780 -> 17780
616
Cov: 17780 -> 17780
Cov: 17780 -> 17780
617
Cov: 17780 -> 17780
Cov: 17780 -> 17780
618
Cov: 17780 -> 17780
Cov: 17780 -> 17780
619
Cov: 17780 -> 17780
Cov: 17780 -> 17780
620
Cov: 17780 -> 17780
Cov: 17780 -> 17780
621
Cov: 17780 -> 17780
Cov: 17780 -> 17780
622
Cov: 17780 -> 17791
Cov: 17791 -> 17791
623
Cov: 17791 -> 17791
Cov: 17791 -> 17791
624
Cov: 17791 -> 17791
Cov: 17791 -> 17791
625
Cov: 17791 -> 17791
Cov: 17791 -> 17791
626
Cov: 17791 -> 17791
Cov: 17791 -> 17791
627
Cov: 17791 -> 17791
Cov: 17791 -> 17791
628
Cov: 17791 -> 17791
Cov: 17791 -> 17791
629
Cov: 17791 -> 17791
Cov: 17791 -> 17791
630
Cov: 17791 -> 17792
Cov: 17792 -> 17792
631
Cov: 17792 -> 17792
Cov: 17792 -> 17792
632
Cov: 17792 -> 17792
Cov: 17792 -> 17792
633
Cov: 17792 -> 17792
Cov: 17792 -> 17792
634
{"exception": "TypeError", "msg": "'getset_descriptor' object is not callable"}
635
Cov: 17792 -> 17792
Cov: 17792 -> 17792
636
Cov: 17792 -> 17792
Cov: 17792 -> 17792
637
Cov: 17792 -> 17792
Cov: 17792 -> 17792
638
Cov: 17792 -> 17793
Cov: 17793 -> 17793
639
Cov: 17793 -> 17797
Cov: 17797 -> 17797
640
Cov: 17797 -> 17797
Cov: 17797 -> 17797
641
Cov: 17797 -> 17797
Cov: 17797 -> 17797
642
Cov: 17797 -> 17797
Cov: 17797 -> 17797
643
Cov: 17797 -> 17797
Cov: 17797 -> 17797
644
Cov: 17797 -> 17810
Cov: 17810 -> 17810
645
Cov: 17810 -> 17810
Cov: 17810 -> 17810
646
{"exception": "TypeError", "msg": "'getset_descriptor' object is not callable"}
647
Cov: 17810 -> 17812
Cov: 17812 -> 17812
648
{"exception": "RuntimeError", "msg": "Please look up dimensions by name, got: name = None."}
649
Cov: 17812 -> 17818
Cov: 17818 -> 17818
650
Cov: 17818 -> 17818
Cov: 17818 -> 17818
651
{"exception": "RuntimeError", "msg": "Expected nested_tensorlist[0].size() > 0 to be true, but got false.  (Could this error message be improved?  If so, please report an enhancement request to PyTorch.)"}
652
Cov: 17818 -> 17850
Cov: 17850 -> 17850
653
Cov: 17850 -> 17850
Cov: 17850 -> 17850
654
Cov: 17850 -> 17850
Cov: 17850 -> 17850
655
Cov: 17850 -> 17850
Cov: 17850 -> 17850
656
Cov: 17850 -> 17850
Cov: 17850 -> 17850
657
Cov: 17850 -> 17850
Cov: 17850 -> 17850
658
Cov: 17850 -> 17850
Cov: 17850 -> 17850
659
Cov: 17850 -> 17854
Cov: 17854 -> 17854
660
Cov: 17854 -> 17854
Cov: 17854 -> 17854
661
Cov: 17854 -> 17856
Cov: 17856 -> 17856
662
Cov: 17856 -> 17871
Cov: 17871 -> 17871
663
Cov: 17871 -> 17899
Cov: 17899 -> 17899
664
{"exception": "RuntimeError", "msg": "This function was deprecated since version 1.9 and is now removed.\nPlease use the `torch.linalg.matrix_rank` function instead. The parameter 'symmetric' was renamed in `torch.linalg.matrix_rank()` to 'hermitian'."}
665
{"exception": "TypeError", "msg": "'getset_descriptor' object is not callable"}
666
Cov: 17899 -> 17902
Cov: 17902 -> 17902
667
Cov: 17902 -> 17923
Cov: 17923 -> 17923
668
Cov: 17923 -> 17923
Cov: 17923 -> 17923
669
Cov: 17923 -> 17923
Cov: 17923 -> 17923
670
Cov: 17923 -> 17926
Cov: 17926 -> 17926
671
Cov: 17926 -> 17976
Cov: 17976 -> 17976
672
Cov: 17976 -> 17977
Cov: 17977 -> 17977
673
Cov: 17977 -> 17977
Cov: 17977 -> 17977
674
Cov: 17977 -> 17980
Cov: 17980 -> 17980
675
Cov: 17980 -> 17980
Cov: 17980 -> 17980
676
Cov: 17980 -> 17993
Cov: 17993 -> 17993
677
Cov: 17993 -> 17995
Cov: 17995 -> 17995
678
Cov: 17995 -> 17995
Cov: 17995 -> 17995
679
{"exception": "TypeError", "msg": "logical_xor(): argument 'other' must be Tensor, not bool"}
680
Cov: 17995 -> 17995
Cov: 17995 -> 17995
681
Cov: 17995 -> 17995
Cov: 17995 -> 17995
682
Cov: 17995 -> 17995
Cov: 17995 -> 17995
683
Cov: 17995 -> 17995
Cov: 17995 -> 17995
684
Cov: 17995 -> 17998
Cov: 17998 -> 17998
685
Cov: 17998 -> 17998
Cov: 17998 -> 17998
686
Cov: 17998 -> 17998
Cov: 17998 -> 17998
687
Cov: 17998 -> 17998
Cov: 17998 -> 17998
688
{"exception": "RuntimeError", "msg": "required rank 4 tensor to use channels_last format"}
689
Cov: 17998 -> 18042
Cov: 18042 -> 18042
690
Cov: 18042 -> 18046
Cov: 18046 -> 18046
691
Cov: 18046 -> 18046
Cov: 18046 -> 18046
692
Cov: 18046 -> 18046
Cov: 18046 -> 18046
693
Cov: 18046 -> 18047
Cov: 18047 -> 18047
694
Cov: 18047 -> 18047
Cov: 18047 -> 18047
695
Cov: 18047 -> 18047
Cov: 18047 -> 18047
696
Cov: 18047 -> 18047
Cov: 18047 -> 18047
697
Cov: 18047 -> 18047
Cov: 18047 -> 18047
698
Cov: 18047 -> 18047
Cov: 18047 -> 18047
699
Cov: 18047 -> 18047
Cov: 18047 -> 18047
700
Cov: 18047 -> 18047
Cov: 18047 -> 18047
701
Cov: 18047 -> 18047
Cov: 18047 -> 18047
702
Cov: 18047 -> 18047
Cov: 18047 -> 18047
703
Cov: 18047 -> 18047
Cov: 18047 -> 18047
704
Cov: 18047 -> 18048
Cov: 18048 -> 18048
705
Cov: 18048 -> 18048
Cov: 18048 -> 18048
706
Cov: 18048 -> 18048
Cov: 18048 -> 18048
707
Cov: 18048 -> 18048
Cov: 18048 -> 18048
708
Cov: 18048 -> 18048
Cov: 18048 -> 18048
709
Cov: 18048 -> 18048
Cov: 18048 -> 18048
710
Cov: 18048 -> 18048
Cov: 18048 -> 18048
711
Cov: 18048 -> 18048
Cov: 18048 -> 18048
712
Cov: 18048 -> 18048
Cov: 18048 -> 18048
713
{"exception": "NotImplementedError", "msg": "Could not run 'aten::sparse_mask' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::sparse_mask' is only available for these backends: [SparseCPU, SparseCUDA, SparseCsrCPU, SparseCsrCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nSparseCPU: registered at aten/src/ATen/RegisterSparseCPU.cpp:1387 [kernel]\nSparseCUDA: registered at aten/src/ATen/RegisterSparseCUDA.cpp:1573 [kernel]\nSparseCsrCPU: registered at aten/src/ATen/RegisterSparseCsrCPU.cpp:1135 [kernel]\nSparseCsrCUDA: registered at aten/src/ATen/RegisterSparseCsrCUDA.cpp:1276 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:153 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:498 [backend fallback]\nFunctionalize: registered at ../aten/src/ATen/FunctionalizeFallbackKernel.cpp:290 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:86 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradMTIA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_0.cpp:16725 [kernel]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:382 [backend fallback]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:249 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:710 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:203 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:161 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:494 [backend fallback]\nPreDispatch: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:165 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:157 [backend fallback]\n"}
714
Cov: 18048 -> 18048
Cov: 18048 -> 18048
715
{"exception": "RuntimeError", "msg": "a leaf Variable that requires grad is being used in an in-place operation."}
716
Cov: 18048 -> 18048
Cov: 18048 -> 18048
717
Cov: 18048 -> 18048
Cov: 18048 -> 18048
718
Cov: 18048 -> 18048
Cov: 18048 -> 18048
719
Cov: 18048 -> 18048
Cov: 18048 -> 18048
720
Cov: 18048 -> 18061
Cov: 18061 -> 18061
721
Cov: 18061 -> 18062
Cov: 18062 -> 18062
722
Cov: 18062 -> 18062
Cov: 18062 -> 18062
723
Cov: 18062 -> 18062
Cov: 18062 -> 18062
724
Cov: 18062 -> 18062
Cov: 18062 -> 18062
725
Cov: 18062 -> 18062
Cov: 18062 -> 18062
726
Cov: 18062 -> 18062
Cov: 18062 -> 18062
727
Cov: 18062 -> 18062
Cov: 18062 -> 18062
728
Cov: 18062 -> 18062
Cov: 18062 -> 18062
729
Cov: 18062 -> 18062
Cov: 18062 -> 18062
730
Cov: 18062 -> 18062
Cov: 18062 -> 18062
731
Cov: 18062 -> 18062
Cov: 18062 -> 18062
732
Cov: 18062 -> 18062
Cov: 18062 -> 18062
733
Cov: 18062 -> 18062
Cov: 18062 -> 18062
734
Cov: 18062 -> 18067
Cov: 18067 -> 18067
735
Cov: 18067 -> 18067
Cov: 18067 -> 18067
736
Cov: 18067 -> 18067
Cov: 18067 -> 18067
737
Cov: 18067 -> 18067
Cov: 18067 -> 18067
738
Cov: 18067 -> 18067
Cov: 18067 -> 18067
739
Cov: 18067 -> 18072
Cov: 18072 -> 18072
740
Cov: 18072 -> 18072
Cov: 18072 -> 18072
741
{"exception": "RuntimeError", "msg": "Index tensor must have the same number of dimensions as input tensor"}
742
Cov: 18072 -> 18072
Cov: 18072 -> 18072
743
Cov: 18072 -> 18077
Cov: 18077 -> 18077
744
Cov: 18077 -> 18077
Cov: 18077 -> 18077
745
Cov: 18077 -> 18077
Cov: 18077 -> 18077
746
Cov: 18077 -> 18077
Cov: 18077 -> 18077
747
Cov: 18077 -> 18077
Cov: 18077 -> 18077
748
Cov: 18077 -> 18083
Cov: 18083 -> 18083
749
Cov: 18083 -> 18087
Cov: 18087 -> 18087
750
Cov: 18087 -> 18091
Cov: 18091 -> 18091
751
{"exception": "TypeError", "msg": "addbmm() received an invalid combination of arguments - got (Tensor), but expected (Tensor batch1, Tensor batch2, *, Number beta, Number alpha)"}
752
Cov: 18091 -> 18091
Cov: 18091 -> 18091
753
Cov: 18091 -> 18091
Cov: 18091 -> 18091
754
Cov: 18091 -> 18091
Cov: 18091 -> 18091
755
Cov: 18091 -> 18091
Cov: 18091 -> 18091
756
Cov: 18091 -> 18091
Cov: 18091 -> 18091
757
Cov: 18091 -> 18091
Cov: 18091 -> 18091
758
{"exception": "TypeError", "msg": "igammac(): argument 'other' (position 1) must be Tensor, not int"}
759
Cov: 18091 -> 18091
Cov: 18091 -> 18091
760
{"exception": "RuntimeError", "msg": "masked_fill_ only supports boolean masks, but got mask with dtype unsigned char"}
761
Cov: 18091 -> 18091
Cov: 18091 -> 18091
762
Cov: 18091 -> 18091
Cov: 18091 -> 18091
763
Cov: 18091 -> 18091
Cov: 18091 -> 18091
764
Cov: 18091 -> 18095
Cov: 18095 -> 18095
765
Cov: 18095 -> 18095
Cov: 18095 -> 18095
766
Cov: 18095 -> 18095
Cov: 18095 -> 18095
767
Cov: 18095 -> 18095
Cov: 18095 -> 18095
768
Cov: 18095 -> 18095
Cov: 18095 -> 18095
769
Cov: 18095 -> 18095
Cov: 18095 -> 18095
770
Cov: 18095 -> 18095
Cov: 18095 -> 18095
771
Cov: 18095 -> 18095
Cov: 18095 -> 18095
772
{"exception": "RuntimeError", "msg": "masked_select: expected BoolTensor for mask"}
773
Cov: 18095 -> 18109
Cov: 18109 -> 18109
774
Cov: 18109 -> 18109
Cov: 18109 -> 18109
775
Cov: 18109 -> 18109
Cov: 18109 -> 18109
776
{"exception": "NotImplementedError", "msg": "Could not run 'aten::q_scale' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::q_scale' is only available for these backends: [QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nQuantizedCPU: registered at aten/src/ATen/RegisterQuantizedCPU.cpp:944 [kernel]\nQuantizedCUDA: registered at aten/src/ATen/RegisterQuantizedCUDA.cpp:459 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:153 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:498 [backend fallback]\nFunctionalize: registered at ../aten/src/ATen/FunctionalizeFallbackKernel.cpp:290 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:86 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradMTIA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_4.cpp:13056 [kernel]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:382 [backend fallback]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:249 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:710 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:203 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:161 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:494 [backend fallback]\nPreDispatch: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:165 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:157 [backend fallback]\n"}
777
Cov: 18109 -> 18109
Cov: 18109 -> 18109
778
Cov: 18109 -> 18116
Cov: 18116 -> 18116
779
Cov: 18116 -> 18116
Cov: 18116 -> 18116
780
Cov: 18116 -> 18116
Cov: 18116 -> 18116
781
Cov: 18116 -> 18116
Cov: 18116 -> 18116
782
Cov: 18116 -> 18116
Cov: 18116 -> 18116
783
Cov: 18116 -> 18116
Cov: 18116 -> 18116
784
Cov: 18116 -> 18116
Cov: 18116 -> 18116
785
Cov: 18116 -> 18116
Cov: 18116 -> 18116
786
Cov: 18116 -> 18116
Cov: 18116 -> 18116
787
Cov: 18116 -> 18116
Cov: 18116 -> 18116
788
{"exception": "RuntimeError", "msg": "any: You passed a dimname (string) to this op in place of a dimension index but it does not yet support this behavior. Please pass a dimension index to work around this."}
789
Cov: 18116 -> 18116
Cov: 18116 -> 18116
790
Cov: 18116 -> 18116
Cov: 18116 -> 18116
791
Cov: 18116 -> 18116
Cov: 18116 -> 18116
792
Cov: 18116 -> 18117
Cov: 18117 -> 18117
793
Cov: 18117 -> 18117
Cov: 18117 -> 18117
794
Cov: 18117 -> 18128
Cov: 18128 -> 18128
795
Cov: 18128 -> 18141
Cov: 18141 -> 18141
796
{"exception": "TypeError", "msg": "'getset_descriptor' object is not callable"}
797
Cov: 18141 -> 18141
Cov: 18141 -> 18141
798
Cov: 18141 -> 18141
Cov: 18141 -> 18141
799
Cov: 18141 -> 18141
Cov: 18141 -> 18141
800
Cov: 18141 -> 18145
Cov: 18145 -> 18145
801
Cov: 18145 -> 18145
Cov: 18145 -> 18145
802
Cov: 18145 -> 18145
Cov: 18145 -> 18145
803
Cov: 18145 -> 18145
Cov: 18145 -> 18145
804
Cov: 18145 -> 18145
Cov: 18145 -> 18145
805
{"exception": "RuntimeError", "msg": "\"histogram_cpu\" not implemented for 'Long'"}
806
Cov: 18145 -> 18145
Cov: 18145 -> 18145
807
Cov: 18145 -> 18145
Cov: 18145 -> 18145
808
Cov: 18145 -> 18151
Cov: 18151 -> 18151
809
Cov: 18151 -> 18151
Cov: 18151 -> 18151
810
Cov: 18151 -> 18160
Cov: 18160 -> 18160
811
Cov: 18160 -> 18160
Cov: 18160 -> 18160
812
Cov: 18160 -> 18160
Cov: 18160 -> 18160
813
Cov: 18160 -> 18160
Cov: 18160 -> 18160
814
Cov: 18160 -> 18163
Cov: 18163 -> 18163
815
Cov: 18163 -> 18163
Cov: 18163 -> 18163
816
Cov: 18163 -> 18163
Cov: 18163 -> 18163
817
Cov: 18163 -> 18163
Cov: 18163 -> 18163
818
Cov: 18163 -> 18168
Cov: 18168 -> 18168
819
Cov: 18168 -> 18168
Cov: 18168 -> 18168
820
Cov: 18168 -> 18168
Cov: 18168 -> 18168
821
Cov: 18168 -> 18168
Cov: 18168 -> 18168
822
Cov: 18168 -> 18168
Cov: 18168 -> 18168
823
Cov: 18168 -> 18168
Cov: 18168 -> 18168
824
{"exception": "RuntimeError", "msg": "source tensor shape must match self tensor shape, excluding the specified dimension. Got self.shape = [2, 3, 3] source.shape = [2, 3]"}
825
Cov: 18168 -> 18168
Cov: 18168 -> 18168
826
Cov: 18168 -> 18168
Cov: 18168 -> 18168
827
Cov: 18168 -> 18168
Cov: 18168 -> 18168
828
Cov: 18168 -> 18168
Cov: 18168 -> 18168
829
Cov: 18168 -> 18173
Cov: 18173 -> 18173
830
Cov: 18173 -> 18173
Cov: 18173 -> 18173
831
Cov: 18173 -> 18206
Cov: 18206 -> 18206
832
Cov: 18206 -> 18206
Cov: 18206 -> 18206
833
Cov: 18206 -> 18206
Cov: 18206 -> 18206
834
Cov: 18206 -> 18206
Cov: 18206 -> 18206
835
Cov: 18206 -> 18206
Cov: 18206 -> 18206
836
Cov: 18206 -> 18217
Cov: 18217 -> 18217
837
Cov: 18217 -> 18217
Cov: 18217 -> 18217
838
Cov: 18217 -> 18217
Cov: 18217 -> 18217
839
Cov: 18217 -> 18217
Cov: 18217 -> 18217
840
{"exception": "RuntimeError", "msg": "element 0 of tensors does not require grad and does not have a grad_fn"}
841
Cov: 18217 -> 18241
Cov: 18241 -> 18241
842
Cov: 18241 -> 18566
Cov: 18566 -> 18566
843
Cov: 18566 -> 18572
Cov: 18572 -> 18572
844
Cov: 18572 -> 18572
Cov: 18572 -> 18572
845
Cov: 18572 -> 18572
Cov: 18572 -> 18572
846
Cov: 18572 -> 18572
Cov: 18572 -> 18572
847
Cov: 18572 -> 18572
Cov: 18572 -> 18572
848
Cov: 18572 -> 18572
Cov: 18572 -> 18572
849
Cov: 18572 -> 18578
Cov: 18578 -> 18578
850
Cov: 18578 -> 18586
Cov: 18586 -> 18586
851
Cov: 18586 -> 18586
Cov: 18586 -> 18586
852
Cov: 18586 -> 18586
Cov: 18586 -> 18586
853
{"exception": "RuntimeError", "msg": "Please look up dimensions by name, got: name = None."}
854
Cov: 18586 -> 18586
Cov: 18586 -> 18586
855
Cov: 18586 -> 18586
Cov: 18586 -> 18586
856
Cov: 18586 -> 18587
Cov: 18587 -> 18587
857
Cov: 18587 -> 18587
Cov: 18587 -> 18587
858
Cov: 18587 -> 18587
Cov: 18587 -> 18587
859
Cov: 18587 -> 18587
Cov: 18587 -> 18587
860
Cov: 18587 -> 18587
Cov: 18587 -> 18587
861
Cov: 18587 -> 18591
Cov: 18591 -> 18591
862
Cov: 18591 -> 18591
Cov: 18591 -> 18591
863
Cov: 18591 -> 18591
Cov: 18591 -> 18591
864
Cov: 18591 -> 18591
Cov: 18591 -> 18591
865
Cov: 18591 -> 18591
Cov: 18591 -> 18591
866
Cov: 18591 -> 18591
Cov: 18591 -> 18591
867
Cov: 18591 -> 18591
Cov: 18591 -> 18591
868
Cov: 18591 -> 18591
Cov: 18591 -> 18591
869
{"exception": "NotImplementedError", "msg": "Could not run 'aten::q_per_channel_zero_points' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::q_per_channel_zero_points' is only available for these backends: [QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nQuantizedCPU: registered at aten/src/ATen/RegisterQuantizedCPU.cpp:944 [kernel]\nQuantizedCUDA: registered at aten/src/ATen/RegisterQuantizedCUDA.cpp:459 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:153 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:498 [backend fallback]\nFunctionalize: registered at ../aten/src/ATen/FunctionalizeFallbackKernel.cpp:290 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:86 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradMTIA: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_1.cpp:16002 [kernel]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:382 [backend fallback]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:249 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:710 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:203 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:161 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:494 [backend fallback]\nPreDispatch: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:165 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:157 [backend fallback]\n"}
870
Cov: 18591 -> 18591
Cov: 18591 -> 18591
871
Cov: 18591 -> 18591
Cov: 18591 -> 18591
872
Cov: 18591 -> 18591
Cov: 18591 -> 18591
873
Cov: 18591 -> 18591
Cov: 18591 -> 18591
874
Cov: 18591 -> 18591
Cov: 18591 -> 18591
875
Cov: 18591 -> 18591
Cov: 18591 -> 18591
876
Cov: 18591 -> 18591
Cov: 18591 -> 18591
877
Cov: 18591 -> 18618
Cov: 18618 -> 18618
878
Cov: 18618 -> 18618
Cov: 18618 -> 18618
879
Cov: 18618 -> 18618
Cov: 18618 -> 18618
880
Cov: 18618 -> 18618
Cov: 18618 -> 18618
881
Cov: 18618 -> 18622
Cov: 18622 -> 18622
882
Cov: 18622 -> 18632
Cov: 18632 -> 18632
883
Cov: 18632 -> 18632
Cov: 18632 -> 18632
884
Cov: 18632 -> 18635
Cov: 18635 -> 18635
885
Cov: 18635 -> 18635
Cov: 18635 -> 18635
886
Cov: 18635 -> 18635
Cov: 18635 -> 18635
887
Cov: 18635 -> 18635
Cov: 18635 -> 18635
888
Cov: 18635 -> 18635
Cov: 18635 -> 18635
889
Cov: 18635 -> 18635
Cov: 18635 -> 18635
890
Cov: 18635 -> 18635
Cov: 18635 -> 18635
891
Cov: 18635 -> 18635
Cov: 18635 -> 18635
892
Cov: 18635 -> 18635
Cov: 18635 -> 18635
893
Cov: 18635 -> 18638
Cov: 18638 -> 18638
894
Cov: 18638 -> 18638
Cov: 18638 -> 18638
895
{"exception": "RuntimeError", "msg": "masked_select: expected BoolTensor for mask"}
896
Cov: 18638 -> 18638
Cov: 18638 -> 18638
897
Cov: 18638 -> 18648
Cov: 18648 -> 18648
898
Cov: 18648 -> 18648
Cov: 18648 -> 18648
899
Cov: 18648 -> 18648
Cov: 18648 -> 18648
900
Cov: 18648 -> 18656
Cov: 18656 -> 18656
901
Cov: 18656 -> 18656
Cov: 18656 -> 18656
902
Cov: 18656 -> 18657
Cov: 18657 -> 18657
903
Cov: 18657 -> 18657
Cov: 18657 -> 18657
904
Cov: 18657 -> 18657
Cov: 18657 -> 18657
905
Cov: 18657 -> 18657
Cov: 18657 -> 18657
906
Cov: 18657 -> 18657
Cov: 18657 -> 18657
907
Cov: 18657 -> 18657
Cov: 18657 -> 18657
908
Cov: 18657 -> 18658
Cov: 18658 -> 18658
909
{"exception": "TypeError", "msg": "uniform_() got an unexpected keyword argument 'from_'"}
910
Cov: 18658 -> 18659
Cov: 18659 -> 18659
911
{"exception": "RuntimeError", "msg": "element 0 of tensors does not require grad and does not have a grad_fn"}
912
Cov: 18659 -> 18689
Cov: 18689 -> 18689
913
Cov: 18689 -> 18689
Cov: 18689 -> 18689
914
Cov: 18689 -> 18689
Cov: 18689 -> 18689
915
Cov: 18689 -> 18690
Cov: 18690 -> 18690
916
Cov: 18690 -> 18690
Cov: 18690 -> 18690
917
Cov: 18690 -> 18690
Cov: 18690 -> 18690
918
Cov: 18690 -> 18691
Cov: 18691 -> 18691
919
Cov: 18691 -> 18698
Cov: 18698 -> 18698
920
Cov: 18698 -> 18698
Cov: 18698 -> 18698
921
Cov: 18698 -> 18698
Cov: 18698 -> 18698
922
Cov: 18698 -> 18698
Cov: 18698 -> 18698
923
Cov: 18698 -> 18698
Cov: 18698 -> 18698
924
Cov: 18698 -> 18698
Cov: 18698 -> 18698
925
Cov: 18698 -> 18698
Cov: 18698 -> 18698
926
Cov: 18698 -> 18698
Cov: 18698 -> 18698
927
Cov: 18698 -> 18698
Cov: 18698 -> 18698
928
Cov: 18698 -> 18698
Cov: 18698 -> 18698
929
Cov: 18698 -> 18698
Cov: 18698 -> 18698
930
Cov: 18698 -> 18698
Cov: 18698 -> 18698
931
{"exception": "RuntimeError", "msg": "index_fill_ only supports a 0-dimensional value tensor, but got tensor with 1 dimension(s)."}
932
Cov: 18698 -> 18698
Cov: 18698 -> 18698
933
Cov: 18698 -> 18698
Cov: 18698 -> 18698
934
Cov: 18698 -> 18698
Cov: 18698 -> 18698
935
Cov: 18698 -> 18698
Cov: 18698 -> 18698
936
Cov: 18698 -> 18698
Cov: 18698 -> 18698
937
Cov: 18698 -> 18698
Cov: 18698 -> 18698
938
Cov: 18698 -> 18698
Cov: 18698 -> 18698
939
Cov: 18698 -> 18707
Cov: 18707 -> 18707
940
Cov: 18707 -> 18707
Cov: 18707 -> 18707
941
Cov: 18707 -> 18707
Cov: 18707 -> 18707
942
Cov: 18707 -> 18713
Cov: 18713 -> 18713
943
Cov: 18713 -> 18713
Cov: 18713 -> 18713
944
Cov: 18713 -> 18713
Cov: 18713 -> 18713
945
Cov: 18713 -> 18721
Cov: 18721 -> 18721
946
{"exception": "RuntimeError", "msg": "element 0 of tensors does not require grad and does not have a grad_fn"}
947
Cov: 18721 -> 18734
Cov: 18734 -> 18734
948
Cov: 18734 -> 18734
Cov: 18734 -> 18734
949
Cov: 18734 -> 18734
Cov: 18734 -> 18734
950
Cov: 18734 -> 18734
Cov: 18734 -> 18734
951
{"exception": "NotImplementedError", "msg": "Could not run 'aten::qscheme' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::qscheme' is only available for these backends: [QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nQuantizedCPU: registered at aten/src/ATen/RegisterQuantizedCPU.cpp:944 [kernel]\nQuantizedCUDA: registered at aten/src/ATen/RegisterQuantizedCUDA.cpp:459 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:153 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:498 [backend fallback]\nFunctionalize: registered at ../aten/src/ATen/FunctionalizeFallbackKernel.cpp:290 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:86 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradMTIA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_0.cpp:16725 [kernel]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:382 [backend fallback]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:249 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:710 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:203 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:161 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:494 [backend fallback]\nPreDispatch: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:165 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:157 [backend fallback]\n"}
952
Cov: 18734 -> 18734
Cov: 18734 -> 18734
953
Cov: 18734 -> 18734
Cov: 18734 -> 18734
954
Cov: 18734 -> 18735
Cov: 18735 -> 18735
955
Cov: 18735 -> 18735
Cov: 18735 -> 18735
956
Cov: 18735 -> 18735
Cov: 18735 -> 18735
957
Cov: 18735 -> 18735
Cov: 18735 -> 18735
958
Cov: 18735 -> 18735
Cov: 18735 -> 18735
959
Cov: 18735 -> 18735
Cov: 18735 -> 18735
960
Cov: 18735 -> 18735
Cov: 18735 -> 18735
961
Cov: 18735 -> 18735
Cov: 18735 -> 18735
962
Cov: 18735 -> 18735
Cov: 18735 -> 18735
963
Cov: 18735 -> 18735
Cov: 18735 -> 18735
964
Cov: 18735 -> 18735
Cov: 18735 -> 18735
965
Cov: 18735 -> 18735
Cov: 18735 -> 18735
966
Cov: 18735 -> 18735
Cov: 18735 -> 18735
967
Cov: 18735 -> 18735
Cov: 18735 -> 18735
968
Cov: 18735 -> 18735
Cov: 18735 -> 18735
969
Cov: 18735 -> 18735
Cov: 18735 -> 18735
970
Cov: 18735 -> 18750
Cov: 18750 -> 18750
971
Cov: 18750 -> 18750
Cov: 18750 -> 18750
972
Cov: 18750 -> 18768
Cov: 18768 -> 18768
973
Cov: 18768 -> 18768
Cov: 18768 -> 18768
974
Cov: 18768 -> 18774
Cov: 18774 -> 18774
975
Cov: 18774 -> 18774
Cov: 18774 -> 18774
976
Cov: 18774 -> 18774
Cov: 18774 -> 18774
977
Cov: 18774 -> 18777
Cov: 18777 -> 18777
978
Cov: 18777 -> 18777
Cov: 18777 -> 18777
979
Cov: 18777 -> 18777
Cov: 18777 -> 18777
980
Cov: 18777 -> 18825
Cov: 18825 -> 18825
981
Cov: 18825 -> 18825
Cov: 18825 -> 18825
982
Cov: 18825 -> 18825
Cov: 18825 -> 18825
983
Cov: 18825 -> 18825
Cov: 18825 -> 18825
984
Cov: 18825 -> 18825
Cov: 18825 -> 18825
985
Cov: 18825 -> 18825
Cov: 18825 -> 18825
986
Cov: 18825 -> 18825
Cov: 18825 -> 18825
987
Cov: 18825 -> 18825
Cov: 18825 -> 18825
988
Cov: 18825 -> 18825
Cov: 18825 -> 18825
989
Cov: 18825 -> 18825
Cov: 18825 -> 18825
990
Cov: 18825 -> 18825
Cov: 18825 -> 18825
991
Cov: 18825 -> 18826
Cov: 18826 -> 18826
992
Cov: 18826 -> 18826
Cov: 18826 -> 18826
993
Cov: 18826 -> 18826
Cov: 18826 -> 18826
994
Cov: 18826 -> 18826
Cov: 18826 -> 18826
995
Cov: 18826 -> 18826
Cov: 18826 -> 18826
996
Cov: 18826 -> 18826
Cov: 18826 -> 18826
997
{"exception": "NotImplementedError", "msg": "Could not run 'aten::sparse_mask' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::sparse_mask' is only available for these backends: [SparseCPU, SparseCUDA, SparseCsrCPU, SparseCsrCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nSparseCPU: registered at aten/src/ATen/RegisterSparseCPU.cpp:1387 [kernel]\nSparseCUDA: registered at aten/src/ATen/RegisterSparseCUDA.cpp:1573 [kernel]\nSparseCsrCPU: registered at aten/src/ATen/RegisterSparseCsrCPU.cpp:1135 [kernel]\nSparseCsrCUDA: registered at aten/src/ATen/RegisterSparseCsrCUDA.cpp:1276 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:153 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:498 [backend fallback]\nFunctionalize: registered at ../aten/src/ATen/FunctionalizeFallbackKernel.cpp:290 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:86 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradMTIA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_0.cpp:16725 [kernel]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:382 [backend fallback]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:249 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:710 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:203 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:161 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:494 [backend fallback]\nPreDispatch: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:165 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:157 [backend fallback]\n"}
998
Cov: 18826 -> 18827
Cov: 18827 -> 18827
999
Cov: 18827 -> 18831
Cov: 18831 -> 18831
1000
Cov: 18831 -> 18831
Cov: 18831 -> 18831
1001
Cov: 18831 -> 18831
Cov: 18831 -> 18831
1002
Cov: 18831 -> 18837
Cov: 18837 -> 18837
1003
Cov: 18837 -> 18837
Cov: 18837 -> 18837
1004
Cov: 18837 -> 18837
Cov: 18837 -> 18837
1005
Cov: 18837 -> 18837
Cov: 18837 -> 18837
1006
Cov: 18837 -> 18837
Cov: 18837 -> 18837
1007
Cov: 18837 -> 18855
Cov: 18855 -> 18855
1008
Cov: 18855 -> 18855
Cov: 18855 -> 18855
1009
Cov: 18855 -> 18870
Cov: 18870 -> 18870
1010
Cov: 18870 -> 18870
Cov: 18870 -> 18870
1011
Cov: 18870 -> 18870
Cov: 18870 -> 18870
1012
Cov: 18870 -> 18870
Cov: 18870 -> 18870
1013
Cov: 18870 -> 18870
Cov: 18870 -> 18870
1014
Cov: 18870 -> 18870
Cov: 18870 -> 18870
1015
Cov: 18870 -> 18870
Cov: 18870 -> 18870
1016
Cov: 18870 -> 18870
Cov: 18870 -> 18870
1017
{"exception": "TypeError", "msg": "igamma(): argument 'other' (position 1) must be Tensor, not int"}
1018
Cov: 18870 -> 18870
Cov: 18870 -> 18870
1019
Cov: 18870 -> 18870
Cov: 18870 -> 18870
1020
Cov: 18870 -> 18870
Cov: 18870 -> 18870
1021
Cov: 18870 -> 18870
Cov: 18870 -> 18870
1022
Cov: 18870 -> 18873
Cov: 18873 -> 18873
1023
Cov: 18873 -> 18873
Cov: 18873 -> 18873
1024
Cov: 18873 -> 18873
Cov: 18873 -> 18873
1025
Cov: 18873 -> 19049
Cov: 19049 -> 19049
1026
Cov: 19049 -> 19061
Cov: 19061 -> 19061
1027
Cov: 19061 -> 19061
Cov: 19061 -> 19061
1028
Cov: 19061 -> 19062
Cov: 19062 -> 19062
1029
{"exception": "RuntimeError", "msg": "\"lshift_cpu\" not implemented for 'Double'"}
1030
Cov: 19062 -> 19062
Cov: 19062 -> 19062
1031
Cov: 19062 -> 19062
Cov: 19062 -> 19062
1032
Cov: 19062 -> 19062
Cov: 19062 -> 19062
1033
Cov: 19062 -> 19062
Cov: 19062 -> 19062
1034
Cov: 19062 -> 19062
Cov: 19062 -> 19062
1035
Cov: 19062 -> 19068
Cov: 19068 -> 19068
1036
Cov: 19068 -> 19068
Cov: 19068 -> 19068
1037
Cov: 19068 -> 19188
Cov: 19188 -> 19188
1038
Cov: 19188 -> 19188
Cov: 19188 -> 19188
1039
Cov: 19188 -> 19188
Cov: 19188 -> 19188
1040
Cov: 19188 -> 19191
Cov: 19191 -> 19191
1041
Cov: 19191 -> 19191
Cov: 19191 -> 19191
1042
Cov: 19191 -> 19191
Cov: 19191 -> 19191
1043
Cov: 19191 -> 19191
Cov: 19191 -> 19191
1044
Cov: 19191 -> 19192
Cov: 19192 -> 19192
1045
Cov: 19192 -> 19192
Cov: 19192 -> 19192
1046
Cov: 19192 -> 19192
Cov: 19192 -> 19192
1047
Cov: 19192 -> 19194
Cov: 19194 -> 19194
1048
Cov: 19194 -> 19194
Cov: 19194 -> 19194
1049
Cov: 19194 -> 19194
Cov: 19194 -> 19194
1050
Cov: 19194 -> 19194
Cov: 19194 -> 19194
1051
Cov: 19194 -> 19194
Cov: 19194 -> 19194
1052
Cov: 19194 -> 19194
Cov: 19194 -> 19194
1053
Cov: 19194 -> 19194
Cov: 19194 -> 19194
1054
Cov: 19194 -> 19194
Cov: 19194 -> 19194
1055
Cov: 19194 -> 19206
Cov: 19206 -> 19206
1056
Cov: 19206 -> 19206
Cov: 19206 -> 19206
1057
{"exception": "RuntimeError", "msg": "vector + matrix @ vector expected, got 2, 2, 1"}
1058
Cov: 19206 -> 19208
Cov: 19208 -> 19208
1059
Cov: 19208 -> 19208
Cov: 19208 -> 19208
1060
Cov: 19208 -> 19208
Cov: 19208 -> 19208
1061
Cov: 19208 -> 19208
Cov: 19208 -> 19208
1062
{"exception": "TypeError", "msg": "must be real number, not NoneType"}
1063
Cov: 19208 -> 19208
Cov: 19208 -> 19208
1064
Cov: 19208 -> 19216
Cov: 19216 -> 19216
1065
Cov: 19216 -> 19216
Cov: 19216 -> 19216
1066
Cov: 19216 -> 19216
Cov: 19216 -> 19216
1067
Cov: 19216 -> 19216
Cov: 19216 -> 19216
1068
Cov: 19216 -> 19217
Cov: 19217 -> 19217
1069
Cov: 19217 -> 19217
Cov: 19217 -> 19217
1070
Cov: 19217 -> 19217
Cov: 19217 -> 19217
1071
Cov: 19217 -> 19217
Cov: 19217 -> 19217
1072
Cov: 19217 -> 19424
Cov: 19424 -> 19424
1073
Cov: 19424 -> 19424
Cov: 19424 -> 19424
1074
Cov: 19424 -> 19424
Cov: 19424 -> 19424
1075
Cov: 19424 -> 19424
Cov: 19424 -> 19424
1076
Cov: 19424 -> 19424
Cov: 19424 -> 19424
1077
Cov: 19424 -> 19424
Cov: 19424 -> 19424
1078
Cov: 19424 -> 19424
Cov: 19424 -> 19424
1079
Cov: 19424 -> 19424
Cov: 19424 -> 19424
1080
Cov: 19424 -> 19424
Cov: 19424 -> 19424
1081
Cov: 19424 -> 19425
Cov: 19425 -> 19425
1082
Cov: 19425 -> 19426
Cov: 19426 -> 19426
1083
Cov: 19426 -> 19426
Cov: 19426 -> 19426
1084
Cov: 19426 -> 19426
Cov: 19426 -> 19426
1085
Cov: 19426 -> 19429
Cov: 19429 -> 19429
1086
Cov: 19429 -> 19433
Cov: 19433 -> 19433
1087
Cov: 19433 -> 19433
Cov: 19433 -> 19433
1088
{"exception": "RuntimeError", "msg": "This function was deprecated since version 1.9 and is now removed. The default behavior has changed from using the upper triangular portion of the matrix by default to using the lower triangular portion.\n\nL, _ = torch.symeig(A, upper=upper) should be replaced with:\nL = torch.linalg.eigvalsh(A, UPLO='U' if upper else 'L')\n\nand\n\nL, V = torch.symeig(A, eigenvectors=True) should be replaced with:\nL, V = torch.linalg.eigh(A, UPLO='U' if upper else 'L')"}
1089
Cov: 19433 -> 19434
Cov: 19434 -> 19434
1090
Cov: 19434 -> 19434
Cov: 19434 -> 19434
1091
Cov: 19434 -> 19434
Cov: 19434 -> 19434
1092
Cov: 19434 -> 19445
Cov: 19445 -> 19445
1093
Cov: 19445 -> 19465
Cov: 19465 -> 19465
1094
{"exception": "TypeError", "msg": "addbmm() missing 1 required positional arguments: \"batch2\""}
1095
{"exception": "TypeError", "msg": "type_as(): argument 'other' (position 1) must be Tensor, not torch.dtype"}
1096
Cov: 19465 -> 19465
Cov: 19465 -> 19465
1097
Cov: 19465 -> 19465
Cov: 19465 -> 19465
1098
Cov: 19465 -> 19465
Cov: 19465 -> 19465
1099
Cov: 19465 -> 19465
Cov: 19465 -> 19465
1100
Cov: 19465 -> 19465
Cov: 19465 -> 19465
1101
Cov: 19465 -> 19465
Cov: 19465 -> 19465
1102
Cov: 19465 -> 19465
Cov: 19465 -> 19465
1103
Cov: 19465 -> 19465
Cov: 19465 -> 19465
1104
Cov: 19465 -> 19465
Cov: 19465 -> 19465
1105
Cov: 19465 -> 19465
Cov: 19465 -> 19465
1106
Cov: 19465 -> 19465
Cov: 19465 -> 19465
1107
{"exception": "RuntimeError", "msg": "scatter(): Expected self.dtype to be equal to src.dtype"}
1108
{"exception": "RuntimeError", "msg": "Creating a Tensor subclass from a class that does not inherit from Tensor is not possible. Make sure your class inherits from Tensor."}
1109
Cov: 19465 -> 19465
Cov: 19465 -> 19465
1110
Cov: 19465 -> 19466
Cov: 19466 -> 19466
1111
Cov: 19466 -> 19476
Cov: 19476 -> 19476
1112
Cov: 19476 -> 19523
Cov: 19523 -> 19523
1113
Cov: 19523 -> 19523
Cov: 19523 -> 19523
1114
Cov: 19523 -> 19524
Cov: 19524 -> 19524
1115
Cov: 19524 -> 19524
Cov: 19524 -> 19524
1116
Cov: 19524 -> 19524
Cov: 19524 -> 19524
1117
Cov: 19524 -> 19524
Cov: 19524 -> 19524
1118
Cov: 19524 -> 19524
Cov: 19524 -> 19524
1119
Cov: 19524 -> 19524
Cov: 19524 -> 19524
1120
Cov: 19524 -> 19524
Cov: 19524 -> 19524
1121
Cov: 19524 -> 19524
Cov: 19524 -> 19524
1122
Cov: 19524 -> 19524
Cov: 19524 -> 19524
1123
Cov: 19524 -> 19527
Cov: 19527 -> 19527
1124
Cov: 19527 -> 19527
Cov: 19527 -> 19527
1125
Cov: 19527 -> 19528
Cov: 19528 -> 19528
1126
Cov: 19528 -> 19528
Cov: 19528 -> 19528
1127
Cov: 19528 -> 19529
Cov: 19529 -> 19529
1128
Cov: 19529 -> 19529
Cov: 19529 -> 19529
1129
Cov: 19529 -> 19529
Cov: 19529 -> 19529
1130
Cov: 19529 -> 19529
Cov: 19529 -> 19529
1131
Cov: 19529 -> 19529
Cov: 19529 -> 19529
1132
Cov: 19529 -> 19529
Cov: 19529 -> 19529
1133
{"exception": "TypeError", "msg": "narrow() received an invalid combination of arguments - got (length=int, start=int, dimension=int, ), but expected one of:\n * (int dim, Tensor start, int length)\n      didn't match because some of the keywords were incorrect: dimension\n * (int dim, int start, int length)\n      didn't match because some of the keywords were incorrect: dimension\n"}
1134
Cov: 19529 -> 19529
Cov: 19529 -> 19529
1135
Cov: 19529 -> 19529
Cov: 19529 -> 19529
1136
Cov: 19529 -> 19529
Cov: 19529 -> 19529
1137
Cov: 19529 -> 19529
Cov: 19529 -> 19529
1138
Cov: 19529 -> 19529
Cov: 19529 -> 19529
1139
Cov: 19529 -> 19529
Cov: 19529 -> 19529
1140
Cov: 19529 -> 19529
Cov: 19529 -> 19529
1141
Cov: 19529 -> 19529
Cov: 19529 -> 19529
1142
Cov: 19529 -> 19529
Cov: 19529 -> 19529
1143
Cov: 19529 -> 19529
Cov: 19529 -> 19529
1144
Cov: 19529 -> 19529
Cov: 19529 -> 19529
1145
{"exception": "RuntimeError", "msg": "1D tensors expected, but got 2D and 2D tensors"}
1146
Cov: 19529 -> 19529
Cov: 19529 -> 19529
1147
Cov: 19529 -> 19529
Cov: 19529 -> 19529
1148
Cov: 19529 -> 19529
Cov: 19529 -> 19529
1149
Cov: 19529 -> 19529
Cov: 19529 -> 19529
1150
Cov: 19529 -> 19529
Cov: 19529 -> 19529
1151
Cov: 19529 -> 19529
Cov: 19529 -> 19529
1152
Cov: 19529 -> 19529
Cov: 19529 -> 19529
1153
Cov: 19529 -> 19529
Cov: 19529 -> 19529
1154
{"exception": "TypeError", "msg": "'getset_descriptor' object is not callable"}
1155
Cov: 19529 -> 19536
Cov: 19536 -> 19536
1156
Cov: 19536 -> 19536
Cov: 19536 -> 19536
1157
Cov: 19536 -> 19537
Cov: 19537 -> 19537
1158
{"exception": "IndexError", "msg": "put_(): Expected source and index to have the same number of elements, but got source.numel() = 6, index.numel() = 2"}
1159
Cov: 19537 -> 19537
Cov: 19537 -> 19537
1160
Cov: 19537 -> 19537
Cov: 19537 -> 19537
1161
Cov: 19537 -> 19537
Cov: 19537 -> 19537
1162
Cov: 19537 -> 19537
Cov: 19537 -> 19537
1163
Cov: 19537 -> 19545
Cov: 19545 -> 19545
1164
{"exception": "RuntimeError", "msg": "element 0 of tensors does not require grad and does not have a grad_fn"}
1165
{"exception": "NotImplementedError", "msg": "Could not run 'aten::q_per_channel_scales' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::q_per_channel_scales' is only available for these backends: [QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nQuantizedCPU: registered at aten/src/ATen/RegisterQuantizedCPU.cpp:944 [kernel]\nQuantizedCUDA: registered at aten/src/ATen/RegisterQuantizedCUDA.cpp:459 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:153 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:498 [backend fallback]\nFunctionalize: registered at ../aten/src/ATen/FunctionalizeFallbackKernel.cpp:290 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:86 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradMTIA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_3.cpp:14610 [kernel]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:382 [backend fallback]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:249 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:710 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:203 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:161 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:494 [backend fallback]\nPreDispatch: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:165 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:157 [backend fallback]\n"}
1166
Cov: 19545 -> 19547
Cov: 19547 -> 19547
1167
Cov: 19547 -> 19548
Cov: 19548 -> 19548
1168
Cov: 19548 -> 19548
Cov: 19548 -> 19548
1169
Cov: 19548 -> 19548
Cov: 19548 -> 19548
1170
Cov: 19548 -> 19552
Cov: 19552 -> 19552
1171
{"exception": "TypeError", "msg": "new_tensor() takes 1 positional argument but 4 were given"}
1172
Cov: 19552 -> 19553
Cov: 19553 -> 19553
1173
Cov: 19553 -> 19554
Cov: 19554 -> 19554
1174
Cov: 19554 -> 19557
Cov: 19557 -> 19557
1175
Cov: 19557 -> 19557
Cov: 19557 -> 19557
1176
{"exception": "RuntimeError", "msg": "masked_fill_ only supports a 0-dimensional value tensor, but got tensor with 1 dimension(s)."}
1177
Cov: 19557 -> 19557
Cov: 19557 -> 19557
1178
Cov: 19557 -> 19557
Cov: 19557 -> 19557
1179
{"exception": "NotImplementedError", "msg": "Could not run 'aten::int_repr' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::int_repr' is only available for these backends: [QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nQuantizedCPU: registered at aten/src/ATen/RegisterQuantizedCPU.cpp:944 [kernel]\nQuantizedCUDA: registered at aten/src/ATen/RegisterQuantizedCUDA.cpp:459 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:153 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:498 [backend fallback]\nFunctionalize: registered at ../aten/src/ATen/FunctionalizeFallbackKernel.cpp:290 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:86 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradMTIA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_0.cpp:16725 [kernel]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:382 [backend fallback]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:249 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:710 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:203 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:161 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:494 [backend fallback]\nPreDispatch: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:165 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:157 [backend fallback]\n"}
1180
Cov: 19557 -> 19557
Cov: 19557 -> 19557
1181
{"exception": "NotImplementedError", "msg": "Could not run 'aten::int_repr' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::int_repr' is only available for these backends: [QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nQuantizedCPU: registered at aten/src/ATen/RegisterQuantizedCPU.cpp:944 [kernel]\nQuantizedCUDA: registered at aten/src/ATen/RegisterQuantizedCUDA.cpp:459 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:153 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:498 [backend fallback]\nFunctionalize: registered at ../aten/src/ATen/FunctionalizeFallbackKernel.cpp:290 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:86 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradMTIA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_0.cpp:16725 [kernel]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:382 [backend fallback]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:249 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:710 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:203 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:161 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:494 [backend fallback]\nPreDispatch: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:165 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:157 [backend fallback]\n"}
1182
Cov: 19557 -> 19557
Cov: 19557 -> 19557
1183
Cov: 19557 -> 19561
Cov: 19561 -> 19561
1184
Cov: 19561 -> 19561
Cov: 19561 -> 19561
1185
Cov: 19561 -> 19561
Cov: 19561 -> 19561
1186
Cov: 19561 -> 19561
Cov: 19561 -> 19561
1187
Cov: 19561 -> 19562
Cov: 19562 -> 19562
1188
Cov: 19562 -> 19562
Cov: 19562 -> 19562
1189
Cov: 19562 -> 19562
Cov: 19562 -> 19562
1190
Cov: 19562 -> 19568
Cov: 19568 -> 19568
1191
Cov: 19568 -> 19572
Cov: 19572 -> 19572
1192
Cov: 19572 -> 19572
Cov: 19572 -> 19572
1193
{"exception": "RuntimeError", "msg": "element 0 of tensors does not require grad and does not have a grad_fn"}
1194
Cov: 19572 -> 19581
Cov: 19581 -> 19581
1195
Cov: 19581 -> 19581
Cov: 19581 -> 19581
1196
Cov: 19581 -> 19582
Cov: 19582 -> 19582
1197
Cov: 19582 -> 19600
Cov: 19600 -> 19600
1198
Cov: 19600 -> 19600
Cov: 19600 -> 19600
1199
Cov: 19600 -> 19600
Cov: 19600 -> 19600
1200
Cov: 19600 -> 19600
Cov: 19600 -> 19600
1201
Cov: 19600 -> 19600
Cov: 19600 -> 19600
1202
Cov: 19600 -> 19600
Cov: 19600 -> 19600
1203
Cov: 19600 -> 19600
Cov: 19600 -> 19600
1204
Cov: 19600 -> 19600
Cov: 19600 -> 19600
1205
Cov: 19600 -> 19600
Cov: 19600 -> 19600
1206
Cov: 19600 -> 19600
Cov: 19600 -> 19600
1207
Cov: 19600 -> 19600
Cov: 19600 -> 19600
1208
Cov: 19600 -> 19600
Cov: 19600 -> 19600
1209
Cov: 19600 -> 19601
Cov: 19601 -> 19601
1210
Cov: 19601 -> 19601
Cov: 19601 -> 19601
1211
Cov: 19601 -> 19659
Cov: 19659 -> 19659
1212
Cov: 19659 -> 19663
Cov: 19663 -> 19663
1213
Cov: 19663 -> 19663
Cov: 19663 -> 19663
1214
{"exception": "RuntimeError", "msg": "Expected nested_tensorlist[0].size() > 0 to be true, but got false.  (Could this error message be improved?  If so, please report an enhancement request to PyTorch.)"}
1215
Cov: 19663 -> 19664
Cov: 19664 -> 19664
1216
Cov: 19664 -> 19667
Cov: 19667 -> 19667
1217
Cov: 19667 -> 19667
Cov: 19667 -> 19667
1218
Cov: 19667 -> 19668
Cov: 19668 -> 19668
1219
Cov: 19668 -> 19668
Cov: 19668 -> 19668
1220
Cov: 19668 -> 19669
Cov: 19669 -> 19669
1221
Cov: 19669 -> 19669
Cov: 19669 -> 19669
1222
Cov: 19669 -> 19671
Cov: 19671 -> 19671
1223
Cov: 19671 -> 19671
Cov: 19671 -> 19671
1224
{"exception": "NotImplementedError", "msg": "Could not run 'aten::q_per_channel_axis' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::q_per_channel_axis' is only available for these backends: [QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nQuantizedCPU: registered at aten/src/ATen/RegisterQuantizedCPU.cpp:944 [kernel]\nQuantizedCUDA: registered at aten/src/ATen/RegisterQuantizedCUDA.cpp:459 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:153 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:498 [backend fallback]\nFunctionalize: registered at ../aten/src/ATen/FunctionalizeFallbackKernel.cpp:290 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:86 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradMTIA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_4.cpp:13056 [kernel]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:382 [backend fallback]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:249 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:710 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:203 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:161 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:494 [backend fallback]\nPreDispatch: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:165 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:157 [backend fallback]\n"}
1225
Cov: 19671 -> 19671
Cov: 19671 -> 19671
1226
Cov: 19671 -> 19671
Cov: 19671 -> 19671
1227
Cov: 19671 -> 19671
Cov: 19671 -> 19671
1228
Cov: 19671 -> 19671
Cov: 19671 -> 19671
1229
Cov: 19671 -> 19671
Cov: 19671 -> 19671
1230
Cov: 19671 -> 19672
Cov: 19672 -> 19672
1231
Cov: 19672 -> 19672
Cov: 19672 -> 19672
1232
{"exception": "RuntimeError", "msg": "Creating a Tensor subclass from a class that does not inherit from Tensor is not possible. Make sure your class inherits from Tensor."}
1233
Cov: 19672 -> 19672
Cov: 19672 -> 19672
1234
Cov: 19672 -> 19672
Cov: 19672 -> 19672
1235
Cov: 19672 -> 19672
Cov: 19672 -> 19672
1236
Cov: 19672 -> 19672
Cov: 19672 -> 19672
1237
Cov: 19672 -> 19676
Cov: 19676 -> 19676
1238
Cov: 19676 -> 19676
Cov: 19676 -> 19676
1239
Cov: 19676 -> 19676
Cov: 19676 -> 19676
1240
Cov: 19676 -> 19676
Cov: 19676 -> 19676
1241
Cov: 19676 -> 19676
Cov: 19676 -> 19676
1242
Cov: 19676 -> 19676
Cov: 19676 -> 19676
1243
Cov: 19676 -> 19676
Cov: 19676 -> 19676
1244
Cov: 19676 -> 19682
Cov: 19682 -> 19682
1245
Cov: 19682 -> 19694
Cov: 19694 -> 19694
1246
Cov: 19694 -> 19695
Cov: 19695 -> 19695
1247
Cov: 19695 -> 19696
Cov: 19696 -> 19696
1248
{"exception": "RuntimeError", "msg": "any: You passed a dimname (string) to this op in place of a dimension index but it does not yet support this behavior. Please pass a dimension index to work around this."}
1249
Cov: 19696 -> 19696
Cov: 19696 -> 19696
1250
Cov: 19696 -> 19696
Cov: 19696 -> 19696
1251
Cov: 19696 -> 19696
Cov: 19696 -> 19696
1252
Cov: 19696 -> 19696
Cov: 19696 -> 19696
1253
{"exception": "RuntimeError", "msg": "The expanded size of the tensor (5) must match the existing size (4) at non-singleton dimension 2.  Target sizes: [10, 3, 5].  Tensor sizes: [10, 3, 4]"}
1254
Cov: 19696 -> 19696
Cov: 19696 -> 19696
1255
Cov: 19696 -> 19703
Cov: 19703 -> 19703
1256
Cov: 19703 -> 19776
Cov: 19776 -> 19776
1257
Cov: 19776 -> 19776
Cov: 19776 -> 19776
1258
Cov: 19776 -> 19801
Cov: 19801 -> 19801
1259
Cov: 19801 -> 19803
Cov: 19803 -> 19803
1260
Cov: 19803 -> 19803
Cov: 19803 -> 19803
1261
Cov: 19803 -> 19803
Cov: 19803 -> 19803
1262
Cov: 19803 -> 19803
Cov: 19803 -> 19803
1263
Cov: 19803 -> 19803
Cov: 19803 -> 19803
1264
Cov: 19803 -> 19803
Cov: 19803 -> 19803
1265
Cov: 19803 -> 19803
Cov: 19803 -> 19803
1266
Cov: 19803 -> 19803
Cov: 19803 -> 19803
1267
Cov: 19803 -> 19809
Cov: 19809 -> 19809
1268
Cov: 19809 -> 19809
Cov: 19809 -> 19809
1269
Cov: 19809 -> 19809
Cov: 19809 -> 19809
1270
Cov: 19809 -> 19827
Cov: 19827 -> 19827
1271
Cov: 19827 -> 19827
Cov: 19827 -> 19827
1272
Cov: 19827 -> 19827
Cov: 19827 -> 19827
1273
{"exception": "RuntimeError", "msg": "vector + matrix @ vector expected, got 2, 2, 1"}
1274
{"exception": "RuntimeError", "msg": "element 0 of tensors does not require grad and does not have a grad_fn"}
1275
Cov: 19827 -> 19828
Cov: 19828 -> 19828
1276
Cov: 19828 -> 19828
Cov: 19828 -> 19828
1277
Cov: 19828 -> 19828
Cov: 19828 -> 19828
1278
Cov: 19828 -> 19828
Cov: 19828 -> 19828
1279
Cov: 19828 -> 19828
Cov: 19828 -> 19828
1280
Cov: 19828 -> 19842
Cov: 19842 -> 19842
1281
Cov: 19842 -> 19842
Cov: 19842 -> 19842
1282
Cov: 19842 -> 19842
Cov: 19842 -> 19842
1283
Cov: 19842 -> 19842
Cov: 19842 -> 19842
1284
Cov: 19842 -> 19842
Cov: 19842 -> 19842
1285
Cov: 19842 -> 19842
Cov: 19842 -> 19842
1286
Cov: 19842 -> 19842
Cov: 19842 -> 19842
1287
Cov: 19842 -> 19842
Cov: 19842 -> 19842
1288
Cov: 19842 -> 19842
Cov: 19842 -> 19842
1289
Cov: 19842 -> 19842
Cov: 19842 -> 19842
1290
Cov: 19842 -> 19842
Cov: 19842 -> 19842
1291
Cov: 19842 -> 19842
Cov: 19842 -> 19842
1292
Cov: 19842 -> 19842
Cov: 19842 -> 19842
1293
Cov: 19842 -> 19842
Cov: 19842 -> 19842
1294
Cov: 19842 -> 19842
Cov: 19842 -> 19842
1295
Cov: 19842 -> 19843
Cov: 19843 -> 19843
1296
Cov: 19843 -> 19843
Cov: 19843 -> 19843
1297
Cov: 19843 -> 19843
Cov: 19843 -> 19843
1298
Cov: 19843 -> 19843
Cov: 19843 -> 19843
1299
Cov: 19843 -> 19845
Cov: 19845 -> 19845
1300
Cov: 19845 -> 19845
Cov: 19845 -> 19845
1301
Cov: 19845 -> 19846
Cov: 19846 -> 19846
1302
{"exception": "RuntimeError", "msg": "matrix_exp: Expected a floating point or complex tensor as input. Got Long"}
1303
Cov: 19846 -> 19846
Cov: 19846 -> 19846
1304
Cov: 19846 -> 19846
Cov: 19846 -> 19846
1305
Cov: 19846 -> 19846
Cov: 19846 -> 19846
1306
Cov: 19846 -> 19846
Cov: 19846 -> 19846
1307
{"exception": "RuntimeError", "msg": "The expanded size of the tensor (5) must match the existing size (4) at non-singleton dimension 2.  Target sizes: [10, 3, 5].  Tensor sizes: [10, 3, 4]"}
1308
Cov: 19846 -> 19846
Cov: 19846 -> 19846
1309
Cov: 19846 -> 20110
Cov: 20110 -> 20110
1310
Cov: 20110 -> 20113
Cov: 20113 -> 20113
1311
Cov: 20113 -> 20114
Cov: 20114 -> 20114
1312
Cov: 20114 -> 20115
Cov: 20115 -> 20115
1313
Cov: 20115 -> 20115
Cov: 20115 -> 20115
1314
Cov: 20115 -> 20115
Cov: 20115 -> 20115
1315
Cov: 20115 -> 20116
Cov: 20116 -> 20116
1316
Cov: 20116 -> 20116
Cov: 20116 -> 20116
1317
Cov: 20116 -> 20117
Cov: 20117 -> 20117
1318
Cov: 20117 -> 20117
Cov: 20117 -> 20117
1319
Cov: 20117 -> 20118
Cov: 20118 -> 20118
1320
Cov: 20118 -> 20118
Cov: 20118 -> 20118
1321
Cov: 20118 -> 20118
Cov: 20118 -> 20118
1322
Cov: 20118 -> 20145
Cov: 20145 -> 20145
1323
Cov: 20145 -> 20145
Cov: 20145 -> 20145
1324
Cov: 20145 -> 20145
Cov: 20145 -> 20145
1325
Cov: 20145 -> 20145
Cov: 20145 -> 20145
1326
Cov: 20145 -> 20145
Cov: 20145 -> 20145
1327
Cov: 20145 -> 20145
Cov: 20145 -> 20145
1328
Cov: 20145 -> 20147
Cov: 20147 -> 20147
1329
Cov: 20147 -> 20147
Cov: 20147 -> 20147
1330
Cov: 20147 -> 20147
Cov: 20147 -> 20147
1331
Cov: 20147 -> 20147
Cov: 20147 -> 20147
1332
Cov: 20147 -> 20147
Cov: 20147 -> 20147
1333
{"exception": "NotImplementedError", "msg": "Could not run 'aten::sparse_mask' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::sparse_mask' is only available for these backends: [SparseCPU, SparseCUDA, SparseCsrCPU, SparseCsrCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nSparseCPU: registered at aten/src/ATen/RegisterSparseCPU.cpp:1387 [kernel]\nSparseCUDA: registered at aten/src/ATen/RegisterSparseCUDA.cpp:1573 [kernel]\nSparseCsrCPU: registered at aten/src/ATen/RegisterSparseCsrCPU.cpp:1135 [kernel]\nSparseCsrCUDA: registered at aten/src/ATen/RegisterSparseCsrCUDA.cpp:1276 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:153 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:498 [backend fallback]\nFunctionalize: registered at ../aten/src/ATen/FunctionalizeFallbackKernel.cpp:290 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:86 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradMTIA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_0.cpp:16725 [kernel]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:382 [backend fallback]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:249 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:710 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:203 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:161 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:494 [backend fallback]\nPreDispatch: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:165 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:157 [backend fallback]\n"}
1334
Cov: 20147 -> 20147
Cov: 20147 -> 20147
1335
Cov: 20147 -> 20148
Cov: 20148 -> 20148
1336
Cov: 20148 -> 20148
Cov: 20148 -> 20148
1337
Cov: 20148 -> 20150
Cov: 20150 -> 20150
1338
Cov: 20150 -> 20150
Cov: 20150 -> 20150
1339
Cov: 20150 -> 20150
Cov: 20150 -> 20150
1340
Cov: 20150 -> 20150
Cov: 20150 -> 20150
1341
Cov: 20150 -> 20151
Cov: 20151 -> 20151
1342
Cov: 20151 -> 20151
Cov: 20151 -> 20151
1343
Cov: 20151 -> 20151
Cov: 20151 -> 20151
1344
Cov: 20151 -> 20155
Cov: 20155 -> 20155
1345
Cov: 20155 -> 20155
Cov: 20155 -> 20155
1346
Cov: 20155 -> 20155
Cov: 20155 -> 20155
1347
Cov: 20155 -> 20167
Cov: 20167 -> 20167
1348
Cov: 20167 -> 20167
Cov: 20167 -> 20167
1349
Cov: 20167 -> 20167
Cov: 20167 -> 20167
1350
Cov: 20167 -> 20167
Cov: 20167 -> 20167
1351
Cov: 20167 -> 20167
Cov: 20167 -> 20167
1352
Cov: 20167 -> 20167
Cov: 20167 -> 20167
1353
Cov: 20167 -> 20167
Cov: 20167 -> 20167
1354
Cov: 20167 -> 20167
Cov: 20167 -> 20167
1355
Cov: 20167 -> 20170
Cov: 20170 -> 20170
1356
Cov: 20170 -> 20170
Cov: 20170 -> 20170
1357
Cov: 20170 -> 20174
Cov: 20174 -> 20174
1358
Cov: 20174 -> 20174
Cov: 20174 -> 20174
1359
Cov: 20174 -> 20174
Cov: 20174 -> 20174
1360
Cov: 20174 -> 20175
Cov: 20175 -> 20175
1361
Cov: 20175 -> 20175
Cov: 20175 -> 20175
1362
Cov: 20175 -> 20175
Cov: 20175 -> 20175
1363
Cov: 20175 -> 20183
Cov: 20183 -> 20183
1364
{"exception": "RuntimeError", "msg": "tensor.sspaddmm(...) can only be called on sparse tensors"}
1365
Cov: 20183 -> 20184
Cov: 20184 -> 20184
1366
{"exception": "RuntimeError", "msg": "element 0 of tensors does not require grad and does not have a grad_fn"}
1367
Cov: 20184 -> 20185
Cov: 20185 -> 20185
1368
Cov: 20185 -> 20185
Cov: 20185 -> 20185
1369
Cov: 20185 -> 20185
Cov: 20185 -> 20185
1370
Cov: 20185 -> 20186
Cov: 20186 -> 20186
1371
Cov: 20186 -> 20187
Cov: 20187 -> 20187
1372
Cov: 20187 -> 20187
Cov: 20187 -> 20187
1373
Cov: 20187 -> 20187
Cov: 20187 -> 20187
1374
Cov: 20187 -> 20187
Cov: 20187 -> 20187
1375
{"exception": "RuntimeError", "msg": "element 0 of tensors does not require grad and does not have a grad_fn"}
1376
Cov: 20187 -> 20189
Cov: 20189 -> 20189
1377
Cov: 20189 -> 20189
Cov: 20189 -> 20189
1378
Cov: 20189 -> 20189
Cov: 20189 -> 20189
1379
Cov: 20189 -> 20189
Cov: 20189 -> 20189
1380
Cov: 20189 -> 20189
Cov: 20189 -> 20189
1381
Cov: 20189 -> 20192
Cov: 20192 -> 20192
1382
Cov: 20192 -> 20202
Cov: 20202 -> 20202
1383
{"exception": "RuntimeError", "msg": "scatter(): Expected self.dtype to be equal to src.dtype"}
1384
Cov: 20202 -> 20202
Cov: 20202 -> 20202
1385
Cov: 20202 -> 20202
Cov: 20202 -> 20202
1386
Cov: 20202 -> 20202
Cov: 20202 -> 20202
1387
Cov: 20202 -> 20202
Cov: 20202 -> 20202
1388
Cov: 20202 -> 20202
Cov: 20202 -> 20202
1389
{"exception": "TypeError", "msg": "random_() missing 2 required positional argument: \"from\", \"to\""}
1390
{"exception": "RuntimeError", "msg": "element 0 of tensors does not require grad and does not have a grad_fn"}
1391
Cov: 20202 -> 20209
Cov: 20209 -> 20209
1392
Cov: 20209 -> 20212
Cov: 20212 -> 20212
1393
Cov: 20212 -> 20212
Cov: 20212 -> 20212
1394
Cov: 20212 -> 20213
Cov: 20213 -> 20213
1395
Cov: 20213 -> 20213
Cov: 20213 -> 20213
1396
Cov: 20213 -> 20216
Cov: 20216 -> 20216
1397
Cov: 20216 -> 20217
Cov: 20217 -> 20217
1398
Cov: 20217 -> 20220
Cov: 20220 -> 20220
1399
{"exception": "RuntimeError", "msg": "element 0 of tensors does not require grad and does not have a grad_fn"}
1400
Cov: 20220 -> 20221
Cov: 20221 -> 20221
1401
Cov: 20221 -> 20223
Cov: 20223 -> 20223
1402
Cov: 20223 -> 20223
Cov: 20223 -> 20223
1403
{"exception": "RuntimeError", "msg": "This function was deprecated since version 1.9 and is now removed. `torch.solve` is deprecated in favor of `torch.linalg.solve`. `torch.linalg.solve` has its arguments reversed and does not return the LU factorization.\n\nTo get the LU factorization see `torch.lu`, which can be used with `torch.lu_solve` or `torch.lu_unpack`.\nX = torch.solve(B, A).solution should be replaced with:\nX = torch.linalg.solve(A, B)"}
1404
Cov: 20223 -> 20229
Cov: 20229 -> 20229
1405
Cov: 20229 -> 20229
Cov: 20229 -> 20229
1406
Cov: 20229 -> 20229
Cov: 20229 -> 20229
1407
Cov: 20229 -> 20242
Cov: 20242 -> 20242
1408
Cov: 20242 -> 20242
Cov: 20242 -> 20242
1409
Cov: 20242 -> 20243
Cov: 20243 -> 20243
1410
Cov: 20243 -> 20243
Cov: 20243 -> 20243
1411
Cov: 20243 -> 20244
Cov: 20244 -> 20244
1412
Cov: 20244 -> 20244
Cov: 20244 -> 20244
1413
{"exception": "RuntimeError", "msg": "size {[4, 5]} is not expandable to size {[2, 3]}."}
1414
Cov: 20244 -> 20244
Cov: 20244 -> 20244
1415
Cov: 20244 -> 20245
Cov: 20245 -> 20245
1416
Cov: 20245 -> 20245
Cov: 20245 -> 20245
1417
Cov: 20245 -> 20246
Cov: 20246 -> 20246
1418
Cov: 20246 -> 20246
Cov: 20246 -> 20246
1419
Cov: 20246 -> 20246
Cov: 20246 -> 20246
1420
Cov: 20246 -> 20256
Cov: 20256 -> 20256
1421
Cov: 20256 -> 20256
Cov: 20256 -> 20256
1422
Cov: 20256 -> 20256
Cov: 20256 -> 20256
1423
Cov: 20256 -> 20259
Cov: 20259 -> 20259
1424
Cov: 20259 -> 20259
Cov: 20259 -> 20259
1425
Cov: 20259 -> 20259
Cov: 20259 -> 20259
1426
Cov: 20259 -> 20259
Cov: 20259 -> 20259
1427
Cov: 20259 -> 20259
Cov: 20259 -> 20259
1428
Cov: 20259 -> 20259
Cov: 20259 -> 20259
1429
Cov: 20259 -> 20259
Cov: 20259 -> 20259
1430
Cov: 20259 -> 20259
Cov: 20259 -> 20259
1431
Cov: 20259 -> 20259
Cov: 20259 -> 20259
1432
Cov: 20259 -> 20259
Cov: 20259 -> 20259
1433
Cov: 20259 -> 20259
Cov: 20259 -> 20259
1434
Cov: 20259 -> 20259
Cov: 20259 -> 20259
1435
Cov: 20259 -> 20276
Cov: 20276 -> 20276
1436
Cov: 20276 -> 20276
Cov: 20276 -> 20276
1437
Cov: 20276 -> 20276
Cov: 20276 -> 20276
1438
{"exception": "RuntimeError", "msg": "required rank 4 tensor to use channels_last format"}
1439
Cov: 20276 -> 20276
Cov: 20276 -> 20276
1440
{"exception": "RuntimeError", "msg": "Error: cannot set number of interop threads after parallel work has started or set_num_interop_threads called"}
1441
Cov: 20276 -> 20276
Cov: 20276 -> 20276
1442
Cov: 20276 -> 20276
Cov: 20276 -> 20276
1443
{"exception": "RuntimeError", "msg": "size {[4, 4]} is not expandable to size {[2, 3, 4]}."}
1444
Cov: 20276 -> 20276
Cov: 20276 -> 20276
1445
Cov: 20276 -> 20277
Cov: 20277 -> 20277
1446
Cov: 20277 -> 20277
Cov: 20277 -> 20277
1447
Cov: 20277 -> 20277
Cov: 20277 -> 20277
1448
Cov: 20277 -> 20277
Cov: 20277 -> 20277
1449
Cov: 20277 -> 20277
Cov: 20277 -> 20277
1450
Cov: 20277 -> 20277
Cov: 20277 -> 20277
1451
Cov: 20277 -> 20282
Cov: 20282 -> 20282
1452
Cov: 20282 -> 20282
Cov: 20282 -> 20282
1453
Cov: 20282 -> 20282
Cov: 20282 -> 20282
1454
Cov: 20282 -> 20282
Cov: 20282 -> 20282
1455
Cov: 20282 -> 20282
Cov: 20282 -> 20282
1456
Cov: 20282 -> 20282
Cov: 20282 -> 20282
1457
Cov: 20282 -> 20286
Cov: 20286 -> 20286
1458
Cov: 20286 -> 20292
Cov: 20292 -> 20292
1459
Cov: 20292 -> 20293
Cov: 20293 -> 20293
1460
Cov: 20293 -> 20293
Cov: 20293 -> 20293
1461
Cov: 20293 -> 20293
Cov: 20293 -> 20293
1462
Cov: 20293 -> 20293
Cov: 20293 -> 20293
1463
Cov: 20293 -> 20293
Cov: 20293 -> 20293
1464
Cov: 20293 -> 20293
Cov: 20293 -> 20293
1465
Cov: 20293 -> 20293
Cov: 20293 -> 20293
1466
Cov: 20293 -> 20294
Cov: 20294 -> 20294
1467
Cov: 20294 -> 20294
Cov: 20294 -> 20294
1468
Cov: 20294 -> 20294
Cov: 20294 -> 20294
1469
Cov: 20294 -> 20295
Cov: 20295 -> 20295
1470
Cov: 20295 -> 20295
Cov: 20295 -> 20295
1471
Cov: 20295 -> 20295
Cov: 20295 -> 20295
1472
{"exception": "TypeError", "msg": "'getset_descriptor' object is not callable"}
1473
Cov: 20295 -> 20295
Cov: 20295 -> 20295
1474
Cov: 20295 -> 20296
Cov: 20296 -> 20296
1475
Cov: 20296 -> 20296
Cov: 20296 -> 20296
1476
Cov: 20296 -> 20297
Cov: 20297 -> 20297
1477
Cov: 20297 -> 20298
Cov: 20298 -> 20298
1478
{"exception": "TypeError", "msg": "random_() missing 2 required positional argument: \"from\", \"to\""}
1479
Cov: 20298 -> 20298
Cov: 20298 -> 20298
1480
Cov: 20298 -> 20299
Cov: 20299 -> 20299
1481
Cov: 20299 -> 20299
Cov: 20299 -> 20299
1482
Cov: 20299 -> 20299
Cov: 20299 -> 20299
1483
Cov: 20299 -> 20299
Cov: 20299 -> 20299
1484
Cov: 20299 -> 20300
Cov: 20300 -> 20300
1485
Cov: 20300 -> 20300
Cov: 20300 -> 20300
1486
Cov: 20300 -> 20300
Cov: 20300 -> 20300
1487
Cov: 20300 -> 20300
Cov: 20300 -> 20300
1488
Cov: 20300 -> 20300
Cov: 20300 -> 20300
1489
Cov: 20300 -> 20300
Cov: 20300 -> 20300
1490
Cov: 20300 -> 20301
Cov: 20301 -> 20301
1491
Cov: 20301 -> 20301
Cov: 20301 -> 20301
1492
Cov: 20301 -> 20301
Cov: 20301 -> 20301
1493
Cov: 20301 -> 20301
Cov: 20301 -> 20301
1494
Cov: 20301 -> 20301
Cov: 20301 -> 20301
1495
Cov: 20301 -> 20301
Cov: 20301 -> 20301
1496
{"exception": "RuntimeError", "msg": "result type Double can't be cast to the desired output type Long"}
1497
Cov: 20301 -> 20301
Cov: 20301 -> 20301
1498
Cov: 20301 -> 20302
Cov: 20302 -> 20302
1499
Cov: 20302 -> 20424
Cov: 20424 -> 20424
1500
Cov: 20424 -> 20424
Cov: 20424 -> 20424
1501
Cov: 20424 -> 20425
Cov: 20425 -> 20425
1502
Cov: 20425 -> 20426
Cov: 20426 -> 20426
1503
Cov: 20426 -> 20426
Cov: 20426 -> 20426
1504
Cov: 20426 -> 20440
Cov: 20440 -> 20440
1505
Cov: 20440 -> 20440
Cov: 20440 -> 20440
1506
Cov: 20440 -> 20440
Cov: 20440 -> 20440
1507
Cov: 20440 -> 20440
Cov: 20440 -> 20440
1508
Cov: 20440 -> 20441
Cov: 20441 -> 20441
1509
Cov: 20441 -> 20441
Cov: 20441 -> 20441
1510
Cov: 20441 -> 20441
Cov: 20441 -> 20441
1511
Cov: 20441 -> 20441
Cov: 20441 -> 20441
1512
Cov: 20441 -> 20441
Cov: 20441 -> 20441
1513
Cov: 20441 -> 20441
Cov: 20441 -> 20441
1514
{"exception": "RuntimeError", "msg": "any: You passed a dimname (string) to this op in place of a dimension index but it does not yet support this behavior. Please pass a dimension index to work around this."}
1515
Cov: 20441 -> 20441
Cov: 20441 -> 20441
1516
Cov: 20441 -> 20441
Cov: 20441 -> 20441
1517
Cov: 20441 -> 20442
Cov: 20442 -> 20442
1518
Cov: 20442 -> 20443
Cov: 20443 -> 20443
1519
Cov: 20443 -> 20443
Cov: 20443 -> 20443
1520
Cov: 20443 -> 20443
Cov: 20443 -> 20443
1521
{"exception": "RuntimeError", "msg": "This function was deprecated since version 1.9 and is now removed. `torch.lstsq` is deprecated in favor of `torch.linalg.lstsq`.\n`torch.linalg.lstsq` has reversed arguments and does not return the QR decomposition in the returned tuple (although it returns other information about the problem).\n\nTo get the QR decomposition consider using `torch.linalg.qr`.\n\nThe returned solution in `torch.lstsq` stored the residuals of the solution in the last m - n columns of the returned value whenever m > n. In torch.linalg.lstsq, the residuals are in the field 'residuals' of the returned named tuple.\n\nThe unpacking of the solution, as in\nX, _ = torch.lstsq(B, A).solution[:A.size(1)]\nshould be replaced with:\nX = torch.linalg.lstsq(A, B).solution"}
1522
Cov: 20443 -> 20448
Cov: 20448 -> 20448
1523
Cov: 20448 -> 20448
Cov: 20448 -> 20448
1524
Cov: 20448 -> 20450
Cov: 20450 -> 20450
1525
Cov: 20450 -> 20450
Cov: 20450 -> 20450
1526
Cov: 20450 -> 20453
Cov: 20453 -> 20453
1527
Cov: 20453 -> 20453
Cov: 20453 -> 20453
1528
Cov: 20453 -> 20453
Cov: 20453 -> 20453
1529
Cov: 20453 -> 20453
Cov: 20453 -> 20453
1530
Cov: 20453 -> 20454
Cov: 20454 -> 20454
1531
Cov: 20454 -> 20454
Cov: 20454 -> 20454
1532
Cov: 20454 -> 20455
Cov: 20455 -> 20455
1533
Cov: 20455 -> 20455
Cov: 20455 -> 20455
1534
Cov: 20455 -> 20455
Cov: 20455 -> 20455
1535
Cov: 20455 -> 20455
Cov: 20455 -> 20455
1536
Cov: 20455 -> 20455
Cov: 20455 -> 20455
1537
Cov: 20455 -> 20455
Cov: 20455 -> 20455
1538
Cov: 20455 -> 20455
Cov: 20455 -> 20455
1539
{"exception": "TypeError", "msg": "random_() received an invalid combination of arguments - got (to=int, from_=int, ), but expected one of:\n * (*, torch.Generator generator)\n      didn't match because some of the keywords were incorrect: to, from_\n * (int from, int to, *, torch.Generator generator)\n * (int to, *, torch.Generator generator)\n"}
1540
Cov: 20455 -> 20455
Cov: 20455 -> 20455
1541
Cov: 20455 -> 20456
Cov: 20456 -> 20456
1542
Cov: 20456 -> 20456
Cov: 20456 -> 20456
1543
Cov: 20456 -> 20456
Cov: 20456 -> 20456
1544
Cov: 20456 -> 20456
Cov: 20456 -> 20456
1545
Cov: 20456 -> 20456
Cov: 20456 -> 20456
1546
Cov: 20456 -> 20456
Cov: 20456 -> 20456
1547
Cov: 20456 -> 20456
Cov: 20456 -> 20456
1548
{"exception": "RuntimeError", "msg": "element 0 of tensors does not require grad and does not have a grad_fn"}
1549
Cov: 20456 -> 20457
Cov: 20457 -> 20457
1550
Cov: 20457 -> 20457
Cov: 20457 -> 20457
1551
{"exception": "RuntimeError", "msg": "element 0 of tensors does not require grad and does not have a grad_fn"}
1552
{"exception": "RuntimeError", "msg": "This function was deprecated since version 1.9 and is now removed. `torch.linalg.eig` returns complex tensors of dtype `cfloat` or `cdouble` rather than real tensors mimicking complex tensors.\n\nL, _ = torch.eig(A) should be replaced with:\nL_complex = torch.linalg.eigvals(A)\n\nand\n\nL, V = torch.eig(A, eigenvectors=True) should be replaced with:\nL_complex, V_complex = torch.linalg.eig(A)"}
1553
Cov: 20457 -> 20461
Cov: 20461 -> 20461
1554
Cov: 20461 -> 20461
Cov: 20461 -> 20461
1555
Cov: 20461 -> 20461
Cov: 20461 -> 20461
1556
Cov: 20461 -> 20462
Cov: 20462 -> 20462
1557
Cov: 20462 -> 20462
Cov: 20462 -> 20462
1558
Cov: 20462 -> 20462
Cov: 20462 -> 20462
1559
Cov: 20462 -> 20462
Cov: 20462 -> 20462
1560
{"exception": "NameError", "msg": "name 'Tensor' is not defined"}
1561
Cov: 20462 -> 20462
Cov: 20462 -> 20462
1562
{"exception": "RuntimeError", "msg": "prelu: Type promoting not supported. Got Float and Double"}
1563
Cov: 20462 -> 20463
Cov: 20463 -> 20463
1564
Cov: 20463 -> 20463
Cov: 20463 -> 20463
1565
Cov: 20463 -> 20463
Cov: 20463 -> 20463
1566
Cov: 20463 -> 20463
Cov: 20463 -> 20463
1567
Cov: 20463 -> 20463
Cov: 20463 -> 20463
1568
Cov: 20463 -> 20467
Cov: 20467 -> 20467
1569
Cov: 20467 -> 20467
Cov: 20467 -> 20467
1570
Cov: 20467 -> 20467
Cov: 20467 -> 20467
1571
Cov: 20467 -> 20470
Cov: 20470 -> 20470
1572
Cov: 20470 -> 20470
Cov: 20470 -> 20470
1573
Cov: 20470 -> 20471
Cov: 20471 -> 20471
1574
Cov: 20471 -> 20471
Cov: 20471 -> 20471
1575
Cov: 20471 -> 20471
Cov: 20471 -> 20471
1576
Cov: 20471 -> 20471
Cov: 20471 -> 20471
1577
Cov: 20471 -> 20471
Cov: 20471 -> 20471
1578
Cov: 20471 -> 20471
Cov: 20471 -> 20471
1579
Cov: 20471 -> 20471
Cov: 20471 -> 20471
1580
Cov: 20471 -> 20472
Cov: 20472 -> 20472
1581
Cov: 20472 -> 20472
Cov: 20472 -> 20472
1582
Cov: 20472 -> 20472
Cov: 20472 -> 20472
1583
Cov: 20472 -> 20472
Cov: 20472 -> 20472
1584
Cov: 20472 -> 20472
Cov: 20472 -> 20472
1585
Cov: 20472 -> 20498
Cov: 20498 -> 20498
1586
Cov: 20498 -> 20498
Cov: 20498 -> 20498
1587
Cov: 20498 -> 20498
Cov: 20498 -> 20498
1588
{"exception": "RuntimeError", "msg": "element 0 of tensors does not require grad and does not have a grad_fn"}
1589
Cov: 20498 -> 20518
Cov: 20518 -> 20518
1590
Cov: 20518 -> 20519
Cov: 20519 -> 20519
1591
Cov: 20519 -> 20519
Cov: 20519 -> 20519
1592
Cov: 20519 -> 20520
Cov: 20520 -> 20520
1593
Cov: 20520 -> 20537
Cov: 20537 -> 20537
1594
Cov: 20537 -> 20537
Cov: 20537 -> 20537
1595
Cov: 20537 -> 20537
Cov: 20537 -> 20537
1596
Cov: 20537 -> 20538
Cov: 20538 -> 20538
1597
Cov: 20538 -> 20538
Cov: 20538 -> 20538
1598
Cov: 20538 -> 20538
Cov: 20538 -> 20538
1599
Cov: 20538 -> 20538
Cov: 20538 -> 20538
1600
Cov: 20538 -> 20538
Cov: 20538 -> 20538
1601
Cov: 20538 -> 20538
Cov: 20538 -> 20538
1602
Cov: 20538 -> 20538
Cov: 20538 -> 20538
1603
Cov: 20538 -> 20538
Cov: 20538 -> 20538
1604
Cov: 20538 -> 20538
Cov: 20538 -> 20538
1605
Cov: 20538 -> 20538
Cov: 20538 -> 20538
1606
Cov: 20538 -> 20538
Cov: 20538 -> 20538
1607
Cov: 20538 -> 20538
Cov: 20538 -> 20538
1608
Cov: 20538 -> 20538
Cov: 20538 -> 20538
1609
Cov: 20538 -> 20538
Cov: 20538 -> 20538
1610
Cov: 20538 -> 20538
Cov: 20538 -> 20538
1611
{"exception": "RuntimeError", "msg": "element 0 of tensors does not require grad and does not have a grad_fn"}
1612
Cov: 20538 -> 20539
Cov: 20539 -> 20539
1613
Cov: 20539 -> 20542
Cov: 20542 -> 20542
1614
Cov: 20542 -> 20546
Cov: 20546 -> 20546
1615
Cov: 20546 -> 20546
Cov: 20546 -> 20546
1616
{"exception": "RuntimeError", "msg": "torch.ormqr: other.shape[-2] must be greater than or equal to tau.shape[-1]"}
1617
{"exception": "NameError", "msg": "name 'math' is not defined"}
1618
Cov: 20546 -> 20546
Cov: 20546 -> 20546
1619
Cov: 20546 -> 20547
Cov: 20547 -> 20547
1620
Cov: 20547 -> 20547
Cov: 20547 -> 20547
1621
{"exception": "RuntimeError", "msg": "Error: cannot set number of interop threads after parallel work has started or set_num_interop_threads called"}
1622
Cov: 20547 -> 20547
Cov: 20547 -> 20547
1623
Cov: 20547 -> 20547
Cov: 20547 -> 20547
1624
Cov: 20547 -> 20547
Cov: 20547 -> 20547
1625
Cov: 20547 -> 20547
Cov: 20547 -> 20547
1626
Cov: 20547 -> 20547
Cov: 20547 -> 20547
1627
Cov: 20547 -> 20547
Cov: 20547 -> 20547
1628
Cov: 20547 -> 20547
Cov: 20547 -> 20547
1629
Cov: 20547 -> 20548
Cov: 20548 -> 20548
1630
Cov: 20548 -> 20548
Cov: 20548 -> 20548
1631
Cov: 20548 -> 20548
Cov: 20548 -> 20548
1632
Cov: 20548 -> 20548
Cov: 20548 -> 20548
1633
Cov: 20548 -> 20548
Cov: 20548 -> 20548
1634
Cov: 20548 -> 20548
Cov: 20548 -> 20548
1635
Cov: 20548 -> 20568
Cov: 20568 -> 20568
1636
{"exception": "RuntimeError", "msg": "expand(torch.LongTensor{[2, 3]}, size=[3]): the number of sizes provided (1) must be greater or equal to the number of dimensions in the tensor (2)"}
1637
{"exception": "RuntimeError", "msg": "This function was deprecated since version 1.9 and is now removed. `torch.solve` is deprecated in favor of `torch.linalg.solve`. `torch.linalg.solve` has its arguments reversed and does not return the LU factorization.\n\nTo get the LU factorization see `torch.lu`, which can be used with `torch.lu_solve` or `torch.lu_unpack`.\nX = torch.solve(B, A).solution should be replaced with:\nX = torch.linalg.solve(A, B)"}
1638
Cov: 20568 -> 20569
Cov: 20569 -> 20569
1639
{"exception": "RuntimeError", "msg": "Default process group has not been initialized, please make sure to call init_process_group."}
1640
Cov: 20569 -> 20578
Cov: 20578 -> 20578
1641
{"exception": "TypeError", "msg": "new_full(): argument 'size' (position 1) must be tuple of ints, not int"}
1642
Cov: 20578 -> 20590
Cov: 20590 -> 20590
1643
Cov: 20590 -> 20590
Cov: 20590 -> 20590
1644
Cov: 20590 -> 20590
Cov: 20590 -> 20590
1645
Cov: 20590 -> 20590
Cov: 20590 -> 20590
1646
Cov: 20590 -> 20590
Cov: 20590 -> 20590
1647
Cov: 20590 -> 20590
Cov: 20590 -> 20590
1648
Cov: 20590 -> 20591
Cov: 20591 -> 20591
1649
Cov: 20591 -> 20591
Cov: 20591 -> 20591
1650
Cov: 20591 -> 20591
Cov: 20591 -> 20591
1651
Cov: 20591 -> 20591
Cov: 20591 -> 20591
1652
Cov: 20591 -> 20591
Cov: 20591 -> 20591
1653
Cov: 20591 -> 20605
Cov: 20605 -> 20605
1654
Cov: 20605 -> 20605
Cov: 20605 -> 20605
1655
Cov: 20605 -> 20606
Cov: 20606 -> 20606
1656
Cov: 20606 -> 20606
Cov: 20606 -> 20606
1657
Cov: 20606 -> 20606
Cov: 20606 -> 20606
1658
Cov: 20606 -> 20607
Cov: 20607 -> 20607
1659
Cov: 20607 -> 20610
Cov: 20610 -> 20610
1660
Cov: 20610 -> 20610
Cov: 20610 -> 20610
1661
Cov: 20610 -> 20610
Cov: 20610 -> 20610
1662
Cov: 20610 -> 20610
Cov: 20610 -> 20610
1663
{"exception": "AssertionError", "msg": "d must require grad"}
1664
Cov: 20610 -> 20610
Cov: 20610 -> 20610
1665
Cov: 20610 -> 20610
Cov: 20610 -> 20610
1666
Cov: 20610 -> 20610
Cov: 20610 -> 20610
1667
Cov: 20610 -> 20610
Cov: 20610 -> 20610
1668
Cov: 20610 -> 20613
Cov: 20613 -> 20613
1669
Cov: 20613 -> 20614
Cov: 20614 -> 20614
1670
{"exception": "TypeError", "msg": "heaviside(): argument 'values' must be Tensor, not float"}
1671
Cov: 20614 -> 20614
Cov: 20614 -> 20614
1672
Cov: 20614 -> 20615
Cov: 20615 -> 20615
1673
Cov: 20615 -> 20615
Cov: 20615 -> 20615
1674
{"exception": "TypeError", "msg": "'getset_descriptor' object is not callable"}
1675
Cov: 20615 -> 20615
Cov: 20615 -> 20615
1676
Cov: 20615 -> 20615
Cov: 20615 -> 20615
1677
Cov: 20615 -> 20615
Cov: 20615 -> 20615
1678
Cov: 20615 -> 20616
Cov: 20616 -> 20616
1679
Cov: 20616 -> 20617
Cov: 20617 -> 20617
1680
{"exception": "NameError", "msg": "name 'Parameter' is not defined"}
1681
Cov: 20617 -> 20617
Cov: 20617 -> 20617
1682
Cov: 20617 -> 20617
Cov: 20617 -> 20617
1683
Cov: 20617 -> 20617
Cov: 20617 -> 20617
1684
Cov: 20617 -> 20617
Cov: 20617 -> 20617
1685
{"exception": "TypeError", "msg": "set_() received an invalid combination of arguments - got (stride=NoneType, size=NoneType, storage_offset=int, source=NoneType, ), but expected one of:\n * ()\n * (torch.Storage source)\n * (torch.Storage source, int storage_offset, tuple of ints size, tuple of ints stride)\n      didn't match because some of the arguments have invalid types: (\u001b[31;1msource=NoneType\u001b[0m, \u001b[32;1mstorage_offset=int\u001b[0m, \u001b[31;1msize=NoneType\u001b[0m, \u001b[31;1mstride=NoneType\u001b[0m, )\n * (Tensor source)\n * (Tensor source, int storage_offset, tuple of ints size, tuple of ints stride)\n      didn't match because some of the arguments have invalid types: (\u001b[31;1msource=NoneType\u001b[0m, \u001b[32;1mstorage_offset=int\u001b[0m, \u001b[31;1msize=NoneType\u001b[0m, \u001b[31;1mstride=NoneType\u001b[0m, )\n"}
1686
Cov: 20617 -> 20618
Cov: 20618 -> 20618
1687
Cov: 20618 -> 20618
Cov: 20618 -> 20618
1688
Cov: 20618 -> 20618
Cov: 20618 -> 20618
1689
Cov: 20618 -> 20748
Cov: 20748 -> 20748
1690
Cov: 20748 -> 20748
Cov: 20748 -> 20748
1691
Cov: 20748 -> 20748
Cov: 20748 -> 20748
1692
Cov: 20748 -> 20748
Cov: 20748 -> 20748
1693
Cov: 20748 -> 20763
Cov: 20763 -> 20763
1694
{"exception": "RuntimeError", "msg": "element 0 of tensors does not require grad and does not have a grad_fn"}
1695
{"exception": "RuntimeError", "msg": "torch.linalg.householder_product: input.shape[-2] must be greater than or equal to input.shape[-1]"}
1696
Cov: 20763 -> 20764
Cov: 20764 -> 20764
1697
Cov: 20764 -> 20764
Cov: 20764 -> 20764
1698
Cov: 20764 -> 20764
Cov: 20764 -> 20764
1699
{"exception": "RuntimeError", "msg": "This function was deprecated since version 1.9 and is now removed. `torch.linalg.eig` returns complex tensors of dtype `cfloat` or `cdouble` rather than real tensors mimicking complex tensors.\n\nL, _ = torch.eig(A) should be replaced with:\nL_complex = torch.linalg.eigvals(A)\n\nand\n\nL, V = torch.eig(A, eigenvectors=True) should be replaced with:\nL_complex, V_complex = torch.linalg.eig(A)"}
1700
Cov: 20764 -> 20764
Cov: 20764 -> 20764
1701
Cov: 20764 -> 20765
Cov: 20765 -> 20765
1702
Cov: 20765 -> 20766
Cov: 20766 -> 20766
1703
Cov: 20766 -> 20767
Cov: 20767 -> 20767
1704
Cov: 20767 -> 20767
Cov: 20767 -> 20767
1705
Cov: 20767 -> 20767
Cov: 20767 -> 20767
1706
{"exception": "RuntimeError", "msg": "element 0 of tensors does not require grad and does not have a grad_fn"}
1707
Cov: 20767 -> 20782
Cov: 20782 -> 20782
1708
Cov: 20782 -> 20782
Cov: 20782 -> 20782
1709
Cov: 20782 -> 20783
Cov: 20783 -> 20783
1710
Cov: 20783 -> 20783
Cov: 20783 -> 20783
1711
{"exception": "RuntimeError", "msg": "Index tensor must have the same number of dimensions as input tensor"}
1712
Cov: 20783 -> 20784
Cov: 20784 -> 20784
1713
Cov: 20784 -> 20784
Cov: 20784 -> 20784
1714
{"exception": "RuntimeError", "msg": "vector + matrix @ vector expected, got 2, 2, 1"}
1715
Cov: 20784 -> 20784
Cov: 20784 -> 20784
1716
Cov: 20784 -> 20786
Cov: 20786 -> 20786
1717
{"exception": "RuntimeError", "msg": "Error: cannot set number of interop threads after parallel work has started or set_num_interop_threads called"}
1718
Cov: 20786 -> 20796
Cov: 20796 -> 20796
1719
Cov: 20796 -> 20796
Cov: 20796 -> 20796
1720
Cov: 20796 -> 20797
Cov: 20797 -> 20797
1721
Cov: 20797 -> 20797
Cov: 20797 -> 20797
1722
Cov: 20797 -> 20798
Cov: 20798 -> 20798
1723
Cov: 20798 -> 20799
Cov: 20799 -> 20799
1724
Cov: 20799 -> 20799
Cov: 20799 -> 20799
1725
Cov: 20799 -> 20799
Cov: 20799 -> 20799
1726
Cov: 20799 -> 20799
Cov: 20799 -> 20799
1727
Cov: 20799 -> 20799
Cov: 20799 -> 20799
1728
Cov: 20799 -> 20799
Cov: 20799 -> 20799
1729
Cov: 20799 -> 20800
Cov: 20800 -> 20800
1730
Cov: 20800 -> 20800
Cov: 20800 -> 20800
1731
Cov: 20800 -> 20800
Cov: 20800 -> 20800
1732
Cov: 20800 -> 20803
Cov: 20803 -> 20803
1733
Cov: 20803 -> 20803
Cov: 20803 -> 20803
1734
Cov: 20803 -> 20803
Cov: 20803 -> 20803
1735
{"exception": "RuntimeError", "msg": "Creating a Tensor subclass from a class that does not inherit from Tensor is not possible. Make sure your class inherits from Tensor."}
1736
Cov: 20803 -> 20803
Cov: 20803 -> 20803
1737
{"exception": "RuntimeError", "msg": "This function was deprecated since version 1.9 and is now removed. `torch.linalg.eig` returns complex tensors of dtype `cfloat` or `cdouble` rather than real tensors mimicking complex tensors.\n\nL, _ = torch.eig(A) should be replaced with:\nL_complex = torch.linalg.eigvals(A)\n\nand\n\nL, V = torch.eig(A, eigenvectors=True) should be replaced with:\nL_complex, V_complex = torch.linalg.eig(A)"}
1738
Cov: 20803 -> 20835
Cov: 20835 -> 20835
1739
Cov: 20835 -> 20835
Cov: 20835 -> 20835
1740
Cov: 20835 -> 20841
Cov: 20841 -> 20841
1741
{"exception": "RuntimeError", "msg": "element 0 of tensors does not require grad and does not have a grad_fn"}
1742
Cov: 20841 -> 20843
Cov: 20843 -> 20843
1743
Cov: 20843 -> 20843
Cov: 20843 -> 20843
1744
Cov: 20843 -> 20843
Cov: 20843 -> 20843
1745
Cov: 20843 -> 20843
Cov: 20843 -> 20843
1746
Cov: 20843 -> 20843
Cov: 20843 -> 20843
1747
Cov: 20843 -> 20843
Cov: 20843 -> 20843
1748
Cov: 20843 -> 20844
Cov: 20844 -> 20844
1749
Cov: 20844 -> 20844
Cov: 20844 -> 20844
1750
Cov: 20844 -> 20845
Cov: 20845 -> 20845
1751
{"exception": "TypeError", "msg": "descriptor 'manual_seed' of 'torch._C.Generator' object needs an argument"}
1752
{"exception": "RuntimeError", "msg": "torch.triangular_solve: Expected b to have at least 2 dimensions, but it has 1 dimensions instead"}
1753
Cov: 20845 -> 20845
Cov: 20845 -> 20845
1754
Cov: 20845 -> 20845
Cov: 20845 -> 20845
1755
{"exception": "TypeError", "msg": "'getset_descriptor' object is not callable"}
1756
Cov: 20845 -> 20845
Cov: 20845 -> 20845
1757
{"exception": "RuntimeError", "msg": "Expected nested_tensorlist[0].size() > 0 to be true, but got false.  (Could this error message be improved?  If so, please report an enhancement request to PyTorch.)"}
1758
Cov: 20845 -> 20846
Cov: 20846 -> 20846
1759
Cov: 20846 -> 20846
Cov: 20846 -> 20846
1760
Cov: 20846 -> 20846
Cov: 20846 -> 20846
1761
Cov: 20846 -> 20846
Cov: 20846 -> 20846
1762
Cov: 20846 -> 20846
Cov: 20846 -> 20846
1763
Cov: 20846 -> 20846
Cov: 20846 -> 20846
1764
Cov: 20846 -> 20846
Cov: 20846 -> 20846
1765
Cov: 20846 -> 20846
Cov: 20846 -> 20846
1766
Cov: 20846 -> 20846
Cov: 20846 -> 20846
1767
Cov: 20846 -> 20846
Cov: 20846 -> 20846
1768
Cov: 20846 -> 20846
Cov: 20846 -> 20846
1769
Cov: 20846 -> 20847
Cov: 20847 -> 20847
1770
Cov: 20847 -> 20848
Cov: 20848 -> 20848
1771
Cov: 20848 -> 20848
Cov: 20848 -> 20848
1772
Cov: 20848 -> 20848
Cov: 20848 -> 20848
1773
Cov: 20848 -> 20848
Cov: 20848 -> 20848
1774
Cov: 20848 -> 20848
Cov: 20848 -> 20848
1775
Cov: 20848 -> 20848
Cov: 20848 -> 20848
1776
Cov: 20848 -> 20848
Cov: 20848 -> 20848
1777
Cov: 20848 -> 20848
Cov: 20848 -> 20848
1778
Cov: 20848 -> 20848
Cov: 20848 -> 20848
1779
Cov: 20848 -> 20848
Cov: 20848 -> 20848
1780
Cov: 20848 -> 20850
Cov: 20850 -> 20850
1781
Cov: 20850 -> 20850
Cov: 20850 -> 20850
1782
Cov: 20850 -> 20850
Cov: 20850 -> 20850
1783
Cov: 20850 -> 20850
Cov: 20850 -> 20850
1784
Cov: 20850 -> 20851
Cov: 20851 -> 20851
1785
Cov: 20851 -> 20851
Cov: 20851 -> 20851
1786
Cov: 20851 -> 20851
Cov: 20851 -> 20851
1787
Cov: 20851 -> 20852
Cov: 20852 -> 20852
1788
Cov: 20852 -> 20852
Cov: 20852 -> 20852
1789
Cov: 20852 -> 20877
Cov: 20877 -> 20877
1790
Cov: 20877 -> 20877
Cov: 20877 -> 20877
1791
Cov: 20877 -> 20877
Cov: 20877 -> 20877
1792
Cov: 20877 -> 20878
Cov: 20878 -> 20878
1793
{"exception": "RuntimeError", "msg": "element 0 of tensors does not require grad and does not have a grad_fn"}
1794
Cov: 20878 -> 20880
Cov: 20880 -> 20880
1795
Cov: 20880 -> 20880
Cov: 20880 -> 20880
1796
Cov: 20880 -> 20880
Cov: 20880 -> 20880
1797
Cov: 20880 -> 20880
Cov: 20880 -> 20880
1798
Cov: 20880 -> 20890
Cov: 20890 -> 20890
1799
{"exception": "ModuleNotFoundError", "msg": "No module named 'PIL'"}
1800
Cov: 20890 -> 23254
Cov: 23254 -> 23254
1801
Cov: 23254 -> 23254
Cov: 23254 -> 23254
1802
Cov: 23254 -> 23261
Cov: 23261 -> 23261
1803
Cov: 23261 -> 23261
Cov: 23261 -> 23261
1804
Cov: 23261 -> 23261
Cov: 23261 -> 23261
1805
Cov: 23261 -> 23262
Cov: 23262 -> 23262
1806
Cov: 23262 -> 23264
Cov: 23264 -> 23264
1807
Cov: 23264 -> 23264
Cov: 23264 -> 23264
1808
Cov: 23264 -> 23264
Cov: 23264 -> 23264
1809
Cov: 23264 -> 23264
Cov: 23264 -> 23264
1810
Cov: 23264 -> 23264
Cov: 23264 -> 23264
1811
Cov: 23264 -> 23265
Cov: 23265 -> 23265
1812
Cov: 23265 -> 23265
Cov: 23265 -> 23265
1813
Cov: 23265 -> 23265
Cov: 23265 -> 23265
1814
Cov: 23265 -> 23265
Cov: 23265 -> 23265
1815
Cov: 23265 -> 23265
Cov: 23265 -> 23265
1816
Cov: 23265 -> 23265
Cov: 23265 -> 23265
1817
Cov: 23265 -> 23268
Cov: 23268 -> 23268
1818
Cov: 23268 -> 23268
Cov: 23268 -> 23268
1819
Cov: 23268 -> 23268
Cov: 23268 -> 23268
1820
Cov: 23268 -> 23269
Cov: 23269 -> 23269
1821
{"exception": "TypeError", "msg": "to_sparse() received an invalid combination of arguments - got (sparseDims=int, ), but expected one of:\n * (*, torch.layout layout, tuple of ints blocksize, int dense_dim)\n * (int sparse_dim)\n      didn't match because some of the keywords were incorrect: sparseDims\n"}
1822
Cov: 23269 -> 23269
Cov: 23269 -> 23269
1823
Cov: 23269 -> 23269
Cov: 23269 -> 23269
1824
Cov: 23269 -> 23269
Cov: 23269 -> 23269
1825
Cov: 23269 -> 23270
Cov: 23270 -> 23270
1826
Cov: 23270 -> 23270
Cov: 23270 -> 23270
1827
Cov: 23270 -> 23270
Cov: 23270 -> 23270
1828
Cov: 23270 -> 23271
Cov: 23271 -> 23271
1829
Cov: 23271 -> 23271
Cov: 23271 -> 23271
1830
Cov: 23271 -> 23271
Cov: 23271 -> 23271
1831
Cov: 23271 -> 23273
Cov: 23273 -> 23273
1832
Cov: 23273 -> 23273
Cov: 23273 -> 23273
1833
Cov: 23273 -> 23273
Cov: 23273 -> 23273
1834
Cov: 23273 -> 23273
Cov: 23273 -> 23273
1835
Cov: 23273 -> 23273
Cov: 23273 -> 23273
1836
Cov: 23273 -> 23273
Cov: 23273 -> 23273
1837
Cov: 23273 -> 23273
Cov: 23273 -> 23273
1838
Cov: 23273 -> 23273
Cov: 23273 -> 23273
1839
Cov: 23273 -> 23273
Cov: 23273 -> 23273
1840
Cov: 23273 -> 23273
Cov: 23273 -> 23273
1841
Cov: 23273 -> 23273
Cov: 23273 -> 23273
1842
Cov: 23273 -> 23273
Cov: 23273 -> 23273
1843
Cov: 23273 -> 23274
Cov: 23274 -> 23274
1844
Cov: 23274 -> 23275
Cov: 23275 -> 23275
1845
Cov: 23275 -> 23276
Cov: 23276 -> 23276
1846
Cov: 23276 -> 23277
Cov: 23277 -> 23277
1847
Cov: 23277 -> 23277
Cov: 23277 -> 23277
1848
Cov: 23277 -> 23278
Cov: 23278 -> 23278
1849
Cov: 23278 -> 23278
Cov: 23278 -> 23278
1850
Cov: 23278 -> 23278
Cov: 23278 -> 23278
1851
Cov: 23278 -> 23278
Cov: 23278 -> 23278
1852
Cov: 23278 -> 23278
Cov: 23278 -> 23278
1853
Cov: 23278 -> 23278
Cov: 23278 -> 23278
1854
{"exception": "RuntimeError", "msg": "masked_fill_ only supports boolean masks, but got mask with dtype unsigned char"}
1855
Cov: 23278 -> 23278
Cov: 23278 -> 23278
1856
Cov: 23278 -> 23278
Cov: 23278 -> 23278
1857
Cov: 23278 -> 23278
Cov: 23278 -> 23278
1858
Cov: 23278 -> 23312
Cov: 23312 -> 23312
1859
Cov: 23312 -> 23312
Cov: 23312 -> 23312
1860
Cov: 23312 -> 23312
Cov: 23312 -> 23312
1861
Cov: 23312 -> 23313
Cov: 23313 -> 23313
1862
Cov: 23313 -> 23313
Cov: 23313 -> 23313
1863
{"exception": "RuntimeError", "msg": "element 0 of tensors does not require grad and does not have a grad_fn"}
1864
Cov: 23313 -> 23321
Cov: 23321 -> 23321
1865
Cov: 23321 -> 23321
Cov: 23321 -> 23321
1866
Cov: 23321 -> 23321
Cov: 23321 -> 23321
1867
Cov: 23321 -> 23321
Cov: 23321 -> 23321
1868
Cov: 23321 -> 23322
Cov: 23322 -> 23322
1869
Cov: 23322 -> 23322
Cov: 23322 -> 23322
1870
Cov: 23322 -> 23323
Cov: 23323 -> 23323
1871
{"exception": "RuntimeError", "msg": "element 0 of tensors does not require grad and does not have a grad_fn"}
1872
Cov: 23323 -> 23324
Cov: 23324 -> 23324
1873
Cov: 23324 -> 23324
Cov: 23324 -> 23324
1874
Cov: 23324 -> 23324
Cov: 23324 -> 23324
1875
Cov: 23324 -> 23324
Cov: 23324 -> 23324
1876
Cov: 23324 -> 23324
Cov: 23324 -> 23324
1877
Cov: 23324 -> 23325
Cov: 23325 -> 23325
1878
Cov: 23325 -> 23325
Cov: 23325 -> 23325
1879
Cov: 23325 -> 23325
Cov: 23325 -> 23325
1880
Cov: 23325 -> 23325
Cov: 23325 -> 23325
1881
Cov: 23325 -> 23325
Cov: 23325 -> 23325
1882
{"exception": "RuntimeError", "msg": "This function was deprecated since version 1.9 and is now removed. `torch.linalg.eig` returns complex tensors of dtype `cfloat` or `cdouble` rather than real tensors mimicking complex tensors.\n\nL, _ = torch.eig(A) should be replaced with:\nL_complex = torch.linalg.eigvals(A)\n\nand\n\nL, V = torch.eig(A, eigenvectors=True) should be replaced with:\nL_complex, V_complex = torch.linalg.eig(A)"}
1883
Cov: 23325 -> 23326
Cov: 23326 -> 23326
1884
Cov: 23326 -> 23327
Cov: 23327 -> 23327
1885
Cov: 23327 -> 23327
Cov: 23327 -> 23327
1886
Cov: 23327 -> 23327
Cov: 23327 -> 23327
1887
Cov: 23327 -> 23327
Cov: 23327 -> 23327
1888
Cov: 23327 -> 23327
Cov: 23327 -> 23327
1889
{"exception": "RuntimeError", "msg": "scatter(): Expected self.dtype to be equal to src.dtype"}
1890
Cov: 23327 -> 23327
Cov: 23327 -> 23327
1891
Cov: 23327 -> 23327
Cov: 23327 -> 23327
1892
Cov: 23327 -> 23327
Cov: 23327 -> 23327
1893
Cov: 23327 -> 23327
Cov: 23327 -> 23327
1894
Cov: 23327 -> 23327
Cov: 23327 -> 23327
1895
Cov: 23327 -> 23327
Cov: 23327 -> 23327
1896
Cov: 23327 -> 23327
Cov: 23327 -> 23327
1897
Cov: 23327 -> 23327
Cov: 23327 -> 23327
1898
Cov: 23327 -> 23327
Cov: 23327 -> 23327
1899
Cov: 23327 -> 23327
Cov: 23327 -> 23327
1900
Cov: 23327 -> 23327
Cov: 23327 -> 23327
1901
Cov: 23327 -> 23327
Cov: 23327 -> 23327
1902
Cov: 23327 -> 23327
Cov: 23327 -> 23327
1903
Cov: 23327 -> 23331
Cov: 23331 -> 23331
1904
Cov: 23331 -> 23331
Cov: 23331 -> 23331
1905
Cov: 23331 -> 23331
Cov: 23331 -> 23331
1906
Cov: 23331 -> 23343
Cov: 23343 -> 23343
1907
Cov: 23343 -> 23344
Cov: 23344 -> 23344
1908
Cov: 23344 -> 23344
Cov: 23344 -> 23344
1909
Cov: 23344 -> 23344
Cov: 23344 -> 23344
1910
Cov: 23344 -> 23344
Cov: 23344 -> 23344
1911
Cov: 23344 -> 23344
Cov: 23344 -> 23344
1912
Cov: 23344 -> 23344
Cov: 23344 -> 23344
1913
Cov: 23344 -> 23344
Cov: 23344 -> 23344
1914
Cov: 23344 -> 23344
Cov: 23344 -> 23344
1915
Cov: 23344 -> 23344
Cov: 23344 -> 23344
1916
Cov: 23344 -> 23344
Cov: 23344 -> 23344
1917
Cov: 23344 -> 23345
Cov: 23345 -> 23345
1918
{"exception": "RuntimeError", "msg": "logdet: A must be batches of square matrices, but they are 3 by 4 matrices"}
1919
Cov: 23345 -> 23345
Cov: 23345 -> 23345
1920
Cov: 23345 -> 23345
Cov: 23345 -> 23345
1921
Cov: 23345 -> 23345
Cov: 23345 -> 23345
1922
Cov: 23345 -> 23345
Cov: 23345 -> 23345
1923
Cov: 23345 -> 23345
Cov: 23345 -> 23345
1924
Cov: 23345 -> 23346
Cov: 23346 -> 23346
1925
Cov: 23346 -> 23346
Cov: 23346 -> 23346
1926
Cov: 23346 -> 23346
Cov: 23346 -> 23346
1927
Cov: 23346 -> 23347
Cov: 23347 -> 23347
1928
{"exception": "RuntimeError", "msg": "any: You passed a dimname (string) to this op in place of a dimension index but it does not yet support this behavior. Please pass a dimension index to work around this."}
1929
Cov: 23347 -> 23347
Cov: 23347 -> 23347
1930
Cov: 23347 -> 23347
Cov: 23347 -> 23347
1931
{"exception": "RuntimeError", "msg": "This function was deprecated since version 1.9 and is now removed. `torch.linalg.eig` returns complex tensors of dtype `cfloat` or `cdouble` rather than real tensors mimicking complex tensors.\n\nL, _ = torch.eig(A) should be replaced with:\nL_complex = torch.linalg.eigvals(A)\n\nand\n\nL, V = torch.eig(A, eigenvectors=True) should be replaced with:\nL_complex, V_complex = torch.linalg.eig(A)"}
1932
Cov: 23347 -> 23348
Cov: 23348 -> 23348
1933
Cov: 23348 -> 23348
Cov: 23348 -> 23348
1934
Cov: 23348 -> 23348
Cov: 23348 -> 23348
1935
Cov: 23348 -> 23349
Cov: 23349 -> 23349
1936
Cov: 23349 -> 23353
Cov: 23353 -> 23353
1937
Cov: 23353 -> 23357
Cov: 23357 -> 23357
1938
Cov: 23357 -> 23357
Cov: 23357 -> 23357
1939
Cov: 23357 -> 23357
Cov: 23357 -> 23357
1940
Cov: 23357 -> 23357
Cov: 23357 -> 23357
1941
Cov: 23357 -> 23360
Cov: 23360 -> 23360
1942
{"exception": "RuntimeError", "msg": "linalg.slogdet: A must be batches of square matrices, but they are 3 by 4 matrices"}
1943
Cov: 23360 -> 23360
Cov: 23360 -> 23360
1944
Cov: 23360 -> 23360
Cov: 23360 -> 23360
1945
Cov: 23360 -> 23360
Cov: 23360 -> 23360
1946
Cov: 23360 -> 23360
Cov: 23360 -> 23360
1947
Cov: 23360 -> 23360
Cov: 23360 -> 23360
1948
Cov: 23360 -> 23366
Cov: 23366 -> 23366
1949
Cov: 23366 -> 23370
Cov: 23370 -> 23370
1950
Cov: 23370 -> 23370
Cov: 23370 -> 23370
1951
Cov: 23370 -> 23370
Cov: 23370 -> 23370
1952
Cov: 23370 -> 23370
Cov: 23370 -> 23370
1953
Cov: 23370 -> 23370
Cov: 23370 -> 23370
1954
Cov: 23370 -> 23370
Cov: 23370 -> 23370
1955
Cov: 23370 -> 23370
Cov: 23370 -> 23370
1956
Cov: 23370 -> 23370
Cov: 23370 -> 23370
1957
Cov: 23370 -> 23370
Cov: 23370 -> 23370
1958
Cov: 23370 -> 23370
Cov: 23370 -> 23370
1959
Cov: 23370 -> 23372
Cov: 23372 -> 23372
1960
Cov: 23372 -> 23372
Cov: 23372 -> 23372
1961
Cov: 23372 -> 23372
Cov: 23372 -> 23372
1962
Cov: 23372 -> 23372
Cov: 23372 -> 23372
1963
Cov: 23372 -> 23372
Cov: 23372 -> 23372
1964
Cov: 23372 -> 23372
Cov: 23372 -> 23372
1965
Cov: 23372 -> 23372
Cov: 23372 -> 23372
1966
{"exception": "TypeError", "msg": "'getset_descriptor' object is not callable"}
1967
Cov: 23372 -> 23372
Cov: 23372 -> 23372
1968
Cov: 23372 -> 23372
Cov: 23372 -> 23372
1969
Cov: 23372 -> 23372
Cov: 23372 -> 23372
1970
Cov: 23372 -> 23372
Cov: 23372 -> 23372
1971
Cov: 23372 -> 23372
Cov: 23372 -> 23372
1972
Cov: 23372 -> 23372
Cov: 23372 -> 23372
1973
Cov: 23372 -> 23372
Cov: 23372 -> 23372
1974
Cov: 23372 -> 23372
Cov: 23372 -> 23372
1975
Cov: 23372 -> 23373
Cov: 23373 -> 23373
1976
Cov: 23373 -> 23373
Cov: 23373 -> 23373
1977
Cov: 23373 -> 23373
Cov: 23373 -> 23373
1978
Cov: 23373 -> 23373
Cov: 23373 -> 23373
1979
{"exception": "NameError", "msg": "name 'Tensor' is not defined"}
1980
Cov: 23373 -> 23374
Cov: 23374 -> 23374
1981
Cov: 23374 -> 23374
Cov: 23374 -> 23374
1982
Cov: 23374 -> 23374
Cov: 23374 -> 23374
1983
Cov: 23374 -> 23379
Cov: 23379 -> 23379
1984
Cov: 23379 -> 23379
Cov: 23379 -> 23379
1985
Cov: 23379 -> 23379
Cov: 23379 -> 23379
1986
Cov: 23379 -> 23379
Cov: 23379 -> 23379
1987
Cov: 23379 -> 23379
Cov: 23379 -> 23379
1988
Cov: 23379 -> 23380
Cov: 23380 -> 23380
1989
Cov: 23380 -> 23380
Cov: 23380 -> 23380
1990
Cov: 23380 -> 23380
Cov: 23380 -> 23380
1991
Cov: 23380 -> 23380
Cov: 23380 -> 23380
1992
Cov: 23380 -> 23380
Cov: 23380 -> 23380
1993
Cov: 23380 -> 23380
Cov: 23380 -> 23380
1994
Cov: 23380 -> 23383
Cov: 23383 -> 23383
1995
Cov: 23383 -> 23384
Cov: 23384 -> 23384
1996
{"exception": "RuntimeError", "msg": "This function was deprecated since version 1.9 and is now removed.\nPlease use the `torch.linalg.matrix_rank` function instead. The parameter 'symmetric' was renamed in `torch.linalg.matrix_rank()` to 'hermitian'."}
1997
Cov: 23384 -> 23385
Cov: 23385 -> 23385
1998
Cov: 23385 -> 23385
Cov: 23385 -> 23385
1999
Cov: 23385 -> 23385
Cov: 23385 -> 23385
2000
Cov: 23385 -> 23385
Cov: 23385 -> 23385
2001
Cov: 23385 -> 23385
Cov: 23385 -> 23385
2002
Cov: 23385 -> 23385
Cov: 23385 -> 23385
2003
Cov: 23385 -> 23385
Cov: 23385 -> 23385
2004
Cov: 23385 -> 23385
Cov: 23385 -> 23385
2005
Cov: 23385 -> 23385
Cov: 23385 -> 23385
2006
Cov: 23385 -> 23385
Cov: 23385 -> 23385
2007
Cov: 23385 -> 23385
Cov: 23385 -> 23385
2008
Cov: 23385 -> 23385
Cov: 23385 -> 23385
2009
Cov: 23385 -> 23386
Cov: 23386 -> 23386
2010
Cov: 23386 -> 23386
Cov: 23386 -> 23386
2011
Cov: 23386 -> 23386
Cov: 23386 -> 23386
2012
Cov: 23386 -> 23386
Cov: 23386 -> 23386
2013
Cov: 23386 -> 23387
Cov: 23387 -> 23387
2014
Cov: 23387 -> 23387
Cov: 23387 -> 23387
2015
Cov: 23387 -> 23387
Cov: 23387 -> 23387
2016
Cov: 23387 -> 23387
Cov: 23387 -> 23387
2017
Cov: 23387 -> 23387
Cov: 23387 -> 23387
2018
Cov: 23387 -> 23439
Cov: 23439 -> 23439
2019
Cov: 23439 -> 23439
Cov: 23439 -> 23439
2020
Cov: 23439 -> 23439
Cov: 23439 -> 23439
2021
Cov: 23439 -> 23440
Cov: 23440 -> 23440
2022
Cov: 23440 -> 23443
Cov: 23443 -> 23443
2023
Cov: 23443 -> 23443
Cov: 23443 -> 23443
2024
Cov: 23443 -> 23443
Cov: 23443 -> 23443
2025
Cov: 23443 -> 23443
Cov: 23443 -> 23443
2026
Cov: 23443 -> 23444
Cov: 23444 -> 23444
2027
Cov: 23444 -> 23445
Cov: 23445 -> 23445
2028
Cov: 23445 -> 23446
Cov: 23446 -> 23446
2029
Cov: 23446 -> 23446
Cov: 23446 -> 23446
2030
Cov: 23446 -> 23447
Cov: 23447 -> 23447
2031
Cov: 23447 -> 23447
Cov: 23447 -> 23447
2032
Cov: 23447 -> 23447
Cov: 23447 -> 23447
2033
Cov: 23447 -> 23447
Cov: 23447 -> 23447
2034
Cov: 23447 -> 23447
Cov: 23447 -> 23447
2035
Cov: 23447 -> 23447
Cov: 23447 -> 23447
2036
Cov: 23447 -> 23447
Cov: 23447 -> 23447
2037
{"exception": "TypeError", "msg": "new_full(): argument 'size' (position 1) must be tuple of ints, not int"}
2038
Cov: 23447 -> 23447
Cov: 23447 -> 23447
2039
Cov: 23447 -> 23449
Cov: 23449 -> 23449
2040
Cov: 23449 -> 23449
Cov: 23449 -> 23449
2041
{"exception": "TypeError", "msg": "heaviside(): argument 'values' (position 1) must be Tensor, not float"}
2042
Cov: 23449 -> 23450
Cov: 23450 -> 23450
2043
Cov: 23450 -> 23469
Cov: 23469 -> 23469
2044
Cov: 23469 -> 23470
Cov: 23470 -> 23470
2045
Cov: 23470 -> 23470
Cov: 23470 -> 23470
2046
Cov: 23470 -> 23470
Cov: 23470 -> 23470
2047
{"exception": "TypeError", "msg": "'getset_descriptor' object is not callable"}
2048
Cov: 23470 -> 23471
Cov: 23471 -> 23471
2049
Cov: 23471 -> 23471
Cov: 23471 -> 23471
2050
Cov: 23471 -> 23471
Cov: 23471 -> 23471
2051
Cov: 23471 -> 23472
Cov: 23472 -> 23472
2052
{"exception": "RuntimeError", "msg": "scatter(): Expected self.dtype to be equal to src.dtype"}
2053
Cov: 23472 -> 23478
Cov: 23478 -> 23478
2054
Cov: 23478 -> 23478
Cov: 23478 -> 23478
2055
Cov: 23478 -> 23479
Cov: 23479 -> 23479
2056
Cov: 23479 -> 23479
Cov: 23479 -> 23479
2057
Cov: 23479 -> 23479
Cov: 23479 -> 23479
2058
Cov: 23479 -> 23479
Cov: 23479 -> 23479
2059
Cov: 23479 -> 23479
Cov: 23479 -> 23479
2060
Cov: 23479 -> 23479
Cov: 23479 -> 23479
2061
Cov: 23479 -> 23479
Cov: 23479 -> 23479
2062
Cov: 23479 -> 23480
Cov: 23480 -> 23480
2063
Cov: 23480 -> 23480
Cov: 23480 -> 23480
2064
Cov: 23480 -> 23480
Cov: 23480 -> 23480
2065
Cov: 23480 -> 23480
Cov: 23480 -> 23480
2066
Cov: 23480 -> 23484
Cov: 23484 -> 23484
2067
Cov: 23484 -> 23484
Cov: 23484 -> 23484
2068
Cov: 23484 -> 23484
Cov: 23484 -> 23484
2069
Cov: 23484 -> 23486
Cov: 23486 -> 23486
2070
Cov: 23486 -> 23486
Cov: 23486 -> 23486
2071
Cov: 23486 -> 23486
Cov: 23486 -> 23486
2072
Cov: 23486 -> 23486
Cov: 23486 -> 23486
2073
Cov: 23486 -> 23486
Cov: 23486 -> 23486
2074
Cov: 23486 -> 23486
Cov: 23486 -> 23486
2075
Cov: 23486 -> 23486
Cov: 23486 -> 23486
2076
{"exception": "RuntimeError", "msg": "expected scalar type Double but found Float"}
2077
Cov: 23486 -> 23486
Cov: 23486 -> 23486
2078
Cov: 23486 -> 23486
Cov: 23486 -> 23486
2079
Cov: 23486 -> 23486
Cov: 23486 -> 23486
2080
Cov: 23486 -> 23487
Cov: 23487 -> 23487
2081
Cov: 23487 -> 23519
Cov: 23519 -> 23519
2082
Cov: 23519 -> 23519
Cov: 23519 -> 23519
2083
Cov: 23519 -> 23519
Cov: 23519 -> 23519
2084
Cov: 23519 -> 23519
Cov: 23519 -> 23519
2085
Cov: 23519 -> 23552
Cov: 23552 -> 23552
2086
Cov: 23552 -> 23552
Cov: 23552 -> 23552
2087
Cov: 23552 -> 23553
Cov: 23553 -> 23553
2088
Cov: 23553 -> 23553
Cov: 23553 -> 23553
2089
{"exception": "TypeError", "msg": "scatter_() received an invalid combination of arguments - got (int, Tensor, Tensor, NoneType), but expected one of:\n * (int dim, Tensor index, Tensor src)\n * (int dim, Tensor index, Tensor src, *, str reduce)\n * (int dim, Tensor index, Number value)\n * (int dim, Tensor index, Number value, *, str reduce)\n"}
2090
Cov: 23553 -> 23556
Cov: 23556 -> 23556
2091
Cov: 23556 -> 23556
Cov: 23556 -> 23556
2092
Cov: 23556 -> 23556
Cov: 23556 -> 23556
2093
Cov: 23556 -> 23557
Cov: 23557 -> 23557
2094
Cov: 23557 -> 23557
Cov: 23557 -> 23557
2095
Cov: 23557 -> 23557
Cov: 23557 -> 23557
2096
Cov: 23557 -> 23557
Cov: 23557 -> 23557
2097
Cov: 23557 -> 23557
Cov: 23557 -> 23557
2098
Cov: 23557 -> 23557
Cov: 23557 -> 23557
2099
Cov: 23557 -> 23557
Cov: 23557 -> 23557
2100
Cov: 23557 -> 23557
Cov: 23557 -> 23557
2101
Cov: 23557 -> 23557
Cov: 23557 -> 23557
2102
Cov: 23557 -> 23557
Cov: 23557 -> 23557
2103
Cov: 23557 -> 23557
Cov: 23557 -> 23557
2104
Cov: 23557 -> 23557
Cov: 23557 -> 23557
2105
Cov: 23557 -> 23557
Cov: 23557 -> 23557
2106
Cov: 23557 -> 23557
Cov: 23557 -> 23557
2107
Cov: 23557 -> 23558
Cov: 23558 -> 23558
2108
Cov: 23558 -> 23558
Cov: 23558 -> 23558
2109
Cov: 23558 -> 23558
Cov: 23558 -> 23558
2110
Cov: 23558 -> 23562
Cov: 23562 -> 23562
2111
Cov: 23562 -> 23562
Cov: 23562 -> 23562
2112
Cov: 23562 -> 23563
Cov: 23563 -> 23563
2113
{"exception": "TypeError", "msg": "'getset_descriptor' object is not callable"}
2114
Cov: 23563 -> 23563
Cov: 23563 -> 23563
2115
Cov: 23563 -> 23564
Cov: 23564 -> 23564
2116
Cov: 23564 -> 23564
Cov: 23564 -> 23564
2117
Cov: 23564 -> 23565
Cov: 23565 -> 23565
2118
Cov: 23565 -> 23566
Cov: 23566 -> 23566
2119
Cov: 23566 -> 23566
Cov: 23566 -> 23566
2120
Cov: 23566 -> 23567
Cov: 23567 -> 23567
2121
Cov: 23567 -> 23567
Cov: 23567 -> 23567
2122
Cov: 23567 -> 23567
Cov: 23567 -> 23567
2123
Cov: 23567 -> 23567
Cov: 23567 -> 23567
2124
Cov: 23567 -> 23567
Cov: 23567 -> 23567
2125
Cov: 23567 -> 23567
Cov: 23567 -> 23567
2126
Cov: 23567 -> 23567
Cov: 23567 -> 23567
2127
Cov: 23567 -> 23568
Cov: 23568 -> 23568
2128
Cov: 23568 -> 23569
Cov: 23569 -> 23569
2129
Cov: 23569 -> 23569
Cov: 23569 -> 23569
2130
Cov: 23569 -> 23570
Cov: 23570 -> 23570
2131
Cov: 23570 -> 23570
Cov: 23570 -> 23570
2132
Cov: 23570 -> 23570
Cov: 23570 -> 23570
2133
Cov: 23570 -> 23570
Cov: 23570 -> 23570
2134
{"exception": "TypeError", "msg": "logical_xor_(): argument 'other' must be Tensor, not bool"}
2135
Cov: 23570 -> 23570
Cov: 23570 -> 23570
2136
Cov: 23570 -> 23571
Cov: 23571 -> 23571
2137
Cov: 23571 -> 23582
Cov: 23582 -> 23582
2138
Cov: 23582 -> 23582
Cov: 23582 -> 23582
2139
Cov: 23582 -> 23582
Cov: 23582 -> 23582
2140
Cov: 23582 -> 23583
Cov: 23583 -> 23583
2141
Cov: 23583 -> 23583
Cov: 23583 -> 23583
2142
Cov: 23583 -> 23583
Cov: 23583 -> 23583
2143
Cov: 23583 -> 23585
Cov: 23585 -> 23585
2144
Cov: 23585 -> 23585
Cov: 23585 -> 23585
2145
Cov: 23585 -> 23589
Cov: 23589 -> 23589
2146
Cov: 23589 -> 23589
Cov: 23589 -> 23589
2147
Cov: 23589 -> 23589
Cov: 23589 -> 23589
2148
Cov: 23589 -> 23589
Cov: 23589 -> 23589
2149
Cov: 23589 -> 23591
Cov: 23591 -> 23591
2150
Cov: 23591 -> 23591
Cov: 23591 -> 23591
2151
Cov: 23591 -> 23591
Cov: 23591 -> 23591
2152
{"exception": "RuntimeError", "msg": "vector + matrix @ vector expected, got 2, 2, 1"}
2153
Cov: 23591 -> 23591
Cov: 23591 -> 23591
2154
{"exception": "TypeError", "msg": "type_as(): argument 'other' (position 1) must be Tensor, not torch.tensortype"}
2155
Cov: 23591 -> 23594
Cov: 23594 -> 23594
2156
{"exception": "NameError", "msg": "name 'randint' is not defined"}
2157
Cov: 23594 -> 23594
Cov: 23594 -> 23594
2158
Cov: 23594 -> 23594
Cov: 23594 -> 23594
2159
{"exception": "TypeError", "msg": "fft_ifftn(): argument 'input' (position 1) must be Tensor, not builtin_function_or_method"}
2160
{"exception": "TypeError", "msg": "addbmm() missing 1 required positional arguments: \"batch2\""}
2161
Cov: 23594 -> 23595
Cov: 23595 -> 23595
2162
Cov: 23595 -> 23595
Cov: 23595 -> 23595
2163
Cov: 23595 -> 23596
Cov: 23596 -> 23596
2164
Cov: 23596 -> 23596
Cov: 23596 -> 23596
2165
Cov: 23596 -> 23596
Cov: 23596 -> 23596
2166
Cov: 23596 -> 23596
Cov: 23596 -> 23596
2167
Cov: 23596 -> 23596
Cov: 23596 -> 23596
2168
Cov: 23596 -> 23596
Cov: 23596 -> 23596
2169
Cov: 23596 -> 23596
Cov: 23596 -> 23596
2170
Cov: 23596 -> 23596
Cov: 23596 -> 23596
2171
Cov: 23596 -> 23596
Cov: 23596 -> 23596
2172
Cov: 23596 -> 23596
Cov: 23596 -> 23596
2173
Cov: 23596 -> 23596
Cov: 23596 -> 23596
2174
Cov: 23596 -> 23596
Cov: 23596 -> 23596
2175
Cov: 23596 -> 23596
Cov: 23596 -> 23596
2176
{"exception": "RuntimeError", "msg": "A must be batches of square matrices, but they are 3 by 4 matrices"}
2177
Cov: 23596 -> 23596
Cov: 23596 -> 23596
2178
Cov: 23596 -> 23596
Cov: 23596 -> 23596
2179
Cov: 23596 -> 23596
Cov: 23596 -> 23596
2180
Cov: 23596 -> 23597
Cov: 23597 -> 23597
2181
Cov: 23597 -> 23597
Cov: 23597 -> 23597
2182
Cov: 23597 -> 23598
Cov: 23598 -> 23598
2183
Cov: 23598 -> 23598
Cov: 23598 -> 23598
2184
Cov: 23598 -> 23598
Cov: 23598 -> 23598
2185
Cov: 23598 -> 23598
Cov: 23598 -> 23598
2186
Cov: 23598 -> 23598
Cov: 23598 -> 23598
2187
Cov: 23598 -> 23598
Cov: 23598 -> 23598
2188
Cov: 23598 -> 23598
Cov: 23598 -> 23598
2189
{"exception": "RuntimeError", "msg": "This function was deprecated since version 1.9 and is now removed. `torch.solve` is deprecated in favor of `torch.linalg.solve`. `torch.linalg.solve` has its arguments reversed and does not return the LU factorization.\n\nTo get the LU factorization see `torch.lu`, which can be used with `torch.lu_solve` or `torch.lu_unpack`.\nX = torch.solve(B, A).solution should be replaced with:\nX = torch.linalg.solve(A, B)"}
2190
Cov: 23598 -> 23599
Cov: 23599 -> 23599
2191
{"exception": "TypeError", "msg": "'getset_descriptor' object is not callable"}
2192
Cov: 23599 -> 23599
Cov: 23599 -> 23599
2193
Cov: 23599 -> 23599
Cov: 23599 -> 23599
2194
Cov: 23599 -> 23600
Cov: 23600 -> 23600
2195
Cov: 23600 -> 23601
Cov: 23601 -> 23601
2196
Cov: 23601 -> 23601
Cov: 23601 -> 23601
2197
Cov: 23601 -> 23601
Cov: 23601 -> 23601
2198
Cov: 23601 -> 23602
Cov: 23602 -> 23602
2199
Cov: 23602 -> 23602
Cov: 23602 -> 23602
2200
Cov: 23602 -> 23602
Cov: 23602 -> 23602
2201
Cov: 23602 -> 23605
Cov: 23605 -> 23605
2202
Cov: 23605 -> 23609
Cov: 23609 -> 23609
2203
Cov: 23609 -> 23609
Cov: 23609 -> 23609
2204
Cov: 23609 -> 23609
Cov: 23609 -> 23609
2205
Cov: 23609 -> 23609
Cov: 23609 -> 23609
2206
Cov: 23609 -> 23610
Cov: 23610 -> 23610
2207
Cov: 23610 -> 23610
Cov: 23610 -> 23610
2208
Cov: 23610 -> 23610
Cov: 23610 -> 23610
2209
Cov: 23610 -> 23610
Cov: 23610 -> 23610
2210
Cov: 23610 -> 23610
Cov: 23610 -> 23610
2211
Cov: 23610 -> 23610
Cov: 23610 -> 23610
2212
Cov: 23610 -> 23611
Cov: 23611 -> 23611
2213
Cov: 23611 -> 23611
Cov: 23611 -> 23611
2214
Cov: 23611 -> 23612
Cov: 23612 -> 23612
2215
{"exception": "TypeError", "msg": "new_full(): argument 'size' (position 1) must be tuple of ints, not int"}
2216
Cov: 23612 -> 23612
Cov: 23612 -> 23612
2217
{"exception": "TypeError", "msg": "index_add() received an invalid combination of arguments - got (tensor=Tensor, index=Tensor, dim=int, ), but expected one of:\n * (int dim, Tensor index, Tensor source, *, Number alpha)\n * (name dim, Tensor index, Tensor source, *, Number alpha)\n"}
2218
Cov: 23612 -> 23612
Cov: 23612 -> 23612
2219
Cov: 23612 -> 23612
Cov: 23612 -> 23612
2220
Cov: 23612 -> 23612
Cov: 23612 -> 23612
2221
Cov: 23612 -> 23612
Cov: 23612 -> 23612
2222
Cov: 23612 -> 23612
Cov: 23612 -> 23612
2223
Cov: 23612 -> 23612
Cov: 23612 -> 23612
2224
Cov: 23612 -> 23612
Cov: 23612 -> 23612
2225
Cov: 23612 -> 23613
Cov: 23613 -> 23613
2226
Cov: 23613 -> 23613
Cov: 23613 -> 23613
2227
Cov: 23613 -> 23613
Cov: 23613 -> 23613
2228
Cov: 23613 -> 23614
Cov: 23614 -> 23614
2229
Cov: 23614 -> 23614
Cov: 23614 -> 23614
2230
Cov: 23614 -> 23614
Cov: 23614 -> 23614
2231
Cov: 23614 -> 23614
Cov: 23614 -> 23614
2232
Cov: 23614 -> 23614
Cov: 23614 -> 23614
2233
Cov: 23614 -> 23614
Cov: 23614 -> 23614
2234
Cov: 23614 -> 23614
Cov: 23614 -> 23614
2235
{"exception": "RuntimeError", "msg": "tensor.sspaddmm(...) can only be called on sparse tensors"}
2236
Cov: 23614 -> 23614
Cov: 23614 -> 23614
2237
{"exception": "TypeError", "msg": "arctanh_() takes no arguments (1 given)"}
2238
{"exception": "NotImplementedError", "msg": "Could not run 'aten::int_repr' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::int_repr' is only available for these backends: [QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nQuantizedCPU: registered at aten/src/ATen/RegisterQuantizedCPU.cpp:944 [kernel]\nQuantizedCUDA: registered at aten/src/ATen/RegisterQuantizedCUDA.cpp:459 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:153 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:498 [backend fallback]\nFunctionalize: registered at ../aten/src/ATen/FunctionalizeFallbackKernel.cpp:290 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:86 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradMTIA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_0.cpp:16725 [kernel]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:382 [backend fallback]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:249 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:710 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:203 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:161 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:494 [backend fallback]\nPreDispatch: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:165 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:157 [backend fallback]\n"}
2239
Cov: 23614 -> 23614
Cov: 23614 -> 23614
2240
Cov: 23614 -> 23614
Cov: 23614 -> 23614
2241
Cov: 23614 -> 23614
Cov: 23614 -> 23614
2242
Cov: 23614 -> 23614
Cov: 23614 -> 23614
2243
{"exception": "NotImplementedError", "msg": "Could not run 'aten::q_per_channel_scales' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::q_per_channel_scales' is only available for these backends: [QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nQuantizedCPU: registered at aten/src/ATen/RegisterQuantizedCPU.cpp:944 [kernel]\nQuantizedCUDA: registered at aten/src/ATen/RegisterQuantizedCUDA.cpp:459 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:153 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:498 [backend fallback]\nFunctionalize: registered at ../aten/src/ATen/FunctionalizeFallbackKernel.cpp:290 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:86 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradMTIA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_3.cpp:14610 [kernel]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:382 [backend fallback]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:249 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:710 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:203 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:161 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:494 [backend fallback]\nPreDispatch: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:165 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:157 [backend fallback]\n"}
2244
Cov: 23614 -> 23614
Cov: 23614 -> 23614
2245
Cov: 23614 -> 23615
Cov: 23615 -> 23615
2246
Cov: 23615 -> 23616
Cov: 23616 -> 23616
2247
Cov: 23616 -> 23616
Cov: 23616 -> 23616
2248
Cov: 23616 -> 23616
Cov: 23616 -> 23616
2249
{"exception": "RuntimeError", "msg": "This function was deprecated since version 1.9 and is now removed. `torch.solve` is deprecated in favor of `torch.linalg.solve`. `torch.linalg.solve` has its arguments reversed and does not return the LU factorization.\n\nTo get the LU factorization see `torch.lu`, which can be used with `torch.lu_solve` or `torch.lu_unpack`.\nX = torch.solve(B, A).solution should be replaced with:\nX = torch.linalg.solve(A, B)"}
2250
Cov: 23616 -> 23635
Cov: 23635 -> 23635
2251
Cov: 23635 -> 23635
Cov: 23635 -> 23635
2252
Cov: 23635 -> 23635
Cov: 23635 -> 23635
2253
Cov: 23635 -> 23635
Cov: 23635 -> 23635
2254
Cov: 23635 -> 23635
Cov: 23635 -> 23635
2255
Cov: 23635 -> 23635
Cov: 23635 -> 23635
2256
Cov: 23635 -> 23635
Cov: 23635 -> 23635
2257
Cov: 23635 -> 23635
Cov: 23635 -> 23635
2258
Cov: 23635 -> 23635
Cov: 23635 -> 23635
2259
Cov: 23635 -> 23635
Cov: 23635 -> 23635
2260
=================================================================
timeout reached. testcase took: 10
[Error] ...
Hangs during coverage collection.
Had to restart coverage executor...
timeout
2261
Cov: 23635 -> 39
Cov: 39 -> 39
2262
Cov: 39 -> 39
Cov: 39 -> 39
2263
Cov: 39 -> 39
Cov: 39 -> 39
2264
Cov: 39 -> 42
Cov: 42 -> 42
2265
{"exception": "RuntimeError", "msg": "\"rshift_cpu\" not implemented for 'Float'"}
2266
Cov: 42 -> 42
Cov: 42 -> 42
2267
Cov: 42 -> 46
Cov: 46 -> 46
2268
Cov: 46 -> 46
Cov: 46 -> 46
2269
Cov: 46 -> 46
Cov: 46 -> 46
2270
Cov: 46 -> 46
Cov: 46 -> 46
2271
Cov: 46 -> 46
Cov: 46 -> 46
2272
Cov: 46 -> 46
Cov: 46 -> 46
2273
Cov: 46 -> 159
Cov: 159 -> 159
2274
{"exception": "RuntimeError", "msg": "scatter(): Expected self.dtype to be equal to src.dtype"}
2275
Cov: 159 -> 159
Cov: 159 -> 159
2276
Cov: 159 -> 159
Cov: 159 -> 159
2277
Cov: 159 -> 159
Cov: 159 -> 159
2278
Cov: 159 -> 187
Cov: 187 -> 187
2279
Cov: 187 -> 187
Cov: 187 -> 187
2280
Cov: 187 -> 187
Cov: 187 -> 187
2281
Cov: 187 -> 189
Cov: 189 -> 189
2282
Cov: 189 -> 189
Cov: 189 -> 189
2283
{"exception": "RuntimeError", "msg": "outer: Expected 1-D argument self, but got 2-D"}
2284
{"exception": "RuntimeError", "msg": "Expected nested_tensorlist[0].size() > 0 to be true, but got false.  (Could this error message be improved?  If so, please report an enhancement request to PyTorch.)"}
2285
Cov: 189 -> 214
Cov: 214 -> 214
2286
Cov: 214 -> 214
Cov: 214 -> 214
2287
Cov: 214 -> 214
Cov: 214 -> 214
2288
Cov: 214 -> 214
Cov: 214 -> 214
2289
Cov: 214 -> 214
Cov: 214 -> 214
2290
Cov: 214 -> 282
Cov: 282 -> 282
2291
Cov: 282 -> 282
Cov: 282 -> 282
2292
Cov: 282 -> 282
Cov: 282 -> 282
2293
Cov: 282 -> 282
Cov: 282 -> 282
2294
Cov: 282 -> 282
Cov: 282 -> 282
2295
Cov: 282 -> 287
Cov: 287 -> 287
2296
Cov: 287 -> 287
Cov: 287 -> 287
2297
Cov: 287 -> 287
Cov: 287 -> 287
2298
Cov: 287 -> 287
Cov: 287 -> 287
2299
Cov: 287 -> 289
Cov: 289 -> 289
2300
Cov: 289 -> 289
Cov: 289 -> 289
2301
Cov: 289 -> 294
Cov: 294 -> 294
2302
Cov: 294 -> 304
Cov: 304 -> 304
2303
Cov: 304 -> 304
Cov: 304 -> 304
2304
Cov: 304 -> 304
Cov: 304 -> 304
2305
Cov: 304 -> 304
Cov: 304 -> 304
2306
Cov: 304 -> 304
Cov: 304 -> 304
2307
Cov: 304 -> 304
Cov: 304 -> 304
2308
Cov: 304 -> 307
Cov: 307 -> 307
2309
Cov: 307 -> 307
Cov: 307 -> 307
2310
Cov: 307 -> 309
Cov: 309 -> 309
2311
Cov: 309 -> 309
Cov: 309 -> 309
2312
Cov: 309 -> 309
Cov: 309 -> 309
2313
Cov: 309 -> 309
Cov: 309 -> 309
2314
Cov: 309 -> 309
Cov: 309 -> 309
2315
Cov: 309 -> 309
Cov: 309 -> 309
2316
Cov: 309 -> 309
Cov: 309 -> 309
2317
Cov: 309 -> 309
Cov: 309 -> 309
2318
Cov: 309 -> 309
Cov: 309 -> 309
2319
Cov: 309 -> 309
Cov: 309 -> 309
2320
Cov: 309 -> 309
Cov: 309 -> 309
2321
Cov: 309 -> 309
Cov: 309 -> 309
2322
{"exception": "TypeError", "msg": "remainder_() received an invalid combination of arguments - got (divisor=int, ), but expected one of:\n * (Tensor other)\n      didn't match because some of the keywords were incorrect: divisor\n * (Number other)\n      didn't match because some of the keywords were incorrect: divisor\n"}
2323
Cov: 309 -> 309
Cov: 309 -> 309
2324
Cov: 309 -> 309
Cov: 309 -> 309
2325
Cov: 309 -> 309
Cov: 309 -> 309
2326
Cov: 309 -> 333
Cov: 333 -> 333
2327
Cov: 333 -> 333
Cov: 333 -> 333
2328
{"exception": "NotImplementedError", "msg": "Could not run 'aten::int_repr' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::int_repr' is only available for these backends: [QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nQuantizedCPU: registered at aten/src/ATen/RegisterQuantizedCPU.cpp:944 [kernel]\nQuantizedCUDA: registered at aten/src/ATen/RegisterQuantizedCUDA.cpp:459 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:153 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:498 [backend fallback]\nFunctionalize: registered at ../aten/src/ATen/FunctionalizeFallbackKernel.cpp:290 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:86 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradMTIA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_0.cpp:16725 [kernel]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:382 [backend fallback]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:249 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:710 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:203 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:161 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:494 [backend fallback]\nPreDispatch: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:165 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:157 [backend fallback]\n"}
2329
Cov: 333 -> 512
Cov: 512 -> 512
2330
Cov: 512 -> 512
Cov: 512 -> 512
2331
Cov: 512 -> 519
Cov: 519 -> 519
2332
Cov: 519 -> 519
Cov: 519 -> 519
2333
Cov: 519 -> 519
Cov: 519 -> 519
2334
{"exception": "RuntimeError", "msg": "vector + matrix @ vector expected, got 1, 3, 1"}
2335
Cov: 519 -> 519
Cov: 519 -> 519
2336
Cov: 519 -> 537
Cov: 537 -> 537
2337
Cov: 537 -> 537
Cov: 537 -> 537
2338
Cov: 537 -> 669
Cov: 669 -> 669
2339
Cov: 669 -> 670
Cov: 670 -> 670
2340
{"exception": "RuntimeError", "msg": "expand(torch.FloatTensor{[2, 3, 4]}, size=[2, 4]): the number of sizes provided (2) must be greater or equal to the number of dimensions in the tensor (3)"}
2341
Cov: 670 -> 670
Cov: 670 -> 670
2342
Cov: 670 -> 670
Cov: 670 -> 670
2343
Cov: 670 -> 670
Cov: 670 -> 670
2344
Cov: 670 -> 670
Cov: 670 -> 670
2345
Cov: 670 -> 670
Cov: 670 -> 670
2346
{"exception": "TypeError", "msg": "must be real number, not NoneType"}
2347
Cov: 670 -> 670
Cov: 670 -> 670
2348
Cov: 670 -> 670
Cov: 670 -> 670
2349
Cov: 670 -> 670
Cov: 670 -> 670
2350
Cov: 670 -> 673
Cov: 673 -> 673
2351
Cov: 673 -> 673
Cov: 673 -> 673
2352
Cov: 673 -> 673
Cov: 673 -> 673
2353
Cov: 673 -> 673
Cov: 673 -> 673
2354
Cov: 673 -> 673
Cov: 673 -> 673
2355
Cov: 673 -> 673
Cov: 673 -> 673
2356
Cov: 673 -> 673
Cov: 673 -> 673
2357
Cov: 673 -> 673
Cov: 673 -> 673
2358
Cov: 673 -> 682
Cov: 682 -> 682
2359
Cov: 682 -> 682
Cov: 682 -> 682
2360
Cov: 682 -> 698
Cov: 698 -> 698
2361
Cov: 698 -> 714
Cov: 714 -> 714
2362
Cov: 714 -> 725
Cov: 725 -> 725
2363
Cov: 725 -> 725
Cov: 725 -> 725
2364
Cov: 725 -> 726
Cov: 726 -> 726
2365
Cov: 726 -> 726
Cov: 726 -> 726
2366
Cov: 726 -> 726
Cov: 726 -> 726
2367
Cov: 726 -> 726
Cov: 726 -> 726
2368
Cov: 726 -> 726
Cov: 726 -> 726
2369
Cov: 726 -> 736
Cov: 736 -> 736
2370
Cov: 736 -> 736
Cov: 736 -> 736
2371
Cov: 736 -> 736
Cov: 736 -> 736
2372
Cov: 736 -> 736
Cov: 736 -> 736
2373
Cov: 736 -> 744
Cov: 744 -> 744
2374
Cov: 744 -> 745
Cov: 745 -> 745
2375
Cov: 745 -> 745
Cov: 745 -> 745
2376
Cov: 745 -> 745
Cov: 745 -> 745
2377
Cov: 745 -> 748
Cov: 748 -> 748
2378
Cov: 748 -> 803
Cov: 803 -> 803
2379
{"exception": "NotImplementedError", "msg": "Could not run 'aten::q_zero_point' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::q_zero_point' is only available for these backends: [QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nQuantizedCPU: registered at aten/src/ATen/RegisterQuantizedCPU.cpp:944 [kernel]\nQuantizedCUDA: registered at aten/src/ATen/RegisterQuantizedCUDA.cpp:459 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:153 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:498 [backend fallback]\nFunctionalize: registered at ../aten/src/ATen/FunctionalizeFallbackKernel.cpp:290 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:86 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradMTIA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_3.cpp:14610 [kernel]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:382 [backend fallback]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:249 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:710 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:203 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:161 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:494 [backend fallback]\nPreDispatch: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:165 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:157 [backend fallback]\n"}
2380
Cov: 803 -> 803
Cov: 803 -> 803
2381
Cov: 803 -> 803
Cov: 803 -> 803
2382
Cov: 803 -> 803
Cov: 803 -> 803
2383
Cov: 803 -> 803
Cov: 803 -> 803
2384
Cov: 803 -> 803
Cov: 803 -> 803
2385
Cov: 803 -> 804
Cov: 804 -> 804
2386
Cov: 804 -> 804
Cov: 804 -> 804
2387
Cov: 804 -> 804
Cov: 804 -> 804
2388
Cov: 804 -> 804
Cov: 804 -> 804
2389
Cov: 804 -> 804
Cov: 804 -> 804
2390
Cov: 804 -> 804
Cov: 804 -> 804
2391
Cov: 804 -> 804
Cov: 804 -> 804
2392
Cov: 804 -> 804
Cov: 804 -> 804
2393
Cov: 804 -> 804
Cov: 804 -> 804
2394
Cov: 804 -> 804
Cov: 804 -> 804
2395
Cov: 804 -> 804
Cov: 804 -> 804
2396
Cov: 804 -> 804
Cov: 804 -> 804
2397
Cov: 804 -> 804
Cov: 804 -> 804
2398
Cov: 804 -> 16963
Cov: 16963 -> 16963
2399
Cov: 16963 -> 16966
Cov: 16966 -> 16966
2400
Cov: 16966 -> 16966
Cov: 16966 -> 16966
2401
Cov: 16966 -> 16966
Cov: 16966 -> 16966
2402
Cov: 16966 -> 16966
Cov: 16966 -> 16966
2403
Cov: 16966 -> 16966
Cov: 16966 -> 16966
2404
Cov: 16966 -> 16966
Cov: 16966 -> 16966
2405
Cov: 16966 -> 16966
Cov: 16966 -> 16966
2406
Cov: 16966 -> 16967
Cov: 16967 -> 16967
2407
Cov: 16967 -> 16967
Cov: 16967 -> 16967
2408
Cov: 16967 -> 16967
Cov: 16967 -> 16967
2409
Cov: 16967 -> 16967
Cov: 16967 -> 16967
2410
Cov: 16967 -> 16984
Cov: 16984 -> 16984
2411
Cov: 16984 -> 16984
Cov: 16984 -> 16984
2412
Cov: 16984 -> 16984
Cov: 16984 -> 16984
2413
Cov: 16984 -> 16984
Cov: 16984 -> 16984
2414
{"exception": "RuntimeError", "msg": "Bad in-place call: input tensor size [3, 2] and output tensor size [2, 3] should match"}
2415
Cov: 16984 -> 16984
Cov: 16984 -> 16984
2416
Cov: 16984 -> 16988
Cov: 16988 -> 16988
2417
Cov: 16988 -> 16994
Cov: 16994 -> 16994
2418
Cov: 16994 -> 17132
Cov: 17132 -> 17132
2419
Cov: 17132 -> 17137
Cov: 17137 -> 17137
2420
Cov: 17137 -> 17155
Cov: 17155 -> 17155
2421
Cov: 17155 -> 17163
Cov: 17163 -> 17163
2422
Cov: 17163 -> 17163
Cov: 17163 -> 17163
2423
Cov: 17163 -> 17163
Cov: 17163 -> 17163
2424
Cov: 17163 -> 17164
Cov: 17164 -> 17164
2425
Cov: 17164 -> 17164
Cov: 17164 -> 17164
2426
Cov: 17164 -> 17164
Cov: 17164 -> 17164
2427
Cov: 17164 -> 17164
Cov: 17164 -> 17164
2428
Cov: 17164 -> 17164
Cov: 17164 -> 17164
2429
{"exception": "RuntimeError", "msg": "tensor.sspaddmm(...) can only be called on sparse tensors"}
2430
Cov: 17164 -> 17164
Cov: 17164 -> 17164
2431
{"exception": "RuntimeError", "msg": "values expected sparse tensor layout but got Strided"}
2432
{"exception": "RuntimeError", "msg": "size mismatch, input: [2, 3], v1: [3], v2: [3]"}
2433
Cov: 17164 -> 17168
Cov: 17168 -> 17168
2434
Cov: 17168 -> 17168
Cov: 17168 -> 17168
2435
Cov: 17168 -> 17186
Cov: 17186 -> 17186
2436
Cov: 17186 -> 17186
Cov: 17186 -> 17186
2437
Cov: 17186 -> 17211
Cov: 17211 -> 17211
2438
Cov: 17211 -> 17212
Cov: 17212 -> 17212
2439
{"exception": "RuntimeError", "msg": "The expanded size of the tensor (2) must match the existing size (3) at non-singleton dimension 1.  Target sizes: [2, 2].  Tensor sizes: [2, 3]"}
2440
Cov: 17212 -> 17213
Cov: 17213 -> 17213
2441
Cov: 17213 -> 17213
Cov: 17213 -> 17213
2442
{"exception": "NotImplementedError", "msg": "Could not run 'aten::q_per_channel_axis' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::q_per_channel_axis' is only available for these backends: [QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nQuantizedCPU: registered at aten/src/ATen/RegisterQuantizedCPU.cpp:944 [kernel]\nQuantizedCUDA: registered at aten/src/ATen/RegisterQuantizedCUDA.cpp:459 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:153 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:498 [backend fallback]\nFunctionalize: registered at ../aten/src/ATen/FunctionalizeFallbackKernel.cpp:290 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:86 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradMTIA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_4.cpp:13056 [kernel]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:382 [backend fallback]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:249 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:710 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:203 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:161 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:494 [backend fallback]\nPreDispatch: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:165 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:157 [backend fallback]\n"}
2443
Cov: 17213 -> 17213
Cov: 17213 -> 17213
2444
Cov: 17213 -> 17213
Cov: 17213 -> 17213
2445
Cov: 17213 -> 17213
Cov: 17213 -> 17213
2446
Cov: 17213 -> 17221
Cov: 17221 -> 17221
2447
Cov: 17221 -> 17221
Cov: 17221 -> 17221
2448
{"exception": "RuntimeError", "msg": "result type Float can't be cast to the desired output type Long"}
2449
Cov: 17221 -> 17221
Cov: 17221 -> 17221
2450
Cov: 17221 -> 17223
Cov: 17223 -> 17223
2451
{"exception": "RuntimeError", "msg": "result type Float can't be cast to the desired output type Long"}
2452
Cov: 17223 -> 17223
Cov: 17223 -> 17223
2453
Cov: 17223 -> 17223
Cov: 17223 -> 17223
2454
Cov: 17223 -> 17223
Cov: 17223 -> 17223
2455
Cov: 17223 -> 17229
Cov: 17229 -> 17229
2456
Cov: 17229 -> 17246
Cov: 17246 -> 17246
2457
Cov: 17246 -> 17246
Cov: 17246 -> 17246
2458
Cov: 17246 -> 17272
Cov: 17272 -> 17272
2459
Cov: 17272 -> 17273
Cov: 17273 -> 17273
2460
{"exception": "RuntimeError", "msg": "the base given to float_power_ has dtype Float but the operation's result requires dtype Double"}
2461
Cov: 17273 -> 17273
Cov: 17273 -> 17273
2462
Cov: 17273 -> 17273
Cov: 17273 -> 17273
2463
Cov: 17273 -> 17273
Cov: 17273 -> 17273
2464
Cov: 17273 -> 17280
Cov: 17280 -> 17280
2465
Cov: 17280 -> 17306
Cov: 17306 -> 17306
2466
Cov: 17306 -> 17306
Cov: 17306 -> 17306
2467
Cov: 17306 -> 17307
Cov: 17307 -> 17307
2468
Cov: 17307 -> 17313
Cov: 17313 -> 17313
2469
Cov: 17313 -> 17313
Cov: 17313 -> 17313
2470
Cov: 17313 -> 17313
Cov: 17313 -> 17313
2471
Cov: 17313 -> 17313
Cov: 17313 -> 17313
2472
Cov: 17313 -> 17313
Cov: 17313 -> 17313
2473
Cov: 17313 -> 17318
Cov: 17318 -> 17318
2474
Cov: 17318 -> 17318
Cov: 17318 -> 17318
2475
Cov: 17318 -> 17318
Cov: 17318 -> 17318
2476
Cov: 17318 -> 17318
Cov: 17318 -> 17318
2477
Cov: 17318 -> 17318
Cov: 17318 -> 17318
2478
Cov: 17318 -> 17318
Cov: 17318 -> 17318
2479
Cov: 17318 -> 17318
Cov: 17318 -> 17318
2480
{"exception": "TypeError", "msg": "addbmm() received an invalid combination of arguments - got (Tensor), but expected (Tensor batch1, Tensor batch2, *, Number beta, Number alpha)"}
2481
Cov: 17318 -> 17335
Cov: 17335 -> 17335
2482
Cov: 17335 -> 17335
Cov: 17335 -> 17335
2483
Cov: 17335 -> 17335
Cov: 17335 -> 17335
2484
Cov: 17335 -> 17335
Cov: 17335 -> 17335
2485
Cov: 17335 -> 17335
Cov: 17335 -> 17335
2486
Cov: 17335 -> 17336
Cov: 17336 -> 17336
2487
Cov: 17336 -> 17336
Cov: 17336 -> 17336
2488
Cov: 17336 -> 17336
Cov: 17336 -> 17336
2489
Cov: 17336 -> 17379
Cov: 17379 -> 17379
2490
{"exception": "ModuleNotFoundError", "msg": "No module named 'PIL'"}
2491
Cov: 17379 -> 19748
Cov: 19748 -> 19748
2492
Cov: 19748 -> 19748
Cov: 19748 -> 19748
2493
Cov: 19748 -> 19748
Cov: 19748 -> 19748
2494
Cov: 19748 -> 19748
Cov: 19748 -> 19748
2495
Cov: 19748 -> 19752
Cov: 19752 -> 19752
2496
Cov: 19752 -> 19755
Cov: 19755 -> 19755
2497
Cov: 19755 -> 19755
Cov: 19755 -> 19755
2498
Cov: 19755 -> 19755
Cov: 19755 -> 19755
2499
Cov: 19755 -> 19755
Cov: 19755 -> 19755
2500
Cov: 19755 -> 19758
Cov: 19758 -> 19758
2501
Cov: 19758 -> 19758
Cov: 19758 -> 19758
2502
Cov: 19758 -> 19758
Cov: 19758 -> 19758
2503
Cov: 19758 -> 19758
Cov: 19758 -> 19758
2504
Cov: 19758 -> 19772
Cov: 19772 -> 19772
2505
Cov: 19772 -> 19772
Cov: 19772 -> 19772
2506
Cov: 19772 -> 19782
Cov: 19782 -> 19782
2507
{"exception": "RuntimeError", "msg": "masked_scatter_ only supports boolean masks, but got mask with dtype Long"}
2508
Cov: 19782 -> 19782
Cov: 19782 -> 19782
2509
Cov: 19782 -> 19784
Cov: 19784 -> 19784
2510
Cov: 19784 -> 19784
Cov: 19784 -> 19784
2511
Cov: 19784 -> 19784
Cov: 19784 -> 19784
2512
Cov: 19784 -> 19784
Cov: 19784 -> 19784
2513
Cov: 19784 -> 19784
Cov: 19784 -> 19784
2514
Cov: 19784 -> 19784
Cov: 19784 -> 19784
2515
Cov: 19784 -> 19784
Cov: 19784 -> 19784
2516
{"exception": "RuntimeError", "msg": "size mismatch, input: [2, 3], v1: [3], v2: [3]"}
2517
Cov: 19784 -> 19785
Cov: 19785 -> 19785
2518
Cov: 19785 -> 19787
Cov: 19787 -> 19787
2519
Cov: 19787 -> 19787
Cov: 19787 -> 19787
2520
Cov: 19787 -> 19788
Cov: 19788 -> 19788
2521
Cov: 19788 -> 19788
Cov: 19788 -> 19788
2522
Cov: 19788 -> 19788
Cov: 19788 -> 19788
2523
Cov: 19788 -> 19788
Cov: 19788 -> 19788
2524
Cov: 19788 -> 19788
Cov: 19788 -> 19788
2525
Cov: 19788 -> 19788
Cov: 19788 -> 19788
2526
Cov: 19788 -> 19788
Cov: 19788 -> 19788
2527
{"exception": "RuntimeError", "msg": "values expected sparse tensor layout but got Strided"}
2528
Cov: 19788 -> 19788
Cov: 19788 -> 19788
2529
Cov: 19788 -> 19788
Cov: 19788 -> 19788
2530
Cov: 19788 -> 19788
Cov: 19788 -> 19788
2531
Cov: 19788 -> 19788
Cov: 19788 -> 19788
2532
{"exception": "RuntimeError", "msg": "Expected nested_tensorlist[0].size() > 0 to be true, but got false.  (Could this error message be improved?  If so, please report an enhancement request to PyTorch.)"}
2533
Cov: 19788 -> 19789
Cov: 19789 -> 19789
2534
Cov: 19789 -> 19792
Cov: 19792 -> 19792
2535
Cov: 19792 -> 19792
Cov: 19792 -> 19792
2536
Cov: 19792 -> 19794
Cov: 19794 -> 19794
2537
Cov: 19794 -> 19795
Cov: 19795 -> 19795
2538
Cov: 19795 -> 19795
Cov: 19795 -> 19795
2539
Cov: 19795 -> 19795
Cov: 19795 -> 19795
2540
Cov: 19795 -> 19795
Cov: 19795 -> 19795
2541
Cov: 19795 -> 19795
Cov: 19795 -> 19795
2542
Cov: 19795 -> 19799
Cov: 19799 -> 19799
2543
Cov: 19799 -> 19799
Cov: 19799 -> 19799
2544
{"exception": "NotImplementedError", "msg": "Could not run 'aten::q_per_channel_axis' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::q_per_channel_axis' is only available for these backends: [QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nQuantizedCPU: registered at aten/src/ATen/RegisterQuantizedCPU.cpp:944 [kernel]\nQuantizedCUDA: registered at aten/src/ATen/RegisterQuantizedCUDA.cpp:459 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:153 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:498 [backend fallback]\nFunctionalize: registered at ../aten/src/ATen/FunctionalizeFallbackKernel.cpp:290 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:86 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradMTIA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_4.cpp:13056 [kernel]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:382 [backend fallback]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:249 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:710 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:203 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:161 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:494 [backend fallback]\nPreDispatch: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:165 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:157 [backend fallback]\n"}
2545
Cov: 19799 -> 19799
Cov: 19799 -> 19799
2546
Cov: 19799 -> 19799
Cov: 19799 -> 19799
2547
Cov: 19799 -> 19799
Cov: 19799 -> 19799
2548
Cov: 19799 -> 19800
Cov: 19800 -> 19800
2549
Cov: 19800 -> 19800
Cov: 19800 -> 19800
2550
Cov: 19800 -> 19800
Cov: 19800 -> 19800
2551
{"exception": "RuntimeError", "msg": "element 0 of tensors does not require grad and does not have a grad_fn"}
2552
Cov: 19800 -> 19846
Cov: 19846 -> 19846
2553
Cov: 19846 -> 19846
Cov: 19846 -> 19846
2554
Cov: 19846 -> 19846
Cov: 19846 -> 19846
2555
Cov: 19846 -> 19846
Cov: 19846 -> 19846
2556
Cov: 19846 -> 19847
Cov: 19847 -> 19847
2557
Cov: 19847 -> 19857
Cov: 19857 -> 19857
2558
Cov: 19857 -> 19857
Cov: 19857 -> 19857
2559
Cov: 19857 -> 19857
Cov: 19857 -> 19857
2560
Cov: 19857 -> 19859
Cov: 19859 -> 19859
2561
Cov: 19859 -> 19859
Cov: 19859 -> 19859
2562
Cov: 19859 -> 19859
Cov: 19859 -> 19859
2563
Cov: 19859 -> 19859
Cov: 19859 -> 19859
2564
Cov: 19859 -> 19860
Cov: 19860 -> 19860
2565
Cov: 19860 -> 19860
Cov: 19860 -> 19860
2566
Cov: 19860 -> 19860
Cov: 19860 -> 19860
2567
Cov: 19860 -> 19860
Cov: 19860 -> 19860
2568
Cov: 19860 -> 19865
Cov: 19865 -> 19865
2569
Cov: 19865 -> 19865
Cov: 19865 -> 19865
2570
Cov: 19865 -> 19866
Cov: 19866 -> 19866
2571
{"exception": "RuntimeError", "msg": "scatter(): Expected self.dtype to be equal to src.dtype"}
2572
Cov: 19866 -> 19878
Cov: 19878 -> 19878
2573
Cov: 19878 -> 19911
Cov: 19911 -> 19911
2574
Cov: 19911 -> 19911
Cov: 19911 -> 19911
2575
Cov: 19911 -> 19911
Cov: 19911 -> 19911
2576
{"exception": "NotImplementedError", "msg": "Could not run 'aten::q_scale' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::q_scale' is only available for these backends: [QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nQuantizedCPU: registered at aten/src/ATen/RegisterQuantizedCPU.cpp:944 [kernel]\nQuantizedCUDA: registered at aten/src/ATen/RegisterQuantizedCUDA.cpp:459 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:153 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:498 [backend fallback]\nFunctionalize: registered at ../aten/src/ATen/FunctionalizeFallbackKernel.cpp:290 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:86 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradMTIA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_4.cpp:13056 [kernel]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:382 [backend fallback]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:249 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:710 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:203 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:161 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:494 [backend fallback]\nPreDispatch: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:165 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:157 [backend fallback]\n"}
2577
Cov: 19911 -> 19911
Cov: 19911 -> 19911
2578
Cov: 19911 -> 19918
Cov: 19918 -> 19918
2579
Cov: 19918 -> 19918
Cov: 19918 -> 19918
2580
Cov: 19918 -> 19918
Cov: 19918 -> 19918
2581
{"exception": "RuntimeError", "msg": "element 0 of tensors does not require grad and does not have a grad_fn"}
2582
Cov: 19918 -> 20031
Cov: 20031 -> 20031
2583
Cov: 20031 -> 20031
Cov: 20031 -> 20031
2584
Cov: 20031 -> 20031
Cov: 20031 -> 20031
2585
Cov: 20031 -> 20035
Cov: 20035 -> 20035
2586
Cov: 20035 -> 20038
Cov: 20038 -> 20038
2587
Cov: 20038 -> 20038
Cov: 20038 -> 20038
2588
Cov: 20038 -> 20038
Cov: 20038 -> 20038
2589
Cov: 20038 -> 20038
Cov: 20038 -> 20038
2590
Cov: 20038 -> 20038
Cov: 20038 -> 20038
2591
{"exception": "TypeError", "msg": "new_full(): argument 'size' (position 1) must be tuple of ints, not int"}
2592
Cov: 20038 -> 20038
Cov: 20038 -> 20038
2593
Cov: 20038 -> 20038
Cov: 20038 -> 20038
2594
Cov: 20038 -> 20038
Cov: 20038 -> 20038
2595
{"exception": "RuntimeError", "msg": "Index tensor must have the same number of dimensions as self tensor"}
2596
Cov: 20038 -> 20046
Cov: 20046 -> 20046
2597
Cov: 20046 -> 20070
Cov: 20070 -> 20070
2598
Cov: 20070 -> 20070
Cov: 20070 -> 20070
2599
Cov: 20070 -> 20070
Cov: 20070 -> 20070
2600
Cov: 20070 -> 20070
Cov: 20070 -> 20070
2601
Cov: 20070 -> 20070
Cov: 20070 -> 20070
2602
Cov: 20070 -> 20070
Cov: 20070 -> 20070
2603
Cov: 20070 -> 20070
Cov: 20070 -> 20070
2604
Cov: 20070 -> 20072
Cov: 20072 -> 20072
2605
Cov: 20072 -> 20072
Cov: 20072 -> 20072
2606
Cov: 20072 -> 20072
Cov: 20072 -> 20072
2607
Cov: 20072 -> 20072
Cov: 20072 -> 20072
2608
Cov: 20072 -> 20072
Cov: 20072 -> 20072
2609
Cov: 20072 -> 20072
Cov: 20072 -> 20072
2610
Cov: 20072 -> 20072
Cov: 20072 -> 20072
2611
Cov: 20072 -> 20073
Cov: 20073 -> 20073
2612
Cov: 20073 -> 20090
Cov: 20090 -> 20090
2613
Cov: 20090 -> 20090
Cov: 20090 -> 20090
2614
Cov: 20090 -> 20090
Cov: 20090 -> 20090
2615
Cov: 20090 -> 20090
Cov: 20090 -> 20090
2616
Cov: 20090 -> 20090
Cov: 20090 -> 20090
2617
Cov: 20090 -> 20090
Cov: 20090 -> 20090
2618
Cov: 20090 -> 20090
Cov: 20090 -> 20090
2619
Cov: 20090 -> 20090
Cov: 20090 -> 20090
2620
{"exception": "RuntimeError", "msg": "Error: cannot set number of interop threads after parallel work has started or set_num_interop_threads called"}
2621
{"exception": "RuntimeError", "msg": "torch.ormqr: other.shape[-2] must be equal to input.shape[-2]"}
2622
Cov: 20090 -> 20090
Cov: 20090 -> 20090
2623
Cov: 20090 -> 20090
Cov: 20090 -> 20090
2624
Cov: 20090 -> 20090
Cov: 20090 -> 20090
2625
Cov: 20090 -> 20090
Cov: 20090 -> 20090
2626
Cov: 20090 -> 20090
Cov: 20090 -> 20090
2627
Cov: 20090 -> 20090
Cov: 20090 -> 20090
2628
Cov: 20090 -> 20090
Cov: 20090 -> 20090
2629
Cov: 20090 -> 20092
Cov: 20092 -> 20092
2630
Cov: 20092 -> 20092
Cov: 20092 -> 20092
2631
Cov: 20092 -> 20109
Cov: 20109 -> 20109
2632
{"exception": "RuntimeError", "msg": "values expected sparse tensor layout but got Strided"}
2633
Cov: 20109 -> 20109
Cov: 20109 -> 20109
2634
Cov: 20109 -> 20129
Cov: 20129 -> 20129
2635
Cov: 20129 -> 20129
Cov: 20129 -> 20129
2636
Cov: 20129 -> 20129
Cov: 20129 -> 20129
2637
Cov: 20129 -> 20129
Cov: 20129 -> 20129
2638
Cov: 20129 -> 20223
Cov: 20223 -> 20223
2639
Cov: 20223 -> 20230
Cov: 20230 -> 20230
2640
Cov: 20230 -> 20230
Cov: 20230 -> 20230
2641
Cov: 20230 -> 20230
Cov: 20230 -> 20230
2642
Cov: 20230 -> 20231
Cov: 20231 -> 20231
2643
Cov: 20231 -> 20231
Cov: 20231 -> 20231
2644
Cov: 20231 -> 20231
Cov: 20231 -> 20231
2645
Cov: 20231 -> 20231
Cov: 20231 -> 20231
2646
Cov: 20231 -> 20231
Cov: 20231 -> 20231
2647
Cov: 20231 -> 20231
Cov: 20231 -> 20231
2648
Cov: 20231 -> 20251
Cov: 20251 -> 20251
2649
Cov: 20251 -> 20251
Cov: 20251 -> 20251
2650
Cov: 20251 -> 20267
Cov: 20267 -> 20267
2651
Cov: 20267 -> 20268
Cov: 20268 -> 20268
2652
Cov: 20268 -> 20268
Cov: 20268 -> 20268
2653
Cov: 20268 -> 20268
Cov: 20268 -> 20268
2654
Cov: 20268 -> 20268
Cov: 20268 -> 20268
2655
Cov: 20268 -> 20281
Cov: 20281 -> 20281
2656
Cov: 20281 -> 20282
Cov: 20282 -> 20282
2657
Cov: 20282 -> 20282
Cov: 20282 -> 20282
2658
Cov: 20282 -> 20282
Cov: 20282 -> 20282
2659
{"exception": "TypeError", "msg": "index_put_(): argument 'indices' (position 1) must be tuple of Tensors, not Tensor"}
2660
{"exception": "RuntimeError", "msg": "masked_fill_ only supports boolean masks, but got mask with dtype unsigned char"}
2661
Cov: 20282 -> 20286
Cov: 20286 -> 20286
2662
Cov: 20286 -> 20286
Cov: 20286 -> 20286
2663
Cov: 20286 -> 20286
Cov: 20286 -> 20286
2664
Cov: 20286 -> 20286
Cov: 20286 -> 20286
2665
Cov: 20286 -> 20286
Cov: 20286 -> 20286
2666
Cov: 20286 -> 20286
Cov: 20286 -> 20286
2667
Cov: 20286 -> 20286
Cov: 20286 -> 20286
2668
Cov: 20286 -> 20298
Cov: 20298 -> 20298
2669
Cov: 20298 -> 20298
Cov: 20298 -> 20298
2670
Cov: 20298 -> 20298
Cov: 20298 -> 20298
2671
Cov: 20298 -> 20298
Cov: 20298 -> 20298
2672
Cov: 20298 -> 20298
Cov: 20298 -> 20298
2673
Cov: 20298 -> 20298
Cov: 20298 -> 20298
2674
Cov: 20298 -> 20298
Cov: 20298 -> 20298
2675
Cov: 20298 -> 20378
Cov: 20378 -> 20378
2676
Cov: 20378 -> 20378
Cov: 20378 -> 20378
2677
Cov: 20378 -> 20378
Cov: 20378 -> 20378
2678
Cov: 20378 -> 20378
Cov: 20378 -> 20378
2679
Cov: 20378 -> 20378
Cov: 20378 -> 20378
2680
Cov: 20378 -> 20378
Cov: 20378 -> 20378
2681
{"exception": "NotImplementedError", "msg": "Could not run 'aten::int_repr' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::int_repr' is only available for these backends: [QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nQuantizedCPU: registered at aten/src/ATen/RegisterQuantizedCPU.cpp:944 [kernel]\nQuantizedCUDA: registered at aten/src/ATen/RegisterQuantizedCUDA.cpp:459 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:153 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:498 [backend fallback]\nFunctionalize: registered at ../aten/src/ATen/FunctionalizeFallbackKernel.cpp:290 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:86 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradMTIA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_0.cpp:16725 [kernel]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:382 [backend fallback]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:249 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:710 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:203 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:161 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:494 [backend fallback]\nPreDispatch: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:165 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:157 [backend fallback]\n"}
2682
Cov: 20378 -> 20379
Cov: 20379 -> 20379
2683
Cov: 20379 -> 20379
Cov: 20379 -> 20379
2684
{"exception": "NotImplementedError", "msg": "Could not run 'aten::q_per_channel_zero_points' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::q_per_channel_zero_points' is only available for these backends: [QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nQuantizedCPU: registered at aten/src/ATen/RegisterQuantizedCPU.cpp:944 [kernel]\nQuantizedCUDA: registered at aten/src/ATen/RegisterQuantizedCUDA.cpp:459 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:153 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:498 [backend fallback]\nFunctionalize: registered at ../aten/src/ATen/FunctionalizeFallbackKernel.cpp:290 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:86 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradMTIA: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_1.cpp:16002 [kernel]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:382 [backend fallback]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:249 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:710 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:203 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:161 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:494 [backend fallback]\nPreDispatch: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:165 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:157 [backend fallback]\n"}
2685
Cov: 20379 -> 20379
Cov: 20379 -> 20379
2686
{"exception": "RuntimeError", "msg": "element 0 of tensors does not require grad and does not have a grad_fn"}
2687
Cov: 20379 -> 20380
Cov: 20380 -> 20380
2688
Cov: 20380 -> 20380
Cov: 20380 -> 20380
2689
Cov: 20380 -> 20380
Cov: 20380 -> 20380
2690
Cov: 20380 -> 20380
Cov: 20380 -> 20380
2691
Cov: 20380 -> 20380
Cov: 20380 -> 20380
2692
Cov: 20380 -> 20381
Cov: 20381 -> 20381
2693
Cov: 20381 -> 20395
Cov: 20395 -> 20395
2694
Cov: 20395 -> 20395
Cov: 20395 -> 20395
2695
Cov: 20395 -> 20395
Cov: 20395 -> 20395
2696
Cov: 20395 -> 20395
Cov: 20395 -> 20395
2697
Cov: 20395 -> 20395
Cov: 20395 -> 20395
2698
Cov: 20395 -> 20395
Cov: 20395 -> 20395
2699
Cov: 20395 -> 20396
Cov: 20396 -> 20396
2700
Cov: 20396 -> 20396
Cov: 20396 -> 20396
2701
Cov: 20396 -> 20396
Cov: 20396 -> 20396
2702
Cov: 20396 -> 20410
Cov: 20410 -> 20410
2703
Cov: 20410 -> 20427
Cov: 20427 -> 20427
2704
Cov: 20427 -> 20427
Cov: 20427 -> 20427
2705
Cov: 20427 -> 20432
Cov: 20432 -> 20432
2706
Cov: 20432 -> 20437
Cov: 20437 -> 20437
2707
Cov: 20437 -> 20438
Cov: 20438 -> 20438
2708
Cov: 20438 -> 20438
Cov: 20438 -> 20438
2709
Cov: 20438 -> 20438
Cov: 20438 -> 20438
2710
Cov: 20438 -> 20438
Cov: 20438 -> 20438
2711
Cov: 20438 -> 20438
Cov: 20438 -> 20438
2712
Cov: 20438 -> 20438
Cov: 20438 -> 20438
2713
Cov: 20438 -> 20438
Cov: 20438 -> 20438
2714
Cov: 20438 -> 20438
Cov: 20438 -> 20438
2715
Cov: 20438 -> 20438
Cov: 20438 -> 20438
2716
{"exception": "RuntimeError", "msg": "This function was deprecated since version 1.9 and is now removed. `torch.lstsq` is deprecated in favor of `torch.linalg.lstsq`.\n`torch.linalg.lstsq` has reversed arguments and does not return the QR decomposition in the returned tuple (although it returns other information about the problem).\n\nTo get the QR decomposition consider using `torch.linalg.qr`.\n\nThe returned solution in `torch.lstsq` stored the residuals of the solution in the last m - n columns of the returned value whenever m > n. In torch.linalg.lstsq, the residuals are in the field 'residuals' of the returned named tuple.\n\nThe unpacking of the solution, as in\nX, _ = torch.lstsq(B, A).solution[:A.size(1)]\nshould be replaced with:\nX = torch.linalg.lstsq(A, B).solution"}
2717
Cov: 20438 -> 20441
Cov: 20441 -> 20441
2718
Cov: 20441 -> 20441
Cov: 20441 -> 20441
2719
Cov: 20441 -> 20443
Cov: 20443 -> 20443
2720
Cov: 20443 -> 20443
Cov: 20443 -> 20443
2721
Cov: 20443 -> 20443
Cov: 20443 -> 20443
2722
Cov: 20443 -> 20443
Cov: 20443 -> 20443
2723
Cov: 20443 -> 20444
Cov: 20444 -> 20444
2724
Cov: 20444 -> 20444
Cov: 20444 -> 20444
2725
Cov: 20444 -> 20445
Cov: 20445 -> 20445
2726
Cov: 20445 -> 20445
Cov: 20445 -> 20445
2727
Cov: 20445 -> 20445
Cov: 20445 -> 20445
2728
Cov: 20445 -> 20455
Cov: 20455 -> 20455
2729
Cov: 20455 -> 20456
Cov: 20456 -> 20456
2730
Cov: 20456 -> 20456
Cov: 20456 -> 20456
2731
Cov: 20456 -> 20456
Cov: 20456 -> 20456
2732
Cov: 20456 -> 20459
Cov: 20459 -> 20459
2733
Cov: 20459 -> 20459
Cov: 20459 -> 20459
2734
Cov: 20459 -> 20459
Cov: 20459 -> 20459
2735
Cov: 20459 -> 20459
Cov: 20459 -> 20459
2736
Cov: 20459 -> 20459
Cov: 20459 -> 20459
2737
Cov: 20459 -> 20459
Cov: 20459 -> 20459
2738
Cov: 20459 -> 20461
Cov: 20461 -> 20461
2739
Cov: 20461 -> 20461
Cov: 20461 -> 20461
2740
Cov: 20461 -> 20461
Cov: 20461 -> 20461
2741
Cov: 20461 -> 20461
Cov: 20461 -> 20461
2742
Cov: 20461 -> 20461
Cov: 20461 -> 20461
2743
Cov: 20461 -> 20461
Cov: 20461 -> 20461
2744
Cov: 20461 -> 20461
Cov: 20461 -> 20461
2745
Cov: 20461 -> 20462
Cov: 20462 -> 20462
2746
Cov: 20462 -> 20462
Cov: 20462 -> 20462
2747
Cov: 20462 -> 20462
Cov: 20462 -> 20462
2748
Cov: 20462 -> 20462
Cov: 20462 -> 20462
2749
Cov: 20462 -> 20462
Cov: 20462 -> 20462
2750
Cov: 20462 -> 20475
Cov: 20475 -> 20475
2751
Cov: 20475 -> 20475
Cov: 20475 -> 20475
2752
Cov: 20475 -> 20475
Cov: 20475 -> 20475
2753
Cov: 20475 -> 20475
Cov: 20475 -> 20475
2754
Cov: 20475 -> 20475
Cov: 20475 -> 20475
2755
Cov: 20475 -> 20475
Cov: 20475 -> 20475
2756
{"exception": "RuntimeError", "msg": "size mismatch, input: [3, 4], v1: [4], v2: [3]"}
2757
Cov: 20475 -> 20475
Cov: 20475 -> 20475
2758
Cov: 20475 -> 20475
Cov: 20475 -> 20475
2759
Cov: 20475 -> 20476
Cov: 20476 -> 20476
2760
Cov: 20476 -> 20476
Cov: 20476 -> 20476
2761
Cov: 20476 -> 20476
Cov: 20476 -> 20476
2762
Cov: 20476 -> 20477
Cov: 20477 -> 20477
2763
{"exception": "RuntimeError", "msg": "Creating a Tensor subclass from a class that does not inherit from Tensor is not possible. Make sure your class inherits from Tensor."}
2764
Cov: 20477 -> 20477
Cov: 20477 -> 20477
2765
Cov: 20477 -> 20477
Cov: 20477 -> 20477
2766
Cov: 20477 -> 20477
Cov: 20477 -> 20477
2767
Cov: 20477 -> 20477
Cov: 20477 -> 20477
2768
Cov: 20477 -> 20478
Cov: 20478 -> 20478
2769
Cov: 20478 -> 20479
Cov: 20479 -> 20479
2770
Cov: 20479 -> 20479
Cov: 20479 -> 20479
2771
Cov: 20479 -> 20479
Cov: 20479 -> 20479
2772
Cov: 20479 -> 20479
Cov: 20479 -> 20479
2773
Cov: 20479 -> 20479
Cov: 20479 -> 20479
2774
Cov: 20479 -> 20479
Cov: 20479 -> 20479
2775
Cov: 20479 -> 20479
Cov: 20479 -> 20479
2776
Cov: 20479 -> 20483
Cov: 20483 -> 20483
2777
Cov: 20483 -> 20483
Cov: 20483 -> 20483
2778
Cov: 20483 -> 20483
Cov: 20483 -> 20483
2779
Cov: 20483 -> 20484
Cov: 20484 -> 20484
2780
Cov: 20484 -> 20485
Cov: 20485 -> 20485
2781
Cov: 20485 -> 20485
Cov: 20485 -> 20485
2782
{"exception": "TypeError", "msg": "histogram() received an invalid combination of arguments - got (int, int, int), but expected one of:\n * (Tensor bins, *, Tensor weight, bool density)\n * (int bins, *, tuple of floats range, Tensor weight, bool density)\n"}
2783
Cov: 20485 -> 20485
Cov: 20485 -> 20485
2784
Cov: 20485 -> 20485
Cov: 20485 -> 20485
2785
Cov: 20485 -> 20486
Cov: 20486 -> 20486
2786
Cov: 20486 -> 20750
Cov: 20750 -> 20750
2787
Cov: 20750 -> 20750
Cov: 20750 -> 20750
2788
Cov: 20750 -> 20751
Cov: 20751 -> 20751
2789
Cov: 20751 -> 20751
Cov: 20751 -> 20751
2790
Cov: 20751 -> 20751
Cov: 20751 -> 20751
2791
Cov: 20751 -> 20759
Cov: 20759 -> 20759
2792
Cov: 20759 -> 20759
Cov: 20759 -> 20759
2793
Cov: 20759 -> 20765
Cov: 20765 -> 20765
2794
Cov: 20765 -> 20765
Cov: 20765 -> 20765
2795
Cov: 20765 -> 20771
Cov: 20771 -> 20771
2796
Cov: 20771 -> 20773
Cov: 20773 -> 20773
2797
Cov: 20773 -> 20774
Cov: 20774 -> 20774
2798
Cov: 20774 -> 20774
Cov: 20774 -> 20774
2799
Cov: 20774 -> 20777
Cov: 20777 -> 20777
2800
Cov: 20777 -> 20795
Cov: 20795 -> 20795
2801
Cov: 20795 -> 20795
Cov: 20795 -> 20795
2802
Cov: 20795 -> 20795
Cov: 20795 -> 20795
2803
Cov: 20795 -> 20795
Cov: 20795 -> 20795
2804
Cov: 20795 -> 20795
Cov: 20795 -> 20795
2805
{"exception": "NotImplementedError", "msg": "Could not run 'aten::sparse_mask' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::sparse_mask' is only available for these backends: [SparseCPU, SparseCUDA, SparseCsrCPU, SparseCsrCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nSparseCPU: registered at aten/src/ATen/RegisterSparseCPU.cpp:1387 [kernel]\nSparseCUDA: registered at aten/src/ATen/RegisterSparseCUDA.cpp:1573 [kernel]\nSparseCsrCPU: registered at aten/src/ATen/RegisterSparseCsrCPU.cpp:1135 [kernel]\nSparseCsrCUDA: registered at aten/src/ATen/RegisterSparseCsrCUDA.cpp:1276 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:153 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:498 [backend fallback]\nFunctionalize: registered at ../aten/src/ATen/FunctionalizeFallbackKernel.cpp:290 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:86 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradMTIA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_0.cpp:16725 [kernel]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:382 [backend fallback]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:249 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:710 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:203 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:161 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:494 [backend fallback]\nPreDispatch: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:165 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:157 [backend fallback]\n"}
2806
Cov: 20795 -> 20795
Cov: 20795 -> 20795
2807
Cov: 20795 -> 20795
Cov: 20795 -> 20795
2808
Cov: 20795 -> 20795
Cov: 20795 -> 20795
2809
{"exception": "RuntimeError", "msg": "1D tensors expected, but got 2D and 2D tensors"}
2810
Cov: 20795 -> 20796
Cov: 20796 -> 20796
2811
Cov: 20796 -> 20796
Cov: 20796 -> 20796
2812
Cov: 20796 -> 20796
Cov: 20796 -> 20796
2813
Cov: 20796 -> 20796
Cov: 20796 -> 20796
2814
{"exception": "RuntimeError", "msg": "index_add_(): self (Float) and source (Long) must have the same scalar type"}
2815
Cov: 20796 -> 20796
Cov: 20796 -> 20796
2816
Cov: 20796 -> 20796
Cov: 20796 -> 20796
2817
Cov: 20796 -> 20796
Cov: 20796 -> 20796
2818
Cov: 20796 -> 20796
Cov: 20796 -> 20796
2819
Cov: 20796 -> 20796
Cov: 20796 -> 20796
2820
Cov: 20796 -> 20797
Cov: 20797 -> 20797
2821
Cov: 20797 -> 20797
Cov: 20797 -> 20797
2822
Cov: 20797 -> 20797
Cov: 20797 -> 20797
2823
Cov: 20797 -> 20826
Cov: 20826 -> 20826
2824
Cov: 20826 -> 20827
Cov: 20827 -> 20827
2825
Cov: 20827 -> 20827
Cov: 20827 -> 20827
2826
Cov: 20827 -> 20827
Cov: 20827 -> 20827
2827
Cov: 20827 -> 20827
Cov: 20827 -> 20827
2828
Cov: 20827 -> 20827
Cov: 20827 -> 20827
2829
Cov: 20827 -> 20827
Cov: 20827 -> 20827
2830
Cov: 20827 -> 20827
Cov: 20827 -> 20827
2831
Cov: 20827 -> 20827
Cov: 20827 -> 20827
2832
{"exception": "RuntimeError", "msg": "torch.linalg.householder_product: input.shape[-2] must be greater than or equal to input.shape[-1]"}
2833
Cov: 20827 -> 20827
Cov: 20827 -> 20827
2834
Cov: 20827 -> 20827
Cov: 20827 -> 20827
2835
Cov: 20827 -> 20827
Cov: 20827 -> 20827
2836
Cov: 20827 -> 20827
Cov: 20827 -> 20827
2837
Cov: 20827 -> 20835
Cov: 20835 -> 20835
2838
Cov: 20835 -> 20835
Cov: 20835 -> 20835
2839
Cov: 20835 -> 20838
Cov: 20838 -> 20838
2840
Cov: 20838 -> 20838
Cov: 20838 -> 20838
2841
Cov: 20838 -> 20838
Cov: 20838 -> 20838
2842
Cov: 20838 -> 20838
Cov: 20838 -> 20838
2843
Cov: 20838 -> 20838
Cov: 20838 -> 20838
2844
{"exception": "RuntimeError", "msg": "This function was deprecated since version 1.9 and is now removed. The default behavior has changed from using the upper triangular portion of the matrix by default to using the lower triangular portion.\n\nL, _ = torch.symeig(A, upper=upper) should be replaced with:\nL = torch.linalg.eigvalsh(A, UPLO='U' if upper else 'L')\n\nand\n\nL, V = torch.symeig(A, eigenvectors=True) should be replaced with:\nL, V = torch.linalg.eigh(A, UPLO='U' if upper else 'L')"}
2845
Cov: 20838 -> 20841
Cov: 20841 -> 20841
2846
Cov: 20841 -> 20841
Cov: 20841 -> 20841
2847
Cov: 20841 -> 20843
Cov: 20843 -> 20843
2848
Cov: 20843 -> 20845
Cov: 20845 -> 20845
2849
Cov: 20845 -> 20845
Cov: 20845 -> 20845
2850
Cov: 20845 -> 20845
Cov: 20845 -> 20845
2851
Cov: 20845 -> 20845
Cov: 20845 -> 20845
2852
Cov: 20845 -> 20845
Cov: 20845 -> 20845
2853
{"exception": "RuntimeError", "msg": "\"gcd_cpu\" not implemented for 'Double'"}
2854
{"exception": "RuntimeError", "msg": "element 0 of tensors does not require grad and does not have a grad_fn"}
2855
Cov: 20845 -> 20849
Cov: 20849 -> 20849
2856
Cov: 20849 -> 20849
Cov: 20849 -> 20849
2857
Cov: 20849 -> 20849
Cov: 20849 -> 20849
2858
Cov: 20849 -> 20853
Cov: 20853 -> 20853
2859
Cov: 20853 -> 20853
Cov: 20853 -> 20853
2860
Cov: 20853 -> 20853
Cov: 20853 -> 20853
2861
Cov: 20853 -> 20853
Cov: 20853 -> 20853
2862
Cov: 20853 -> 20853
Cov: 20853 -> 20853
2863
Cov: 20853 -> 20853
Cov: 20853 -> 20853
2864
Cov: 20853 -> 20853
Cov: 20853 -> 20853
2865
Cov: 20853 -> 20857
Cov: 20857 -> 20857
2866
Cov: 20857 -> 20858
Cov: 20858 -> 20858
2867
Cov: 20858 -> 20858
Cov: 20858 -> 20858
2868
Cov: 20858 -> 20877
Cov: 20877 -> 20877
2869
Cov: 20877 -> 20877
Cov: 20877 -> 20877
2870
{"exception": "RuntimeError", "msg": "required rank 4 tensor to use channels_last format"}
2871
Cov: 20877 -> 20895
Cov: 20895 -> 20895
2872
Cov: 20895 -> 20895
Cov: 20895 -> 20895
2873
Cov: 20895 -> 20895
Cov: 20895 -> 20895
2874
Cov: 20895 -> 20897
Cov: 20897 -> 20897
2875
{"exception": "RuntimeError", "msg": "vector + matrix @ vector expected, got 2, 2, 1"}
2876
Cov: 20897 -> 20897
Cov: 20897 -> 20897
2877
Cov: 20897 -> 20897
Cov: 20897 -> 20897
2878
Cov: 20897 -> 20913
Cov: 20913 -> 20913
2879
{"exception": "RuntimeError", "msg": "element 0 of tensors does not require grad and does not have a grad_fn"}
2880
Cov: 20913 -> 20915
Cov: 20915 -> 20915
2881
Cov: 20915 -> 20962
Cov: 20962 -> 20962
2882
Cov: 20962 -> 20962
Cov: 20962 -> 20962
2883
Cov: 20962 -> 20962
Cov: 20962 -> 20962
2884
Cov: 20962 -> 20962
Cov: 20962 -> 20962
2885
Cov: 20962 -> 20972
Cov: 20972 -> 20972
2886
Cov: 20972 -> 20972
Cov: 20972 -> 20972
2887
Cov: 20972 -> 20972
Cov: 20972 -> 20972
2888
Cov: 20972 -> 20972
Cov: 20972 -> 20972
2889
Cov: 20972 -> 20972
Cov: 20972 -> 20972
2890
Cov: 20972 -> 20972
Cov: 20972 -> 20972
2891
Cov: 20972 -> 20972
Cov: 20972 -> 20972
2892
Cov: 20972 -> 20972
Cov: 20972 -> 20972
2893
Cov: 20972 -> 20972
Cov: 20972 -> 20972
2894
Cov: 20972 -> 20972
Cov: 20972 -> 20972
2895
Cov: 20972 -> 20972
Cov: 20972 -> 20972
2896
Cov: 20972 -> 20972
Cov: 20972 -> 20972
2897
Cov: 20972 -> 20972
Cov: 20972 -> 20972
2898
Cov: 20972 -> 20972
Cov: 20972 -> 20972
2899
Cov: 20972 -> 20972
Cov: 20972 -> 20972
2900
Cov: 20972 -> 20973
Cov: 20973 -> 20973
2901
Cov: 20973 -> 20973
Cov: 20973 -> 20973
2902
Cov: 20973 -> 20978
Cov: 20978 -> 20978
2903
Cov: 20978 -> 20982
Cov: 20982 -> 20982
2904
Cov: 20982 -> 20982
Cov: 20982 -> 20982
2905
Cov: 20982 -> 20992
Cov: 20992 -> 20992
2906
Cov: 20992 -> 20998
Cov: 20998 -> 20998
2907
Cov: 20998 -> 20998
Cov: 20998 -> 20998
2908
Cov: 20998 -> 20998
Cov: 20998 -> 20998
2909
Cov: 20998 -> 20998
Cov: 20998 -> 20998
2910
{"exception": "RuntimeError", "msg": "values expected sparse tensor layout but got Strided"}
2911
Cov: 20998 -> 20998
Cov: 20998 -> 20998
2912
Cov: 20998 -> 20998
Cov: 20998 -> 20998
2913
Cov: 20998 -> 20998
Cov: 20998 -> 20998
2914
Cov: 20998 -> 20998
Cov: 20998 -> 20998
2915
Cov: 20998 -> 20998
Cov: 20998 -> 20998
2916
Cov: 20998 -> 20998
Cov: 20998 -> 20998
2917
{"exception": "RuntimeError", "msg": "element 0 of tensors does not require grad and does not have a grad_fn"}
2918
Cov: 20998 -> 21002
Cov: 21002 -> 21002
2919
Cov: 21002 -> 21003
Cov: 21003 -> 21003
2920
Cov: 21003 -> 21003
Cov: 21003 -> 21003
2921
Cov: 21003 -> 21003
Cov: 21003 -> 21003
2922
Cov: 21003 -> 21004
Cov: 21004 -> 21004
2923
Cov: 21004 -> 21004
Cov: 21004 -> 21004
2924
Cov: 21004 -> 21004
Cov: 21004 -> 21004
2925
Cov: 21004 -> 21004
Cov: 21004 -> 21004
2926
{"exception": "RuntimeError", "msg": "nanmean(): expected input to have floating point or complex dtype but got Long"}
2927
{"exception": "RuntimeError", "msg": "\"bitwise_not_cpu\" not implemented for 'Float'"}
2928
{"exception": "RuntimeError", "msg": "indices expected sparse coordinate tensor layout but got Strided"}
2929
Cov: 21004 -> 21004
Cov: 21004 -> 21004
2930
Cov: 21004 -> 21004
Cov: 21004 -> 21004
2931
Cov: 21004 -> 21004
Cov: 21004 -> 21004
2932
Cov: 21004 -> 21004
Cov: 21004 -> 21004
2933
Cov: 21004 -> 21004
Cov: 21004 -> 21004
2934
{"exception": "TypeError", "msg": "must be real number, not NoneType"}
2935
Cov: 21004 -> 21004
Cov: 21004 -> 21004
2936
Cov: 21004 -> 21004
Cov: 21004 -> 21004
2937
Cov: 21004 -> 21004
Cov: 21004 -> 21004
2938
Cov: 21004 -> 21004
Cov: 21004 -> 21004
2939
Cov: 21004 -> 21004
Cov: 21004 -> 21004
2940
Cov: 21004 -> 21004
Cov: 21004 -> 21004
2941
Cov: 21004 -> 21004
Cov: 21004 -> 21004
2942
Cov: 21004 -> 21004
Cov: 21004 -> 21004
2943
Cov: 21004 -> 21005
Cov: 21005 -> 21005
2944
Cov: 21005 -> 21005
Cov: 21005 -> 21005
2945
Cov: 21005 -> 21005
Cov: 21005 -> 21005
2946
Cov: 21005 -> 21005
Cov: 21005 -> 21005
2947
Cov: 21005 -> 21005
Cov: 21005 -> 21005
2948
Cov: 21005 -> 21005
Cov: 21005 -> 21005
2949
Cov: 21005 -> 21005
Cov: 21005 -> 21005
2950
Cov: 21005 -> 21005
Cov: 21005 -> 21005
2951
Cov: 21005 -> 21005
Cov: 21005 -> 21005
2952
Cov: 21005 -> 21005
Cov: 21005 -> 21005
2953
Cov: 21005 -> 21005
Cov: 21005 -> 21005
2954
Cov: 21005 -> 21005
Cov: 21005 -> 21005
2955
Cov: 21005 -> 21005
Cov: 21005 -> 21005
2956
Cov: 21005 -> 21005
Cov: 21005 -> 21005
2957
Cov: 21005 -> 21005
Cov: 21005 -> 21005
2958
Cov: 21005 -> 21005
Cov: 21005 -> 21005
2959
Cov: 21005 -> 21006
Cov: 21006 -> 21006
2960
Cov: 21006 -> 21006
Cov: 21006 -> 21006
2961
{"exception": "RuntimeError", "msg": "result type Float can't be cast to the desired output type Long"}
2962
Cov: 21006 -> 21006
Cov: 21006 -> 21006
2963
Cov: 21006 -> 21010
Cov: 21010 -> 21010
2964
Cov: 21010 -> 21010
Cov: 21010 -> 21010
2965
Cov: 21010 -> 21010
Cov: 21010 -> 21010
2966
Cov: 21010 -> 21010
Cov: 21010 -> 21010
2967
Cov: 21010 -> 21011
Cov: 21011 -> 21011
2968
{"exception": "RuntimeError", "msg": "put_ does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True)'. You can turn off determinism just for this operation, or you can use the 'warn_only=True' option, if that's acceptable for your application. You can also file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation."}
2969
Cov: 21011 -> 21011
Cov: 21011 -> 21011
2970
Cov: 21011 -> 21011
Cov: 21011 -> 21011
2971
Cov: 21011 -> 21011
Cov: 21011 -> 21011
2972
Cov: 21011 -> 21011
Cov: 21011 -> 21011
2973
Cov: 21011 -> 21011
Cov: 21011 -> 21011
2974
Cov: 21011 -> 21011
Cov: 21011 -> 21011
2975
Cov: 21011 -> 21011
Cov: 21011 -> 21011
2976
Cov: 21011 -> 21011
Cov: 21011 -> 21011
2977
Cov: 21011 -> 21011
Cov: 21011 -> 21011
2978
Cov: 21011 -> 21011
Cov: 21011 -> 21011
2979
Cov: 21011 -> 21011
Cov: 21011 -> 21011
2980
Cov: 21011 -> 21011
Cov: 21011 -> 21011
2981
Cov: 21011 -> 21011
Cov: 21011 -> 21011
2982
Cov: 21011 -> 21012
Cov: 21012 -> 21012
2983
Cov: 21012 -> 21012
Cov: 21012 -> 21012
2984
Cov: 21012 -> 21012
Cov: 21012 -> 21012
2985
Cov: 21012 -> 21012
Cov: 21012 -> 21012
2986
{"exception": "RuntimeError", "msg": "\"lcm_cpu\" not implemented for 'Float'"}
2987
Cov: 21012 -> 21012
Cov: 21012 -> 21012
2988
Cov: 21012 -> 21013
Cov: 21013 -> 21013
2989
Cov: 21013 -> 21013
Cov: 21013 -> 21013
2990
Cov: 21013 -> 21013
Cov: 21013 -> 21013
2991
Cov: 21013 -> 21014
Cov: 21014 -> 21014
2992
Cov: 21014 -> 21014
Cov: 21014 -> 21014
2993
Cov: 21014 -> 21014
Cov: 21014 -> 21014
2994
Cov: 21014 -> 21014
Cov: 21014 -> 21014
2995
Cov: 21014 -> 21014
Cov: 21014 -> 21014
2996
Cov: 21014 -> 21014
Cov: 21014 -> 21014
2997
Cov: 21014 -> 21014
Cov: 21014 -> 21014
2998
Cov: 21014 -> 21015
Cov: 21015 -> 21015
2999
Cov: 21015 -> 21015
Cov: 21015 -> 21015
3000
{"exception": "NotImplementedError", "msg": "Could not run 'aten::q_per_channel_zero_points' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::q_per_channel_zero_points' is only available for these backends: [QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nQuantizedCPU: registered at aten/src/ATen/RegisterQuantizedCPU.cpp:944 [kernel]\nQuantizedCUDA: registered at aten/src/ATen/RegisterQuantizedCUDA.cpp:459 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:153 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:498 [backend fallback]\nFunctionalize: registered at ../aten/src/ATen/FunctionalizeFallbackKernel.cpp:290 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:86 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradMTIA: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_1.cpp:16002 [kernel]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:382 [backend fallback]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:249 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:710 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:203 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:161 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:494 [backend fallback]\nPreDispatch: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:165 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:157 [backend fallback]\n"}
3001
Cov: 21015 -> 21015
Cov: 21015 -> 21015
3002
Cov: 21015 -> 21015
Cov: 21015 -> 21015
3003
Cov: 21015 -> 21015
Cov: 21015 -> 21015
3004
Cov: 21015 -> 21015
Cov: 21015 -> 21015
3005
Cov: 21015 -> 21015
Cov: 21015 -> 21015
3006
Cov: 21015 -> 21015
Cov: 21015 -> 21015
3007
Cov: 21015 -> 21027
Cov: 21027 -> 21027
3008
{"exception": "RuntimeError", "msg": "element 0 of tensors does not require grad and does not have a grad_fn"}
3009
Cov: 21027 -> 21029
Cov: 21029 -> 21029
3010
Cov: 21029 -> 21029
Cov: 21029 -> 21029
3011
Cov: 21029 -> 21029
Cov: 21029 -> 21029
3012
Cov: 21029 -> 21034
Cov: 21034 -> 21034
3013
Cov: 21034 -> 21034
Cov: 21034 -> 21034
3014
Cov: 21034 -> 21037
Cov: 21037 -> 21037
3015
Cov: 21037 -> 21037
Cov: 21037 -> 21037
3016
Cov: 21037 -> 21037
Cov: 21037 -> 21037
3017
Cov: 21037 -> 21037
Cov: 21037 -> 21037
3018
Cov: 21037 -> 21037
Cov: 21037 -> 21037
3019
Cov: 21037 -> 21038
Cov: 21038 -> 21038
3020
Cov: 21038 -> 21039
Cov: 21039 -> 21039
3021
Cov: 21039 -> 21039
Cov: 21039 -> 21039
3022
Cov: 21039 -> 21039
Cov: 21039 -> 21039
3023
{"exception": "RuntimeError", "msg": "expand(torch.FloatTensor{[10, 3, 4]}, size=[3, 5]): the number of sizes provided (2) must be greater or equal to the number of dimensions in the tensor (3)"}
3024
Cov: 21039 -> 21039
Cov: 21039 -> 21039
3025
{"exception": "RuntimeError", "msg": "\"hypot_cpu\" not implemented for 'Long'"}
3026
Cov: 21039 -> 21039
Cov: 21039 -> 21039
3027
Cov: 21039 -> 21040
Cov: 21040 -> 21040
3028
Cov: 21040 -> 21040
Cov: 21040 -> 21040
3029
Cov: 21040 -> 21040
Cov: 21040 -> 21040
3030
{"exception": "RuntimeError", "msg": "the base given to float_power_ has dtype Long but the operation's result requires dtype Double"}
3031
Cov: 21040 -> 21040
Cov: 21040 -> 21040
3032
Cov: 21040 -> 21040
Cov: 21040 -> 21040
3033
Cov: 21040 -> 21040
Cov: 21040 -> 21040
3034
Cov: 21040 -> 21040
Cov: 21040 -> 21040
3035
Cov: 21040 -> 21040
Cov: 21040 -> 21040
3036
Cov: 21040 -> 21040
Cov: 21040 -> 21040
3037
Cov: 21040 -> 21040
Cov: 21040 -> 21040
3038
Cov: 21040 -> 21040
Cov: 21040 -> 21040
3039
Cov: 21040 -> 21040
Cov: 21040 -> 21040
3040
Cov: 21040 -> 21040
Cov: 21040 -> 21040
3041
Cov: 21040 -> 21040
Cov: 21040 -> 21040
3042
Cov: 21040 -> 21040
Cov: 21040 -> 21040
3043
Cov: 21040 -> 21040
Cov: 21040 -> 21040
3044
Cov: 21040 -> 21040
Cov: 21040 -> 21040
3045
Cov: 21040 -> 21040
Cov: 21040 -> 21040
3046
Cov: 21040 -> 21040
Cov: 21040 -> 21040
3047
Cov: 21040 -> 21040
Cov: 21040 -> 21040
3048
Cov: 21040 -> 21040
Cov: 21040 -> 21040
3049
Cov: 21040 -> 21045
Cov: 21045 -> 21045
3050
Cov: 21045 -> 21045
Cov: 21045 -> 21045
3051
Cov: 21045 -> 21045
Cov: 21045 -> 21045
3052
Cov: 21045 -> 21053
Cov: 21053 -> 21053
3053
Cov: 21053 -> 21053
Cov: 21053 -> 21053
3054
Cov: 21053 -> 21053
Cov: 21053 -> 21053
3055
Cov: 21053 -> 21053
Cov: 21053 -> 21053
3056
Cov: 21053 -> 21057
Cov: 21057 -> 21057
3057
Cov: 21057 -> 21059
Cov: 21059 -> 21059
3058
Cov: 21059 -> 21059
Cov: 21059 -> 21059
3059
Cov: 21059 -> 21060
Cov: 21060 -> 21060
3060
Cov: 21060 -> 21206
Cov: 21206 -> 21206
3061
Cov: 21206 -> 21206
Cov: 21206 -> 21206
3062
Cov: 21206 -> 21206
Cov: 21206 -> 21206
3063
Cov: 21206 -> 21206
Cov: 21206 -> 21206
3064
Cov: 21206 -> 21211
Cov: 21211 -> 21211
3065
Cov: 21211 -> 21211
Cov: 21211 -> 21211
3066
Cov: 21211 -> 21211
Cov: 21211 -> 21211
3067
Cov: 21211 -> 21211
Cov: 21211 -> 21211
3068
{"exception": "RuntimeError", "msg": "outer: Expected 1-D argument self, but got 2-D"}
3069
Cov: 21211 -> 21211
Cov: 21211 -> 21211
3070
Cov: 21211 -> 21211
Cov: 21211 -> 21211
3071
Cov: 21211 -> 21214
Cov: 21214 -> 21214
3072
Cov: 21214 -> 21214
Cov: 21214 -> 21214
3073
Cov: 21214 -> 21214
Cov: 21214 -> 21214
3074
Cov: 21214 -> 21214
Cov: 21214 -> 21214
3075
Cov: 21214 -> 21218
Cov: 21218 -> 21218
3076
Cov: 21218 -> 21218
Cov: 21218 -> 21218
3077
Cov: 21218 -> 21218
Cov: 21218 -> 21218
3078
Cov: 21218 -> 21220
Cov: 21220 -> 21220
3079
{"exception": "TypeError", "msg": "map_(): argument 'other' (position 1) must be Tensor, not function"}
3080
Cov: 21220 -> 21220
Cov: 21220 -> 21220
3081
Cov: 21220 -> 21220
Cov: 21220 -> 21220
3082
Cov: 21220 -> 21220
Cov: 21220 -> 21220
3083
Cov: 21220 -> 21220
Cov: 21220 -> 21220
3084
Cov: 21220 -> 21220
Cov: 21220 -> 21220
3085
Cov: 21220 -> 21220
Cov: 21220 -> 21220
3086
Cov: 21220 -> 21238
Cov: 21238 -> 21238
3087
Cov: 21238 -> 21238
Cov: 21238 -> 21238
3088
Cov: 21238 -> 21238
Cov: 21238 -> 21238
3089
Cov: 21238 -> 21238
Cov: 21238 -> 21238
3090
Cov: 21238 -> 21238
Cov: 21238 -> 21238
3091
Cov: 21238 -> 21239
Cov: 21239 -> 21239
3092
Cov: 21239 -> 21239
Cov: 21239 -> 21239
3093
Cov: 21239 -> 21240
Cov: 21240 -> 21240
3094
Cov: 21240 -> 21240
Cov: 21240 -> 21240
3095
Cov: 21240 -> 21241
Cov: 21241 -> 21241
3096
Cov: 21241 -> 21241
Cov: 21241 -> 21241
3097
Cov: 21241 -> 21241
Cov: 21241 -> 21241
3098
Cov: 21241 -> 21241
Cov: 21241 -> 21241
3099
Cov: 21241 -> 21241
Cov: 21241 -> 21241
3100
Cov: 21241 -> 21241
Cov: 21241 -> 21241
3101
Cov: 21241 -> 21241
Cov: 21241 -> 21241
3102
{"exception": "RuntimeError", "msg": "Index tensor must have the same number of dimensions as self tensor"}
3103
Cov: 21241 -> 21241
Cov: 21241 -> 21241
3104
{"exception": "RuntimeError", "msg": "the base given to float_power_ has dtype Float but the operation's result requires dtype Double"}
3105
Cov: 21241 -> 21241
Cov: 21241 -> 21241
3106
Cov: 21241 -> 21242
Cov: 21242 -> 21242
3107
Cov: 21242 -> 21242
Cov: 21242 -> 21242
3108
Cov: 21242 -> 21242
Cov: 21242 -> 21242
3109
{"exception": "TypeError", "msg": "angle() takes no arguments (1 given)"}
3110
Cov: 21242 -> 21242
Cov: 21242 -> 21242
3111
Cov: 21242 -> 21243
Cov: 21243 -> 21243
3112
Cov: 21243 -> 21243
Cov: 21243 -> 21243
3113
Cov: 21243 -> 21243
Cov: 21243 -> 21243
3114
Cov: 21243 -> 21243
Cov: 21243 -> 21243
3115
Cov: 21243 -> 21250
Cov: 21250 -> 21250
3116
Cov: 21250 -> 21250
Cov: 21250 -> 21250
3117
Cov: 21250 -> 21250
Cov: 21250 -> 21250
3118
Cov: 21250 -> 21250
Cov: 21250 -> 21250
3119
Cov: 21250 -> 21250
Cov: 21250 -> 21250
3120
Cov: 21250 -> 21250
Cov: 21250 -> 21250
3121
Cov: 21250 -> 21250
Cov: 21250 -> 21250
3122
Cov: 21250 -> 21250
Cov: 21250 -> 21250
3123
Cov: 21250 -> 21250
Cov: 21250 -> 21250
3124
Cov: 21250 -> 21250
Cov: 21250 -> 21250
3125
Cov: 21250 -> 21250
Cov: 21250 -> 21250
3126
Cov: 21250 -> 21250
Cov: 21250 -> 21250
3127
Cov: 21250 -> 21251
Cov: 21251 -> 21251
3128
Cov: 21251 -> 21251
Cov: 21251 -> 21251
3129
Cov: 21251 -> 21252
Cov: 21252 -> 21252
3130
Cov: 21252 -> 21252
Cov: 21252 -> 21252
3131
Cov: 21252 -> 21252
Cov: 21252 -> 21252
3132
Cov: 21252 -> 21252
Cov: 21252 -> 21252
3133
Cov: 21252 -> 21252
Cov: 21252 -> 21252
3134
Cov: 21252 -> 21252
Cov: 21252 -> 21252
3135
Cov: 21252 -> 21252
Cov: 21252 -> 21252
3136
Cov: 21252 -> 21258
Cov: 21258 -> 21258
3137
Cov: 21258 -> 21258
Cov: 21258 -> 21258
3138
Cov: 21258 -> 21258
Cov: 21258 -> 21258
3139
Cov: 21258 -> 21258
Cov: 21258 -> 21258
3140
Cov: 21258 -> 21258
Cov: 21258 -> 21258
3141
Cov: 21258 -> 21258
Cov: 21258 -> 21258
3142
Cov: 21258 -> 21263
Cov: 21263 -> 21263
3143
Cov: 21263 -> 21278
Cov: 21278 -> 21278
3144
Cov: 21278 -> 21282
Cov: 21282 -> 21282
3145
Cov: 21282 -> 21283
Cov: 21283 -> 21283
3146
Cov: 21283 -> 21283
Cov: 21283 -> 21283
3147
Cov: 21283 -> 21283
Cov: 21283 -> 21283
3148
Cov: 21283 -> 21283
Cov: 21283 -> 21283
3149
Cov: 21283 -> 21283
Cov: 21283 -> 21283
3150
Cov: 21283 -> 21297
Cov: 21297 -> 21297
3151
Cov: 21297 -> 21297
Cov: 21297 -> 21297
3152
{"exception": "RuntimeError", "msg": "result type Float can't be cast to the desired output type Long"}
3153
Cov: 21297 -> 21297
Cov: 21297 -> 21297
3154
Cov: 21297 -> 21297
Cov: 21297 -> 21297
3155
Cov: 21297 -> 21305
Cov: 21305 -> 21305
3156
Cov: 21305 -> 21305
Cov: 21305 -> 21305
3157
Cov: 21305 -> 21306
Cov: 21306 -> 21306
3158
Cov: 21306 -> 21306
Cov: 21306 -> 21306
3159
Cov: 21306 -> 21306
Cov: 21306 -> 21306
3160
Cov: 21306 -> 21307
Cov: 21307 -> 21307
3161
Cov: 21307 -> 21307
Cov: 21307 -> 21307
3162
Cov: 21307 -> 21307
Cov: 21307 -> 21307
3163
Cov: 21307 -> 21335
Cov: 21335 -> 21335
3164
Cov: 21335 -> 21335
Cov: 21335 -> 21335
3165
Cov: 21335 -> 21335
Cov: 21335 -> 21335
3166
Cov: 21335 -> 21341
Cov: 21341 -> 21341
3167
Cov: 21341 -> 21341
Cov: 21341 -> 21341
3168
Cov: 21341 -> 21341
Cov: 21341 -> 21341
3169
Cov: 21341 -> 21341
Cov: 21341 -> 21341
3170
Cov: 21341 -> 21341
Cov: 21341 -> 21341
3171
Cov: 21341 -> 21344
Cov: 21344 -> 21344
3172
Cov: 21344 -> 21344
Cov: 21344 -> 21344
3173
Cov: 21344 -> 21344
Cov: 21344 -> 21344
3174
{"exception": "_LinAlgError", "msg": "torch.linalg.solve: The solver failed because the input matrix is singular."}
3175
Cov: 21344 -> 21344
Cov: 21344 -> 21344
3176
Cov: 21344 -> 21344
Cov: 21344 -> 21344
3177
Cov: 21344 -> 21344
Cov: 21344 -> 21344
3178
Cov: 21344 -> 21344
Cov: 21344 -> 21344
3179
Cov: 21344 -> 21344
Cov: 21344 -> 21344
3180
Cov: 21344 -> 21349
Cov: 21349 -> 21349
3181
Cov: 21349 -> 21349
Cov: 21349 -> 21349
3182
Cov: 21349 -> 21349
Cov: 21349 -> 21349
3183
Cov: 21349 -> 21354
Cov: 21354 -> 21354
3184
Cov: 21354 -> 21358
Cov: 21358 -> 21358
3185
Cov: 21358 -> 21358
Cov: 21358 -> 21358
3186
Cov: 21358 -> 21358
Cov: 21358 -> 21358
3187
Cov: 21358 -> 21358
Cov: 21358 -> 21358
3188
Cov: 21358 -> 21358
Cov: 21358 -> 21358
3189
Cov: 21358 -> 21358
Cov: 21358 -> 21358
3190
Cov: 21358 -> 21358
Cov: 21358 -> 21358
3191
Cov: 21358 -> 21358
Cov: 21358 -> 21358
3192
Cov: 21358 -> 21402
Cov: 21402 -> 21402
3193
Cov: 21402 -> 21402
Cov: 21402 -> 21402
3194
Cov: 21402 -> 21402
Cov: 21402 -> 21402
3195
Cov: 21402 -> 21408
Cov: 21408 -> 21408
3196
Cov: 21408 -> 21409
Cov: 21409 -> 21409
3197
Cov: 21409 -> 21409
Cov: 21409 -> 21409
3198
Cov: 21409 -> 21409
Cov: 21409 -> 21409
3199
{"exception": "IndexError", "msg": "index_copy_(): Number of indices (3) should be equal to source.size(dim) (4)"}
3200
Cov: 21409 -> 21409
Cov: 21409 -> 21409
3201
Cov: 21409 -> 21409
Cov: 21409 -> 21409
3202
Cov: 21409 -> 21409
Cov: 21409 -> 21409
3203
Cov: 21409 -> 21409
Cov: 21409 -> 21409
3204
Cov: 21409 -> 21409
Cov: 21409 -> 21409
3205
Cov: 21409 -> 21409
Cov: 21409 -> 21409
3206
{"exception": "TypeError", "msg": "arctanh_() takes no keyword arguments"}
3207
{"exception": "RuntimeError", "msg": "torch.dsplit attempted to split along dimension 2, but the size of the dimension 5 is not divisible by the split_size 2!"}
3208
Cov: 21409 -> 21409
Cov: 21409 -> 21409
3209
Cov: 21409 -> 21409
Cov: 21409 -> 21409
3210
Cov: 21409 -> 21409
Cov: 21409 -> 21409
3211
Cov: 21409 -> 21409
Cov: 21409 -> 21409
3212
Cov: 21409 -> 21409
Cov: 21409 -> 21409
3213
Cov: 21409 -> 21409
Cov: 21409 -> 21409
3214
{"exception": "TypeError", "msg": "'getset_descriptor' object is not callable"}
3215
Cov: 21409 -> 21409
Cov: 21409 -> 21409
3216
Cov: 21409 -> 21409
Cov: 21409 -> 21409
3217
Cov: 21409 -> 21409
Cov: 21409 -> 21409
3218
Cov: 21409 -> 21409
Cov: 21409 -> 21409
3219
Cov: 21409 -> 21409
Cov: 21409 -> 21409
3220
Cov: 21409 -> 21409
Cov: 21409 -> 21409
3221
Cov: 21409 -> 21411
Cov: 21411 -> 21411
3222
Cov: 21411 -> 21411
Cov: 21411 -> 21411
3223
Cov: 21411 -> 21411
Cov: 21411 -> 21411
3224
Cov: 21411 -> 21411
Cov: 21411 -> 21411
3225
Cov: 21411 -> 21411
Cov: 21411 -> 21411
3226
Cov: 21411 -> 21418
Cov: 21418 -> 21418
3227
Cov: 21418 -> 21418
Cov: 21418 -> 21418
3228
Cov: 21418 -> 21418
Cov: 21418 -> 21418
3229
Cov: 21418 -> 21418
Cov: 21418 -> 21418
3230
Cov: 21418 -> 21418
Cov: 21418 -> 21418
3231
Cov: 21418 -> 21418
Cov: 21418 -> 21418
3232
{"exception": "RuntimeError", "msg": "torch.ormqr: Expected tau to have one dimension less than input, but got tau.ndim equal to 3 and input.ndim is equal to 3"}
3233
Cov: 21418 -> 21418
Cov: 21418 -> 21418
3234
Cov: 21418 -> 21418
Cov: 21418 -> 21418
3235
Cov: 21418 -> 21418
Cov: 21418 -> 21418
3236
Cov: 21418 -> 21418
Cov: 21418 -> 21418
3237
Cov: 21418 -> 21418
Cov: 21418 -> 21418
3238
Cov: 21418 -> 21418
Cov: 21418 -> 21418
3239
Cov: 21418 -> 21418
Cov: 21418 -> 21418
3240
Cov: 21418 -> 21418
Cov: 21418 -> 21418
3241
Cov: 21418 -> 21418
Cov: 21418 -> 21418
3242
Cov: 21418 -> 21418
Cov: 21418 -> 21418
3243
Cov: 21418 -> 21418
Cov: 21418 -> 21418
3244
Cov: 21418 -> 21418
Cov: 21418 -> 21418
3245
Cov: 21418 -> 21418
Cov: 21418 -> 21418
3246
Cov: 21418 -> 21418
Cov: 21418 -> 21418
3247
Cov: 21418 -> 21420
Cov: 21420 -> 21420
3248
Cov: 21420 -> 21420
Cov: 21420 -> 21420
3249
Cov: 21420 -> 21420
Cov: 21420 -> 21420
3250
Cov: 21420 -> 21421
Cov: 21421 -> 21421
3251
Cov: 21421 -> 21421
Cov: 21421 -> 21421
3252
Cov: 21421 -> 21421
Cov: 21421 -> 21421
3253
Cov: 21421 -> 21422
Cov: 21422 -> 21422
3254
{"exception": "NameError", "msg": "name 'Tensor' is not defined"}
3255
Cov: 21422 -> 21428
Cov: 21428 -> 21428
3256
Cov: 21428 -> 21429
Cov: 21429 -> 21429
3257
{"exception": "RuntimeError", "msg": "Index tensor must have the same number of dimensions as input tensor"}
3258
Cov: 21429 -> 21429
Cov: 21429 -> 21429
3259
Cov: 21429 -> 21429
Cov: 21429 -> 21429
3260
Cov: 21429 -> 21429
Cov: 21429 -> 21429
3261
Cov: 21429 -> 21429
Cov: 21429 -> 21429
3262
Cov: 21429 -> 21429
Cov: 21429 -> 21429
3263
Cov: 21429 -> 21430
Cov: 21430 -> 21430
3264
Cov: 21430 -> 21430
Cov: 21430 -> 21430
3265
Cov: 21430 -> 21430
Cov: 21430 -> 21430
3266
Cov: 21430 -> 21430
Cov: 21430 -> 21430
3267
Cov: 21430 -> 21430
Cov: 21430 -> 21430
3268
Cov: 21430 -> 21430
Cov: 21430 -> 21430
3269
Cov: 21430 -> 21430
Cov: 21430 -> 21430
3270
Cov: 21430 -> 21430
Cov: 21430 -> 21430
3271
Cov: 21430 -> 21431
Cov: 21431 -> 21431
3272
Cov: 21431 -> 21431
Cov: 21431 -> 21431
3273
Cov: 21431 -> 21431
Cov: 21431 -> 21431
3274
Cov: 21431 -> 21431
Cov: 21431 -> 21431
3275
Cov: 21431 -> 21431
Cov: 21431 -> 21431
3276
Cov: 21431 -> 21431
Cov: 21431 -> 21431
3277
Cov: 21431 -> 21431
Cov: 21431 -> 21431
3278
Cov: 21431 -> 21445
Cov: 21445 -> 21445
3279
Cov: 21445 -> 21445
Cov: 21445 -> 21445
3280
Cov: 21445 -> 21445
Cov: 21445 -> 21445
3281
Cov: 21445 -> 21446
Cov: 21446 -> 21446
3282
Cov: 21446 -> 21446
Cov: 21446 -> 21446
3283
{"exception": "RuntimeError", "msg": "required rank 4 tensor to use channels_last format"}
3284
Cov: 21446 -> 21446
Cov: 21446 -> 21446
3285
Cov: 21446 -> 21446
Cov: 21446 -> 21446
3286
Cov: 21446 -> 21446
Cov: 21446 -> 21446
3287
Cov: 21446 -> 21446
Cov: 21446 -> 21446
3288
Cov: 21446 -> 21446
Cov: 21446 -> 21446
3289
{"exception": "RuntimeError", "msg": "The expanded size of the tensor (2) must match the existing size (3) at non-singleton dimension 1.  Target sizes: [3, 2].  Tensor sizes: [2, 3]"}
3290
{"exception": "NotImplementedError", "msg": "Could not run 'aten::sparse_mask' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::sparse_mask' is only available for these backends: [SparseCPU, SparseCUDA, SparseCsrCPU, SparseCsrCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nSparseCPU: registered at aten/src/ATen/RegisterSparseCPU.cpp:1387 [kernel]\nSparseCUDA: registered at aten/src/ATen/RegisterSparseCUDA.cpp:1573 [kernel]\nSparseCsrCPU: registered at aten/src/ATen/RegisterSparseCsrCPU.cpp:1135 [kernel]\nSparseCsrCUDA: registered at aten/src/ATen/RegisterSparseCsrCUDA.cpp:1276 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:153 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:498 [backend fallback]\nFunctionalize: registered at ../aten/src/ATen/FunctionalizeFallbackKernel.cpp:290 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:86 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradMTIA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_0.cpp:16725 [kernel]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:382 [backend fallback]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:249 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:710 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:203 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:161 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:494 [backend fallback]\nPreDispatch: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:165 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:157 [backend fallback]\n"}
3291
Cov: 21446 -> 21446
Cov: 21446 -> 21446
3292
Cov: 21446 -> 21446
Cov: 21446 -> 21446
3293
Cov: 21446 -> 21446
Cov: 21446 -> 21446
3294
Cov: 21446 -> 21446
Cov: 21446 -> 21446
3295
Cov: 21446 -> 21446
Cov: 21446 -> 21446
3296
Cov: 21446 -> 21447
Cov: 21447 -> 21447
3297
Cov: 21447 -> 21447
Cov: 21447 -> 21447
3298
Cov: 21447 -> 21450
Cov: 21450 -> 21450
3299
Cov: 21450 -> 21450
Cov: 21450 -> 21450
3300
Cov: 21450 -> 21450
Cov: 21450 -> 21450
3301
Cov: 21450 -> 21450
Cov: 21450 -> 21450
3302
Cov: 21450 -> 21451
Cov: 21451 -> 21451
3303
Cov: 21451 -> 21451
Cov: 21451 -> 21451
3304
Cov: 21451 -> 21451
Cov: 21451 -> 21451
3305
Cov: 21451 -> 21451
Cov: 21451 -> 21451
3306
Cov: 21451 -> 21451
Cov: 21451 -> 21451
3307
Cov: 21451 -> 21452
Cov: 21452 -> 21452
3308
Cov: 21452 -> 21453
Cov: 21453 -> 21453
3309
Cov: 21453 -> 21453
Cov: 21453 -> 21453
3310
Cov: 21453 -> 21457
Cov: 21457 -> 21457
3311
Cov: 21457 -> 21459
Cov: 21459 -> 21459
3312
Cov: 21459 -> 21459
Cov: 21459 -> 21459
3313
Cov: 21459 -> 21459
Cov: 21459 -> 21459
3314
Cov: 21459 -> 21460
Cov: 21460 -> 21460
3315
Cov: 21460 -> 21460
Cov: 21460 -> 21460
3316
Cov: 21460 -> 21460
Cov: 21460 -> 21460
3317
Cov: 21460 -> 21460
Cov: 21460 -> 21460
3318
Cov: 21460 -> 21460
Cov: 21460 -> 21460
3319
Cov: 21460 -> 21460
Cov: 21460 -> 21460
3320
{"exception": "TypeError", "msg": "'getset_descriptor' object is not callable"}
3321
Cov: 21460 -> 21460
Cov: 21460 -> 21460
3322
Cov: 21460 -> 21460
Cov: 21460 -> 21460
3323
Cov: 21460 -> 21472
Cov: 21472 -> 21472
3324
Cov: 21472 -> 21473
Cov: 21473 -> 21473
3325
Cov: 21473 -> 21479
Cov: 21479 -> 21479
3326
Cov: 21479 -> 21479
Cov: 21479 -> 21479
3327
Cov: 21479 -> 21481
Cov: 21481 -> 21481
3328
Cov: 21481 -> 21481
Cov: 21481 -> 21481
3329
Cov: 21481 -> 21481
Cov: 21481 -> 21481
3330
Cov: 21481 -> 21482
Cov: 21482 -> 21482
3331
Cov: 21482 -> 21482
Cov: 21482 -> 21482
3332
Cov: 21482 -> 21482
Cov: 21482 -> 21482
3333
Cov: 21482 -> 21482
Cov: 21482 -> 21482
3334
Cov: 21482 -> 21483
Cov: 21483 -> 21483
3335
Cov: 21483 -> 21483
Cov: 21483 -> 21483
3336
Cov: 21483 -> 21483
Cov: 21483 -> 21483
3337
Cov: 21483 -> 21483
Cov: 21483 -> 21483
3338
Cov: 21483 -> 21483
Cov: 21483 -> 21483
3339
Cov: 21483 -> 21483
Cov: 21483 -> 21483
3340
Cov: 21483 -> 21483
Cov: 21483 -> 21483
3341
Cov: 21483 -> 21483
Cov: 21483 -> 21483
3342
Cov: 21483 -> 21483
Cov: 21483 -> 21483
3343
Cov: 21483 -> 21484
Cov: 21484 -> 21484
3344
Cov: 21484 -> 21484
Cov: 21484 -> 21484
3345
Cov: 21484 -> 21484
Cov: 21484 -> 21484
3346
Cov: 21484 -> 21484
Cov: 21484 -> 21484
3347
Cov: 21484 -> 21484
Cov: 21484 -> 21484
3348
Cov: 21484 -> 21484
Cov: 21484 -> 21484
3349
Cov: 21484 -> 21485
Cov: 21485 -> 21485
3350
Cov: 21485 -> 21490
Cov: 21490 -> 21490
3351
Cov: 21490 -> 21526
Cov: 21526 -> 21526
3352
Cov: 21526 -> 21526
Cov: 21526 -> 21526
3353
Cov: 21526 -> 21526
Cov: 21526 -> 21526
3354
Cov: 21526 -> 21526
Cov: 21526 -> 21526
3355
Cov: 21526 -> 21557
Cov: 21557 -> 21557
3356
Cov: 21557 -> 21557
Cov: 21557 -> 21557
3357
Cov: 21557 -> 21557
Cov: 21557 -> 21557
3358
Cov: 21557 -> 21559
Cov: 21559 -> 21559
3359
Cov: 21559 -> 21566
Cov: 21566 -> 21566
3360
Cov: 21566 -> 21566
Cov: 21566 -> 21566
3361
Cov: 21566 -> 21566
Cov: 21566 -> 21566
3362
Cov: 21566 -> 21575
Cov: 21575 -> 21575
3363
{"exception": "RuntimeError", "msg": "Error: cannot set number of interop threads after parallel work has started or set_num_interop_threads called"}
3364
Cov: 21575 -> 21580
Cov: 21580 -> 21580
3365
Cov: 21580 -> 21580
Cov: 21580 -> 21580
3366
Cov: 21580 -> 21581
Cov: 21581 -> 21581
3367
Cov: 21581 -> 21582
Cov: 21582 -> 21582
3368
Cov: 21582 -> 21582
Cov: 21582 -> 21582
3369
Cov: 21582 -> 21582
Cov: 21582 -> 21582
3370
Cov: 21582 -> 21582
Cov: 21582 -> 21582
3371
Cov: 21582 -> 21582
Cov: 21582 -> 21582
3372
Cov: 21582 -> 21582
Cov: 21582 -> 21582
3373
Cov: 21582 -> 21582
Cov: 21582 -> 21582
3374
Cov: 21582 -> 21582
Cov: 21582 -> 21582
3375
Cov: 21582 -> 21582
Cov: 21582 -> 21582
3376
Cov: 21582 -> 21583
Cov: 21583 -> 21583
3377
Cov: 21583 -> 21583
Cov: 21583 -> 21583
3378
Cov: 21583 -> 21583
Cov: 21583 -> 21583
3379
Cov: 21583 -> 21583
Cov: 21583 -> 21583
3380
Cov: 21583 -> 21583
Cov: 21583 -> 21583
3381
Cov: 21583 -> 21583
Cov: 21583 -> 21583
3382
Cov: 21583 -> 21583
Cov: 21583 -> 21583
3383
Cov: 21583 -> 21585
Cov: 21585 -> 21585
3384
Cov: 21585 -> 21585
Cov: 21585 -> 21585
3385
Cov: 21585 -> 21585
Cov: 21585 -> 21585
3386
Cov: 21585 -> 21585
Cov: 21585 -> 21585
3387
Cov: 21585 -> 21592
Cov: 21592 -> 21592
3388
Cov: 21592 -> 21592
Cov: 21592 -> 21592
3389
Cov: 21592 -> 21592
Cov: 21592 -> 21592
3390
Cov: 21592 -> 21612
Cov: 21612 -> 21612
3391
Cov: 21612 -> 21612
Cov: 21612 -> 21612
3392
Cov: 21612 -> 21612
Cov: 21612 -> 21612
3393
Cov: 21612 -> 21612
Cov: 21612 -> 21612
3394
Cov: 21612 -> 21612
Cov: 21612 -> 21612
3395
Cov: 21612 -> 21612
Cov: 21612 -> 21612
3396
Cov: 21612 -> 21612
Cov: 21612 -> 21612
3397
Cov: 21612 -> 21612
Cov: 21612 -> 21612
3398
Cov: 21612 -> 21612
Cov: 21612 -> 21612
3399
Cov: 21612 -> 21612
Cov: 21612 -> 21612
3400
Cov: 21612 -> 21612
Cov: 21612 -> 21612
3401
Cov: 21612 -> 21614
Cov: 21614 -> 21614
3402
Cov: 21614 -> 21614
Cov: 21614 -> 21614
3403
Cov: 21614 -> 21614
Cov: 21614 -> 21614
3404
Cov: 21614 -> 21615
Cov: 21615 -> 21615
3405
Cov: 21615 -> 21615
Cov: 21615 -> 21615
3406
Cov: 21615 -> 21615
Cov: 21615 -> 21615
3407
Cov: 21615 -> 21615
Cov: 21615 -> 21615
3408
Cov: 21615 -> 21616
Cov: 21616 -> 21616
3409
Cov: 21616 -> 21616
Cov: 21616 -> 21616
3410
Cov: 21616 -> 21618
Cov: 21618 -> 21618
3411
Cov: 21618 -> 21618
Cov: 21618 -> 21618
3412
Cov: 21618 -> 21618
Cov: 21618 -> 21618
3413
{"exception": "RuntimeError", "msg": "The expanded size of the tensor (5) must match the existing size (4) at non-singleton dimension 2.  Target sizes: [2, 3, 5].  Tensor sizes: [2, 3, 4]"}
3414
Cov: 21618 -> 21618
Cov: 21618 -> 21618
3415
Cov: 21618 -> 21618
Cov: 21618 -> 21618
3416
Cov: 21618 -> 21618
Cov: 21618 -> 21618
3417
{"exception": "RuntimeError", "msg": "result type Double can't be cast to the desired output type Long"}
3418
Cov: 21618 -> 21618
Cov: 21618 -> 21618
3419
Cov: 21618 -> 21618
Cov: 21618 -> 21618
3420
Cov: 21618 -> 21624
Cov: 21624 -> 21624
3421
Cov: 21624 -> 21625
Cov: 21625 -> 21625
3422
Cov: 21625 -> 21630
Cov: 21630 -> 21630
3423
Cov: 21630 -> 21630
Cov: 21630 -> 21630
3424
Cov: 21630 -> 21630
Cov: 21630 -> 21630
3425
Cov: 21630 -> 21633
Cov: 21633 -> 21633
3426
Cov: 21633 -> 21633
Cov: 21633 -> 21633
3427
Cov: 21633 -> 21633
Cov: 21633 -> 21633
3428
Cov: 21633 -> 21634
Cov: 21634 -> 21634
3429
Cov: 21634 -> 21637
Cov: 21637 -> 21637
3430
Cov: 21637 -> 21637
Cov: 21637 -> 21637
3431
Cov: 21637 -> 21637
Cov: 21637 -> 21637
3432
Cov: 21637 -> 21637
Cov: 21637 -> 21637
3433
Cov: 21637 -> 21638
Cov: 21638 -> 21638
3434
Cov: 21638 -> 21638
Cov: 21638 -> 21638
3435
Cov: 21638 -> 21638
Cov: 21638 -> 21638
3436
Cov: 21638 -> 21641
Cov: 21641 -> 21641
3437
Cov: 21641 -> 21653
Cov: 21653 -> 21653
3438
Cov: 21653 -> 21653
Cov: 21653 -> 21653
3439
{"exception": "TypeError", "msg": "histogram() received an invalid combination of arguments - got (numpy.ndarray, int, density=bool, weight=NoneType, range=tuple), but expected one of:\n * (Tensor bins, *, Tensor weight, bool density)\n * (int bins, *, tuple of floats range, Tensor weight, bool density)\n      didn't match because some of the arguments have invalid types: (\u001b[31;1mnumpy.ndarray\u001b[0m, \u001b[31;1mint\u001b[0m, \u001b[31;1mrange=tuple of (int, int)\u001b[0m, \u001b[31;1mweight=NoneType\u001b[0m, \u001b[32;1mdensity=bool\u001b[0m)\n"}
3440
Cov: 21653 -> 21655
Cov: 21655 -> 21655
3441
Cov: 21655 -> 21656
Cov: 21656 -> 21656
3442
Cov: 21656 -> 21660
Cov: 21660 -> 21660
3443
{"exception": "RuntimeError", "msg": "outer: Expected 1-D argument self, but got 2-D"}
3444
Cov: 21660 -> 21660
Cov: 21660 -> 21660
3445
{"exception": "RuntimeError", "msg": "t_() expects a tensor with <= 2 dimensions, but self is 4D"}
3446
Cov: 21660 -> 21660
Cov: 21660 -> 21660
3447
Cov: 21660 -> 21660
Cov: 21660 -> 21660
3448
Cov: 21660 -> 21661
Cov: 21661 -> 21661
3449
Cov: 21661 -> 21661
Cov: 21661 -> 21661
3450
Cov: 21661 -> 21661
Cov: 21661 -> 21661
3451
Cov: 21661 -> 21661
Cov: 21661 -> 21661
3452
Cov: 21661 -> 21661
Cov: 21661 -> 21661
3453
Cov: 21661 -> 21662
Cov: 21662 -> 21662
3454
{"exception": "RuntimeError", "msg": "\"lshift_cpu\" not implemented for 'Float'"}
3455
{"exception": "RuntimeError", "msg": "This function was deprecated since version 1.9 and is now removed. `torch.lstsq` is deprecated in favor of `torch.linalg.lstsq`.\n`torch.linalg.lstsq` has reversed arguments and does not return the QR decomposition in the returned tuple (although it returns other information about the problem).\n\nTo get the QR decomposition consider using `torch.linalg.qr`.\n\nThe returned solution in `torch.lstsq` stored the residuals of the solution in the last m - n columns of the returned value whenever m > n. In torch.linalg.lstsq, the residuals are in the field 'residuals' of the returned named tuple.\n\nThe unpacking of the solution, as in\nX, _ = torch.lstsq(B, A).solution[:A.size(1)]\nshould be replaced with:\nX = torch.linalg.lstsq(A, B).solution"}
3456
{"exception": "ValueError", "msg": "var has negative entry/entries"}
3457
Cov: 21662 -> 21670
Cov: 21670 -> 21670
3458
{"exception": "RuntimeError", "msg": "outer: Expected 1-D argument self, but got 2-D"}
3459
Cov: 21670 -> 21670
Cov: 21670 -> 21670
3460
Cov: 21670 -> 21670
Cov: 21670 -> 21670
3461
Cov: 21670 -> 21670
Cov: 21670 -> 21670
3462
Cov: 21670 -> 21671
Cov: 21671 -> 21671
3463
Cov: 21671 -> 21671
Cov: 21671 -> 21671
3464
Cov: 21671 -> 21671
Cov: 21671 -> 21671
3465
Cov: 21671 -> 21671
Cov: 21671 -> 21671
3466
Cov: 21671 -> 21671
Cov: 21671 -> 21671
3467
Cov: 21671 -> 21671
Cov: 21671 -> 21671
3468
Cov: 21671 -> 21720
Cov: 21720 -> 21720
3469
Cov: 21720 -> 21721
Cov: 21721 -> 21721
3470
Cov: 21721 -> 21721
Cov: 21721 -> 21721
3471
Cov: 21721 -> 21721
Cov: 21721 -> 21721
3472
Cov: 21721 -> 21721
Cov: 21721 -> 21721
3473
Cov: 21721 -> 21722
Cov: 21722 -> 21722
3474
Cov: 21722 -> 21722
Cov: 21722 -> 21722
3475
{"exception": "IndexError", "msg": "index_copy_(): Index should have dimension 1 or 0 (got 2)"}
3476
Cov: 21722 -> 21723
Cov: 21723 -> 21723
3477
Cov: 21723 -> 21724
Cov: 21724 -> 21724
3478
Cov: 21724 -> 21725
Cov: 21725 -> 21725
3479
Cov: 21725 -> 21725
Cov: 21725 -> 21725
3480
Cov: 21725 -> 21726
Cov: 21726 -> 21726
3481
Cov: 21726 -> 21726
Cov: 21726 -> 21726
3482
Cov: 21726 -> 21726
Cov: 21726 -> 21726
3483
Cov: 21726 -> 21727
Cov: 21727 -> 21727
3484
Cov: 21727 -> 21727
Cov: 21727 -> 21727
3485
Cov: 21727 -> 21727
Cov: 21727 -> 21727
3486
Cov: 21727 -> 21728
Cov: 21728 -> 21728
3487
Cov: 21728 -> 21728
Cov: 21728 -> 21728
3488
Cov: 21728 -> 21734
Cov: 21734 -> 21734
3489
Cov: 21734 -> 21735
Cov: 21735 -> 21735
3490
Cov: 21735 -> 21735
Cov: 21735 -> 21735
3491
Cov: 21735 -> 21736
Cov: 21736 -> 21736
3492
Cov: 21736 -> 21736
Cov: 21736 -> 21736
3493
Cov: 21736 -> 21736
Cov: 21736 -> 21736
3494
Cov: 21736 -> 21736
Cov: 21736 -> 21736
3495
Cov: 21736 -> 21736
Cov: 21736 -> 21736
3496
Cov: 21736 -> 21736
Cov: 21736 -> 21736
3497
Cov: 21736 -> 21736
Cov: 21736 -> 21736
3498
Cov: 21736 -> 21736
Cov: 21736 -> 21736
3499
Cov: 21736 -> 21736
Cov: 21736 -> 21736
3500
Cov: 21736 -> 21736
Cov: 21736 -> 21736
3501
Cov: 21736 -> 21736
Cov: 21736 -> 21736
3502
Cov: 21736 -> 21736
Cov: 21736 -> 21736
3503
Cov: 21736 -> 21737
Cov: 21737 -> 21737
3504
Cov: 21737 -> 21737
Cov: 21737 -> 21737
3505
Cov: 21737 -> 21738
Cov: 21738 -> 21738
3506
Cov: 21738 -> 21738
Cov: 21738 -> 21738
3507
Cov: 21738 -> 21738
Cov: 21738 -> 21738
3508
Cov: 21738 -> 21739
Cov: 21739 -> 21739
3509
Cov: 21739 -> 21739
Cov: 21739 -> 21739
3510
Cov: 21739 -> 21739
Cov: 21739 -> 21739
3511
{"exception": "RuntimeError", "msg": "any: You passed a dimname (string) to this op in place of a dimension index but it does not yet support this behavior. Please pass a dimension index to work around this."}
3512
Cov: 21739 -> 21742
Cov: 21742 -> 21742
3513
Cov: 21742 -> 21742
Cov: 21742 -> 21742
3514
Cov: 21742 -> 21742
Cov: 21742 -> 21742
3515
Cov: 21742 -> 21742
Cov: 21742 -> 21742
3516
Cov: 21742 -> 21742
Cov: 21742 -> 21742
3517
Cov: 21742 -> 21744
Cov: 21744 -> 21744
3518
Cov: 21744 -> 21744
Cov: 21744 -> 21744
3519
Cov: 21744 -> 21744
Cov: 21744 -> 21744
3520
Cov: 21744 -> 21745
Cov: 21745 -> 21745
3521
Cov: 21745 -> 21745
Cov: 21745 -> 21745
3522
Cov: 21745 -> 21745
Cov: 21745 -> 21745
3523
Cov: 21745 -> 21746
Cov: 21746 -> 21746
3524
Cov: 21746 -> 21746
Cov: 21746 -> 21746
3525
Cov: 21746 -> 21746
Cov: 21746 -> 21746
3526
Cov: 21746 -> 21746
Cov: 21746 -> 21746
3527
Cov: 21746 -> 21747
Cov: 21747 -> 21747
3528
Cov: 21747 -> 21748
Cov: 21748 -> 21748
3529
Cov: 21748 -> 21748
Cov: 21748 -> 21748
3530
Cov: 21748 -> 21785
Cov: 21785 -> 21785
3531
Cov: 21785 -> 21785
Cov: 21785 -> 21785
3532
Cov: 21785 -> 21785
Cov: 21785 -> 21785
3533
Cov: 21785 -> 21785
Cov: 21785 -> 21785
3534
Cov: 21785 -> 21785
Cov: 21785 -> 21785
3535
Cov: 21785 -> 21785
Cov: 21785 -> 21785
3536
Cov: 21785 -> 21785
Cov: 21785 -> 21785
3537
Cov: 21785 -> 21785
Cov: 21785 -> 21785
3538
Cov: 21785 -> 21786
Cov: 21786 -> 21786
3539
Cov: 21786 -> 21786
Cov: 21786 -> 21786
3540
{"exception": "RuntimeError", "msg": "masked_select: expected BoolTensor for mask"}
3541
Cov: 21786 -> 21786
Cov: 21786 -> 21786
3542
Cov: 21786 -> 21786
Cov: 21786 -> 21786
3543
Cov: 21786 -> 21786
Cov: 21786 -> 21786
3544
Cov: 21786 -> 21786
Cov: 21786 -> 21786
3545
Cov: 21786 -> 21786
Cov: 21786 -> 21786
3546
{"exception": "RuntimeError", "msg": "vector + matrix @ vector expected, got 2, 2, 1"}
3547
Cov: 21786 -> 21803
Cov: 21803 -> 21803
3548
{"exception": "TypeError", "msg": "'getset_descriptor' object is not callable"}
3549
Cov: 21803 -> 21803
Cov: 21803 -> 21803
3550
Cov: 21803 -> 21803
Cov: 21803 -> 21803
3551
Cov: 21803 -> 21803
Cov: 21803 -> 21803
3552
Cov: 21803 -> 21803
Cov: 21803 -> 21803
3553
Cov: 21803 -> 21803
Cov: 21803 -> 21803
3554
Cov: 21803 -> 21804
Cov: 21804 -> 21804
3555
Cov: 21804 -> 21804
Cov: 21804 -> 21804
3556
Cov: 21804 -> 21804
Cov: 21804 -> 21804
3557
Cov: 21804 -> 21804
Cov: 21804 -> 21804
3558
Cov: 21804 -> 21804
Cov: 21804 -> 21804
3559
Cov: 21804 -> 21804
Cov: 21804 -> 21804
3560
Cov: 21804 -> 21804
Cov: 21804 -> 21804
3561
Cov: 21804 -> 21804
Cov: 21804 -> 21804
3562
Cov: 21804 -> 21804
Cov: 21804 -> 21804
3563
Cov: 21804 -> 21804
Cov: 21804 -> 21804
3564
Cov: 21804 -> 21804
Cov: 21804 -> 21804
3565
Cov: 21804 -> 21804
Cov: 21804 -> 21804
3566
Cov: 21804 -> 21804
Cov: 21804 -> 21804
3567
Cov: 21804 -> 21804
Cov: 21804 -> 21804
3568
Cov: 21804 -> 21804
Cov: 21804 -> 21804
3569
Cov: 21804 -> 21804
Cov: 21804 -> 21804
3570
Cov: 21804 -> 21804
Cov: 21804 -> 21804
3571
Cov: 21804 -> 21804
Cov: 21804 -> 21804
3572
Cov: 21804 -> 21804
Cov: 21804 -> 21804
3573
Cov: 21804 -> 21804
Cov: 21804 -> 21804
3574
Cov: 21804 -> 21804
Cov: 21804 -> 21804
3575
Cov: 21804 -> 21804
Cov: 21804 -> 21804
3576
{"exception": "RuntimeError", "msg": "torch.vsplit attempted to split along dimension 0, but the size of the dimension 2 is not divisible by the split_size 3!"}
3577
Cov: 21804 -> 21804
Cov: 21804 -> 21804
3578
Cov: 21804 -> 21804
Cov: 21804 -> 21804
3579
Cov: 21804 -> 21804
Cov: 21804 -> 21804
3580
Cov: 21804 -> 21804
Cov: 21804 -> 21804
3581
Cov: 21804 -> 21804
Cov: 21804 -> 21804
3582
Cov: 21804 -> 21804
Cov: 21804 -> 21804
3583
Cov: 21804 -> 21804
Cov: 21804 -> 21804
3584
{"exception": "TypeError", "msg": "heaviside(): argument 'values' must be Tensor, not int"}
3585
Cov: 21804 -> 21804
Cov: 21804 -> 21804
3586
Cov: 21804 -> 21805
Cov: 21805 -> 21805
3587
Cov: 21805 -> 21806
Cov: 21806 -> 21806
3588
Cov: 21806 -> 21810
Cov: 21810 -> 21810
3589
Cov: 21810 -> 21810
Cov: 21810 -> 21810
3590
{"exception": "NotImplementedError", "msg": "Could not run 'aten::q_per_channel_zero_points' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::q_per_channel_zero_points' is only available for these backends: [QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nQuantizedCPU: registered at aten/src/ATen/RegisterQuantizedCPU.cpp:944 [kernel]\nQuantizedCUDA: registered at aten/src/ATen/RegisterQuantizedCUDA.cpp:459 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:153 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:498 [backend fallback]\nFunctionalize: registered at ../aten/src/ATen/FunctionalizeFallbackKernel.cpp:290 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:86 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradMTIA: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_1.cpp:16002 [kernel]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:382 [backend fallback]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:249 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:710 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:203 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:161 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:494 [backend fallback]\nPreDispatch: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:165 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:157 [backend fallback]\n"}
3591
Cov: 21810 -> 21810
Cov: 21810 -> 21810
3592
Cov: 21810 -> 21810
Cov: 21810 -> 21810
3593
Cov: 21810 -> 21810
Cov: 21810 -> 21810
3594
Cov: 21810 -> 21811
Cov: 21811 -> 21811
3595
Cov: 21811 -> 21811
Cov: 21811 -> 21811
3596
Cov: 21811 -> 21813
Cov: 21813 -> 21813
3597
Cov: 21813 -> 21814
Cov: 21814 -> 21814
3598
Cov: 21814 -> 21814
Cov: 21814 -> 21814
3599
Cov: 21814 -> 21815
Cov: 21815 -> 21815
3600
Cov: 21815 -> 21815
Cov: 21815 -> 21815
3601
{"exception": "TypeError", "msg": "vsplit() received an invalid combination of arguments - got (split_size_or_sections=int, ), but expected one of:\n * (int sections)\n      didn't match because some of the keywords were incorrect: split_size_or_sections\n * (tuple of ints indices)\n      didn't match because some of the keywords were incorrect: split_size_or_sections\n"}
3602
Cov: 21815 -> 21815
Cov: 21815 -> 21815
3603
Cov: 21815 -> 21815
Cov: 21815 -> 21815
3604
Cov: 21815 -> 21815
Cov: 21815 -> 21815
3605
Cov: 21815 -> 21815
Cov: 21815 -> 21815
3606
Cov: 21815 -> 21815
Cov: 21815 -> 21815
3607
Cov: 21815 -> 21826
Cov: 21826 -> 21826
3608
Cov: 21826 -> 21827
Cov: 21827 -> 21827
3609
Cov: 21827 -> 21827
Cov: 21827 -> 21827
3610
{"exception": "TypeError", "msg": "index_put(): argument 'indices' (position 1) must be tuple of Tensors, not Tensor"}
3611
Cov: 21827 -> 21828
Cov: 21828 -> 21828
3612
Cov: 21828 -> 21828
Cov: 21828 -> 21828
3613
Cov: 21828 -> 21828
Cov: 21828 -> 21828
3614
Cov: 21828 -> 21828
Cov: 21828 -> 21828
3615
Cov: 21828 -> 21828
Cov: 21828 -> 21828
3616
Cov: 21828 -> 21829
Cov: 21829 -> 21829
3617
Cov: 21829 -> 21829
Cov: 21829 -> 21829
3618
Cov: 21829 -> 21829
Cov: 21829 -> 21829
3619
Cov: 21829 -> 21829
Cov: 21829 -> 21829
3620
Cov: 21829 -> 21829
Cov: 21829 -> 21829
3621
Cov: 21829 -> 21829
Cov: 21829 -> 21829
3622
Cov: 21829 -> 21830
Cov: 21830 -> 21830
3623
Cov: 21830 -> 21831
Cov: 21831 -> 21831
3624
Cov: 21831 -> 21836
Cov: 21836 -> 21836
3625
Cov: 21836 -> 21836
Cov: 21836 -> 21836
3626
Cov: 21836 -> 21836
Cov: 21836 -> 21836
3627
Cov: 21836 -> 21836
Cov: 21836 -> 21836
3628
Cov: 21836 -> 21836
Cov: 21836 -> 21836
3629
Cov: 21836 -> 21836
Cov: 21836 -> 21836
3630
Cov: 21836 -> 21841
Cov: 21841 -> 21841
3631
Cov: 21841 -> 21841
Cov: 21841 -> 21841
3632
Cov: 21841 -> 21841
Cov: 21841 -> 21841
3633
Cov: 21841 -> 21842
Cov: 21842 -> 21842
3634
{"exception": "RuntimeError", "msg": "The size of tensor a (10) must match the size of tensor b (2) at non-singleton dimension 0"}
3635
Cov: 21842 -> 21842
Cov: 21842 -> 21842
3636
Cov: 21842 -> 21842
Cov: 21842 -> 21842
3637
Cov: 21842 -> 21842
Cov: 21842 -> 21842
3638
Cov: 21842 -> 21842
Cov: 21842 -> 21842
3639
Cov: 21842 -> 21842
Cov: 21842 -> 21842
3640
Cov: 21842 -> 21843
Cov: 21843 -> 21843
3641
{"exception": "TypeError", "msg": "'getset_descriptor' object is not callable"}
3642
Cov: 21843 -> 21843
Cov: 21843 -> 21843
3643
Cov: 21843 -> 21843
Cov: 21843 -> 21843
3644
Cov: 21843 -> 21845
Cov: 21845 -> 21845
3645
Cov: 21845 -> 21845
Cov: 21845 -> 21845
3646
Cov: 21845 -> 22009
Cov: 22009 -> 22009
3647
Cov: 22009 -> 22009
Cov: 22009 -> 22009
3648
Cov: 22009 -> 22009
Cov: 22009 -> 22009
3649
Cov: 22009 -> 22009
Cov: 22009 -> 22009
3650
Cov: 22009 -> 22015
Cov: 22015 -> 22015
3651
Cov: 22015 -> 22015
Cov: 22015 -> 22015
3652
Cov: 22015 -> 22015
Cov: 22015 -> 22015
3653
Cov: 22015 -> 22015
Cov: 22015 -> 22015
3654
Cov: 22015 -> 22015
Cov: 22015 -> 22015
3655
Cov: 22015 -> 22015
Cov: 22015 -> 22015
3656
Cov: 22015 -> 22015
Cov: 22015 -> 22015
3657
Cov: 22015 -> 22048
Cov: 22048 -> 22048
3658
Cov: 22048 -> 22048
Cov: 22048 -> 22048
3659
Cov: 22048 -> 22048
Cov: 22048 -> 22048
3660
Cov: 22048 -> 22048
Cov: 22048 -> 22048
3661
Cov: 22048 -> 22050
Cov: 22050 -> 22050
3662
Cov: 22050 -> 22050
Cov: 22050 -> 22050
3663
Cov: 22050 -> 22050
Cov: 22050 -> 22050
3664
Cov: 22050 -> 22050
Cov: 22050 -> 22050
3665
Cov: 22050 -> 22050
Cov: 22050 -> 22050
3666
Cov: 22050 -> 22050
Cov: 22050 -> 22050
3667
Cov: 22050 -> 22053
Cov: 22053 -> 22053
3668
Cov: 22053 -> 22053
Cov: 22053 -> 22053
3669
Cov: 22053 -> 22054
Cov: 22054 -> 22054
3670
Cov: 22054 -> 22055
Cov: 22055 -> 22055
3671
Cov: 22055 -> 22061
Cov: 22061 -> 22061
3672
Cov: 22061 -> 22061
Cov: 22061 -> 22061
3673
Cov: 22061 -> 22061
Cov: 22061 -> 22061
3674
Cov: 22061 -> 22061
Cov: 22061 -> 22061
3675
Cov: 22061 -> 22061
Cov: 22061 -> 22061
3676
Cov: 22061 -> 22061
Cov: 22061 -> 22061
3677
Cov: 22061 -> 22061
Cov: 22061 -> 22061
3678
Cov: 22061 -> 22061
Cov: 22061 -> 22061
3679
Cov: 22061 -> 22061
Cov: 22061 -> 22061
3680
Cov: 22061 -> 22076
Cov: 22076 -> 22076
3681
Cov: 22076 -> 22077
Cov: 22077 -> 22077
3682
{"exception": "RuntimeError", "msg": "masked_scatter_ only supports boolean masks, but got mask with dtype Byte"}
3683
Cov: 22077 -> 22077
Cov: 22077 -> 22077
3684
Cov: 22077 -> 22077
Cov: 22077 -> 22077
3685
Cov: 22077 -> 22077
Cov: 22077 -> 22077
3686
Cov: 22077 -> 22077
Cov: 22077 -> 22077
3687
Cov: 22077 -> 22077
Cov: 22077 -> 22077
3688
Cov: 22077 -> 22077
Cov: 22077 -> 22077
3689
Cov: 22077 -> 22078
Cov: 22078 -> 22078
3690
Cov: 22078 -> 22078
Cov: 22078 -> 22078
3691
Cov: 22078 -> 22079
Cov: 22079 -> 22079
3692
Cov: 22079 -> 22079
Cov: 22079 -> 22079
3693
Cov: 22079 -> 22079
Cov: 22079 -> 22079
3694
Cov: 22079 -> 22079
Cov: 22079 -> 22079
3695
Cov: 22079 -> 22079
Cov: 22079 -> 22079
3696
Cov: 22079 -> 22079
Cov: 22079 -> 22079
3697
Cov: 22079 -> 22079
Cov: 22079 -> 22079
3698
Cov: 22079 -> 22079
Cov: 22079 -> 22079
3699
Cov: 22079 -> 22079
Cov: 22079 -> 22079
3700
Cov: 22079 -> 22080
Cov: 22080 -> 22080
3701
Cov: 22080 -> 22081
Cov: 22081 -> 22081
3702
Cov: 22081 -> 22081
Cov: 22081 -> 22081
3703
Cov: 22081 -> 22081
Cov: 22081 -> 22081
3704
Cov: 22081 -> 22081
Cov: 22081 -> 22081
3705
Cov: 22081 -> 22081
Cov: 22081 -> 22081
3706
Cov: 22081 -> 22082
Cov: 22082 -> 22082
3707
Cov: 22082 -> 22082
Cov: 22082 -> 22082
3708
Cov: 22082 -> 22083
Cov: 22083 -> 22083
3709
Cov: 22083 -> 22083
Cov: 22083 -> 22083
3710
Cov: 22083 -> 22083
Cov: 22083 -> 22083
3711
Cov: 22083 -> 22083
Cov: 22083 -> 22083
3712
Cov: 22083 -> 22083
Cov: 22083 -> 22083
3713
Cov: 22083 -> 22083
Cov: 22083 -> 22083
3714
Cov: 22083 -> 22084
Cov: 22084 -> 22084
3715
Cov: 22084 -> 22084
Cov: 22084 -> 22084
3716
Cov: 22084 -> 22084
Cov: 22084 -> 22084
3717
Cov: 22084 -> 22084
Cov: 22084 -> 22084
3718
Cov: 22084 -> 22085
Cov: 22085 -> 22085
3719
Cov: 22085 -> 22085
Cov: 22085 -> 22085
3720
Cov: 22085 -> 22085
Cov: 22085 -> 22085
3721
Cov: 22085 -> 22085
Cov: 22085 -> 22085
3722
Cov: 22085 -> 22086
Cov: 22086 -> 22086
3723
Cov: 22086 -> 22086
Cov: 22086 -> 22086
3724
{"exception": "RuntimeError", "msg": "Creating a Tensor subclass from a class that does not inherit from Tensor is not possible. Make sure your class inherits from Tensor."}
3725
Cov: 22086 -> 22092
Cov: 22092 -> 22092
3726
Cov: 22092 -> 22092
Cov: 22092 -> 22092
3727
Cov: 22092 -> 22093
Cov: 22093 -> 22093
3728
{"exception": "TypeError", "msg": "index_add_() missing 1 required positional arguments: \"source\""}
3729
Cov: 22093 -> 22093
Cov: 22093 -> 22093
3730
Cov: 22093 -> 22099
Cov: 22099 -> 22099
3731
Cov: 22099 -> 22100
Cov: 22100 -> 22100
3732
Cov: 22100 -> 22104
Cov: 22104 -> 22104
3733
Cov: 22104 -> 22104
Cov: 22104 -> 22104
3734
Cov: 22104 -> 22105
Cov: 22105 -> 22105
3735
Cov: 22105 -> 22105
Cov: 22105 -> 22105
3736
Cov: 22105 -> 22105
Cov: 22105 -> 22105
3737
Cov: 22105 -> 22105
Cov: 22105 -> 22105
3738
Cov: 22105 -> 22106
Cov: 22106 -> 22106
3739
Cov: 22106 -> 22106
Cov: 22106 -> 22106
3740
Cov: 22106 -> 22106
Cov: 22106 -> 22106
3741
{"exception": "TypeError", "msg": "must be real number, not NoneType"}
3742
Cov: 22106 -> 22110
Cov: 22110 -> 22110
3743
{"exception": "RuntimeError", "msg": "element 0 of tensors does not require grad and does not have a grad_fn"}
3744
Cov: 22110 -> 22113
Cov: 22113 -> 22113
3745
Cov: 22113 -> 22114
Cov: 22114 -> 22114
3746
Cov: 22114 -> 22114
Cov: 22114 -> 22114
3747
Cov: 22114 -> 22114
Cov: 22114 -> 22114
3748
Cov: 22114 -> 22117
Cov: 22117 -> 22117
3749
Cov: 22117 -> 22117
Cov: 22117 -> 22117
3750
Cov: 22117 -> 22119
Cov: 22119 -> 22119
3751
Cov: 22119 -> 22119
Cov: 22119 -> 22119
3752
Cov: 22119 -> 22119
Cov: 22119 -> 22119
3753
Cov: 22119 -> 22119
Cov: 22119 -> 22119
3754
Cov: 22119 -> 22119
Cov: 22119 -> 22119
3755
Cov: 22119 -> 22119
Cov: 22119 -> 22119
3756
Cov: 22119 -> 22120
Cov: 22120 -> 22120
3757
Cov: 22120 -> 22120
Cov: 22120 -> 22120
3758
Cov: 22120 -> 22120
Cov: 22120 -> 22120
3759
{"exception": "TypeError", "msg": "'getset_descriptor' object is not callable"}
3760
Cov: 22120 -> 22126
Cov: 22126 -> 22126
3761
Cov: 22126 -> 22126
Cov: 22126 -> 22126
3762
Cov: 22126 -> 22126
Cov: 22126 -> 22126
3763
Cov: 22126 -> 22126
Cov: 22126 -> 22126
3764
Cov: 22126 -> 22126
Cov: 22126 -> 22126
3765
Cov: 22126 -> 22126
Cov: 22126 -> 22126
3766
Cov: 22126 -> 22127
Cov: 22127 -> 22127
3767
Cov: 22127 -> 22127
Cov: 22127 -> 22127
3768
Cov: 22127 -> 22128
Cov: 22128 -> 22128
3769
{"exception": "RuntimeError", "msg": "element 0 of tensors does not require grad and does not have a grad_fn"}
3770
{"exception": "RuntimeError", "msg": "corrcoef(): expected input to have two or fewer dimensions but got an input with 3 dimensions"}
3771
Cov: 22128 -> 22129
Cov: 22129 -> 22129
3772
{"exception": "RuntimeError", "msg": "element 0 of tensors does not require grad and does not have a grad_fn"}
3773
Cov: 22129 -> 22130
Cov: 22130 -> 22130
3774
Cov: 22130 -> 22130
Cov: 22130 -> 22130
3775
Cov: 22130 -> 22130
Cov: 22130 -> 22130
3776
Cov: 22130 -> 22130
Cov: 22130 -> 22130
3777
Cov: 22130 -> 22130
Cov: 22130 -> 22130
3778
Cov: 22130 -> 22130
Cov: 22130 -> 22130
3779
Cov: 22130 -> 22130
Cov: 22130 -> 22130
3780
Cov: 22130 -> 22130
Cov: 22130 -> 22130
3781
Cov: 22130 -> 22130
Cov: 22130 -> 22130
3782
Cov: 22130 -> 22130
Cov: 22130 -> 22130
3783
Cov: 22130 -> 22130
Cov: 22130 -> 22130
3784
Cov: 22130 -> 22130
Cov: 22130 -> 22130
3785
Cov: 22130 -> 22130
Cov: 22130 -> 22130
3786
Cov: 22130 -> 22130
Cov: 22130 -> 22130
3787
Cov: 22130 -> 22130
Cov: 22130 -> 22130
3788
Cov: 22130 -> 22130
Cov: 22130 -> 22130
3789
Cov: 22130 -> 22130
Cov: 22130 -> 22130
3790
Cov: 22130 -> 22130
Cov: 22130 -> 22130
3791
Cov: 22130 -> 22308
Cov: 22308 -> 22308
3792
Cov: 22308 -> 22309
Cov: 22309 -> 22309
3793
Cov: 22309 -> 22309
Cov: 22309 -> 22309
3794
Cov: 22309 -> 22309
Cov: 22309 -> 22309
3795
Cov: 22309 -> 22310
Cov: 22310 -> 22310
3796
Cov: 22310 -> 22310
Cov: 22310 -> 22310
3797
Cov: 22310 -> 22311
Cov: 22311 -> 22311
3798
Cov: 22311 -> 22311
Cov: 22311 -> 22311
3799
Cov: 22311 -> 22311
Cov: 22311 -> 22311
3800
{"exception": "RuntimeError", "msg": "element 0 of tensors does not require grad and does not have a grad_fn"}
3801
Cov: 22311 -> 22312
Cov: 22312 -> 22312
3802
Cov: 22312 -> 22312
Cov: 22312 -> 22312
3803
Cov: 22312 -> 22330
Cov: 22330 -> 22330
3804
Cov: 22330 -> 22330
Cov: 22330 -> 22330
3805
Cov: 22330 -> 22330
Cov: 22330 -> 22330
3806
Cov: 22330 -> 22331
Cov: 22331 -> 22331
3807
Cov: 22331 -> 22339
Cov: 22339 -> 22339
3808
Cov: 22339 -> 22339
Cov: 22339 -> 22339
3809
Cov: 22339 -> 22445
Cov: 22445 -> 22445
3810
Cov: 22445 -> 22445
Cov: 22445 -> 22445
3811
Cov: 22445 -> 22446
Cov: 22446 -> 22446
3812
Cov: 22446 -> 22446
Cov: 22446 -> 22446
3813
Cov: 22446 -> 22446
Cov: 22446 -> 22446
3814
Cov: 22446 -> 22447
Cov: 22447 -> 22447
3815
{"exception": "RuntimeError", "msg": "Index tensor must have the same number of dimensions as self tensor"}
3816
Cov: 22447 -> 22447
Cov: 22447 -> 22447
3817
Cov: 22447 -> 22447
Cov: 22447 -> 22447
3818
Cov: 22447 -> 22447
Cov: 22447 -> 22447
3819
Cov: 22447 -> 22447
Cov: 22447 -> 22447
3820
{"exception": "RuntimeError", "msg": "This function was deprecated since version 1.9 and is now removed. `torch.linalg.eig` returns complex tensors of dtype `cfloat` or `cdouble` rather than real tensors mimicking complex tensors.\n\nL, _ = torch.eig(A) should be replaced with:\nL_complex = torch.linalg.eigvals(A)\n\nand\n\nL, V = torch.eig(A, eigenvectors=True) should be replaced with:\nL_complex, V_complex = torch.linalg.eig(A)"}
3821
Cov: 22447 -> 22454
Cov: 22454 -> 22454
3822
Cov: 22454 -> 22454
Cov: 22454 -> 22454
3823
Cov: 22454 -> 22454
Cov: 22454 -> 22454
3824
Cov: 22454 -> 22454
Cov: 22454 -> 22454
3825
{"exception": "RuntimeError", "msg": "element 0 of tensors does not require grad and does not have a grad_fn"}
3826
Cov: 22454 -> 22455
Cov: 22455 -> 22455
3827
{"exception": "RuntimeError", "msg": "a Tensor with 150528 elements cannot be converted to Scalar"}
3828
Cov: 22455 -> 22455
Cov: 22455 -> 22455
3829
Cov: 22455 -> 22455
Cov: 22455 -> 22455
3830
Cov: 22455 -> 22455
Cov: 22455 -> 22455
3831
{"exception": "NameError", "msg": "name 'random' is not defined"}
3832
{"exception": "TypeError", "msg": "'getset_descriptor' object is not callable"}
3833
Cov: 22455 -> 22455
Cov: 22455 -> 22455
3834
Cov: 22455 -> 22455
Cov: 22455 -> 22455
3835
Cov: 22455 -> 22455
Cov: 22455 -> 22455
3836
Cov: 22455 -> 22455
Cov: 22455 -> 22455
3837
Cov: 22455 -> 22455
Cov: 22455 -> 22455
3838
Cov: 22455 -> 22455
Cov: 22455 -> 22455
3839
{"exception": "RuntimeError", "msg": "vector + matrix @ vector expected, got 2, 2, 1"}
3840
Cov: 22455 -> 22455
Cov: 22455 -> 22455
3841
Cov: 22455 -> 22456
Cov: 22456 -> 22456
3842
Cov: 22456 -> 22456
Cov: 22456 -> 22456
3843
Cov: 22456 -> 22457
Cov: 22457 -> 22457
3844
{"exception": "RuntimeError", "msg": "Expected nested_tensorlist[0].size() > 0 to be true, but got false.  (Could this error message be improved?  If so, please report an enhancement request to PyTorch.)"}
3845
Cov: 22457 -> 22458
Cov: 22458 -> 22458
3846
Cov: 22458 -> 22458
Cov: 22458 -> 22458
3847
{"exception": "RuntimeError", "msg": "element 0 of tensors does not require grad and does not have a grad_fn"}
3848
Cov: 22458 -> 22492
Cov: 22492 -> 22492
3849
Cov: 22492 -> 22493
Cov: 22493 -> 22493
3850
{"exception": "NameError", "msg": "name 'input_tensor' is not defined"}
3851
{"exception": "RuntimeError", "msg": "element 0 of tensors does not require grad and does not have a grad_fn"}
3852
Cov: 22493 -> 22494
Cov: 22494 -> 22494
3853
Cov: 22494 -> 22495
Cov: 22495 -> 22495
3854
Cov: 22495 -> 22495
Cov: 22495 -> 22495
3855
Cov: 22495 -> 22495
Cov: 22495 -> 22495
3856
Cov: 22495 -> 22495
Cov: 22495 -> 22495
3857
Cov: 22495 -> 22495
Cov: 22495 -> 22495
3858
Cov: 22495 -> 22495
Cov: 22495 -> 22495
3859
Cov: 22495 -> 22495
Cov: 22495 -> 22495
3860
Cov: 22495 -> 22495
Cov: 22495 -> 22495
3861
Cov: 22495 -> 22495
Cov: 22495 -> 22495
3862
Cov: 22495 -> 22495
Cov: 22495 -> 22495
3863
Cov: 22495 -> 22495
Cov: 22495 -> 22495
3864
Cov: 22495 -> 22495
Cov: 22495 -> 22495
3865
Cov: 22495 -> 22495
Cov: 22495 -> 22495
3866
Cov: 22495 -> 22495
Cov: 22495 -> 22495
3867
Cov: 22495 -> 22495
Cov: 22495 -> 22495
3868
Cov: 22495 -> 22495
Cov: 22495 -> 22495
3869
{"exception": "TypeError", "msg": "index_put(): argument 'indices' (position 1) must be tuple of Tensors, not Tensor"}
3870
Cov: 22495 -> 22495
Cov: 22495 -> 22495
3871
Cov: 22495 -> 22495
Cov: 22495 -> 22495
3872
{"exception": "RuntimeError", "msg": "size {[2, 3]} is not expandable to size {[5, 5]}."}
3873
{"exception": "NameError", "msg": "name 'device' is not defined"}
3874
{"exception": "RuntimeError", "msg": "This function was deprecated since version 1.9 and is now removed. `torch.lstsq` is deprecated in favor of `torch.linalg.lstsq`.\n`torch.linalg.lstsq` has reversed arguments and does not return the QR decomposition in the returned tuple (although it returns other information about the problem).\n\nTo get the QR decomposition consider using `torch.linalg.qr`.\n\nThe returned solution in `torch.lstsq` stored the residuals of the solution in the last m - n columns of the returned value whenever m > n. In torch.linalg.lstsq, the residuals are in the field 'residuals' of the returned named tuple.\n\nThe unpacking of the solution, as in\nX, _ = torch.lstsq(B, A).solution[:A.size(1)]\nshould be replaced with:\nX = torch.linalg.lstsq(A, B).solution"}
3875
Cov: 22495 -> 22758
Cov: 22758 -> 22758
3876
Cov: 22758 -> 22797
Cov: 22797 -> 22797
3877
Cov: 22797 -> 22797
Cov: 22797 -> 22797
3878
Cov: 22797 -> 22797
Cov: 22797 -> 22797
3879
Cov: 22797 -> 22797
Cov: 22797 -> 22797
3880
Cov: 22797 -> 22797
Cov: 22797 -> 22797
3881
{"exception": "RuntimeError", "msg": "tensor.sspaddmm(...) can only be called on sparse tensors"}
3882
Cov: 22797 -> 22797
Cov: 22797 -> 22797
3883
Cov: 22797 -> 22801
Cov: 22801 -> 22801
3884
{"exception": "RuntimeError", "msg": "\"log_normal_cpu\" not implemented for 'Long'"}
3885
Cov: 22801 -> 22802
Cov: 22802 -> 22802
3886
Cov: 22802 -> 22803
Cov: 22803 -> 22803
3887
{"exception": "RuntimeError", "msg": "any: You passed a dimname (string) to this op in place of a dimension index but it does not yet support this behavior. Please pass a dimension index to work around this."}
3888
Cov: 22803 -> 22803
Cov: 22803 -> 22803
3889
Cov: 22803 -> 22803
Cov: 22803 -> 22803
3890
Cov: 22803 -> 22803
Cov: 22803 -> 22803
3891
Cov: 22803 -> 22803
Cov: 22803 -> 22803
3892
Cov: 22803 -> 22803
Cov: 22803 -> 22803
3893
Cov: 22803 -> 22803
Cov: 22803 -> 22803
3894
{"exception": "TypeError", "msg": "'getset_descriptor' object is not callable"}
3895
Cov: 22803 -> 22803
Cov: 22803 -> 22803
3896
Cov: 22803 -> 22804
Cov: 22804 -> 22804
3897
{"exception": "RuntimeError", "msg": "element 0 of tensors does not require grad and does not have a grad_fn"}
3898
{"exception": "RuntimeError", "msg": "Index tensor must have the same number of dimensions as self tensor"}
3899
Cov: 22804 -> 22806
Cov: 22806 -> 22806
3900
Cov: 22806 -> 22809
Cov: 22809 -> 22809
3901
Cov: 22809 -> 22809
Cov: 22809 -> 22809
3902
{"exception": "RuntimeError", "msg": "\"igamma_cpu\" not implemented for 'Long'"}
3903
Cov: 22809 -> 22809
Cov: 22809 -> 22809
3904
Cov: 22809 -> 22809
Cov: 22809 -> 22809
3905
Cov: 22809 -> 22809
Cov: 22809 -> 22809
3906
Cov: 22809 -> 22809
Cov: 22809 -> 22809
3907
Cov: 22809 -> 22809
Cov: 22809 -> 22809
3908
Cov: 22809 -> 22809
Cov: 22809 -> 22809
3909
Cov: 22809 -> 22809
Cov: 22809 -> 22809
3910
Cov: 22809 -> 22809
Cov: 22809 -> 22809
3911
Cov: 22809 -> 22810
Cov: 22810 -> 22810
3912
{"exception": "RuntimeError", "msg": "tensor.sspaddmm(...) can only be called on sparse tensors"}
3913
Cov: 22810 -> 22810
Cov: 22810 -> 22810
3914
Cov: 22810 -> 22812
Cov: 22812 -> 22812
3915
Cov: 22812 -> 22812
Cov: 22812 -> 22812
3916
Cov: 22812 -> 22812
Cov: 22812 -> 22812
3917
Cov: 22812 -> 22812
Cov: 22812 -> 22812
3918
Cov: 22812 -> 22812
Cov: 22812 -> 22812
3919
Cov: 22812 -> 22812
Cov: 22812 -> 22812
3920
Cov: 22812 -> 22812
Cov: 22812 -> 22812
3921
Cov: 22812 -> 22812
Cov: 22812 -> 22812
3922
Cov: 22812 -> 22812
Cov: 22812 -> 22812
3923
Cov: 22812 -> 22812
Cov: 22812 -> 22812
3924
Cov: 22812 -> 22813
Cov: 22813 -> 22813
3925
Cov: 22813 -> 22813
Cov: 22813 -> 22813
3926
Cov: 22813 -> 22813
Cov: 22813 -> 22813
3927
Cov: 22813 -> 22823
Cov: 22823 -> 22823
3928
Cov: 22823 -> 22823
Cov: 22823 -> 22823
3929
Cov: 22823 -> 22827
Cov: 22827 -> 22827
3930
Cov: 22827 -> 22828
Cov: 22828 -> 22828
3931
Cov: 22828 -> 22828
Cov: 22828 -> 22828
3932
Cov: 22828 -> 22828
Cov: 22828 -> 22828
3933
Cov: 22828 -> 22828
Cov: 22828 -> 22828
3934
Cov: 22828 -> 22828
Cov: 22828 -> 22828
3935
Cov: 22828 -> 22828
Cov: 22828 -> 22828
3936
Cov: 22828 -> 22829
Cov: 22829 -> 22829
3937
Cov: 22829 -> 22830
Cov: 22830 -> 22830
3938
{"exception": "RuntimeError", "msg": "element 0 of tensors does not require grad and does not have a grad_fn"}
3939
Cov: 22830 -> 22843
Cov: 22843 -> 22843
3940
Cov: 22843 -> 22843
Cov: 22843 -> 22843
3941
Cov: 22843 -> 22843
Cov: 22843 -> 22843
3942
Cov: 22843 -> 22844
Cov: 22844 -> 22844
3943
Cov: 22844 -> 22844
Cov: 22844 -> 22844
3944
Cov: 22844 -> 22844
Cov: 22844 -> 22844
3945
Cov: 22844 -> 22844
Cov: 22844 -> 22844
3946
Cov: 22844 -> 22850
Cov: 22850 -> 22850
3947
Cov: 22850 -> 22850
Cov: 22850 -> 22850
3948
Cov: 22850 -> 22850
Cov: 22850 -> 22850
3949
Cov: 22850 -> 22853
Cov: 22853 -> 22853
3950
Cov: 22853 -> 22854
Cov: 22854 -> 22854
3951
Cov: 22854 -> 22854
Cov: 22854 -> 22854
3952
Cov: 22854 -> 22854
Cov: 22854 -> 22854
3953
{"exception": "NotImplementedError", "msg": "Could not run 'aten::sparse_mask' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::sparse_mask' is only available for these backends: [SparseCPU, SparseCUDA, SparseCsrCPU, SparseCsrCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nSparseCPU: registered at aten/src/ATen/RegisterSparseCPU.cpp:1387 [kernel]\nSparseCUDA: registered at aten/src/ATen/RegisterSparseCUDA.cpp:1573 [kernel]\nSparseCsrCPU: registered at aten/src/ATen/RegisterSparseCsrCPU.cpp:1135 [kernel]\nSparseCsrCUDA: registered at aten/src/ATen/RegisterSparseCsrCUDA.cpp:1276 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:153 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:498 [backend fallback]\nFunctionalize: registered at ../aten/src/ATen/FunctionalizeFallbackKernel.cpp:290 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:86 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradMTIA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_0.cpp:16725 [kernel]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:382 [backend fallback]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:249 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:710 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:203 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:161 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:494 [backend fallback]\nPreDispatch: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:165 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:157 [backend fallback]\n"}
3954
Cov: 22854 -> 22854
Cov: 22854 -> 22854
3955
Cov: 22854 -> 22854
Cov: 22854 -> 22854
3956
Cov: 22854 -> 22854
Cov: 22854 -> 22854
3957
Cov: 22854 -> 22856
Cov: 22856 -> 22856
3958
Cov: 22856 -> 22856
Cov: 22856 -> 22856
3959
Cov: 22856 -> 22856
Cov: 22856 -> 22856
3960
Cov: 22856 -> 22856
Cov: 22856 -> 22856
3961
Cov: 22856 -> 22856
Cov: 22856 -> 22856
3962
Cov: 22856 -> 22856
Cov: 22856 -> 22856
3963
Cov: 22856 -> 22856
Cov: 22856 -> 22856
3964
Cov: 22856 -> 22856
Cov: 22856 -> 22856
3965
Cov: 22856 -> 22856
Cov: 22856 -> 22856
3966
Cov: 22856 -> 22856
Cov: 22856 -> 22856
3967
Cov: 22856 -> 22856
Cov: 22856 -> 22856
3968
Cov: 22856 -> 22856
Cov: 22856 -> 22856
3969
Cov: 22856 -> 22856
Cov: 22856 -> 22856
3970
{"exception": "RuntimeError", "msg": "The expanded size of the tensor (4) must match the existing size (2) at non-singleton dimension 0.  Target sizes: [4, 3, 4].  Tensor sizes: [2, 3, 4]"}
3971
Cov: 22856 -> 22864
Cov: 22864 -> 22864
3972
Cov: 22864 -> 22864
Cov: 22864 -> 22864
3973
Cov: 22864 -> 22864
Cov: 22864 -> 22864
3974
Cov: 22864 -> 22867
Cov: 22867 -> 22867
3975
Cov: 22867 -> 22867
Cov: 22867 -> 22867
3976
Cov: 22867 -> 22867
Cov: 22867 -> 22867
3977
Cov: 22867 -> 22867
Cov: 22867 -> 22867
3978
Cov: 22867 -> 22867
Cov: 22867 -> 22867
3979
Cov: 22867 -> 22867
Cov: 22867 -> 22867
3980
Cov: 22867 -> 22867
Cov: 22867 -> 22867
3981
Cov: 22867 -> 22867
Cov: 22867 -> 22867
3982
Cov: 22867 -> 22867
Cov: 22867 -> 22867
3983
Cov: 22867 -> 22867
Cov: 22867 -> 22867
3984
Cov: 22867 -> 22867
Cov: 22867 -> 22867
3985
Cov: 22867 -> 22884
Cov: 22884 -> 22884
3986
Cov: 22884 -> 22884
Cov: 22884 -> 22884
3987
Cov: 22884 -> 22885
Cov: 22885 -> 22885
3988
Cov: 22885 -> 22885
Cov: 22885 -> 22885
3989
Cov: 22885 -> 22920
Cov: 22920 -> 22920
3990
Cov: 22920 -> 22920
Cov: 22920 -> 22920
3991
Cov: 22920 -> 22920
Cov: 22920 -> 22920
3992
Cov: 22920 -> 22920
Cov: 22920 -> 22920
3993
Cov: 22920 -> 22920
Cov: 22920 -> 22920
3994
Cov: 22920 -> 22920
Cov: 22920 -> 22920
3995
Cov: 22920 -> 22920
Cov: 22920 -> 22920
3996
{"exception": "TypeError", "msg": "'getset_descriptor' object is not callable"}
3997
Cov: 22920 -> 22920
Cov: 22920 -> 22920
3998
Cov: 22920 -> 22921
Cov: 22921 -> 22921
3999
Cov: 22921 -> 22921
Cov: 22921 -> 22921
4000
Cov: 22921 -> 22921
Cov: 22921 -> 22921
4001
Cov: 22921 -> 22922
Cov: 22922 -> 22922
4002
Cov: 22922 -> 22922
Cov: 22922 -> 22922
4003
Cov: 22922 -> 22922
Cov: 22922 -> 22922
4004
{"exception": "RuntimeError", "msg": "any: You passed a dimname (string) to this op in place of a dimension index but it does not yet support this behavior. Please pass a dimension index to work around this."}
4005
Cov: 22922 -> 22922
Cov: 22922 -> 22922
4006
Cov: 22922 -> 22922
Cov: 22922 -> 22922
4007
Cov: 22922 -> 22923
Cov: 22923 -> 22923
4008
Cov: 22923 -> 22923
Cov: 22923 -> 22923
4009
Cov: 22923 -> 22923
Cov: 22923 -> 22923
4010
Cov: 22923 -> 22923
Cov: 22923 -> 22923
4011
Cov: 22923 -> 22923
Cov: 22923 -> 22923
4012
Cov: 22923 -> 22923
Cov: 22923 -> 22923
4013
Cov: 22923 -> 22923
Cov: 22923 -> 22923
4014
Cov: 22923 -> 22923
Cov: 22923 -> 22923
4015
Cov: 22923 -> 22923
Cov: 22923 -> 22923
4016
Cov: 22923 -> 22923
Cov: 22923 -> 22923
4017
Cov: 22923 -> 22923
Cov: 22923 -> 22923
4018
Cov: 22923 -> 22923
Cov: 22923 -> 22923
4019
Cov: 22923 -> 22923
Cov: 22923 -> 22923
4020
Cov: 22923 -> 22951
Cov: 22951 -> 22951
4021
Cov: 22951 -> 22951
Cov: 22951 -> 22951
4022
Cov: 22951 -> 22952
Cov: 22952 -> 22952
4023
Cov: 22952 -> 22952
Cov: 22952 -> 22952
4024
Cov: 22952 -> 22952
Cov: 22952 -> 22952
4025
Cov: 22952 -> 22953
Cov: 22953 -> 22953
4026
Cov: 22953 -> 22967
Cov: 22967 -> 22967
4027
Cov: 22967 -> 22973
Cov: 22973 -> 22973
4028
Cov: 22973 -> 22973
Cov: 22973 -> 22973
4029
Cov: 22973 -> 22973
Cov: 22973 -> 22973
4030
Cov: 22973 -> 22973
Cov: 22973 -> 22973
4031
Cov: 22973 -> 22973
Cov: 22973 -> 22973
4032
{"exception": "TypeError", "msg": "descriptor 'manual_seed' of 'torch._C.Generator' object needs an argument"}
4033
Cov: 22973 -> 22973
Cov: 22973 -> 22973
4034
{"exception": "TypeError", "msg": "descriptor 'new_tensor' for 'torch._C._TensorBase' objects doesn't apply to a 'list' object"}
4035
Cov: 22973 -> 22973
Cov: 22973 -> 22973
4036
Cov: 22973 -> 22973
Cov: 22973 -> 22973
4037
Cov: 22973 -> 22973
Cov: 22973 -> 22973
4038
Cov: 22973 -> 22973
Cov: 22973 -> 22973
4039
Cov: 22973 -> 22974
Cov: 22974 -> 22974
4040
Cov: 22974 -> 22974
Cov: 22974 -> 22974
4041
Cov: 22974 -> 22974
Cov: 22974 -> 22974
4042
Cov: 22974 -> 22974
Cov: 22974 -> 22974
4043
Cov: 22974 -> 22974
Cov: 22974 -> 22974
4044
Cov: 22974 -> 22975
Cov: 22975 -> 22975
4045
Cov: 22975 -> 22975
Cov: 22975 -> 22975
4046
Cov: 22975 -> 22975
Cov: 22975 -> 22975
4047
Cov: 22975 -> 22975
Cov: 22975 -> 22975
4048
{"exception": "TypeError", "msg": "addmv_() received an invalid combination of arguments - got (Tensor), but expected (Tensor mat, Tensor vec, *, Number beta, Number alpha)"}
4049
Cov: 22975 -> 22975
Cov: 22975 -> 22975
4050
Cov: 22975 -> 22975
Cov: 22975 -> 22975
4051
Cov: 22975 -> 22975
Cov: 22975 -> 22975
4052
Cov: 22975 -> 22975
Cov: 22975 -> 22975
4053
Cov: 22975 -> 22975
Cov: 22975 -> 22975
4054
Cov: 22975 -> 22975
Cov: 22975 -> 22975
4055
Cov: 22975 -> 22975
Cov: 22975 -> 22975
4056
Cov: 22975 -> 22976
Cov: 22976 -> 22976
4057
Cov: 22976 -> 22977
Cov: 22977 -> 22977
4058
Cov: 22977 -> 22978
Cov: 22978 -> 22978
4059
Cov: 22978 -> 22978
Cov: 22978 -> 22978
4060
Cov: 22978 -> 22978
Cov: 22978 -> 22978
4061
Cov: 22978 -> 22978
Cov: 22978 -> 22978
4062
Cov: 22978 -> 22978
Cov: 22978 -> 22978
4063
Cov: 22978 -> 22978
Cov: 22978 -> 22978
4064
Cov: 22978 -> 22981
Cov: 22981 -> 22981
4065
Cov: 22981 -> 22981
Cov: 22981 -> 22981
4066
Cov: 22981 -> 22983
Cov: 22983 -> 22983
4067
Cov: 22983 -> 22983
Cov: 22983 -> 22983
4068
Cov: 22983 -> 22983
Cov: 22983 -> 22983
4069
Cov: 22983 -> 22983
Cov: 22983 -> 22983
4070
Cov: 22983 -> 22983
Cov: 22983 -> 22983
4071
Cov: 22983 -> 22983
Cov: 22983 -> 22983
4072
Cov: 22983 -> 22983
Cov: 22983 -> 22983
4073
Cov: 22983 -> 22983
Cov: 22983 -> 22983
4074
Cov: 22983 -> 22983
Cov: 22983 -> 22983
4075
Cov: 22983 -> 22983
Cov: 22983 -> 22983
4076
Cov: 22983 -> 22987
Cov: 22987 -> 22987
4077
Cov: 22987 -> 22987
Cov: 22987 -> 22987
4078
Cov: 22987 -> 22987
Cov: 22987 -> 22987
4079
Cov: 22987 -> 22989
Cov: 22989 -> 22989
4080
Cov: 22989 -> 22989
Cov: 22989 -> 22989
4081
Cov: 22989 -> 22989
Cov: 22989 -> 22989
4082
Cov: 22989 -> 22996
Cov: 22996 -> 22996
4083
Cov: 22996 -> 22996
Cov: 22996 -> 22996
4084
Cov: 22996 -> 22996
Cov: 22996 -> 22996
4085
Cov: 22996 -> 22997
Cov: 22997 -> 22997
4086
Cov: 22997 -> 22998
Cov: 22998 -> 22998
4087
Cov: 22998 -> 22998
Cov: 22998 -> 22998
4088
Cov: 22998 -> 22998
Cov: 22998 -> 22998
4089
Cov: 22998 -> 22998
Cov: 22998 -> 22998
4090
Cov: 22998 -> 22998
Cov: 22998 -> 22998
4091
Cov: 22998 -> 22998
Cov: 22998 -> 22998
4092
Cov: 22998 -> 22999
Cov: 22999 -> 22999
4093
Cov: 22999 -> 23000
Cov: 23000 -> 23000
4094
Cov: 23000 -> 23000
Cov: 23000 -> 23000
4095
Cov: 23000 -> 23000
Cov: 23000 -> 23000
4096
Cov: 23000 -> 23000
Cov: 23000 -> 23000
4097
Cov: 23000 -> 23000
Cov: 23000 -> 23000
4098
Cov: 23000 -> 23000
Cov: 23000 -> 23000
4099
Cov: 23000 -> 23000
Cov: 23000 -> 23000
4100
Cov: 23000 -> 23000
Cov: 23000 -> 23000
4101
Cov: 23000 -> 23000
Cov: 23000 -> 23000
4102
Cov: 23000 -> 23000
Cov: 23000 -> 23000
4103
Cov: 23000 -> 23000
Cov: 23000 -> 23000
4104
Cov: 23000 -> 23000
Cov: 23000 -> 23000
4105
Cov: 23000 -> 23000
Cov: 23000 -> 23000
4106
Cov: 23000 -> 23000
Cov: 23000 -> 23000
4107
Cov: 23000 -> 23000
Cov: 23000 -> 23000
4108
Cov: 23000 -> 23000
Cov: 23000 -> 23000
4109
Cov: 23000 -> 23000
Cov: 23000 -> 23000
4110
Cov: 23000 -> 23000
Cov: 23000 -> 23000
4111
Cov: 23000 -> 23000
Cov: 23000 -> 23000
4112
Cov: 23000 -> 23001
Cov: 23001 -> 23001
4113
Cov: 23001 -> 23001
Cov: 23001 -> 23001
4114
Cov: 23001 -> 23002
Cov: 23002 -> 23002
4115
Cov: 23002 -> 23002
Cov: 23002 -> 23002
4116
Cov: 23002 -> 23002
Cov: 23002 -> 23002
4117
Cov: 23002 -> 23003
Cov: 23003 -> 23003
4118
Cov: 23003 -> 23003
Cov: 23003 -> 23003
4119
Cov: 23003 -> 23003
Cov: 23003 -> 23003
4120
Cov: 23003 -> 23003
Cov: 23003 -> 23003
4121
Cov: 23003 -> 23009
Cov: 23009 -> 23009
4122
Cov: 23009 -> 23010
Cov: 23010 -> 23010
4123
Cov: 23010 -> 23018
Cov: 23018 -> 23018
4124
Cov: 23018 -> 23018
Cov: 23018 -> 23018
4125
Cov: 23018 -> 23018
Cov: 23018 -> 23018
4126
Cov: 23018 -> 23018
Cov: 23018 -> 23018
4127
Cov: 23018 -> 23018
Cov: 23018 -> 23018
4128
Cov: 23018 -> 23018
Cov: 23018 -> 23018
4129
Cov: 23018 -> 23018
Cov: 23018 -> 23018
4130
Cov: 23018 -> 23018
Cov: 23018 -> 23018
4131
Cov: 23018 -> 23018
Cov: 23018 -> 23018
4132
{"exception": "TypeError", "msg": "silu(): argument 'input' (position 1) must be Tensor, not builtin_function_or_method"}
4133
Cov: 23018 -> 23019
Cov: 23019 -> 23019
4134
Cov: 23019 -> 23019
Cov: 23019 -> 23019
4135
Cov: 23019 -> 23019
Cov: 23019 -> 23019
4136
Cov: 23019 -> 23019
Cov: 23019 -> 23019
4137
Cov: 23019 -> 23019
Cov: 23019 -> 23019
4138
Cov: 23019 -> 23019
Cov: 23019 -> 23019
4139
Cov: 23019 -> 23019
Cov: 23019 -> 23019
4140
Cov: 23019 -> 23021
Cov: 23021 -> 23021
4141
Cov: 23021 -> 23021
Cov: 23021 -> 23021
4142
Cov: 23021 -> 23022
Cov: 23022 -> 23022
4143
Cov: 23022 -> 23023
Cov: 23023 -> 23023
4144
Cov: 23023 -> 23024
Cov: 23024 -> 23024
4145
Cov: 23024 -> 23025
Cov: 23025 -> 23025
4146
Cov: 23025 -> 23025
Cov: 23025 -> 23025
4147
Cov: 23025 -> 23025
Cov: 23025 -> 23025
4148
Cov: 23025 -> 23025
Cov: 23025 -> 23025
4149
Cov: 23025 -> 23025
Cov: 23025 -> 23025
4150
Cov: 23025 -> 23027
Cov: 23027 -> 23027
4151
Cov: 23027 -> 23027
Cov: 23027 -> 23027
4152
Cov: 23027 -> 23027
Cov: 23027 -> 23027
4153
Cov: 23027 -> 23027
Cov: 23027 -> 23027
4154
{"exception": "RuntimeError", "msg": "torch.ormqr: Expected tau to have one dimension less than input, but got tau.ndim equal to 2 and input.ndim is equal to 2"}
4155
{"exception": "TypeError", "msg": "'getset_descriptor' object is not callable"}
4156
Cov: 23027 -> 23027
Cov: 23027 -> 23027
4157
Cov: 23027 -> 23027
Cov: 23027 -> 23027
4158
Cov: 23027 -> 23027
Cov: 23027 -> 23027
4159
Cov: 23027 -> 23027
Cov: 23027 -> 23027
4160
Cov: 23027 -> 23027
Cov: 23027 -> 23027
4161
Cov: 23027 -> 23027
Cov: 23027 -> 23027
4162
Cov: 23027 -> 23027
Cov: 23027 -> 23027
4163
Cov: 23027 -> 23027
Cov: 23027 -> 23027
4164
Cov: 23027 -> 23027
Cov: 23027 -> 23027
4165
Cov: 23027 -> 23027
Cov: 23027 -> 23027
4166
Cov: 23027 -> 23027
Cov: 23027 -> 23027
4167
Cov: 23027 -> 23027
Cov: 23027 -> 23027
4168
Cov: 23027 -> 23027
Cov: 23027 -> 23027
4169
Cov: 23027 -> 23027
Cov: 23027 -> 23027
4170
{"exception": "TypeError", "msg": "map_(): argument 'other' (position 1) must be Tensor, not function"}
4171
Cov: 23027 -> 23027
Cov: 23027 -> 23027
4172
Cov: 23027 -> 23027
Cov: 23027 -> 23027
4173
Cov: 23027 -> 23027
Cov: 23027 -> 23027
4174
Cov: 23027 -> 23034
Cov: 23034 -> 23034
4175
Cov: 23034 -> 23034
Cov: 23034 -> 23034
4176
Cov: 23034 -> 23034
Cov: 23034 -> 23034
4177
Cov: 23034 -> 23034
Cov: 23034 -> 23034
4178
Cov: 23034 -> 23034
Cov: 23034 -> 23034
4179
Cov: 23034 -> 23034
Cov: 23034 -> 23034
4180
Cov: 23034 -> 23034
Cov: 23034 -> 23034
4181
Cov: 23034 -> 23034
Cov: 23034 -> 23034
4182
{"exception": "TypeError", "msg": "fix() takes no arguments (1 given)"}
4183
Cov: 23034 -> 23034
Cov: 23034 -> 23034
4184
Cov: 23034 -> 23034
Cov: 23034 -> 23034
4185
Cov: 23034 -> 23034
Cov: 23034 -> 23034
4186
Cov: 23034 -> 23035
Cov: 23035 -> 23035
4187
Cov: 23035 -> 23057
Cov: 23057 -> 23057
4188
Cov: 23057 -> 23057
Cov: 23057 -> 23057
4189
{"exception": "RuntimeError", "msg": "Creating a Tensor subclass from a class that does not inherit from Tensor is not possible. Make sure your class inherits from Tensor."}
4190
Cov: 23057 -> 23057
Cov: 23057 -> 23057
4191
Cov: 23057 -> 23057
Cov: 23057 -> 23057
4192
Cov: 23057 -> 23057
Cov: 23057 -> 23057
4193
Cov: 23057 -> 23133
Cov: 23133 -> 23133
4194
Cov: 23133 -> 23133
Cov: 23133 -> 23133
4195
Cov: 23133 -> 23133
Cov: 23133 -> 23133
4196
Cov: 23133 -> 23134
Cov: 23134 -> 23134
4197
Cov: 23134 -> 23135
Cov: 23135 -> 23135
4198
Cov: 23135 -> 23135
Cov: 23135 -> 23135
4199
Cov: 23135 -> 23135
Cov: 23135 -> 23135
4200
Cov: 23135 -> 23135
Cov: 23135 -> 23135
4201
Cov: 23135 -> 23135
Cov: 23135 -> 23135
4202
Cov: 23135 -> 23135
Cov: 23135 -> 23135
4203
Cov: 23135 -> 23135
Cov: 23135 -> 23135
4204
Cov: 23135 -> 23135
Cov: 23135 -> 23135
4205
Cov: 23135 -> 23135
Cov: 23135 -> 23135
4206
Cov: 23135 -> 23135
Cov: 23135 -> 23135
4207
{"exception": "TypeError", "msg": "'getset_descriptor' object is not callable"}
4208
Cov: 23135 -> 23136
Cov: 23136 -> 23136
4209
Cov: 23136 -> 23136
Cov: 23136 -> 23136
4210
Cov: 23136 -> 23136
Cov: 23136 -> 23136
4211
Cov: 23136 -> 23136
Cov: 23136 -> 23136
4212
Cov: 23136 -> 23136
Cov: 23136 -> 23136
4213
Cov: 23136 -> 23136
Cov: 23136 -> 23136
4214
Cov: 23136 -> 23136
Cov: 23136 -> 23136
4215
Cov: 23136 -> 23136
Cov: 23136 -> 23136
4216
Cov: 23136 -> 23153
Cov: 23153 -> 23153
4217
Cov: 23153 -> 23153
Cov: 23153 -> 23153
4218
Cov: 23153 -> 23153
Cov: 23153 -> 23153
4219
Cov: 23153 -> 23153
Cov: 23153 -> 23153
4220
Cov: 23153 -> 23154
Cov: 23154 -> 23154
4221
Cov: 23154 -> 23154
Cov: 23154 -> 23154
4222
Cov: 23154 -> 23154
Cov: 23154 -> 23154
4223
Cov: 23154 -> 23155
Cov: 23155 -> 23155
4224
{"exception": "RuntimeError", "msg": "Index tensor must have the same number of dimensions as src tensor"}
4225
Cov: 23155 -> 23155
Cov: 23155 -> 23155
4226
Cov: 23155 -> 23155
Cov: 23155 -> 23155
4227
Cov: 23155 -> 23155
Cov: 23155 -> 23155
4228
Cov: 23155 -> 23156
Cov: 23156 -> 23156
4229
Cov: 23156 -> 23156
Cov: 23156 -> 23156
4230
Cov: 23156 -> 23161
Cov: 23161 -> 23161
4231
Cov: 23161 -> 23197
Cov: 23197 -> 23197
4232
Cov: 23197 -> 23197
Cov: 23197 -> 23197
4233
Cov: 23197 -> 23197
Cov: 23197 -> 23197
4234
Cov: 23197 -> 23197
Cov: 23197 -> 23197
4235
Cov: 23197 -> 23197
Cov: 23197 -> 23197
4236
Cov: 23197 -> 23197
Cov: 23197 -> 23197
4237
Cov: 23197 -> 23197
Cov: 23197 -> 23197
4238
Cov: 23197 -> 23199
Cov: 23199 -> 23199
4239
Cov: 23199 -> 23199
Cov: 23199 -> 23199
4240
Cov: 23199 -> 23199
Cov: 23199 -> 23199
4241
Cov: 23199 -> 23199
Cov: 23199 -> 23199
4242
Cov: 23199 -> 23199
Cov: 23199 -> 23199
4243
Cov: 23199 -> 23199
Cov: 23199 -> 23199
4244
Cov: 23199 -> 23199
Cov: 23199 -> 23199
4245
Cov: 23199 -> 23199
Cov: 23199 -> 23199
4246
{"exception": "TypeError", "msg": "orgqr() missing 1 required positional arguments: \"input2\""}
4247
{"exception": "RuntimeError", "msg": "Error: cannot set number of interop threads after parallel work has started or set_num_interop_threads called"}
4248
Cov: 23199 -> 23200
Cov: 23200 -> 23200
4249
Cov: 23200 -> 23200
Cov: 23200 -> 23200
4250
{"exception": "TypeError", "msg": "'getset_descriptor' object is not callable"}
4251
Cov: 23200 -> 23200
Cov: 23200 -> 23200
4252
Cov: 23200 -> 23200
Cov: 23200 -> 23200
4253
Cov: 23200 -> 23200
Cov: 23200 -> 23200
4254
Cov: 23200 -> 23200
Cov: 23200 -> 23200
4255
Cov: 23200 -> 23200
Cov: 23200 -> 23200
4256
Cov: 23200 -> 23203
Cov: 23203 -> 23203
4257
Cov: 23203 -> 23203
Cov: 23203 -> 23203
4258
{"exception": "RuntimeError", "msg": "element 0 of tensors does not require grad and does not have a grad_fn"}
4259
Cov: 23203 -> 23203
Cov: 23203 -> 23203
4260
Cov: 23203 -> 23203
Cov: 23203 -> 23203
4261
Cov: 23203 -> 23203
Cov: 23203 -> 23203
4262
{"exception": "RuntimeError", "msg": "masked_scatter_ only supports boolean masks, but got mask with dtype Byte"}
4263
Cov: 23203 -> 23204
Cov: 23204 -> 23204
4264
Cov: 23204 -> 23204
Cov: 23204 -> 23204
4265
Cov: 23204 -> 23204
Cov: 23204 -> 23204
4266
Cov: 23204 -> 23204
Cov: 23204 -> 23204
4267
Cov: 23204 -> 23205
Cov: 23205 -> 23205
4268
Cov: 23205 -> 23205
Cov: 23205 -> 23205
4269
Cov: 23205 -> 23206
Cov: 23206 -> 23206
4270
Cov: 23206 -> 23206
Cov: 23206 -> 23206
4271
{"exception": "RuntimeError", "msg": "This function was deprecated since version 1.9 and is now removed. The default behavior has changed from using the upper triangular portion of the matrix by default to using the lower triangular portion.\n\nL, _ = torch.symeig(A, upper=upper) should be replaced with:\nL = torch.linalg.eigvalsh(A, UPLO='U' if upper else 'L')\n\nand\n\nL, V = torch.symeig(A, eigenvectors=True) should be replaced with:\nL, V = torch.linalg.eigh(A, UPLO='U' if upper else 'L')"}
4272
Cov: 23206 -> 23207
Cov: 23207 -> 23207
4273
Cov: 23207 -> 23207
Cov: 23207 -> 23207
4274
Cov: 23207 -> 23207
Cov: 23207 -> 23207
4275
Cov: 23207 -> 23207
Cov: 23207 -> 23207
4276
Cov: 23207 -> 23207
Cov: 23207 -> 23207
4277
Cov: 23207 -> 23207
Cov: 23207 -> 23207
4278
Cov: 23207 -> 23208
Cov: 23208 -> 23208
4279
{"exception": "NotImplementedError", "msg": "Could not run 'aten::sparse_mask' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::sparse_mask' is only available for these backends: [SparseCPU, SparseCUDA, SparseCsrCPU, SparseCsrCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nSparseCPU: registered at aten/src/ATen/RegisterSparseCPU.cpp:1387 [kernel]\nSparseCUDA: registered at aten/src/ATen/RegisterSparseCUDA.cpp:1573 [kernel]\nSparseCsrCPU: registered at aten/src/ATen/RegisterSparseCsrCPU.cpp:1135 [kernel]\nSparseCsrCUDA: registered at aten/src/ATen/RegisterSparseCsrCUDA.cpp:1276 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:153 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:498 [backend fallback]\nFunctionalize: registered at ../aten/src/ATen/FunctionalizeFallbackKernel.cpp:290 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:86 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradMTIA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_0.cpp:16725 [kernel]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:382 [backend fallback]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:249 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:710 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:203 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:161 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:494 [backend fallback]\nPreDispatch: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:165 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:157 [backend fallback]\n"}
4280
Cov: 23208 -> 23209
Cov: 23209 -> 23209
4281
{"exception": "TypeError", "msg": "descriptor 'new_tensor' for 'torch._C._TensorBase' objects doesn't apply to a 'numpy.ndarray' object"}
4282
Cov: 23209 -> 23209
Cov: 23209 -> 23209
4283
Cov: 23209 -> 23210
Cov: 23210 -> 23210
4284
Cov: 23210 -> 23211
Cov: 23211 -> 23211
4285
Cov: 23211 -> 23212
Cov: 23212 -> 23212
4286
Cov: 23212 -> 23212
Cov: 23212 -> 23212
4287
Cov: 23212 -> 23212
Cov: 23212 -> 23212
4288
{"exception": "TypeError", "msg": "'getset_descriptor' object is not callable"}
4289
Cov: 23212 -> 23212
Cov: 23212 -> 23212
4290
Cov: 23212 -> 23214
Cov: 23214 -> 23214
4291
Cov: 23214 -> 23214
Cov: 23214 -> 23214
4292
Cov: 23214 -> 23214
Cov: 23214 -> 23214
4293
Cov: 23214 -> 23214
Cov: 23214 -> 23214
4294
Cov: 23214 -> 23215
Cov: 23215 -> 23215
4295
Cov: 23215 -> 23215
Cov: 23215 -> 23215
4296
Cov: 23215 -> 23215
Cov: 23215 -> 23215
4297
Cov: 23215 -> 23215
Cov: 23215 -> 23215
4298
Cov: 23215 -> 23218
Cov: 23218 -> 23218
4299
Cov: 23218 -> 23218
Cov: 23218 -> 23218
4300
Cov: 23218 -> 23219
Cov: 23219 -> 23219
4301
{"exception": "_LinAlgError", "msg": "cholesky: The factorization could not be completed because the input is not positive-definite (the leading minor of order 2 is not positive-definite)."}
4302
Cov: 23219 -> 23219
Cov: 23219 -> 23219
4303
Cov: 23219 -> 23220
Cov: 23220 -> 23220
4304
{"exception": "TypeError", "msg": "ldexp_(): argument 'other' (position 1) must be Tensor, not int"}
4305
Cov: 23220 -> 23221
Cov: 23221 -> 23221
4306
Cov: 23221 -> 23221
Cov: 23221 -> 23221
4307
Cov: 23221 -> 23221
Cov: 23221 -> 23221
4308
Cov: 23221 -> 23221
Cov: 23221 -> 23221
4309
Cov: 23221 -> 23221
Cov: 23221 -> 23221
4310
Cov: 23221 -> 23221
Cov: 23221 -> 23221
4311
Cov: 23221 -> 23222
Cov: 23222 -> 23222
4312
Cov: 23222 -> 23222
Cov: 23222 -> 23222
4313
Cov: 23222 -> 23233
Cov: 23233 -> 23233
4314
Cov: 23233 -> 23234
Cov: 23234 -> 23234
4315
Cov: 23234 -> 23237
Cov: 23237 -> 23237
4316
Cov: 23237 -> 23237
Cov: 23237 -> 23237
4317
Cov: 23237 -> 23237
Cov: 23237 -> 23237
4318
Cov: 23237 -> 23237
Cov: 23237 -> 23237
4319
Cov: 23237 -> 23237
Cov: 23237 -> 23237
4320
Cov: 23237 -> 23238
Cov: 23238 -> 23238
4321
Cov: 23238 -> 23238
Cov: 23238 -> 23238
4322
Cov: 23238 -> 23238
Cov: 23238 -> 23238
4323
Cov: 23238 -> 23238
Cov: 23238 -> 23238
4324
Cov: 23238 -> 23238
Cov: 23238 -> 23238
4325
Cov: 23238 -> 23238
Cov: 23238 -> 23238
4326
Cov: 23238 -> 23238
Cov: 23238 -> 23238
4327
{"exception": "TypeError", "msg": "'getset_descriptor' object is not callable"}
4328
{"exception": "RuntimeError", "msg": "kl_div: Integral inputs not supported."}
4329
Cov: 23238 -> 23239
Cov: 23239 -> 23239
4330
Cov: 23239 -> 23239
Cov: 23239 -> 23239
4331
Cov: 23239 -> 23239
Cov: 23239 -> 23239
4332
Cov: 23239 -> 23240
Cov: 23240 -> 23240
4333
Cov: 23240 -> 23240
Cov: 23240 -> 23240
4334
Cov: 23240 -> 23241
Cov: 23241 -> 23241
4335
Cov: 23241 -> 23241
Cov: 23241 -> 23241
4336
Cov: 23241 -> 23241
Cov: 23241 -> 23241
4337
Cov: 23241 -> 23241
Cov: 23241 -> 23241
4338
Cov: 23241 -> 23241
Cov: 23241 -> 23241
4339
Cov: 23241 -> 23241
Cov: 23241 -> 23241
4340
Cov: 23241 -> 23241
Cov: 23241 -> 23241
4341
{"exception": "TypeError", "msg": "'getset_descriptor' object is not callable"}
4342
Cov: 23241 -> 23241
Cov: 23241 -> 23241
4343
Cov: 23241 -> 23241
Cov: 23241 -> 23241
4344
Cov: 23241 -> 23249
Cov: 23249 -> 23249
4345
Cov: 23249 -> 23250
Cov: 23250 -> 23250
4346
Cov: 23250 -> 23250
Cov: 23250 -> 23250
4347
Cov: 23250 -> 23250
Cov: 23250 -> 23250
4348
Cov: 23250 -> 23250
Cov: 23250 -> 23250
4349
Cov: 23250 -> 23250
Cov: 23250 -> 23250
4350
Cov: 23250 -> 23250
Cov: 23250 -> 23250
4351
Cov: 23250 -> 23257
Cov: 23257 -> 23257
4352
Cov: 23257 -> 23257
Cov: 23257 -> 23257
4353
Cov: 23257 -> 23264
Cov: 23264 -> 23264
4354
Cov: 23264 -> 23264
Cov: 23264 -> 23264
4355
Cov: 23264 -> 23264
Cov: 23264 -> 23264
4356
Cov: 23264 -> 23264
Cov: 23264 -> 23264
4357
Cov: 23264 -> 23264
Cov: 23264 -> 23264
4358
Cov: 23264 -> 23264
Cov: 23264 -> 23264
4359
Cov: 23264 -> 23264
Cov: 23264 -> 23264
4360
Cov: 23264 -> 23264
Cov: 23264 -> 23264
4361
Cov: 23264 -> 23265
Cov: 23265 -> 23265
4362
Cov: 23265 -> 23265
Cov: 23265 -> 23265
4363
Cov: 23265 -> 23265
Cov: 23265 -> 23265
4364
Cov: 23265 -> 23266
Cov: 23266 -> 23266
4365
Cov: 23266 -> 23266
Cov: 23266 -> 23266
4366
{"exception": "RuntimeError", "msg": "tensor.sspaddmm(...) can only be called on sparse tensors"}
4367
Cov: 23266 -> 23268
Cov: 23268 -> 23268
4368
Cov: 23268 -> 23268
Cov: 23268 -> 23268
4369
Cov: 23268 -> 23268
Cov: 23268 -> 23268
4370
Cov: 23268 -> 23268
Cov: 23268 -> 23268
4371
Cov: 23268 -> 23268
Cov: 23268 -> 23268
4372
Cov: 23268 -> 23268
Cov: 23268 -> 23268
4373
{"exception": "RuntimeError", "msg": "scatter(): Expected self.dtype to be equal to src.dtype"}
4374
Cov: 23268 -> 23268
Cov: 23268 -> 23268
4375
Cov: 23268 -> 23288
Cov: 23288 -> 23288
4376
Cov: 23288 -> 23289
Cov: 23289 -> 23289
4377
Cov: 23289 -> 23289
Cov: 23289 -> 23289
4378
Cov: 23289 -> 23289
Cov: 23289 -> 23289
4379
Cov: 23289 -> 23290
Cov: 23290 -> 23290
4380
Cov: 23290 -> 23291
Cov: 23291 -> 23291
4381
Cov: 23291 -> 23372
Cov: 23372 -> 23372
4382
Cov: 23372 -> 23372
Cov: 23372 -> 23372
4383
Cov: 23372 -> 23372
Cov: 23372 -> 23372
4384
{"exception": "RuntimeError", "msg": "expand(torch.FloatTensor{[10, 3, 4]}, size=[3, 5]): the number of sizes provided (2) must be greater or equal to the number of dimensions in the tensor (3)"}
4385
Cov: 23372 -> 23372
Cov: 23372 -> 23372
4386
Cov: 23372 -> 23378
Cov: 23378 -> 23378
4387
{"exception": "RuntimeError", "msg": "scatter(): Expected self.dtype to be equal to src.dtype"}
4388
Cov: 23378 -> 23378
Cov: 23378 -> 23378
4389
Cov: 23378 -> 23379
Cov: 23379 -> 23379
4390
{"exception": "RuntimeError", "msg": "put_ does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True)'. You can turn off determinism just for this operation, or you can use the 'warn_only=True' option, if that's acceptable for your application. You can also file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation."}
4391
Cov: 23379 -> 23379
Cov: 23379 -> 23379
4392
Cov: 23379 -> 23379
Cov: 23379 -> 23379
4393
{"exception": "RuntimeError", "msg": "element 0 of tensors does not require grad and does not have a grad_fn"}
4394
Cov: 23379 -> 23380
Cov: 23380 -> 23380
4395
Cov: 23380 -> 23380
Cov: 23380 -> 23380
4396
Cov: 23380 -> 23380
Cov: 23380 -> 23380
4397
Cov: 23380 -> 23380
Cov: 23380 -> 23380
4398
Cov: 23380 -> 23392
Cov: 23392 -> 23392
4399
Cov: 23392 -> 23393
Cov: 23393 -> 23393
4400
Cov: 23393 -> 23393
Cov: 23393 -> 23393
4401
Cov: 23393 -> 23394
Cov: 23394 -> 23394
4402
Cov: 23394 -> 23394
Cov: 23394 -> 23394
4403
Cov: 23394 -> 23394
Cov: 23394 -> 23394
4404
Cov: 23394 -> 23395
Cov: 23395 -> 23395
4405
Cov: 23395 -> 23396
Cov: 23396 -> 23396
4406
Cov: 23396 -> 23396
Cov: 23396 -> 23396
4407
{"exception": "RuntimeError", "msg": "A must be batches of square matrices, but they are 3 by 2 matrices"}
4408
Cov: 23396 -> 23396
Cov: 23396 -> 23396
4409
Cov: 23396 -> 23396
Cov: 23396 -> 23396
4410
Cov: 23396 -> 23397
Cov: 23397 -> 23397
4411
Cov: 23397 -> 23397
Cov: 23397 -> 23397
4412
Cov: 23397 -> 23397
Cov: 23397 -> 23397
4413
{"exception": "RuntimeError", "msg": "\"bitwise_and_cpu\" not implemented for 'Float'"}
4414
Cov: 23397 -> 23397
Cov: 23397 -> 23397
4415
Cov: 23397 -> 23397
Cov: 23397 -> 23397
4416
Cov: 23397 -> 23398
Cov: 23398 -> 23398
4417
Cov: 23398 -> 23399
Cov: 23399 -> 23399
4418
Cov: 23399 -> 23399
Cov: 23399 -> 23399
4419
Cov: 23399 -> 23399
Cov: 23399 -> 23399
4420
Cov: 23399 -> 23399
Cov: 23399 -> 23399
4421
Cov: 23399 -> 23399
Cov: 23399 -> 23399
4422
Cov: 23399 -> 23399
Cov: 23399 -> 23399
4423
Cov: 23399 -> 23399
Cov: 23399 -> 23399
4424
Cov: 23399 -> 23399
Cov: 23399 -> 23399
4425
Cov: 23399 -> 23399
Cov: 23399 -> 23399
4426
Cov: 23399 -> 23399
Cov: 23399 -> 23399
4427
Cov: 23399 -> 23399
Cov: 23399 -> 23399
4428
Cov: 23399 -> 23399
Cov: 23399 -> 23399
4429
Cov: 23399 -> 23401
Cov: 23401 -> 23401
4430
Cov: 23401 -> 23401
Cov: 23401 -> 23401
4431
Cov: 23401 -> 23402
Cov: 23402 -> 23402
4432
Cov: 23402 -> 23419
Cov: 23419 -> 23419
4433
{"exception": "RuntimeError", "msg": "result type Float can't be cast to the desired output type Long"}
4434
Cov: 23419 -> 23420
Cov: 23420 -> 23420
4435
Cov: 23420 -> 23421
Cov: 23421 -> 23421
4436
Cov: 23421 -> 23422
Cov: 23422 -> 23422
4437
Cov: 23422 -> 23422
Cov: 23422 -> 23422
4438
Cov: 23422 -> 23422
Cov: 23422 -> 23422
4439
Cov: 23422 -> 23422
Cov: 23422 -> 23422
4440
Cov: 23422 -> 23422
Cov: 23422 -> 23422
4441
Cov: 23422 -> 23422
Cov: 23422 -> 23422
4442
Cov: 23422 -> 23422
Cov: 23422 -> 23422
4443
Cov: 23422 -> 23422
Cov: 23422 -> 23422
4444
Cov: 23422 -> 23422
Cov: 23422 -> 23422
4445
Cov: 23422 -> 23427
Cov: 23427 -> 23427
4446
Cov: 23427 -> 23428
Cov: 23428 -> 23428
4447
Cov: 23428 -> 23429
Cov: 23429 -> 23429
4448
Cov: 23429 -> 23429
Cov: 23429 -> 23429
4449
Cov: 23429 -> 23429
Cov: 23429 -> 23429
4450
Cov: 23429 -> 23429
Cov: 23429 -> 23429
4451
Cov: 23429 -> 23429
Cov: 23429 -> 23429
4452
Cov: 23429 -> 23429
Cov: 23429 -> 23429
4453
Cov: 23429 -> 23430
Cov: 23430 -> 23430
4454
Cov: 23430 -> 23430
Cov: 23430 -> 23430
4455
Cov: 23430 -> 23430
Cov: 23430 -> 23430
4456
Cov: 23430 -> 23430
Cov: 23430 -> 23430
4457
Cov: 23430 -> 23432
Cov: 23432 -> 23432
4458
Cov: 23432 -> 23433
Cov: 23433 -> 23433
4459
Cov: 23433 -> 23434
Cov: 23434 -> 23434
4460
Cov: 23434 -> 23434
Cov: 23434 -> 23434
4461
Cov: 23434 -> 23435
Cov: 23435 -> 23435
4462
{"exception": "RuntimeError", "msg": "The expanded size of the tensor (5) must match the existing size (4) at non-singleton dimension 2.  Target sizes: [10, 3, 5].  Tensor sizes: [10, 3, 4]"}
4463
{"exception": "TypeError", "msg": "<lambda>() takes 1 positional argument but 2 were given"}
4464
{"exception": "NameError", "msg": "name 'Normal' is not defined"}
4465
Cov: 23435 -> 23445
Cov: 23445 -> 23445
4466
Cov: 23445 -> 23445
Cov: 23445 -> 23445
4467
Cov: 23445 -> 23445
Cov: 23445 -> 23445
4468
Cov: 23445 -> 23445
Cov: 23445 -> 23445
4469
Cov: 23445 -> 23446
Cov: 23446 -> 23446
4470
Cov: 23446 -> 23450
Cov: 23450 -> 23450
4471
Cov: 23450 -> 23450
Cov: 23450 -> 23450
4472
Cov: 23450 -> 23450
Cov: 23450 -> 23450
4473
Cov: 23450 -> 23450
Cov: 23450 -> 23450
4474
Cov: 23450 -> 23451
Cov: 23451 -> 23451
4475
Cov: 23451 -> 23451
Cov: 23451 -> 23451
4476
Cov: 23451 -> 23451
Cov: 23451 -> 23451
4477
Cov: 23451 -> 23451
Cov: 23451 -> 23451
4478
Cov: 23451 -> 23451
Cov: 23451 -> 23451
4479
Cov: 23451 -> 23451
Cov: 23451 -> 23451
4480
Cov: 23451 -> 23452
Cov: 23452 -> 23452
4481
Cov: 23452 -> 23452
Cov: 23452 -> 23452
4482
Cov: 23452 -> 23452
Cov: 23452 -> 23452
4483
Cov: 23452 -> 23457
Cov: 23457 -> 23457
4484
Cov: 23457 -> 23457
Cov: 23457 -> 23457
4485
Cov: 23457 -> 23457
Cov: 23457 -> 23457
4486
Cov: 23457 -> 23487
Cov: 23487 -> 23487
4487
Cov: 23487 -> 23487
Cov: 23487 -> 23487
4488
Cov: 23487 -> 23487
Cov: 23487 -> 23487
4489
Cov: 23487 -> 23487
Cov: 23487 -> 23487
4490
Cov: 23487 -> 23488
Cov: 23488 -> 23488
4491
Cov: 23488 -> 23488
Cov: 23488 -> 23488
4492
Cov: 23488 -> 23488
Cov: 23488 -> 23488
4493
Cov: 23488 -> 23488
Cov: 23488 -> 23488
4494
Cov: 23488 -> 23488
Cov: 23488 -> 23488
4495
Cov: 23488 -> 23488
Cov: 23488 -> 23488
4496
Cov: 23488 -> 23489
Cov: 23489 -> 23489
4497
Cov: 23489 -> 23489
Cov: 23489 -> 23489
4498
Cov: 23489 -> 23489
Cov: 23489 -> 23489
4499
Cov: 23489 -> 23489
Cov: 23489 -> 23489
4500
Cov: 23489 -> 23489
Cov: 23489 -> 23489
4501
Cov: 23489 -> 23490
Cov: 23490 -> 23490
4502
Cov: 23490 -> 23490
Cov: 23490 -> 23490
4503
Cov: 23490 -> 23490
Cov: 23490 -> 23490
4504
Cov: 23490 -> 23490
Cov: 23490 -> 23490
4505
Cov: 23490 -> 23493
Cov: 23493 -> 23493
4506
Cov: 23493 -> 23493
Cov: 23493 -> 23493
4507
Cov: 23493 -> 23493
Cov: 23493 -> 23493
4508
Cov: 23493 -> 23493
Cov: 23493 -> 23493
4509
Cov: 23493 -> 23494
Cov: 23494 -> 23494
4510
Cov: 23494 -> 23494
Cov: 23494 -> 23494
4511
Cov: 23494 -> 23494
Cov: 23494 -> 23494
4512
{"exception": "RuntimeError", "msg": "element 0 of tensors does not require grad and does not have a grad_fn"}
4513
Cov: 23494 -> 23495
Cov: 23495 -> 23495
4514
Cov: 23495 -> 23495
Cov: 23495 -> 23495
4515
Cov: 23495 -> 23495
Cov: 23495 -> 23495
4516
Cov: 23495 -> 23495
Cov: 23495 -> 23495
4517
Cov: 23495 -> 23495
Cov: 23495 -> 23495
4518
Cov: 23495 -> 23495
Cov: 23495 -> 23495
4519
Cov: 23495 -> 23495
Cov: 23495 -> 23495
4520
Cov: 23495 -> 23495
Cov: 23495 -> 23495
4521
Cov: 23495 -> 23495
Cov: 23495 -> 23495
4522
Cov: 23495 -> 23495
Cov: 23495 -> 23495
4523
Cov: 23495 -> 23495
Cov: 23495 -> 23495
4524
Cov: 23495 -> 23495
Cov: 23495 -> 23495
4525
Cov: 23495 -> 23495
Cov: 23495 -> 23495
4526
Cov: 23495 -> 23496
Cov: 23496 -> 23496
4527
Cov: 23496 -> 23496
Cov: 23496 -> 23496
4528
Cov: 23496 -> 23496
Cov: 23496 -> 23496
4529
Cov: 23496 -> 23496
Cov: 23496 -> 23496
4530
Cov: 23496 -> 23497
Cov: 23497 -> 23497
4531
Cov: 23497 -> 23497
Cov: 23497 -> 23497
4532
Cov: 23497 -> 23498
Cov: 23498 -> 23498
4533
Cov: 23498 -> 23498
Cov: 23498 -> 23498
4534
Cov: 23498 -> 23498
Cov: 23498 -> 23498
4535
Cov: 23498 -> 23498
Cov: 23498 -> 23498
4536
Cov: 23498 -> 23498
Cov: 23498 -> 23498
4537
Cov: 23498 -> 23498
Cov: 23498 -> 23498
4538
Cov: 23498 -> 23498
Cov: 23498 -> 23498
4539
Cov: 23498 -> 23498
Cov: 23498 -> 23498
4540
Cov: 23498 -> 23501
Cov: 23501 -> 23501
4541
Cov: 23501 -> 23501
Cov: 23501 -> 23501
4542
Cov: 23501 -> 23501
Cov: 23501 -> 23501
4543
Cov: 23501 -> 23893
Cov: 23893 -> 23893
4544
{"exception": "RuntimeError", "msg": "The expanded size of the tensor (3) must match the existing size (2) at non-singleton dimension 1.  Target sizes: [2, 3, 3].  Tensor sizes: [2, 3]"}
4545
Cov: 23893 -> 23895
Cov: 23895 -> 23895
4546
Cov: 23895 -> 23896
Cov: 23896 -> 23896
4547
Cov: 23896 -> 23896
Cov: 23896 -> 23896
4548
Cov: 23896 -> 23896
Cov: 23896 -> 23896
4549
Cov: 23896 -> 23896
Cov: 23896 -> 23896
4550
Cov: 23896 -> 23901
Cov: 23901 -> 23901
4551
Cov: 23901 -> 23901
Cov: 23901 -> 23901
4552
Cov: 23901 -> 23902
Cov: 23902 -> 23902
4553
Cov: 23902 -> 23902
Cov: 23902 -> 23902
4554
Cov: 23902 -> 23902
Cov: 23902 -> 23902
4555
Cov: 23902 -> 23902
Cov: 23902 -> 23902
4556
Cov: 23902 -> 23902
Cov: 23902 -> 23902
4557
{"exception": "RuntimeError", "msg": "element 0 of tensors does not require grad and does not have a grad_fn"}
4558
Cov: 23902 -> 23903
Cov: 23903 -> 23903
4559
Cov: 23903 -> 23904
Cov: 23904 -> 23904
4560
Cov: 23904 -> 23904
Cov: 23904 -> 23904
4561
Cov: 23904 -> 23904
Cov: 23904 -> 23904
4562
Cov: 23904 -> 23905
Cov: 23905 -> 23905
4563
Cov: 23905 -> 23906
Cov: 23906 -> 23906
4564
{"exception": "RuntimeError", "msg": "element 0 of tensors does not require grad and does not have a grad_fn"}
4565
Cov: 23906 -> 23907
Cov: 23907 -> 23907
4566
Cov: 23907 -> 23908
Cov: 23908 -> 23908
4567
Cov: 23908 -> 23908
Cov: 23908 -> 23908
4568
Cov: 23908 -> 23908
Cov: 23908 -> 23908
4569
Cov: 23908 -> 23908
Cov: 23908 -> 23908
4570
Cov: 23908 -> 23908
Cov: 23908 -> 23908
4571
Cov: 23908 -> 23908
Cov: 23908 -> 23908
4572
Cov: 23908 -> 23908
Cov: 23908 -> 23908
4573
Cov: 23908 -> 23909
Cov: 23909 -> 23909
4574
Cov: 23909 -> 23909
Cov: 23909 -> 23909
4575
Cov: 23909 -> 23909
Cov: 23909 -> 23909
4576
Cov: 23909 -> 23909
Cov: 23909 -> 23909
4577
Cov: 23909 -> 23909
Cov: 23909 -> 23909
4578
Cov: 23909 -> 23910
Cov: 23910 -> 23910
4579
Cov: 23910 -> 23910
Cov: 23910 -> 23910
4580
Cov: 23910 -> 23911
Cov: 23911 -> 23911
4581
Cov: 23911 -> 23911
Cov: 23911 -> 23911
4582
Cov: 23911 -> 23911
Cov: 23911 -> 23911
4583
Cov: 23911 -> 23911
Cov: 23911 -> 23911
4584
Cov: 23911 -> 23911
Cov: 23911 -> 23911
4585
Cov: 23911 -> 23911
Cov: 23911 -> 23911
4586
Cov: 23911 -> 23911
Cov: 23911 -> 23911
4587
Cov: 23911 -> 23911
Cov: 23911 -> 23911
4588
Cov: 23911 -> 23911
Cov: 23911 -> 23911
4589
Cov: 23911 -> 23911
Cov: 23911 -> 23911
4590
Cov: 23911 -> 23911
Cov: 23911 -> 23911
4591
Cov: 23911 -> 23911
Cov: 23911 -> 23911
4592
Cov: 23911 -> 23911
Cov: 23911 -> 23911
4593
Cov: 23911 -> 23911
Cov: 23911 -> 23911
4594
Cov: 23911 -> 23912
Cov: 23912 -> 23912
4595
Cov: 23912 -> 23912
Cov: 23912 -> 23912
4596
Cov: 23912 -> 23912
Cov: 23912 -> 23912
4597
Cov: 23912 -> 23912
Cov: 23912 -> 23912
4598
Cov: 23912 -> 23912
Cov: 23912 -> 23912
4599
{"exception": "RuntimeError", "msg": "element 0 of tensors does not require grad and does not have a grad_fn"}
4600
Cov: 23912 -> 23914
Cov: 23914 -> 23914
4601
Cov: 23914 -> 23914
Cov: 23914 -> 23914
4602
Cov: 23914 -> 23914
Cov: 23914 -> 23914
4603
Cov: 23914 -> 23915
Cov: 23915 -> 23915
4604
Cov: 23915 -> 23916
Cov: 23916 -> 23916
4605
Cov: 23916 -> 23916
Cov: 23916 -> 23916
4606
Cov: 23916 -> 23916
Cov: 23916 -> 23916
4607
{"exception": "TypeError", "msg": "index_put_(): argument 'indices' (position 1) must be tuple of Tensors, not Tensor"}
4608
Cov: 23916 -> 23916
Cov: 23916 -> 23916
4609
Cov: 23916 -> 23917
Cov: 23917 -> 23917
4610
Cov: 23917 -> 23917
Cov: 23917 -> 23917
4611
Cov: 23917 -> 23917
Cov: 23917 -> 23917
4612
Cov: 23917 -> 23917
Cov: 23917 -> 23917
4613
Cov: 23917 -> 23918
Cov: 23918 -> 23918
4614
Cov: 23918 -> 23918
Cov: 23918 -> 23918
4615
Cov: 23918 -> 23918
Cov: 23918 -> 23918
4616
Cov: 23918 -> 23918
Cov: 23918 -> 23918
4617
{"exception": "RuntimeError", "msg": "torch.ormqr: Expected other to have the same number of dimensions as input, but got other.ndim equal to 2 and input.ndim is equal to 3"}
4618
Cov: 23918 -> 23918
Cov: 23918 -> 23918
4619
Cov: 23918 -> 23918
Cov: 23918 -> 23918
4620
Cov: 23918 -> 23918
Cov: 23918 -> 23918
4621
Cov: 23918 -> 23918
Cov: 23918 -> 23918
4622
Cov: 23918 -> 23918
Cov: 23918 -> 23918
4623
Cov: 23918 -> 23918
Cov: 23918 -> 23918
4624
Cov: 23918 -> 23918
Cov: 23918 -> 23918
4625
Cov: 23918 -> 23920
Cov: 23920 -> 23920
4626
Cov: 23920 -> 23920
Cov: 23920 -> 23920
4627
Cov: 23920 -> 23920
Cov: 23920 -> 23920
4628
{"exception": "TypeError", "msg": "new_full(): argument 'size' (position 1) must be tuple of ints, not int"}
4629
Cov: 23920 -> 23921
Cov: 23921 -> 23921
4630
Cov: 23921 -> 23921
Cov: 23921 -> 23921
4631
Cov: 23921 -> 23921
Cov: 23921 -> 23921
4632
Cov: 23921 -> 23921
Cov: 23921 -> 23921
4633
Cov: 23921 -> 23921
Cov: 23921 -> 23921
4634
Cov: 23921 -> 23921
Cov: 23921 -> 23921
4635
Cov: 23921 -> 23922
Cov: 23922 -> 23922
4636
Cov: 23922 -> 23922
Cov: 23922 -> 23922
4637
Cov: 23922 -> 23923
Cov: 23923 -> 23923
4638
Cov: 23923 -> 23923
Cov: 23923 -> 23923
4639
Cov: 23923 -> 23923
Cov: 23923 -> 23923
4640
Cov: 23923 -> 23924
Cov: 23924 -> 23924
4641
Cov: 23924 -> 23925
Cov: 23925 -> 23925
4642
Cov: 23925 -> 23925
Cov: 23925 -> 23925
4643
Cov: 23925 -> 23925
Cov: 23925 -> 23925
4644
Cov: 23925 -> 23925
Cov: 23925 -> 23925
4645
Cov: 23925 -> 23925
Cov: 23925 -> 23925
4646
Cov: 23925 -> 23926
Cov: 23926 -> 23926
4647
Cov: 23926 -> 23927
Cov: 23927 -> 23927
4648
Cov: 23927 -> 23927
Cov: 23927 -> 23927
4649
Cov: 23927 -> 23927
Cov: 23927 -> 23927
4650
Cov: 23927 -> 23927
Cov: 23927 -> 23927
4651
Cov: 23927 -> 23927
Cov: 23927 -> 23927
4652
Cov: 23927 -> 23927
Cov: 23927 -> 23927
4653
Cov: 23927 -> 23928
Cov: 23928 -> 23928
4654
{"exception": "RuntimeError", "msg": "istft requires a complex-valued input tensor matching the output from stft with return_complex=True."}
4655
Cov: 23928 -> 23945
Cov: 23945 -> 23945
4656
Cov: 23945 -> 23945
Cov: 23945 -> 23945
4657
Cov: 23945 -> 23948
Cov: 23948 -> 23948
4658
Cov: 23948 -> 23948
Cov: 23948 -> 23948
4659
Cov: 23948 -> 23948
Cov: 23948 -> 23948
4660
{"exception": "RuntimeError", "msg": "This function was deprecated since version 1.9 and is now removed. `torch.linalg.eig` returns complex tensors of dtype `cfloat` or `cdouble` rather than real tensors mimicking complex tensors.\n\nL, _ = torch.eig(A) should be replaced with:\nL_complex = torch.linalg.eigvals(A)\n\nand\n\nL, V = torch.eig(A, eigenvectors=True) should be replaced with:\nL_complex, V_complex = torch.linalg.eig(A)"}
4661
Cov: 23948 -> 23949
Cov: 23949 -> 23949
4662
Cov: 23949 -> 23949
Cov: 23949 -> 23949
4663
Cov: 23949 -> 23949
Cov: 23949 -> 23949
4664
Cov: 23949 -> 23950
Cov: 23950 -> 23950
4665
Cov: 23950 -> 23950
Cov: 23950 -> 23950
4666
Cov: 23950 -> 23950
Cov: 23950 -> 23950
4667
Cov: 23950 -> 23950
Cov: 23950 -> 23950
4668
Cov: 23950 -> 23950
Cov: 23950 -> 23950
4669
Cov: 23950 -> 23950
Cov: 23950 -> 23950
4670
Cov: 23950 -> 23951
Cov: 23951 -> 23951
4671
Cov: 23951 -> 23951
Cov: 23951 -> 23951
4672
Cov: 23951 -> 23951
Cov: 23951 -> 23951
4673
Cov: 23951 -> 23952
Cov: 23952 -> 23952
4674
Cov: 23952 -> 23952
Cov: 23952 -> 23952
4675
Cov: 23952 -> 23952
Cov: 23952 -> 23952
4676
Cov: 23952 -> 23952
Cov: 23952 -> 23952
4677
Cov: 23952 -> 23952
Cov: 23952 -> 23952
4678
Cov: 23952 -> 23952
Cov: 23952 -> 23952
4679
Cov: 23952 -> 23953
Cov: 23953 -> 23953
4680
Cov: 23953 -> 23953
Cov: 23953 -> 23953
4681
Cov: 23953 -> 23953
Cov: 23953 -> 23953
4682
Cov: 23953 -> 23953
Cov: 23953 -> 23953
4683
{"exception": "RuntimeError", "msg": "1D tensors expected, but got 2D and 2D tensors"}
4684
{"exception": "RuntimeError", "msg": "This function was deprecated since version 1.9 and is now removed. The default behavior has changed from using the upper triangular portion of the matrix by default to using the lower triangular portion.\n\nL, _ = torch.symeig(A, upper=upper) should be replaced with:\nL = torch.linalg.eigvalsh(A, UPLO='U' if upper else 'L')\n\nand\n\nL, V = torch.symeig(A, eigenvectors=True) should be replaced with:\nL, V = torch.linalg.eigh(A, UPLO='U' if upper else 'L')"}
4685
Cov: 23953 -> 23955
Cov: 23955 -> 23955
4686
Cov: 23955 -> 23955
Cov: 23955 -> 23955
4687
Cov: 23955 -> 23955
Cov: 23955 -> 23955
4688
Cov: 23955 -> 23955
Cov: 23955 -> 23955
4689
Cov: 23955 -> 23955
Cov: 23955 -> 23955
4690
Cov: 23955 -> 23956
Cov: 23956 -> 23956
4691
Cov: 23956 -> 23956
Cov: 23956 -> 23956
4692
Cov: 23956 -> 23956
Cov: 23956 -> 23956
4693
Cov: 23956 -> 23956
Cov: 23956 -> 23956
4694
Cov: 23956 -> 23956
Cov: 23956 -> 23956
4695
Cov: 23956 -> 23956
Cov: 23956 -> 23956
4696
Cov: 23956 -> 23956
Cov: 23956 -> 23956
4697
Cov: 23956 -> 23959
Cov: 23959 -> 23959
4698
Cov: 23959 -> 23960
Cov: 23960 -> 23960
4699
Cov: 23960 -> 23960
Cov: 23960 -> 23960
4700
Cov: 23960 -> 23960
Cov: 23960 -> 23960
4701
Cov: 23960 -> 23960
Cov: 23960 -> 23960
4702
Cov: 23960 -> 23961
Cov: 23961 -> 23961
4703
Cov: 23961 -> 23961
Cov: 23961 -> 23961
4704
Cov: 23961 -> 23961
Cov: 23961 -> 23961
4705
Cov: 23961 -> 23961
Cov: 23961 -> 23961
4706
Cov: 23961 -> 23962
Cov: 23962 -> 23962
4707
{"exception": "RuntimeError", "msg": "1D tensors expected, but got 2D and 2D tensors"}
4708
Cov: 23962 -> 23963
Cov: 23963 -> 23963
4709
{"exception": "TypeError", "msg": "tile(): argument 'dims' must be tuple of ints, not int"}
4710
Cov: 23963 -> 23964
Cov: 23964 -> 23964
4711
Cov: 23964 -> 23964
Cov: 23964 -> 23964
4712
Cov: 23964 -> 23965
Cov: 23965 -> 23965
4713
{"exception": "TypeError", "msg": "arctanh_() takes no arguments (1 given)"}
4714
Cov: 23965 -> 23965
Cov: 23965 -> 23965
4715
Cov: 23965 -> 23965
Cov: 23965 -> 23965
4716
Cov: 23965 -> 23965
Cov: 23965 -> 23965
4717
Cov: 23965 -> 23965
Cov: 23965 -> 23965
4718
{"exception": "RuntimeError", "msg": "Expected nested_tensorlist[0].size() > 0 to be true, but got false.  (Could this error message be improved?  If so, please report an enhancement request to PyTorch.)"}
4719
Cov: 23965 -> 23966
Cov: 23966 -> 23966
4720
Cov: 23966 -> 23966
Cov: 23966 -> 23966
4721
Cov: 23966 -> 23966
Cov: 23966 -> 23966
4722
Cov: 23966 -> 23966
Cov: 23966 -> 23966
4723
Cov: 23966 -> 23966
Cov: 23966 -> 23966
4724
Cov: 23966 -> 23966
Cov: 23966 -> 23966
4725
Cov: 23966 -> 23966
Cov: 23966 -> 23966
4726
Cov: 23966 -> 23966
Cov: 23966 -> 23966
4727
Cov: 23966 -> 23966
Cov: 23966 -> 23966
4728
Cov: 23966 -> 23967
Cov: 23967 -> 23967
4729
{"exception": "RuntimeError", "msg": "element 0 of tensors does not require grad and does not have a grad_fn"}
4730
Cov: 23967 -> 23968
Cov: 23968 -> 23968
4731
Cov: 23968 -> 23969
Cov: 23969 -> 23969
4732
Cov: 23969 -> 23969
Cov: 23969 -> 23969
4733
Cov: 23969 -> 23969
Cov: 23969 -> 23969
4734
Cov: 23969 -> 23969
Cov: 23969 -> 23969
4735
{"exception": "NameError", "msg": "name '_input_tensor' is not defined"}
4736
{"exception": "TypeError", "msg": "equal(): argument 'other' (position 1) must be Tensor, not float"}
4737
Cov: 23969 -> 23969
Cov: 23969 -> 23969
4738
Cov: 23969 -> 23969
Cov: 23969 -> 23969
4739
Cov: 23969 -> 23969
Cov: 23969 -> 23969
4740
Cov: 23969 -> 23969
Cov: 23969 -> 23969
4741
{"exception": "RuntimeError", "msg": "This function was deprecated since version 1.9 and is now removed. `torch.solve` is deprecated in favor of `torch.linalg.solve`. `torch.linalg.solve` has its arguments reversed and does not return the LU factorization.\n\nTo get the LU factorization see `torch.lu`, which can be used with `torch.lu_solve` or `torch.lu_unpack`.\nX = torch.solve(B, A).solution should be replaced with:\nX = torch.linalg.solve(A, B)"}
4742
Cov: 23969 -> 23974
Cov: 23974 -> 23974
4743
Cov: 23974 -> 23974
Cov: 23974 -> 23974
4744
Cov: 23974 -> 23974
Cov: 23974 -> 23974
4745
Cov: 23974 -> 23974
Cov: 23974 -> 23974
4746
Cov: 23974 -> 23975
Cov: 23975 -> 23975
4747
Cov: 23975 -> 23975
Cov: 23975 -> 23975
4748
Cov: 23975 -> 23976
Cov: 23976 -> 23976
4749
Cov: 23976 -> 23976
Cov: 23976 -> 23976
4750
Cov: 23976 -> 23977
Cov: 23977 -> 23977
4751
Cov: 23977 -> 23977
Cov: 23977 -> 23977
4752
Cov: 23977 -> 23977
Cov: 23977 -> 23977
4753
Cov: 23977 -> 23978
Cov: 23978 -> 23978
4754
Cov: 23978 -> 23978
Cov: 23978 -> 23978
4755
Cov: 23978 -> 23978
Cov: 23978 -> 23978
4756
Cov: 23978 -> 23989
Cov: 23989 -> 23989
4757
Cov: 23989 -> 23989
Cov: 23989 -> 23989
4758
{"exception": "TypeError", "msg": "descriptor 'new_tensor' for 'torch._C._TensorBase' objects doesn't apply to a 'list' object"}
4759
Cov: 23989 -> 23989
Cov: 23989 -> 23989
4760
Cov: 23989 -> 23989
Cov: 23989 -> 23989
4761
Cov: 23989 -> 23989
Cov: 23989 -> 23989
4762
Cov: 23989 -> 23989
Cov: 23989 -> 23989
4763
Cov: 23989 -> 23989
Cov: 23989 -> 23989
4764
{"exception": "TypeError", "msg": "index_add_() missing 1 required positional arguments: \"source\""}
4765
Cov: 23989 -> 23989
Cov: 23989 -> 23989
4766
Cov: 23989 -> 23989
Cov: 23989 -> 23989
4767
Cov: 23989 -> 23989
Cov: 23989 -> 23989
4768
Cov: 23989 -> 23989
Cov: 23989 -> 23989
4769
Cov: 23989 -> 23989
Cov: 23989 -> 23989
4770
Cov: 23989 -> 23989
Cov: 23989 -> 23989
4771
{"exception": "RuntimeError", "msg": "The expanded size of the tensor (3) must match the existing size (2) at non-singleton dimension 0.  Target sizes: [3, 3].  Tensor sizes: [2, 3]"}
4772
{"exception": "RuntimeError", "msg": "stft requires the return_complex parameter be given for real inputs, and will further require that return_complex=True in a future PyTorch release."}
4773
{"exception": "NameError", "msg": "name 'from_type' is not defined"}
4774
Cov: 23989 -> 24000
Cov: 24000 -> 24000
4775
Cov: 24000 -> 24077
Cov: 24077 -> 24077
4776
Cov: 24077 -> 24077
Cov: 24077 -> 24077
4777
Cov: 24077 -> 24078
Cov: 24078 -> 24078
4778
Cov: 24078 -> 24078
Cov: 24078 -> 24078
4779
Cov: 24078 -> 24078
Cov: 24078 -> 24078
4780
Cov: 24078 -> 24078
Cov: 24078 -> 24078
4781
Cov: 24078 -> 24078
Cov: 24078 -> 24078
4782
Cov: 24078 -> 24078
Cov: 24078 -> 24078
4783
{"exception": "RuntimeError", "msg": "\"histogramdd\" not implemented for 'Int'"}
4784
Cov: 24078 -> 24078
Cov: 24078 -> 24078
4785
Cov: 24078 -> 24078
Cov: 24078 -> 24078
4786
Cov: 24078 -> 24078
Cov: 24078 -> 24078
4787
Cov: 24078 -> 24078
Cov: 24078 -> 24078
4788
Cov: 24078 -> 24078
Cov: 24078 -> 24078
4789
Cov: 24078 -> 24078
Cov: 24078 -> 24078
4790
Cov: 24078 -> 24078
Cov: 24078 -> 24078
4791
Cov: 24078 -> 24078
Cov: 24078 -> 24078
4792
Cov: 24078 -> 24078
Cov: 24078 -> 24078
4793
Cov: 24078 -> 24078
Cov: 24078 -> 24078
4794
Cov: 24078 -> 24078
Cov: 24078 -> 24078
4795
Cov: 24078 -> 24078
Cov: 24078 -> 24078
4796
Cov: 24078 -> 24078
Cov: 24078 -> 24078
4797
Cov: 24078 -> 24078
Cov: 24078 -> 24078
4798
Cov: 24078 -> 24080
Cov: 24080 -> 24080
4799
Cov: 24080 -> 24080
Cov: 24080 -> 24080
4800
Cov: 24080 -> 24082
Cov: 24082 -> 24082
4801
Cov: 24082 -> 24082
Cov: 24082 -> 24082
4802
Cov: 24082 -> 24083
Cov: 24083 -> 24083
4803
Cov: 24083 -> 24083
Cov: 24083 -> 24083
4804
Cov: 24083 -> 24083
Cov: 24083 -> 24083
4805
Cov: 24083 -> 24083
Cov: 24083 -> 24083
4806
Cov: 24083 -> 24083
Cov: 24083 -> 24083
4807
Cov: 24083 -> 24083
Cov: 24083 -> 24083
4808
{"exception": "RuntimeError", "msg": "inner() the last dimension must match on both input tensors but got shapes [4, 3] and [3, 4]"}
4809
Cov: 24083 -> 24083
Cov: 24083 -> 24083
4810
Cov: 24083 -> 24083
Cov: 24083 -> 24083
4811
Cov: 24083 -> 24083
Cov: 24083 -> 24083
4812
Cov: 24083 -> 24083
Cov: 24083 -> 24083
4813
Cov: 24083 -> 24083
Cov: 24083 -> 24083
4814
Cov: 24083 -> 24083
Cov: 24083 -> 24083
4815
Cov: 24083 -> 24083
Cov: 24083 -> 24083
4816
Cov: 24083 -> 24083
Cov: 24083 -> 24083
4817
Cov: 24083 -> 24083
Cov: 24083 -> 24083
4818
Cov: 24083 -> 24083
Cov: 24083 -> 24083
4819
{"exception": "RuntimeError", "msg": "any: You passed a dimname (string) to this op in place of a dimension index but it does not yet support this behavior. Please pass a dimension index to work around this."}
4820
Cov: 24083 -> 24083
Cov: 24083 -> 24083
4821
Cov: 24083 -> 24083
Cov: 24083 -> 24083
4822
Cov: 24083 -> 24083
Cov: 24083 -> 24083
4823
Cov: 24083 -> 24083
Cov: 24083 -> 24083
4824
Cov: 24083 -> 24083
Cov: 24083 -> 24083
4825
Cov: 24083 -> 24083
Cov: 24083 -> 24083
4826
Cov: 24083 -> 24083
Cov: 24083 -> 24083
4827
Cov: 24083 -> 24083
Cov: 24083 -> 24083
4828
Cov: 24083 -> 24092
Cov: 24092 -> 24092
4829
Cov: 24092 -> 24093
Cov: 24093 -> 24093
4830
Cov: 24093 -> 24093
Cov: 24093 -> 24093
4831
Cov: 24093 -> 24093
Cov: 24093 -> 24093
4832
Cov: 24093 -> 24093
Cov: 24093 -> 24093
4833
Cov: 24093 -> 24094
Cov: 24094 -> 24094
4834
Cov: 24094 -> 24094
Cov: 24094 -> 24094
4835
Cov: 24094 -> 24094
Cov: 24094 -> 24094
4836
Cov: 24094 -> 24094
Cov: 24094 -> 24094
4837
Cov: 24094 -> 24095
Cov: 24095 -> 24095
4838
Cov: 24095 -> 24095
Cov: 24095 -> 24095
4839
Cov: 24095 -> 24095
Cov: 24095 -> 24095
4840
Cov: 24095 -> 24097
Cov: 24097 -> 24097
4841
Cov: 24097 -> 24097
Cov: 24097 -> 24097
4842
Cov: 24097 -> 24097
Cov: 24097 -> 24097
4843
Cov: 24097 -> 24098
Cov: 24098 -> 24098
4844
Cov: 24098 -> 24098
Cov: 24098 -> 24098
4845
Cov: 24098 -> 24098
Cov: 24098 -> 24098
4846
Cov: 24098 -> 24098
Cov: 24098 -> 24098
4847
Cov: 24098 -> 24098
Cov: 24098 -> 24098
4848
Cov: 24098 -> 24098
Cov: 24098 -> 24098
4849
Cov: 24098 -> 24098
Cov: 24098 -> 24098
4850
Cov: 24098 -> 24099
Cov: 24099 -> 24099
4851
Cov: 24099 -> 24099
Cov: 24099 -> 24099
4852
Cov: 24099 -> 24099
Cov: 24099 -> 24099
4853
Cov: 24099 -> 24099
Cov: 24099 -> 24099
4854
Cov: 24099 -> 24099
Cov: 24099 -> 24099
4855
Cov: 24099 -> 24100
Cov: 24100 -> 24100
4856
Cov: 24100 -> 24100
Cov: 24100 -> 24100
4857
Cov: 24100 -> 24100
Cov: 24100 -> 24100
4858
Cov: 24100 -> 24100
Cov: 24100 -> 24100
4859
Cov: 24100 -> 24100
Cov: 24100 -> 24100
4860
Cov: 24100 -> 24100
Cov: 24100 -> 24100
4861
Cov: 24100 -> 24100
Cov: 24100 -> 24100
4862
Cov: 24100 -> 24101
Cov: 24101 -> 24101
4863
Cov: 24101 -> 24101
Cov: 24101 -> 24101
4864
Cov: 24101 -> 24101
Cov: 24101 -> 24101
4865
Cov: 24101 -> 24101
Cov: 24101 -> 24101
4866
Cov: 24101 -> 24156
Cov: 24156 -> 24156
4867
Cov: 24156 -> 24156
Cov: 24156 -> 24156
4868
Cov: 24156 -> 24156
Cov: 24156 -> 24156
4869
Cov: 24156 -> 24156
Cov: 24156 -> 24156
4870
{"exception": "RuntimeError", "msg": "This function was deprecated since version 1.9 and is now removed. `torch.linalg.eig` returns complex tensors of dtype `cfloat` or `cdouble` rather than real tensors mimicking complex tensors.\n\nL, _ = torch.eig(A) should be replaced with:\nL_complex = torch.linalg.eigvals(A)\n\nand\n\nL, V = torch.eig(A, eigenvectors=True) should be replaced with:\nL_complex, V_complex = torch.linalg.eig(A)"}
4871
Cov: 24156 -> 24157
Cov: 24157 -> 24157
4872
{"exception": "TypeError", "msg": "'getset_descriptor' object is not callable"}
4873
Cov: 24157 -> 24158
Cov: 24158 -> 24158
4874
Cov: 24158 -> 24159
Cov: 24159 -> 24159
4875
Cov: 24159 -> 24159
Cov: 24159 -> 24159
4876
Cov: 24159 -> 24160
Cov: 24160 -> 24160
4877
Cov: 24160 -> 24160
Cov: 24160 -> 24160
4878
{"exception": "RuntimeError", "msg": "Please look up dimensions by name, got: name = None."}
4879
Cov: 24160 -> 24161
Cov: 24161 -> 24161
4880
{"exception": "RuntimeError", "msg": "logdet: A must be batches of square matrices, but they are 4 by 5 matrices"}
4881
Cov: 24161 -> 24162
Cov: 24162 -> 24162
4882
Cov: 24162 -> 24162
Cov: 24162 -> 24162
4883
Cov: 24162 -> 24162
Cov: 24162 -> 24162
4884
Cov: 24162 -> 24163
Cov: 24163 -> 24163
4885
Cov: 24163 -> 24163
Cov: 24163 -> 24163
4886
Cov: 24163 -> 24163
Cov: 24163 -> 24163
4887
Cov: 24163 -> 24166
Cov: 24166 -> 24166
4888
Cov: 24166 -> 24166
Cov: 24166 -> 24166
4889
Cov: 24166 -> 24170
Cov: 24170 -> 24170
4890
Cov: 24170 -> 24170
Cov: 24170 -> 24170
4891
Cov: 24170 -> 24170
Cov: 24170 -> 24170
4892
Cov: 24170 -> 24170
Cov: 24170 -> 24170
4893
Cov: 24170 -> 24170
Cov: 24170 -> 24170
4894
Cov: 24170 -> 24170
Cov: 24170 -> 24170
4895
Cov: 24170 -> 24170
Cov: 24170 -> 24170
4896
Cov: 24170 -> 24170
Cov: 24170 -> 24170
4897
Cov: 24170 -> 24171
Cov: 24171 -> 24171
4898
{"exception": "RuntimeError", "msg": "element 0 of tensors does not require grad and does not have a grad_fn"}
4899
Cov: 24171 -> 24172
Cov: 24172 -> 24172
4900
Cov: 24172 -> 24172
Cov: 24172 -> 24172
4901
Cov: 24172 -> 24172
Cov: 24172 -> 24172
4902
Cov: 24172 -> 24172
Cov: 24172 -> 24172
4903
Cov: 24172 -> 24173
Cov: 24173 -> 24173
4904
Cov: 24173 -> 24173
Cov: 24173 -> 24173
4905
Cov: 24173 -> 24173
Cov: 24173 -> 24173
4906
{"exception": "TypeError", "msg": "hsplit() received an invalid combination of arguments - got (split_size_or_sections=int, ), but expected one of:\n * (int sections)\n      didn't match because some of the keywords were incorrect: split_size_or_sections\n * (tuple of ints indices)\n      didn't match because some of the keywords were incorrect: split_size_or_sections\n"}
4907
Cov: 24173 -> 24173
Cov: 24173 -> 24173
4908
Cov: 24173 -> 24173
Cov: 24173 -> 24173
4909
{"exception": "TypeError", "msg": "symeig() got an unexpected keyword argument 'upper'"}
4910
Cov: 24173 -> 24174
Cov: 24174 -> 24174
4911
Cov: 24174 -> 24174
Cov: 24174 -> 24174
4912
Cov: 24174 -> 24174
Cov: 24174 -> 24174
4913
Cov: 24174 -> 24175
Cov: 24175 -> 24175
4914
Cov: 24175 -> 24175
Cov: 24175 -> 24175
4915
Cov: 24175 -> 24175
Cov: 24175 -> 24175
4916
Cov: 24175 -> 24175
Cov: 24175 -> 24175
4917
Cov: 24175 -> 24181
Cov: 24181 -> 24181
4918
Cov: 24181 -> 24181
Cov: 24181 -> 24181
4919
Cov: 24181 -> 24181
Cov: 24181 -> 24181
4920
Cov: 24181 -> 24181
Cov: 24181 -> 24181
4921
Cov: 24181 -> 24181
Cov: 24181 -> 24181
4922
Cov: 24181 -> 24181
Cov: 24181 -> 24181
4923
Cov: 24181 -> 24182
Cov: 24182 -> 24182
4924
Cov: 24182 -> 24182
Cov: 24182 -> 24182
4925
Cov: 24182 -> 24182
Cov: 24182 -> 24182
4926
Cov: 24182 -> 24182
Cov: 24182 -> 24182
4927
Cov: 24182 -> 24182
Cov: 24182 -> 24182
4928
{"exception": "TypeError", "msg": "'getset_descriptor' object is not callable"}
4929
Cov: 24182 -> 24182
Cov: 24182 -> 24182
4930
{"exception": "TypeError", "msg": "index_put_(): argument 'indices' (position 1) must be tuple of Tensors, not Tensor"}
4931
Cov: 24182 -> 24182
Cov: 24182 -> 24182
4932
Cov: 24182 -> 24182
Cov: 24182 -> 24182
4933
Cov: 24182 -> 24182
Cov: 24182 -> 24182
4934
{"exception": "TypeError", "msg": "scatter_() received an invalid combination of arguments - got (int, Tensor, Tensor, NoneType), but expected one of:\n * (int dim, Tensor index, Tensor src)\n * (int dim, Tensor index, Tensor src, *, str reduce)\n * (int dim, Tensor index, Number value)\n * (int dim, Tensor index, Number value, *, str reduce)\n"}
4935
Cov: 24182 -> 24182
Cov: 24182 -> 24182
4936
Cov: 24182 -> 24182
Cov: 24182 -> 24182
4937
Cov: 24182 -> 24182
Cov: 24182 -> 24182
4938
Cov: 24182 -> 24182
Cov: 24182 -> 24182
4939
Cov: 24182 -> 24182
Cov: 24182 -> 24182
4940
Cov: 24182 -> 24183
Cov: 24183 -> 24183
4941
Cov: 24183 -> 24184
Cov: 24184 -> 24184
4942
Cov: 24184 -> 24184
Cov: 24184 -> 24184
4943
Cov: 24184 -> 24184
Cov: 24184 -> 24184
4944
Cov: 24184 -> 24184
Cov: 24184 -> 24184
4945
Cov: 24184 -> 24185
Cov: 24185 -> 24185
4946
Cov: 24185 -> 24185
Cov: 24185 -> 24185
4947
Cov: 24185 -> 24185
Cov: 24185 -> 24185
4948
{"exception": "TypeError", "msg": "index_put(): argument 'indices' (position 1) must be tuple of Tensors, not Tensor"}
4949
Cov: 24185 -> 24185
Cov: 24185 -> 24185
4950
Cov: 24185 -> 24185
Cov: 24185 -> 24185
4951
Cov: 24185 -> 24185
Cov: 24185 -> 24185
4952
Cov: 24185 -> 24185
Cov: 24185 -> 24185
4953
{"exception": "RuntimeError", "msg": "vector + matrix @ vector expected, got 2, 2, 1"}
4954
Cov: 24185 -> 24185
Cov: 24185 -> 24185
4955
Cov: 24185 -> 24185
Cov: 24185 -> 24185
4956
Cov: 24185 -> 24186
Cov: 24186 -> 24186
4957
Cov: 24186 -> 24186
Cov: 24186 -> 24186
4958
Cov: 24186 -> 24186
Cov: 24186 -> 24186
4959
Cov: 24186 -> 24186
Cov: 24186 -> 24186
4960
Cov: 24186 -> 24186
Cov: 24186 -> 24186
4961
Cov: 24186 -> 24186
Cov: 24186 -> 24186
4962
Cov: 24186 -> 24186
Cov: 24186 -> 24186
4963
Cov: 24186 -> 24187
Cov: 24187 -> 24187
4964
Cov: 24187 -> 24194
Cov: 24194 -> 24194
4965
Cov: 24194 -> 24194
Cov: 24194 -> 24194
4966
{"exception": "RuntimeError", "msg": "The expanded size of the tensor (5) must match the existing size (3) at non-singleton dimension 2.  Target sizes: [2, 3, 5].  Tensor sizes: [2, 3]"}
4967
Cov: 24194 -> 24194
Cov: 24194 -> 24194
4968
Cov: 24194 -> 24194
Cov: 24194 -> 24194
4969
Cov: 24194 -> 24196
Cov: 24196 -> 24196
4970
Cov: 24196 -> 24197
Cov: 24197 -> 24197
4971
Cov: 24197 -> 24198
Cov: 24198 -> 24198
4972
Cov: 24198 -> 24198
Cov: 24198 -> 24198
4973
Cov: 24198 -> 24198
Cov: 24198 -> 24198
4974
Cov: 24198 -> 24199
Cov: 24199 -> 24199
4975
Cov: 24199 -> 24199
Cov: 24199 -> 24199
4976
Cov: 24199 -> 24199
Cov: 24199 -> 24199
4977
Cov: 24199 -> 24199
Cov: 24199 -> 24199
4978
Cov: 24199 -> 24200
Cov: 24200 -> 24200
4979
Cov: 24200 -> 24200
Cov: 24200 -> 24200
4980
Cov: 24200 -> 24200
Cov: 24200 -> 24200
4981
Cov: 24200 -> 24200
Cov: 24200 -> 24200
4982
Cov: 24200 -> 24200
Cov: 24200 -> 24200
4983
Cov: 24200 -> 24200
Cov: 24200 -> 24200
4984
Cov: 24200 -> 24201
Cov: 24201 -> 24201
4985
Cov: 24201 -> 24201
Cov: 24201 -> 24201
4986
Cov: 24201 -> 24201
Cov: 24201 -> 24201
4987
Cov: 24201 -> 24201
Cov: 24201 -> 24201
4988
Cov: 24201 -> 24201
Cov: 24201 -> 24201
4989
Cov: 24201 -> 24201
Cov: 24201 -> 24201
4990
Cov: 24201 -> 24201
Cov: 24201 -> 24201
4991
Cov: 24201 -> 24202
Cov: 24202 -> 24202
4992
Cov: 24202 -> 24202
Cov: 24202 -> 24202
4993
Cov: 24202 -> 24202
Cov: 24202 -> 24202
4994
Cov: 24202 -> 24202
Cov: 24202 -> 24202
4995
Cov: 24202 -> 24202
Cov: 24202 -> 24202
4996
Cov: 24202 -> 24202
Cov: 24202 -> 24202
4997
Cov: 24202 -> 24202
Cov: 24202 -> 24202
4998
Cov: 24202 -> 24203
Cov: 24203 -> 24203
4999
Cov: 24203 -> 24203
Cov: 24203 -> 24203
5000
{"exception": "RuntimeError", "msg": "masked_select: expected BoolTensor for mask"}
5001
Cov: 24203 -> 24203
Cov: 24203 -> 24203
5002
Cov: 24203 -> 24204
Cov: 24204 -> 24204
5003
Cov: 24204 -> 24206
Cov: 24206 -> 24206
5004
Cov: 24206 -> 24208
Cov: 24208 -> 24208
5005
Cov: 24208 -> 24211
Cov: 24211 -> 24211
5006
Cov: 24211 -> 24211
Cov: 24211 -> 24211
5007
Cov: 24211 -> 24211
Cov: 24211 -> 24211
5008
Cov: 24211 -> 24211
Cov: 24211 -> 24211
5009
Cov: 24211 -> 24213
Cov: 24213 -> 24213
5010
Cov: 24213 -> 24213
Cov: 24213 -> 24213
5011
Cov: 24213 -> 24214
Cov: 24214 -> 24214
5012
Cov: 24214 -> 24214
Cov: 24214 -> 24214
5013
Cov: 24214 -> 24214
Cov: 24214 -> 24214
5014
Cov: 24214 -> 24214
Cov: 24214 -> 24214
5015
Cov: 24214 -> 24216
Cov: 24216 -> 24216
5016
Cov: 24216 -> 24217
Cov: 24217 -> 24217
5017
Cov: 24217 -> 24217
Cov: 24217 -> 24217
5018
Cov: 24217 -> 24217
Cov: 24217 -> 24217
5019
Cov: 24217 -> 24217
Cov: 24217 -> 24217
5020
Cov: 24217 -> 24217
Cov: 24217 -> 24217
5021
Cov: 24217 -> 24217
Cov: 24217 -> 24217
5022
Cov: 24217 -> 24217
Cov: 24217 -> 24217
5023
Cov: 24217 -> 24217
Cov: 24217 -> 24217
5024
Cov: 24217 -> 24217
Cov: 24217 -> 24217
5025
Cov: 24217 -> 24217
Cov: 24217 -> 24217
5026
Cov: 24217 -> 24218
Cov: 24218 -> 24218
5027
Cov: 24218 -> 24218
Cov: 24218 -> 24218
5028
Cov: 24218 -> 24233
Cov: 24233 -> 24233
5029
Cov: 24233 -> 24234
Cov: 24234 -> 24234
5030
Cov: 24234 -> 24234
Cov: 24234 -> 24234
5031
Cov: 24234 -> 24234
Cov: 24234 -> 24234
5032
Cov: 24234 -> 24235
Cov: 24235 -> 24235
5033
Cov: 24235 -> 24235
Cov: 24235 -> 24235
5034
Cov: 24235 -> 24235
Cov: 24235 -> 24235
5035
Cov: 24235 -> 24236
Cov: 24236 -> 24236
5036
Cov: 24236 -> 24236
Cov: 24236 -> 24236
5037
Cov: 24236 -> 24236
Cov: 24236 -> 24236
5038
Cov: 24236 -> 24237
Cov: 24237 -> 24237
5039
Cov: 24237 -> 24237
Cov: 24237 -> 24237
5040
Cov: 24237 -> 24238
Cov: 24238 -> 24238
5041
Cov: 24238 -> 24238
Cov: 24238 -> 24238
5042
Cov: 24238 -> 24238
Cov: 24238 -> 24238
5043
Cov: 24238 -> 24238
Cov: 24238 -> 24238
5044
Cov: 24238 -> 24238
Cov: 24238 -> 24238
5045
Cov: 24238 -> 24238
Cov: 24238 -> 24238
5046
Cov: 24238 -> 24238
Cov: 24238 -> 24238
5047
Cov: 24238 -> 24238
Cov: 24238 -> 24238
5048
Cov: 24238 -> 24238
Cov: 24238 -> 24238
5049
Cov: 24238 -> 24238
Cov: 24238 -> 24238
5050
Cov: 24238 -> 24238
Cov: 24238 -> 24238
5051
Cov: 24238 -> 24238
Cov: 24238 -> 24238
5052
Cov: 24238 -> 24238
Cov: 24238 -> 24238
5053
Cov: 24238 -> 24238
Cov: 24238 -> 24238
5054
Cov: 24238 -> 24239
Cov: 24239 -> 24239
5055
Cov: 24239 -> 24239
Cov: 24239 -> 24239
5056
Cov: 24239 -> 24239
Cov: 24239 -> 24239
5057
Cov: 24239 -> 24239
Cov: 24239 -> 24239
5058
Cov: 24239 -> 24239
Cov: 24239 -> 24239
5059
Cov: 24239 -> 24239
Cov: 24239 -> 24239
5060
Cov: 24239 -> 24239
Cov: 24239 -> 24239
5061
Cov: 24239 -> 24239
Cov: 24239 -> 24239
5062
Cov: 24239 -> 24239
Cov: 24239 -> 24239
5063
Cov: 24239 -> 24239
Cov: 24239 -> 24239
5064
Cov: 24239 -> 24239
Cov: 24239 -> 24239
5065
Cov: 24239 -> 24240
Cov: 24240 -> 24240
5066
Cov: 24240 -> 24245
Cov: 24245 -> 24245
5067
Cov: 24245 -> 24246
Cov: 24246 -> 24246
5068
Cov: 24246 -> 24246
Cov: 24246 -> 24246
5069
{"exception": "RuntimeError", "msg": "tensor.sspaddmm(...) can only be called on sparse tensors"}
5070
Cov: 24246 -> 24246
Cov: 24246 -> 24246
5071
Cov: 24246 -> 24246
Cov: 24246 -> 24246
5072
{"exception": "RuntimeError", "msg": "linalg.det: Expected a floating point or complex tensor as input. Got Long"}
5073
Cov: 24246 -> 24247
Cov: 24247 -> 24247
5074
Cov: 24247 -> 24247
Cov: 24247 -> 24247
5075
Cov: 24247 -> 24247
Cov: 24247 -> 24247
5076
Cov: 24247 -> 24247
Cov: 24247 -> 24247
5077
Cov: 24247 -> 24247
Cov: 24247 -> 24247
5078
Cov: 24247 -> 24256
Cov: 24256 -> 24256
5079
Cov: 24256 -> 24257
Cov: 24257 -> 24257
5080
{"exception": "RuntimeError", "msg": "expand(torch.FloatTensor{[10, 3, 4]}, size=[3, 5]): the number of sizes provided (2) must be greater or equal to the number of dimensions in the tensor (3)"}
5081
Cov: 24257 -> 24257
Cov: 24257 -> 24257
5082
Cov: 24257 -> 24258
Cov: 24258 -> 24258
5083
Cov: 24258 -> 24258
Cov: 24258 -> 24258
5084
Cov: 24258 -> 24258
Cov: 24258 -> 24258
5085
Cov: 24258 -> 24258
Cov: 24258 -> 24258
5086
Cov: 24258 -> 24259
Cov: 24259 -> 24259
5087
Cov: 24259 -> 24259
Cov: 24259 -> 24259
5088
Cov: 24259 -> 24259
Cov: 24259 -> 24259
5089
{"exception": "TypeError", "msg": "'getset_descriptor' object is not callable"}
5090
Cov: 24259 -> 24259
Cov: 24259 -> 24259
5091
Cov: 24259 -> 24259
Cov: 24259 -> 24259
5092
{"exception": "TypeError", "msg": "copy_() missing 1 required positional arguments: \"other\""}
5093
Cov: 24259 -> 24259
Cov: 24259 -> 24259
5094
Cov: 24259 -> 24259
Cov: 24259 -> 24259
5095
Cov: 24259 -> 24259
Cov: 24259 -> 24259
5096
Cov: 24259 -> 24259
Cov: 24259 -> 24259
5097
Cov: 24259 -> 24259
Cov: 24259 -> 24259
5098
Cov: 24259 -> 24260
Cov: 24260 -> 24260
5099
Cov: 24260 -> 24260
Cov: 24260 -> 24260
5100
Cov: 24260 -> 24260
Cov: 24260 -> 24260
5101
Cov: 24260 -> 24263
Cov: 24263 -> 24263
5102
Cov: 24263 -> 24263
Cov: 24263 -> 24263
5103
Cov: 24263 -> 24263
Cov: 24263 -> 24263
5104
Cov: 24263 -> 24263
Cov: 24263 -> 24263
5105
Cov: 24263 -> 24263
Cov: 24263 -> 24263
5106
Cov: 24263 -> 24263
Cov: 24263 -> 24263
5107
{"exception": "RuntimeError", "msg": "\"lshift_cpu\" not implemented for 'Float'"}
5108
Cov: 24263 -> 24263
Cov: 24263 -> 24263
5109
Cov: 24263 -> 24263
Cov: 24263 -> 24263
5110
Cov: 24263 -> 24263
Cov: 24263 -> 24263
5111
Cov: 24263 -> 24264
Cov: 24264 -> 24264
5112
Cov: 24264 -> 24264
Cov: 24264 -> 24264
5113
Cov: 24264 -> 24265
Cov: 24265 -> 24265
5114
Cov: 24265 -> 24273
Cov: 24273 -> 24273
5115
Cov: 24273 -> 24273
Cov: 24273 -> 24273
5116
Cov: 24273 -> 24273
Cov: 24273 -> 24273
5117
Cov: 24273 -> 24274
Cov: 24274 -> 24274
5118
{"exception": "TypeError", "msg": "igamma(): argument 'other' must be Tensor, not int"}
5119
Cov: 24274 -> 24274
Cov: 24274 -> 24274
5120
Cov: 24274 -> 24274
Cov: 24274 -> 24274
5121
Cov: 24274 -> 24274
Cov: 24274 -> 24274
5122
Cov: 24274 -> 24274
Cov: 24274 -> 24274
5123
Cov: 24274 -> 24274
Cov: 24274 -> 24274
5124
Cov: 24274 -> 24274
Cov: 24274 -> 24274
5125
Cov: 24274 -> 24274
Cov: 24274 -> 24274
5126
Cov: 24274 -> 24274
Cov: 24274 -> 24274
5127
Cov: 24274 -> 24275
Cov: 24275 -> 24275
5128
Cov: 24275 -> 24275
Cov: 24275 -> 24275
5129
Cov: 24275 -> 24275
Cov: 24275 -> 24275
5130
Cov: 24275 -> 24275
Cov: 24275 -> 24275
5131
Cov: 24275 -> 24276
Cov: 24276 -> 24276
5132
Cov: 24276 -> 24279
Cov: 24279 -> 24279
5133
Cov: 24279 -> 24281
Cov: 24281 -> 24281
5134
Cov: 24281 -> 24281
Cov: 24281 -> 24281
5135
Cov: 24281 -> 24282
Cov: 24282 -> 24282
5136
Cov: 24282 -> 24282
Cov: 24282 -> 24282
5137
Cov: 24282 -> 24282
Cov: 24282 -> 24282
5138
Cov: 24282 -> 24282
Cov: 24282 -> 24282
5139
{"exception": "RuntimeError", "msg": "expand(torch.FloatTensor{[10, 3, 4]}, size=[3, 5]): the number of sizes provided (2) must be greater or equal to the number of dimensions in the tensor (3)"}
5140
Cov: 24282 -> 24282
Cov: 24282 -> 24282
5141
Cov: 24282 -> 24282
Cov: 24282 -> 24282
5142
Cov: 24282 -> 24282
Cov: 24282 -> 24282
5143
Cov: 24282 -> 24282
Cov: 24282 -> 24282
5144
Cov: 24282 -> 24282
Cov: 24282 -> 24282
5145
{"exception": "RuntimeError", "msg": "Expected size for first two dimensions of batch2 tensor to be: [2, 4] but got: [2, 3]."}
5146
Cov: 24282 -> 24282
Cov: 24282 -> 24282
5147
Cov: 24282 -> 24282
Cov: 24282 -> 24282
5148
Cov: 24282 -> 24283
Cov: 24283 -> 24283
5149
Cov: 24283 -> 24283
Cov: 24283 -> 24283
5150
Cov: 24283 -> 24283
Cov: 24283 -> 24283
5151
Cov: 24283 -> 24283
Cov: 24283 -> 24283
5152
Cov: 24283 -> 24283
Cov: 24283 -> 24283
5153
Cov: 24283 -> 24283
Cov: 24283 -> 24283
5154
Cov: 24283 -> 24283
Cov: 24283 -> 24283
5155
Cov: 24283 -> 24284
Cov: 24284 -> 24284
5156
{"exception": "NotImplementedError", "msg": "Could not run 'aten::qscheme' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::qscheme' is only available for these backends: [QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nQuantizedCPU: registered at aten/src/ATen/RegisterQuantizedCPU.cpp:944 [kernel]\nQuantizedCUDA: registered at aten/src/ATen/RegisterQuantizedCUDA.cpp:459 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:153 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:498 [backend fallback]\nFunctionalize: registered at ../aten/src/ATen/FunctionalizeFallbackKernel.cpp:290 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:86 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradMTIA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_0.cpp:16725 [kernel]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:382 [backend fallback]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:249 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:710 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:203 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:161 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:494 [backend fallback]\nPreDispatch: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:165 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:157 [backend fallback]\n"}
5157
Cov: 24284 -> 24286
Cov: 24286 -> 24286
5158
Cov: 24286 -> 24286
Cov: 24286 -> 24286
5159
Cov: 24286 -> 24286
Cov: 24286 -> 24286
5160
Cov: 24286 -> 24286
Cov: 24286 -> 24286
5161
Cov: 24286 -> 24286
Cov: 24286 -> 24286
5162
Cov: 24286 -> 24316
Cov: 24316 -> 24316
5163
Cov: 24316 -> 24316
Cov: 24316 -> 24316
5164
Cov: 24316 -> 24316
Cov: 24316 -> 24316
5165
Cov: 24316 -> 24316
Cov: 24316 -> 24316
5166
Cov: 24316 -> 24316
Cov: 24316 -> 24316
5167
Cov: 24316 -> 24316
Cov: 24316 -> 24316
5168
Cov: 24316 -> 24317
Cov: 24317 -> 24317
5169
Cov: 24317 -> 24317
Cov: 24317 -> 24317
5170
Cov: 24317 -> 24317
Cov: 24317 -> 24317
5171
Cov: 24317 -> 24317
Cov: 24317 -> 24317
5172
Cov: 24317 -> 24317
Cov: 24317 -> 24317
5173
Cov: 24317 -> 24319
Cov: 24319 -> 24319
5174
Cov: 24319 -> 24319
Cov: 24319 -> 24319
5175
Cov: 24319 -> 24319
Cov: 24319 -> 24319
5176
{"exception": "RuntimeError", "msg": "a Tensor with 4 elements cannot be converted to Scalar"}
5177
Cov: 24319 -> 24321
Cov: 24321 -> 24321
5178
Cov: 24321 -> 24321
Cov: 24321 -> 24321
5179
Cov: 24321 -> 24321
Cov: 24321 -> 24321
5180
Cov: 24321 -> 24322
Cov: 24322 -> 24322
5181
Cov: 24322 -> 24322
Cov: 24322 -> 24322
5182
Cov: 24322 -> 24322
Cov: 24322 -> 24322
5183
Cov: 24322 -> 24322
Cov: 24322 -> 24322
5184
Cov: 24322 -> 24323
Cov: 24323 -> 24323
5185
Cov: 24323 -> 24324
Cov: 24324 -> 24324
5186
Cov: 24324 -> 24324
Cov: 24324 -> 24324
5187
Cov: 24324 -> 24324
Cov: 24324 -> 24324
5188
Cov: 24324 -> 24324
Cov: 24324 -> 24324
5189
Cov: 24324 -> 24324
Cov: 24324 -> 24324
5190
Cov: 24324 -> 24324
Cov: 24324 -> 24324
5191
Cov: 24324 -> 24324
Cov: 24324 -> 24324
5192
{"exception": "RuntimeError", "msg": "Please look up dimensions by name, got: name = None."}
5193
Cov: 24324 -> 24325
Cov: 24325 -> 24325
5194
Cov: 24325 -> 24325
Cov: 24325 -> 24325
5195
Cov: 24325 -> 24340
Cov: 24340 -> 24340
5196
{"exception": "TypeError", "msg": "dsplit() received an invalid combination of arguments - got (split_size_or_sections=int, ), but expected one of:\n * (int sections)\n      didn't match because some of the keywords were incorrect: split_size_or_sections\n * (tuple of ints indices)\n      didn't match because some of the keywords were incorrect: split_size_or_sections\n"}
5197
Cov: 24340 -> 24340
Cov: 24340 -> 24340
5198
Cov: 24340 -> 24340
Cov: 24340 -> 24340
5199
Cov: 24340 -> 24340
Cov: 24340 -> 24340
5200
Cov: 24340 -> 24340
Cov: 24340 -> 24340
5201
{"exception": "RuntimeError", "msg": "element 0 of tensors does not require grad and does not have a grad_fn"}
5202
Cov: 24340 -> 24341
Cov: 24341 -> 24341
5203
Cov: 24341 -> 24341
Cov: 24341 -> 24341
5204
Cov: 24341 -> 24342
Cov: 24342 -> 24342
5205
Cov: 24342 -> 24343
Cov: 24343 -> 24343
5206
Cov: 24343 -> 24344
Cov: 24344 -> 24344
5207
Cov: 24344 -> 24344
Cov: 24344 -> 24344
5208
Cov: 24344 -> 24344
Cov: 24344 -> 24344
5209
Cov: 24344 -> 24344
Cov: 24344 -> 24344
5210
Cov: 24344 -> 24344
Cov: 24344 -> 24344
5211
Cov: 24344 -> 24344
Cov: 24344 -> 24344
5212
Cov: 24344 -> 24344
Cov: 24344 -> 24344
5213
Cov: 24344 -> 24344
Cov: 24344 -> 24344
5214
Cov: 24344 -> 24344
Cov: 24344 -> 24344
5215
Cov: 24344 -> 24344
Cov: 24344 -> 24344
5216
Cov: 24344 -> 24345
Cov: 24345 -> 24345
5217
Cov: 24345 -> 24345
Cov: 24345 -> 24345
5218
Cov: 24345 -> 24345
Cov: 24345 -> 24345
5219
Cov: 24345 -> 24346
Cov: 24346 -> 24346
5220
Cov: 24346 -> 24346
Cov: 24346 -> 24346
5221
Cov: 24346 -> 24351
Cov: 24351 -> 24351
5222
Cov: 24351 -> 24351
Cov: 24351 -> 24351
5223
Cov: 24351 -> 24351
Cov: 24351 -> 24351
5224
Cov: 24351 -> 24351
Cov: 24351 -> 24351
5225
Cov: 24351 -> 24353
Cov: 24353 -> 24353
5226
Cov: 24353 -> 24353
Cov: 24353 -> 24353
5227
{"exception": "RuntimeError", "msg": "put_ does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True)'. You can turn off determinism just for this operation, or you can use the 'warn_only=True' option, if that's acceptable for your application. You can also file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation."}
5228
Cov: 24353 -> 24353
Cov: 24353 -> 24353
5229
Cov: 24353 -> 24353
Cov: 24353 -> 24353
5230
Cov: 24353 -> 24353
Cov: 24353 -> 24353
5231
Cov: 24353 -> 24353
Cov: 24353 -> 24353
5232
{"exception": "RuntimeError", "msg": "norm(): input dtype should be either floating point or complex. Got Long instead."}
5233
Cov: 24353 -> 24354
Cov: 24354 -> 24354
5234
Cov: 24354 -> 24354
Cov: 24354 -> 24354
5235
Cov: 24354 -> 24354
Cov: 24354 -> 24354
5236
Cov: 24354 -> 24354
Cov: 24354 -> 24354
5237
Cov: 24354 -> 24354
Cov: 24354 -> 24354
5238
Cov: 24354 -> 24354
Cov: 24354 -> 24354
5239
{"exception": "RuntimeError", "msg": "result type Double can't be cast to the desired output type Long"}
5240
Cov: 24354 -> 24355
Cov: 24355 -> 24355
5241
Cov: 24355 -> 24355
Cov: 24355 -> 24355
5242
Cov: 24355 -> 24356
Cov: 24356 -> 24356
5243
Cov: 24356 -> 24356
Cov: 24356 -> 24356
5244
Cov: 24356 -> 24356
Cov: 24356 -> 24356
5245
{"exception": "RuntimeError", "msg": "This function was deprecated since version 1.9 and is now removed. The default behavior has changed from using the upper triangular portion of the matrix by default to using the lower triangular portion.\n\nL, _ = torch.symeig(A, upper=upper) should be replaced with:\nL = torch.linalg.eigvalsh(A, UPLO='U' if upper else 'L')\n\nand\n\nL, V = torch.symeig(A, eigenvectors=True) should be replaced with:\nL, V = torch.linalg.eigh(A, UPLO='U' if upper else 'L')"}
5246
Cov: 24356 -> 24356
Cov: 24356 -> 24356
5247
Cov: 24356 -> 24356
Cov: 24356 -> 24356
5248
{"exception": "RuntimeError", "msg": "masked_scatter: expected self and source to have same dtypes but gotDouble and Long"}
5249
Cov: 24356 -> 24356
Cov: 24356 -> 24356
5250
Cov: 24356 -> 24357
Cov: 24357 -> 24357
5251
Cov: 24357 -> 24370
Cov: 24370 -> 24370
5252
Cov: 24370 -> 24370
Cov: 24370 -> 24370
5253
Cov: 24370 -> 24370
Cov: 24370 -> 24370
5254
Cov: 24370 -> 24370
Cov: 24370 -> 24370
5255
Cov: 24370 -> 24370
Cov: 24370 -> 24370
5256
{"exception": "RuntimeError", "msg": "linalg.slogdet: A must be batches of square matrices, but they are 3 by 4 matrices"}
5257
Cov: 24370 -> 24370
Cov: 24370 -> 24370
5258
Cov: 24370 -> 24375
Cov: 24375 -> 24375
5259
Cov: 24375 -> 24375
Cov: 24375 -> 24375
5260
Cov: 24375 -> 24375
Cov: 24375 -> 24375
5261
Cov: 24375 -> 24375
Cov: 24375 -> 24375
5262
Cov: 24375 -> 24375
Cov: 24375 -> 24375
5263
Cov: 24375 -> 24378
Cov: 24378 -> 24378
5264
Cov: 24378 -> 24379
Cov: 24379 -> 24379
5265
Cov: 24379 -> 24380
Cov: 24380 -> 24380
5266
Cov: 24380 -> 24380
Cov: 24380 -> 24380
5267
Cov: 24380 -> 24381
Cov: 24381 -> 24381
5268
Cov: 24381 -> 24480
Cov: 24480 -> 24480
5269
Cov: 24480 -> 24481
Cov: 24481 -> 24481
5270
Cov: 24481 -> 24481
Cov: 24481 -> 24481
5271
Cov: 24481 -> 24481
Cov: 24481 -> 24481
5272
Cov: 24481 -> 24482
Cov: 24482 -> 24482
5273
Cov: 24482 -> 24482
Cov: 24482 -> 24482
5274
Cov: 24482 -> 24482
Cov: 24482 -> 24482
5275
Cov: 24482 -> 24482
Cov: 24482 -> 24482
5276
Cov: 24482 -> 24483
Cov: 24483 -> 24483
5277
Cov: 24483 -> 24483
Cov: 24483 -> 24483
5278
Cov: 24483 -> 24483
Cov: 24483 -> 24483
5279
Cov: 24483 -> 24483
Cov: 24483 -> 24483
5280
Cov: 24483 -> 24483
Cov: 24483 -> 24483
5281
Cov: 24483 -> 24483
Cov: 24483 -> 24483
5282
Cov: 24483 -> 24483
Cov: 24483 -> 24483
5283
Cov: 24483 -> 24483
Cov: 24483 -> 24483
5284
Cov: 24483 -> 24483
Cov: 24483 -> 24483
5285
Cov: 24483 -> 24483
Cov: 24483 -> 24483
5286
Cov: 24483 -> 24483
Cov: 24483 -> 24483
5287
Cov: 24483 -> 24483
Cov: 24483 -> 24483
5288
Cov: 24483 -> 24483
Cov: 24483 -> 24483
5289
Cov: 24483 -> 24483
Cov: 24483 -> 24483
5290
{"exception": "TypeError", "msg": "ldexp(): argument 'other' (position 1) must be Tensor, not int"}
5291
Cov: 24483 -> 24483
Cov: 24483 -> 24483
5292
Cov: 24483 -> 24483
Cov: 24483 -> 24483
5293
Cov: 24483 -> 24483
Cov: 24483 -> 24483
5294
Cov: 24483 -> 24484
Cov: 24484 -> 24484
5295
Cov: 24484 -> 24586
Cov: 24586 -> 24586
5296
Cov: 24586 -> 24586
Cov: 24586 -> 24586
5297
Cov: 24586 -> 24586
Cov: 24586 -> 24586
5298
Cov: 24586 -> 24586
Cov: 24586 -> 24586
5299
Cov: 24586 -> 24586
Cov: 24586 -> 24586
5300
Cov: 24586 -> 24586
Cov: 24586 -> 24586
5301
Cov: 24586 -> 24586
Cov: 24586 -> 24586
5302
Cov: 24586 -> 24586
Cov: 24586 -> 24586
5303
Cov: 24586 -> 24586
Cov: 24586 -> 24586
5304
Cov: 24586 -> 24586
Cov: 24586 -> 24586
5305
Cov: 24586 -> 24586
Cov: 24586 -> 24586
5306
Cov: 24586 -> 24587
Cov: 24587 -> 24587
5307
Cov: 24587 -> 24587
Cov: 24587 -> 24587
5308
Cov: 24587 -> 24587
Cov: 24587 -> 24587
5309
Cov: 24587 -> 24587
Cov: 24587 -> 24587
5310
Cov: 24587 -> 24587
Cov: 24587 -> 24587
5311
Cov: 24587 -> 24587
Cov: 24587 -> 24587
5312
Cov: 24587 -> 24588
Cov: 24588 -> 24588
5313
Cov: 24588 -> 24588
Cov: 24588 -> 24588
5314
Cov: 24588 -> 24588
Cov: 24588 -> 24588
5315
{"exception": "RuntimeError", "msg": "index_add_(): self (Double) and source (Long) must have the same scalar type"}
5316
Cov: 24588 -> 24588
Cov: 24588 -> 24588
5317
Cov: 24588 -> 24588
Cov: 24588 -> 24588
5318
Cov: 24588 -> 24589
Cov: 24589 -> 24589
5319
Cov: 24589 -> 24589
Cov: 24589 -> 24589
5320
Cov: 24589 -> 24589
Cov: 24589 -> 24589
5321
Cov: 24589 -> 24589
Cov: 24589 -> 24589
5322
Cov: 24589 -> 24589
Cov: 24589 -> 24589
5323
Cov: 24589 -> 24591
Cov: 24591 -> 24591
5324
Cov: 24591 -> 24592
Cov: 24592 -> 24592
5325
Cov: 24592 -> 24592
Cov: 24592 -> 24592
5326
Cov: 24592 -> 24593
Cov: 24593 -> 24593
5327
Cov: 24593 -> 24593
Cov: 24593 -> 24593
5328
Cov: 24593 -> 24593
Cov: 24593 -> 24593
5329
Cov: 24593 -> 24594
Cov: 24594 -> 24594
5330
Cov: 24594 -> 24595
Cov: 24595 -> 24595
5331
Cov: 24595 -> 24596
Cov: 24596 -> 24596
5332
Cov: 24596 -> 24596
Cov: 24596 -> 24596
5333
Cov: 24596 -> 24596
Cov: 24596 -> 24596
5334
Cov: 24596 -> 24596
Cov: 24596 -> 24596
5335
{"exception": "RuntimeError", "msg": "1D tensors expected, but got 2D and 2D tensors"}
5336
Cov: 24596 -> 24596
Cov: 24596 -> 24596
5337
Cov: 24596 -> 24596
Cov: 24596 -> 24596
5338
{"exception": "RuntimeError", "msg": "the base given to float_power_ has dtype Float but the operation's result requires dtype Double"}
5339
Cov: 24596 -> 24596
Cov: 24596 -> 24596
5340
Cov: 24596 -> 24596
Cov: 24596 -> 24596
5341
Cov: 24596 -> 24596
Cov: 24596 -> 24596
5342
Cov: 24596 -> 24596
Cov: 24596 -> 24596
5343
Cov: 24596 -> 24597
Cov: 24597 -> 24597
5344
Cov: 24597 -> 24598
Cov: 24598 -> 24598
5345
Cov: 24598 -> 24598
Cov: 24598 -> 24598
5346
Cov: 24598 -> 24598
Cov: 24598 -> 24598
5347
Cov: 24598 -> 24598
Cov: 24598 -> 24598
5348
Cov: 24598 -> 24598
Cov: 24598 -> 24598
5349
Cov: 24598 -> 24598
Cov: 24598 -> 24598
5350
Cov: 24598 -> 24598
Cov: 24598 -> 24598
5351
Cov: 24598 -> 24602
Cov: 24602 -> 24602
5352
Cov: 24602 -> 24602
Cov: 24602 -> 24602
5353
Cov: 24602 -> 24603
Cov: 24603 -> 24603
5354
Cov: 24603 -> 24604
Cov: 24604 -> 24604
5355
Cov: 24604 -> 24604
Cov: 24604 -> 24604
5356
Cov: 24604 -> 24604
Cov: 24604 -> 24604
5357
Cov: 24604 -> 24604
Cov: 24604 -> 24604
5358
Cov: 24604 -> 24604
Cov: 24604 -> 24604
5359
Cov: 24604 -> 24605
Cov: 24605 -> 24605
5360
Cov: 24605 -> 24606
Cov: 24606 -> 24606
5361
Cov: 24606 -> 24607
Cov: 24607 -> 24607
5362
Cov: 24607 -> 24608
Cov: 24608 -> 24608
5363
Cov: 24608 -> 24608
Cov: 24608 -> 24608
5364
Cov: 24608 -> 24608
Cov: 24608 -> 24608
5365
Cov: 24608 -> 24608
Cov: 24608 -> 24608
5366
Cov: 24608 -> 24608
Cov: 24608 -> 24608
5367
Cov: 24608 -> 24608
Cov: 24608 -> 24608
5368
Cov: 24608 -> 24609
Cov: 24609 -> 24609
5369
Cov: 24609 -> 24609
Cov: 24609 -> 24609
5370
Cov: 24609 -> 24609
Cov: 24609 -> 24609
5371
Cov: 24609 -> 24609
Cov: 24609 -> 24609
5372
Cov: 24609 -> 24614
Cov: 24614 -> 24614
5373
Cov: 24614 -> 24615
Cov: 24615 -> 24615
5374
Cov: 24615 -> 24615
Cov: 24615 -> 24615
5375
Cov: 24615 -> 24615
Cov: 24615 -> 24615
5376
Cov: 24615 -> 24615
Cov: 24615 -> 24615
5377
Cov: 24615 -> 24615
Cov: 24615 -> 24615
5378
Cov: 24615 -> 24615
Cov: 24615 -> 24615
5379
Cov: 24615 -> 24615
Cov: 24615 -> 24615
5380
Cov: 24615 -> 24615
Cov: 24615 -> 24615
5381
Cov: 24615 -> 24615
Cov: 24615 -> 24615
5382
Cov: 24615 -> 24615
Cov: 24615 -> 24615
5383
Cov: 24615 -> 24616
Cov: 24616 -> 24616
5384
Cov: 24616 -> 24616
Cov: 24616 -> 24616
5385
{"exception": "RuntimeError", "msg": "tensor.sspaddmm(...) can only be called on sparse tensors"}
5386
Cov: 24616 -> 24616
Cov: 24616 -> 24616
5387
Cov: 24616 -> 24617
Cov: 24617 -> 24617
5388
Cov: 24617 -> 24617
Cov: 24617 -> 24617
5389
{"exception": "NameError", "msg": "name 'x' is not defined"}
5390
Cov: 24617 -> 24617
Cov: 24617 -> 24617
5391
Cov: 24617 -> 24617
Cov: 24617 -> 24617
5392
Cov: 24617 -> 24618
Cov: 24618 -> 24618
5393
Cov: 24618 -> 24618
Cov: 24618 -> 24618
5394
{"exception": "RuntimeError", "msg": "istft requires a complex-valued input tensor matching the output from stft with return_complex=True."}
5395
Cov: 24618 -> 24619
Cov: 24619 -> 24619
5396
Cov: 24619 -> 24619
Cov: 24619 -> 24619
5397
Cov: 24619 -> 24619
Cov: 24619 -> 24619
5398
Cov: 24619 -> 24619
Cov: 24619 -> 24619
5399
Cov: 24619 -> 24619
Cov: 24619 -> 24619
5400
Cov: 24619 -> 24619
Cov: 24619 -> 24619
5401
Cov: 24619 -> 24619
Cov: 24619 -> 24619
5402
Cov: 24619 -> 24619
Cov: 24619 -> 24619
5403
Cov: 24619 -> 24619
Cov: 24619 -> 24619
5404
Cov: 24619 -> 24619
Cov: 24619 -> 24619
5405
Cov: 24619 -> 24619
Cov: 24619 -> 24619
5406
Cov: 24619 -> 24619
Cov: 24619 -> 24619
5407
Cov: 24619 -> 24619
Cov: 24619 -> 24619
5408
Cov: 24619 -> 24625
Cov: 24625 -> 24625
5409
Cov: 24625 -> 24625
Cov: 24625 -> 24625
5410
Cov: 24625 -> 24625
Cov: 24625 -> 24625
5411
Cov: 24625 -> 24693
Cov: 24693 -> 24693
5412
Cov: 24693 -> 24693
Cov: 24693 -> 24693
5413
Cov: 24693 -> 24693
Cov: 24693 -> 24693
5414
Cov: 24693 -> 24727
Cov: 24727 -> 24727
5415
Cov: 24727 -> 24727
Cov: 24727 -> 24727
5416
Cov: 24727 -> 24727
Cov: 24727 -> 24727
5417
Cov: 24727 -> 24727
Cov: 24727 -> 24727
5418
{"exception": "TypeError", "msg": "new_full(): argument 'size' (position 1) must be tuple of ints, not int"}
5419
Cov: 24727 -> 24727
Cov: 24727 -> 24727
5420
Cov: 24727 -> 24727
Cov: 24727 -> 24727
5421
Cov: 24727 -> 24727
Cov: 24727 -> 24727
5422
Cov: 24727 -> 24727
Cov: 24727 -> 24727
5423
Cov: 24727 -> 24727
Cov: 24727 -> 24727
5424
Cov: 24727 -> 24727
Cov: 24727 -> 24727
5425
Cov: 24727 -> 24727
Cov: 24727 -> 24727
5426
Cov: 24727 -> 24727
Cov: 24727 -> 24727
5427
{"exception": "_LinAlgError", "msg": "linalg.inv: The diagonal element 3 is zero, the inversion could not be completed because the input matrix is singular."}
5428
Cov: 24727 -> 24727
Cov: 24727 -> 24727
5429
Cov: 24727 -> 24727
Cov: 24727 -> 24727
5430
Cov: 24727 -> 24727
Cov: 24727 -> 24727
5431
Cov: 24727 -> 24727
Cov: 24727 -> 24727
5432
Cov: 24727 -> 24727
Cov: 24727 -> 24727
5433
Cov: 24727 -> 24727
Cov: 24727 -> 24727
5434
Cov: 24727 -> 24727
Cov: 24727 -> 24727
5435
Cov: 24727 -> 24727
Cov: 24727 -> 24727
5436
Cov: 24727 -> 24727
Cov: 24727 -> 24727
5437
Cov: 24727 -> 24728
Cov: 24728 -> 24728
5438
Cov: 24728 -> 24729
Cov: 24729 -> 24729
5439
Cov: 24729 -> 24729
Cov: 24729 -> 24729
5440
Cov: 24729 -> 24729
Cov: 24729 -> 24729
5441
Cov: 24729 -> 24729
Cov: 24729 -> 24729
5442
Cov: 24729 -> 24729
Cov: 24729 -> 24729
5443
Cov: 24729 -> 24729
Cov: 24729 -> 24729
5444
Cov: 24729 -> 24729
Cov: 24729 -> 24729
5445
Cov: 24729 -> 24729
Cov: 24729 -> 24729
5446
Cov: 24729 -> 24729
Cov: 24729 -> 24729
5447
Cov: 24729 -> 24730
Cov: 24730 -> 24730
5448
Cov: 24730 -> 24730
Cov: 24730 -> 24730
5449
Cov: 24730 -> 24730
Cov: 24730 -> 24730
5450
Cov: 24730 -> 24730
Cov: 24730 -> 24730
5451
Cov: 24730 -> 24730
Cov: 24730 -> 24730
5452
Cov: 24730 -> 24731
Cov: 24731 -> 24731
5453
Cov: 24731 -> 24731
Cov: 24731 -> 24731
5454
Cov: 24731 -> 24731
Cov: 24731 -> 24731
5455
Cov: 24731 -> 24731
Cov: 24731 -> 24731
5456
Cov: 24731 -> 24734
Cov: 24734 -> 24734
5457
Cov: 24734 -> 24735
Cov: 24735 -> 24735
5458
Cov: 24735 -> 24735
Cov: 24735 -> 24735
5459
Cov: 24735 -> 24735
Cov: 24735 -> 24735
5460
Cov: 24735 -> 24735
Cov: 24735 -> 24735
5461
Cov: 24735 -> 24735
Cov: 24735 -> 24735
5462
Cov: 24735 -> 24736
Cov: 24736 -> 24736
5463
Cov: 24736 -> 24736
Cov: 24736 -> 24736
5464
{"exception": "RuntimeError", "msg": "torch.dsplit requires a tensor with at least 3 dimension, but got a tensor with 2 dimensions!"}
5465
Cov: 24736 -> 24736
Cov: 24736 -> 24736
5466
{"exception": "TypeError", "msg": "atanh_() takes no keyword arguments"}
5467
Cov: 24736 -> 24741
Cov: 24741 -> 24741
5468
Cov: 24741 -> 24741
Cov: 24741 -> 24741
5469
Cov: 24741 -> 24741
Cov: 24741 -> 24741
5470
Cov: 24741 -> 24741
Cov: 24741 -> 24741
5471
Cov: 24741 -> 24741
Cov: 24741 -> 24741
5472
Cov: 24741 -> 24741
Cov: 24741 -> 24741
5473
Cov: 24741 -> 24741
Cov: 24741 -> 24741
5474
Cov: 24741 -> 24741
Cov: 24741 -> 24741
5475
Cov: 24741 -> 24742
Cov: 24742 -> 24742
5476
Cov: 24742 -> 24742
Cov: 24742 -> 24742
5477
Cov: 24742 -> 24742
Cov: 24742 -> 24742
5478
{"exception": "RuntimeError", "msg": "This function was deprecated since version 1.9 and is now removed.\nPlease use the `torch.linalg.matrix_rank` function instead. The parameter 'symmetric' was renamed in `torch.linalg.matrix_rank()` to 'hermitian'."}
5479
Cov: 24742 -> 24746
Cov: 24746 -> 24746
5480
Cov: 24746 -> 24746
Cov: 24746 -> 24746
5481
Cov: 24746 -> 24747
Cov: 24747 -> 24747
5482
Cov: 24747 -> 24747
Cov: 24747 -> 24747
5483
Cov: 24747 -> 24747
Cov: 24747 -> 24747
5484
Cov: 24747 -> 24747
Cov: 24747 -> 24747
5485
Cov: 24747 -> 24748
Cov: 24748 -> 24748
5486
Cov: 24748 -> 24748
Cov: 24748 -> 24748
5487
Cov: 24748 -> 24748
Cov: 24748 -> 24748
5488
Cov: 24748 -> 24749
Cov: 24749 -> 24749
5489
Cov: 24749 -> 24749
Cov: 24749 -> 24749
5490
Cov: 24749 -> 24749
Cov: 24749 -> 24749
5491
Cov: 24749 -> 24749
Cov: 24749 -> 24749
5492
Cov: 24749 -> 24750
Cov: 24750 -> 24750
5493
Cov: 24750 -> 24751
Cov: 24751 -> 24751
5494
Cov: 24751 -> 24752
Cov: 24752 -> 24752
5495
Cov: 24752 -> 24752
Cov: 24752 -> 24752
5496
{"exception": "NameError", "msg": "name '_input_tensor' is not defined"}
5497
Cov: 24752 -> 24752
Cov: 24752 -> 24752
5498
Cov: 24752 -> 24752
Cov: 24752 -> 24752
5499
Cov: 24752 -> 24752
Cov: 24752 -> 24752
5500
{"exception": "TypeError", "msg": "map_(): argument 'other' (position 1) must be Tensor, not function"}
5501
Cov: 24752 -> 24752
Cov: 24752 -> 24752
5502
Cov: 24752 -> 24752
Cov: 24752 -> 24752
5503
Cov: 24752 -> 24752
Cov: 24752 -> 24752
5504
Cov: 24752 -> 24752
Cov: 24752 -> 24752
5505
{"exception": "TypeError", "msg": "heaviside(): argument 'values' must be Tensor, not NoneType"}
5506
Cov: 24752 -> 24753
Cov: 24753 -> 24753
5507
Cov: 24753 -> 24753
Cov: 24753 -> 24753
5508
Cov: 24753 -> 24754
Cov: 24754 -> 24754
5509
Cov: 24754 -> 24754
Cov: 24754 -> 24754
5510
Cov: 24754 -> 24754
Cov: 24754 -> 24754
5511
Cov: 24754 -> 24754
Cov: 24754 -> 24754
5512
Cov: 24754 -> 24754
Cov: 24754 -> 24754
5513
Cov: 24754 -> 24754
Cov: 24754 -> 24754
5514
Cov: 24754 -> 24754
Cov: 24754 -> 24754
5515
Cov: 24754 -> 24754
Cov: 24754 -> 24754
5516
Cov: 24754 -> 24754
Cov: 24754 -> 24754
5517
Cov: 24754 -> 24755
Cov: 24755 -> 24755
5518
Cov: 24755 -> 24755
Cov: 24755 -> 24755
5519
Cov: 24755 -> 24755
Cov: 24755 -> 24755
5520
Cov: 24755 -> 24755
Cov: 24755 -> 24755
5521
Cov: 24755 -> 24755
Cov: 24755 -> 24755
5522
Cov: 24755 -> 24755
Cov: 24755 -> 24755
5523
Cov: 24755 -> 24755
Cov: 24755 -> 24755
5524
Cov: 24755 -> 24755
Cov: 24755 -> 24755
5525
Cov: 24755 -> 24756
Cov: 24756 -> 24756
5526
Cov: 24756 -> 24756
Cov: 24756 -> 24756
5527
Cov: 24756 -> 24756
Cov: 24756 -> 24756
5528
Cov: 24756 -> 24756
Cov: 24756 -> 24756
5529
{"exception": "TypeError", "msg": "'getset_descriptor' object is not callable"}
5530
Cov: 24756 -> 24756
Cov: 24756 -> 24756
5531
Cov: 24756 -> 24756
Cov: 24756 -> 24756
5532
Cov: 24756 -> 24756
Cov: 24756 -> 24756
5533
Cov: 24756 -> 24756
Cov: 24756 -> 24756
5534
Cov: 24756 -> 24756
Cov: 24756 -> 24756
5535
Cov: 24756 -> 24756
Cov: 24756 -> 24756
5536
Cov: 24756 -> 24756
Cov: 24756 -> 24756
5537
Cov: 24756 -> 24756
Cov: 24756 -> 24756
5538
Cov: 24756 -> 24756
Cov: 24756 -> 24756
5539
Cov: 24756 -> 24757
Cov: 24757 -> 24757
5540
Cov: 24757 -> 24758
Cov: 24758 -> 24758
5541
Cov: 24758 -> 24758
Cov: 24758 -> 24758
5542
Cov: 24758 -> 24758
Cov: 24758 -> 24758
5543
Cov: 24758 -> 24758
Cov: 24758 -> 24758
5544
Cov: 24758 -> 24758
Cov: 24758 -> 24758
5545
Cov: 24758 -> 24759
Cov: 24759 -> 24759
5546
Cov: 24759 -> 24759
Cov: 24759 -> 24759
5547
Cov: 24759 -> 24759
Cov: 24759 -> 24759
5548
Cov: 24759 -> 24760
Cov: 24760 -> 24760
5549
Cov: 24760 -> 24766
Cov: 24766 -> 24766
5550
{"exception": "TypeError", "msg": "'getset_descriptor' object is not callable"}
5551
Cov: 24766 -> 24766
Cov: 24766 -> 24766
5552
Cov: 24766 -> 24767
Cov: 24767 -> 24767
5553
Cov: 24767 -> 24767
Cov: 24767 -> 24767
5554
Cov: 24767 -> 24767
Cov: 24767 -> 24767
5555
Cov: 24767 -> 24767
Cov: 24767 -> 24767
5556
Cov: 24767 -> 24767
Cov: 24767 -> 24767
5557
Cov: 24767 -> 24767
Cov: 24767 -> 24767
5558
Cov: 24767 -> 24767
Cov: 24767 -> 24767
5559
Cov: 24767 -> 24767
Cov: 24767 -> 24767
5560
Cov: 24767 -> 24767
Cov: 24767 -> 24767
5561
Cov: 24767 -> 24767
Cov: 24767 -> 24767
5562
{"exception": "RuntimeError", "msg": "element 0 of tensors does not require grad and does not have a grad_fn"}
5563
Cov: 24767 -> 24769
Cov: 24769 -> 24769
5564
Cov: 24769 -> 24769
Cov: 24769 -> 24769
5565
Cov: 24769 -> 24773
Cov: 24773 -> 24773
5566
Cov: 24773 -> 24774
Cov: 24774 -> 24774
5567
Cov: 24774 -> 24774
Cov: 24774 -> 24774
5568
Cov: 24774 -> 24774
Cov: 24774 -> 24774
5569
Cov: 24774 -> 24774
Cov: 24774 -> 24774
5570
Cov: 24774 -> 24774
Cov: 24774 -> 24774
5571
Cov: 24774 -> 24774
Cov: 24774 -> 24774
5572
Cov: 24774 -> 24775
Cov: 24775 -> 24775
5573
Cov: 24775 -> 24775
Cov: 24775 -> 24775
5574
Cov: 24775 -> 24776
Cov: 24776 -> 24776
5575
Cov: 24776 -> 24776
Cov: 24776 -> 24776
5576
Cov: 24776 -> 24776
Cov: 24776 -> 24776
5577
{"exception": "RuntimeError", "msg": "element 0 of tensors does not require grad and does not have a grad_fn"}
5578
Cov: 24776 -> 24776
Cov: 24776 -> 24776
5579
Cov: 24776 -> 24776
Cov: 24776 -> 24776
5580
Cov: 24776 -> 24777
Cov: 24777 -> 24777
5581
{"exception": "TypeError", "msg": "addr_() received an invalid combination of arguments - got (Tensor), but expected (Tensor vec1, Tensor vec2, *, Number beta, Number alpha)"}
5582
Cov: 24777 -> 24777
Cov: 24777 -> 24777
5583
Cov: 24777 -> 24777
Cov: 24777 -> 24777
5584
Cov: 24777 -> 24777
Cov: 24777 -> 24777
5585
Cov: 24777 -> 24777
Cov: 24777 -> 24777
5586
Cov: 24777 -> 24777
Cov: 24777 -> 24777
5587
Cov: 24777 -> 24777
Cov: 24777 -> 24777
5588
{"exception": "RuntimeError", "msg": "element 0 of tensors does not require grad and does not have a grad_fn"}
5589
Cov: 24777 -> 24785
Cov: 24785 -> 24785
5590
Cov: 24785 -> 24785
Cov: 24785 -> 24785
5591
Cov: 24785 -> 24785
Cov: 24785 -> 24785
5592
Cov: 24785 -> 24785
Cov: 24785 -> 24785
5593
Cov: 24785 -> 24785
Cov: 24785 -> 24785
5594
Cov: 24785 -> 24785
Cov: 24785 -> 24785
5595
Cov: 24785 -> 24785
Cov: 24785 -> 24785
5596
{"exception": "NotImplementedError", "msg": "Could not run 'aten::q_per_channel_scales' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::q_per_channel_scales' is only available for these backends: [QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nQuantizedCPU: registered at aten/src/ATen/RegisterQuantizedCPU.cpp:944 [kernel]\nQuantizedCUDA: registered at aten/src/ATen/RegisterQuantizedCUDA.cpp:459 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:153 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:498 [backend fallback]\nFunctionalize: registered at ../aten/src/ATen/FunctionalizeFallbackKernel.cpp:290 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:86 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradMTIA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_3.cpp:14610 [kernel]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:382 [backend fallback]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:249 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:710 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:203 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:161 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:494 [backend fallback]\nPreDispatch: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:165 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:157 [backend fallback]\n"}
5597
{"exception": "RuntimeError", "msg": "Index tensor must have the same number of dimensions as input tensor"}
5598
Cov: 24785 -> 24786
Cov: 24786 -> 24786
5599
Cov: 24786 -> 24786
Cov: 24786 -> 24786
5600
Cov: 24786 -> 24787
Cov: 24787 -> 24787
5601
Cov: 24787 -> 24787
Cov: 24787 -> 24787
5602
Cov: 24787 -> 24791
Cov: 24791 -> 24791
5603
Cov: 24791 -> 24791
Cov: 24791 -> 24791
5604
Cov: 24791 -> 24791
Cov: 24791 -> 24791
5605
Cov: 24791 -> 24791
Cov: 24791 -> 24791
5606
Cov: 24791 -> 24791
Cov: 24791 -> 24791
5607
Cov: 24791 -> 24791
Cov: 24791 -> 24791
5608
Cov: 24791 -> 24791
Cov: 24791 -> 24791
5609
Cov: 24791 -> 24791
Cov: 24791 -> 24791
5610
Cov: 24791 -> 24791
Cov: 24791 -> 24791
5611
Cov: 24791 -> 24796
Cov: 24796 -> 24796
5612
Cov: 24796 -> 24796
Cov: 24796 -> 24796
5613
Cov: 24796 -> 24796
Cov: 24796 -> 24796
5614
Cov: 24796 -> 24796
Cov: 24796 -> 24796
5615
Cov: 24796 -> 24796
Cov: 24796 -> 24796
5616
Cov: 24796 -> 24796
Cov: 24796 -> 24796
5617
Cov: 24796 -> 24796
Cov: 24796 -> 24796
5618
Cov: 24796 -> 24796
Cov: 24796 -> 24796
5619
Cov: 24796 -> 24797
Cov: 24797 -> 24797
5620
Cov: 24797 -> 24798
Cov: 24798 -> 24798
5621
Cov: 24798 -> 24798
Cov: 24798 -> 24798
5622
Cov: 24798 -> 24798
Cov: 24798 -> 24798
5623
Cov: 24798 -> 24798
Cov: 24798 -> 24798
5624
Cov: 24798 -> 24799
Cov: 24799 -> 24799
5625
Cov: 24799 -> 24799
Cov: 24799 -> 24799
5626
Cov: 24799 -> 24799
Cov: 24799 -> 24799
5627
Cov: 24799 -> 24800
Cov: 24800 -> 24800
5628
Cov: 24800 -> 24800
Cov: 24800 -> 24800
5629
Cov: 24800 -> 24800
Cov: 24800 -> 24800
5630
Cov: 24800 -> 24801
Cov: 24801 -> 24801
5631
Cov: 24801 -> 24801
Cov: 24801 -> 24801
5632
Cov: 24801 -> 24801
Cov: 24801 -> 24801
5633
Cov: 24801 -> 24802
Cov: 24802 -> 24802
5634
Cov: 24802 -> 24802
Cov: 24802 -> 24802
5635
Cov: 24802 -> 24802
Cov: 24802 -> 24802
5636
Cov: 24802 -> 24802
Cov: 24802 -> 24802
5637
Cov: 24802 -> 24802
Cov: 24802 -> 24802
5638
{"exception": "TypeError", "msg": "'getset_descriptor' object is not callable"}
5639
Cov: 24802 -> 24802
Cov: 24802 -> 24802
5640
Cov: 24802 -> 24802
Cov: 24802 -> 24802
5641
Cov: 24802 -> 24802
Cov: 24802 -> 24802
5642
Cov: 24802 -> 24802
Cov: 24802 -> 24802
5643
{"exception": "RuntimeError", "msg": "element 0 of tensors does not require grad and does not have a grad_fn"}
5644
Cov: 24802 -> 24804
Cov: 24804 -> 24804
5645
{"exception": "NotImplementedError", "msg": "Could not run 'aten::q_per_channel_scales' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::q_per_channel_scales' is only available for these backends: [QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nQuantizedCPU: registered at aten/src/ATen/RegisterQuantizedCPU.cpp:944 [kernel]\nQuantizedCUDA: registered at aten/src/ATen/RegisterQuantizedCUDA.cpp:459 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:153 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:498 [backend fallback]\nFunctionalize: registered at ../aten/src/ATen/FunctionalizeFallbackKernel.cpp:290 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:86 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradMTIA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_3.cpp:14610 [kernel]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:382 [backend fallback]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:249 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:710 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:203 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:161 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:494 [backend fallback]\nPreDispatch: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:165 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:157 [backend fallback]\n"}
5646
Cov: 24804 -> 24804
Cov: 24804 -> 24804
5647
Cov: 24804 -> 24804
Cov: 24804 -> 24804
5648
Cov: 24804 -> 24804
Cov: 24804 -> 24804
5649
Cov: 24804 -> 24804
Cov: 24804 -> 24804
5650
Cov: 24804 -> 24804
Cov: 24804 -> 24804
5651
Cov: 24804 -> 24804
Cov: 24804 -> 24804
5652
{"exception": "RuntimeError", "msg": "This function was deprecated since version 1.9 and is now removed. `torch.linalg.eig` returns complex tensors of dtype `cfloat` or `cdouble` rather than real tensors mimicking complex tensors.\n\nL, _ = torch.eig(A) should be replaced with:\nL_complex = torch.linalg.eigvals(A)\n\nand\n\nL, V = torch.eig(A, eigenvectors=True) should be replaced with:\nL_complex, V_complex = torch.linalg.eig(A)"}
5653
Cov: 24804 -> 24806
Cov: 24806 -> 24806
5654
{"exception": "RuntimeError", "msg": "function missing required argument 'byte_order' (pos 2)"}
5655
Cov: 24806 -> 24815
Cov: 24815 -> 24815
5656
Cov: 24815 -> 24815
Cov: 24815 -> 24815
5657
Cov: 24815 -> 24815
Cov: 24815 -> 24815
5658
Cov: 24815 -> 24816
Cov: 24816 -> 24816
5659
Cov: 24816 -> 24816
Cov: 24816 -> 24816
5660
Cov: 24816 -> 24816
Cov: 24816 -> 24816
5661
Cov: 24816 -> 24816
Cov: 24816 -> 24816
5662
Cov: 24816 -> 24816
Cov: 24816 -> 24816
5663
Cov: 24816 -> 24816
Cov: 24816 -> 24816
5664
Cov: 24816 -> 24817
Cov: 24817 -> 24817
5665
Cov: 24817 -> 24817
Cov: 24817 -> 24817
5666
{"exception": "NotImplementedError", "msg": "Could not run 'aten::sparse_mask' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::sparse_mask' is only available for these backends: [SparseCPU, SparseCUDA, SparseCsrCPU, SparseCsrCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nSparseCPU: registered at aten/src/ATen/RegisterSparseCPU.cpp:1387 [kernel]\nSparseCUDA: registered at aten/src/ATen/RegisterSparseCUDA.cpp:1573 [kernel]\nSparseCsrCPU: registered at aten/src/ATen/RegisterSparseCsrCPU.cpp:1135 [kernel]\nSparseCsrCUDA: registered at aten/src/ATen/RegisterSparseCsrCUDA.cpp:1276 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:153 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:498 [backend fallback]\nFunctionalize: registered at ../aten/src/ATen/FunctionalizeFallbackKernel.cpp:290 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:86 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradMTIA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_0.cpp:16725 [kernel]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:382 [backend fallback]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:249 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:710 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:203 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:161 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:494 [backend fallback]\nPreDispatch: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:165 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:157 [backend fallback]\n"}
5667
Cov: 24817 -> 24817
Cov: 24817 -> 24817
5668
{"exception": "RuntimeError", "msg": "This function was deprecated since version 1.9 and is now removed. `torch.lstsq` is deprecated in favor of `torch.linalg.lstsq`.\n`torch.linalg.lstsq` has reversed arguments and does not return the QR decomposition in the returned tuple (although it returns other information about the problem).\n\nTo get the QR decomposition consider using `torch.linalg.qr`.\n\nThe returned solution in `torch.lstsq` stored the residuals of the solution in the last m - n columns of the returned value whenever m > n. In torch.linalg.lstsq, the residuals are in the field 'residuals' of the returned named tuple.\n\nThe unpacking of the solution, as in\nX, _ = torch.lstsq(B, A).solution[:A.size(1)]\nshould be replaced with:\nX = torch.linalg.lstsq(A, B).solution"}
5669
Cov: 24817 -> 24819
Cov: 24819 -> 24819
5670
Cov: 24819 -> 24819
Cov: 24819 -> 24819
5671
Cov: 24819 -> 24819
Cov: 24819 -> 24819
5672
Cov: 24819 -> 24819
Cov: 24819 -> 24819
5673
Cov: 24819 -> 24819
Cov: 24819 -> 24819
5674
Cov: 24819 -> 24819
Cov: 24819 -> 24819
5675
Cov: 24819 -> 24819
Cov: 24819 -> 24819
5676
Cov: 24819 -> 24930
Cov: 24930 -> 24930
5677
Cov: 24930 -> 24930
Cov: 24930 -> 24930
5678
Cov: 24930 -> 24930
Cov: 24930 -> 24930
5679
Cov: 24930 -> 24930
Cov: 24930 -> 24930
5680
Cov: 24930 -> 24930
Cov: 24930 -> 24930
5681
Cov: 24930 -> 24930
Cov: 24930 -> 24930
5682
Cov: 24930 -> 24931
Cov: 24931 -> 24931
5683
Cov: 24931 -> 24931
Cov: 24931 -> 24931
5684
Cov: 24931 -> 24931
Cov: 24931 -> 24931
5685
Cov: 24931 -> 24932
Cov: 24932 -> 24932
5686
Cov: 24932 -> 24932
Cov: 24932 -> 24932
5687
Cov: 24932 -> 24932
Cov: 24932 -> 24932
5688
Cov: 24932 -> 24932
Cov: 24932 -> 24932
5689
Cov: 24932 -> 24932
Cov: 24932 -> 24932
5690
Cov: 24932 -> 24932
Cov: 24932 -> 24932
5691
{"exception": "RuntimeError", "msg": "element 0 of tensors does not require grad and does not have a grad_fn"}
5692
{"exception": "TypeError", "msg": "index_add_() missing 1 required positional arguments: \"source\""}
5693
Cov: 24932 -> 24933
Cov: 24933 -> 24933
5694
Cov: 24933 -> 24934
Cov: 24934 -> 24934
5695
Cov: 24934 -> 24935
Cov: 24935 -> 24935
5696
Cov: 24935 -> 24936
Cov: 24936 -> 24936
5697
Cov: 24936 -> 24936
Cov: 24936 -> 24936
5698
Cov: 24936 -> 24936
Cov: 24936 -> 24936
5699
Cov: 24936 -> 24936
Cov: 24936 -> 24936
5700
{"exception": "RuntimeError", "msg": "max_unpooling2d_forward_out does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True)'. You can turn off determinism just for this operation, or you can use the 'warn_only=True' option, if that's acceptable for your application. You can also file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation."}
5701
Cov: 24936 -> 24937
Cov: 24937 -> 24937
5702
Cov: 24937 -> 24939
Cov: 24939 -> 24939
5703
Cov: 24939 -> 24940
Cov: 24940 -> 24940
5704
Cov: 24940 -> 24940
Cov: 24940 -> 24940
5705
Cov: 24940 -> 24941
Cov: 24941 -> 24941
5706
Cov: 24941 -> 24945
Cov: 24945 -> 24945
5707
Cov: 24945 -> 24945
Cov: 24945 -> 24945
5708
Cov: 24945 -> 24946
Cov: 24946 -> 24946
5709
Cov: 24946 -> 24947
Cov: 24947 -> 24947
5710
Cov: 24947 -> 24987
Cov: 24987 -> 24987
5711
Cov: 24987 -> 24987
Cov: 24987 -> 24987
5712
Cov: 24987 -> 24987
Cov: 24987 -> 24987
5713
Cov: 24987 -> 24988
Cov: 24988 -> 24988
5714
Cov: 24988 -> 24988
Cov: 24988 -> 24988
5715
Cov: 24988 -> 24988
Cov: 24988 -> 24988
5716
{"exception": "RuntimeError", "msg": "expand(torch.FloatTensor{[10, 3, 4]}, size=[3, 5]): the number of sizes provided (2) must be greater or equal to the number of dimensions in the tensor (3)"}
5717
Cov: 24988 -> 24988
Cov: 24988 -> 24988
5718
{"exception": "NotImplementedError", "msg": "Could not run 'aten::q_per_channel_scales' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::q_per_channel_scales' is only available for these backends: [QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nQuantizedCPU: registered at aten/src/ATen/RegisterQuantizedCPU.cpp:944 [kernel]\nQuantizedCUDA: registered at aten/src/ATen/RegisterQuantizedCUDA.cpp:459 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:153 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:498 [backend fallback]\nFunctionalize: registered at ../aten/src/ATen/FunctionalizeFallbackKernel.cpp:290 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:86 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradMTIA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_3.cpp:14610 [kernel]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:382 [backend fallback]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:249 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:710 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:203 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:161 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:494 [backend fallback]\nPreDispatch: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:165 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:157 [backend fallback]\n"}
5719
Cov: 24988 -> 24988
Cov: 24988 -> 24988
5720
Cov: 24988 -> 24988
Cov: 24988 -> 24988
5721
Cov: 24988 -> 24989
Cov: 24989 -> 24989
5722
Cov: 24989 -> 24989
Cov: 24989 -> 24989
5723
Cov: 24989 -> 24989
Cov: 24989 -> 24989
5724
Cov: 24989 -> 24989
Cov: 24989 -> 24989
5725
Cov: 24989 -> 24989
Cov: 24989 -> 24989
5726
Cov: 24989 -> 24989
Cov: 24989 -> 24989
5727
Cov: 24989 -> 24993
Cov: 24993 -> 24993
5728
Cov: 24993 -> 24993
Cov: 24993 -> 24993
5729
Cov: 24993 -> 24993
Cov: 24993 -> 24993
5730
Cov: 24993 -> 24993
Cov: 24993 -> 24993
5731
Cov: 24993 -> 24993
Cov: 24993 -> 24993
5732
Cov: 24993 -> 24994
Cov: 24994 -> 24994
5733
Cov: 24994 -> 24995
Cov: 24995 -> 24995
5734
Cov: 24995 -> 24995
Cov: 24995 -> 24995
5735
Cov: 24995 -> 24995
Cov: 24995 -> 24995
5736
Cov: 24995 -> 24995
Cov: 24995 -> 24995
5737
Cov: 24995 -> 24996
Cov: 24996 -> 24996
5738
Cov: 24996 -> 24997
Cov: 24997 -> 24997
5739
Cov: 24997 -> 24998
Cov: 24998 -> 24998
5740
Cov: 24998 -> 24998
Cov: 24998 -> 24998
5741
Cov: 24998 -> 24998
Cov: 24998 -> 24998
5742
Cov: 24998 -> 24998
Cov: 24998 -> 24998
5743
Cov: 24998 -> 24998
Cov: 24998 -> 24998
5744
Cov: 24998 -> 24998
Cov: 24998 -> 24998
5745
Cov: 24998 -> 24998
Cov: 24998 -> 24998
5746
Cov: 24998 -> 24998
Cov: 24998 -> 24998
5747
Cov: 24998 -> 24998
Cov: 24998 -> 24998
5748
Cov: 24998 -> 24998
Cov: 24998 -> 24998
5749
Cov: 24998 -> 24998
Cov: 24998 -> 24998
5750
Cov: 24998 -> 24998
Cov: 24998 -> 24998
5751
Cov: 24998 -> 24998
Cov: 24998 -> 24998
5752
Cov: 24998 -> 24998
Cov: 24998 -> 24998
5753
Cov: 24998 -> 24999
Cov: 24999 -> 24999
5754
Cov: 24999 -> 24999
Cov: 24999 -> 24999
5755
Cov: 24999 -> 24999
Cov: 24999 -> 24999
5756
Cov: 24999 -> 24999
Cov: 24999 -> 24999
5757
Cov: 24999 -> 25003
Cov: 25003 -> 25003
5758
Cov: 25003 -> 25003
Cov: 25003 -> 25003
5759
Cov: 25003 -> 25003
Cov: 25003 -> 25003
5760
Cov: 25003 -> 25003
Cov: 25003 -> 25003
5761
Cov: 25003 -> 25003
Cov: 25003 -> 25003
5762
Cov: 25003 -> 25003
Cov: 25003 -> 25003
5763
Cov: 25003 -> 25004
Cov: 25004 -> 25004
5764
Cov: 25004 -> 25004
Cov: 25004 -> 25004
5765
Cov: 25004 -> 25004
Cov: 25004 -> 25004
5766
Cov: 25004 -> 25004
Cov: 25004 -> 25004
5767
Cov: 25004 -> 25004
Cov: 25004 -> 25004
5768
Cov: 25004 -> 25004
Cov: 25004 -> 25004
5769
Cov: 25004 -> 25005
Cov: 25005 -> 25005
5770
Cov: 25005 -> 25006
Cov: 25006 -> 25006
5771
Cov: 25006 -> 25006
Cov: 25006 -> 25006
5772
Cov: 25006 -> 25006
Cov: 25006 -> 25006
5773
Cov: 25006 -> 25008
Cov: 25008 -> 25008
5774
{"exception": "TypeError", "msg": "to_sparse() received an invalid combination of arguments - got (sparseDims=int, ), but expected one of:\n * (*, torch.layout layout, tuple of ints blocksize, int dense_dim)\n * (int sparse_dim)\n      didn't match because some of the keywords were incorrect: sparseDims\n"}
5775
Cov: 25008 -> 25009
Cov: 25009 -> 25009
5776
Cov: 25009 -> 25009
Cov: 25009 -> 25009
5777
Cov: 25009 -> 25009
Cov: 25009 -> 25009
5778
Cov: 25009 -> 25009
Cov: 25009 -> 25009
5779
Cov: 25009 -> 25011
Cov: 25011 -> 25011
5780
{"exception": "NotImplementedError", "msg": "Could not run 'aten::q_per_channel_axis' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::q_per_channel_axis' is only available for these backends: [QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nQuantizedCPU: registered at aten/src/ATen/RegisterQuantizedCPU.cpp:944 [kernel]\nQuantizedCUDA: registered at aten/src/ATen/RegisterQuantizedCUDA.cpp:459 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:153 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:498 [backend fallback]\nFunctionalize: registered at ../aten/src/ATen/FunctionalizeFallbackKernel.cpp:290 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:86 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradMTIA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_4.cpp:13056 [kernel]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:382 [backend fallback]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:249 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:710 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:203 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:161 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:494 [backend fallback]\nPreDispatch: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:165 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:157 [backend fallback]\n"}
5781
Cov: 25011 -> 25011
Cov: 25011 -> 25011
5782
Cov: 25011 -> 25011
Cov: 25011 -> 25011
5783
Cov: 25011 -> 25011
Cov: 25011 -> 25011
5784
Cov: 25011 -> 25011
Cov: 25011 -> 25011
5785
Cov: 25011 -> 25013
Cov: 25013 -> 25013
5786
{"exception": "RuntimeError", "msg": "Index tensor must have the same number of dimensions as input tensor"}
5787
Cov: 25013 -> 25013
Cov: 25013 -> 25013
5788
Cov: 25013 -> 25013
Cov: 25013 -> 25013
5789
{"exception": "RuntimeError", "msg": "This function was deprecated since version 1.9 and is now removed. `torch.lstsq` is deprecated in favor of `torch.linalg.lstsq`.\n`torch.linalg.lstsq` has reversed arguments and does not return the QR decomposition in the returned tuple (although it returns other information about the problem).\n\nTo get the QR decomposition consider using `torch.linalg.qr`.\n\nThe returned solution in `torch.lstsq` stored the residuals of the solution in the last m - n columns of the returned value whenever m > n. In torch.linalg.lstsq, the residuals are in the field 'residuals' of the returned named tuple.\n\nThe unpacking of the solution, as in\nX, _ = torch.lstsq(B, A).solution[:A.size(1)]\nshould be replaced with:\nX = torch.linalg.lstsq(A, B).solution"}
5790
Cov: 25013 -> 25015
Cov: 25015 -> 25015
5791
Cov: 25015 -> 25015
Cov: 25015 -> 25015
5792
Cov: 25015 -> 25015
Cov: 25015 -> 25015
5793
Cov: 25015 -> 25015
Cov: 25015 -> 25015
5794
Cov: 25015 -> 25018
Cov: 25018 -> 25018
5795
Cov: 25018 -> 25018
Cov: 25018 -> 25018
5796
Cov: 25018 -> 25018
Cov: 25018 -> 25018
5797
Cov: 25018 -> 25018
Cov: 25018 -> 25018
5798
Cov: 25018 -> 25018
Cov: 25018 -> 25018
5799
Cov: 25018 -> 25018
Cov: 25018 -> 25018
5800
Cov: 25018 -> 25018
Cov: 25018 -> 25018
5801
Cov: 25018 -> 25018
Cov: 25018 -> 25018
5802
Cov: 25018 -> 25018
Cov: 25018 -> 25018
5803
Cov: 25018 -> 25018
Cov: 25018 -> 25018
5804
Cov: 25018 -> 25018
Cov: 25018 -> 25018
5805
Cov: 25018 -> 25018
Cov: 25018 -> 25018
5806
Cov: 25018 -> 25018
Cov: 25018 -> 25018
5807
Cov: 25018 -> 25018
Cov: 25018 -> 25018
5808
Cov: 25018 -> 25018
Cov: 25018 -> 25018
5809
Cov: 25018 -> 25018
Cov: 25018 -> 25018
5810
Cov: 25018 -> 25018
Cov: 25018 -> 25018
5811
Cov: 25018 -> 25018
Cov: 25018 -> 25018
5812
Cov: 25018 -> 25018
Cov: 25018 -> 25018
5813
Cov: 25018 -> 25018
Cov: 25018 -> 25018
5814
Cov: 25018 -> 25019
Cov: 25019 -> 25019
5815
Cov: 25019 -> 25019
Cov: 25019 -> 25019
5816
Cov: 25019 -> 25020
Cov: 25020 -> 25020
5817
Cov: 25020 -> 25020
Cov: 25020 -> 25020
5818
Cov: 25020 -> 25020
Cov: 25020 -> 25020
5819
Cov: 25020 -> 25020
Cov: 25020 -> 25020
5820
Cov: 25020 -> 25020
Cov: 25020 -> 25020
5821
Cov: 25020 -> 25020
Cov: 25020 -> 25020
5822
Cov: 25020 -> 25020
Cov: 25020 -> 25020
5823
Cov: 25020 -> 25020
Cov: 25020 -> 25020
5824
Cov: 25020 -> 25020
Cov: 25020 -> 25020
5825
Cov: 25020 -> 25020
Cov: 25020 -> 25020
5826
Cov: 25020 -> 25020
Cov: 25020 -> 25020
5827
Cov: 25020 -> 25020
Cov: 25020 -> 25020
5828
Cov: 25020 -> 25021
Cov: 25021 -> 25021
5829
Cov: 25021 -> 25022
Cov: 25022 -> 25022
5830
Cov: 25022 -> 25022
Cov: 25022 -> 25022
5831
Cov: 25022 -> 25022
Cov: 25022 -> 25022
5832
Cov: 25022 -> 25022
Cov: 25022 -> 25022
5833
Cov: 25022 -> 25023
Cov: 25023 -> 25023
5834
Cov: 25023 -> 25023
Cov: 25023 -> 25023
5835
Cov: 25023 -> 25025
Cov: 25025 -> 25025
5836
Cov: 25025 -> 25025
Cov: 25025 -> 25025
5837
Cov: 25025 -> 25025
Cov: 25025 -> 25025
5838
{"exception": "RuntimeError", "msg": "logdet: A must be batches of square matrices, but they are 3 by 4 matrices"}
5839
Cov: 25025 -> 25025
Cov: 25025 -> 25025
5840
Cov: 25025 -> 25025
Cov: 25025 -> 25025
5841
Cov: 25025 -> 25025
Cov: 25025 -> 25025
5842
Cov: 25025 -> 25025
Cov: 25025 -> 25025
5843
Cov: 25025 -> 25025
Cov: 25025 -> 25025
5844
Cov: 25025 -> 25025
Cov: 25025 -> 25025
5845
Cov: 25025 -> 25025
Cov: 25025 -> 25025
5846
Cov: 25025 -> 25025
Cov: 25025 -> 25025
5847
Cov: 25025 -> 25025
Cov: 25025 -> 25025
5848
{"exception": "NotImplementedError", "msg": "Could not run 'aten::qscheme' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::qscheme' is only available for these backends: [QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nQuantizedCPU: registered at aten/src/ATen/RegisterQuantizedCPU.cpp:944 [kernel]\nQuantizedCUDA: registered at aten/src/ATen/RegisterQuantizedCUDA.cpp:459 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:153 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:498 [backend fallback]\nFunctionalize: registered at ../aten/src/ATen/FunctionalizeFallbackKernel.cpp:290 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:86 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradMTIA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_0.cpp:16725 [kernel]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:382 [backend fallback]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:249 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:710 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:203 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:161 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:494 [backend fallback]\nPreDispatch: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:165 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:157 [backend fallback]\n"}
5849
{"exception": "RuntimeError", "msg": "Please look up dimensions by name, got: name = None."}
5850
Cov: 25025 -> 25025
Cov: 25025 -> 25025
5851
Cov: 25025 -> 25025
Cov: 25025 -> 25025
5852
Cov: 25025 -> 25026
Cov: 25026 -> 25026
5853
Cov: 25026 -> 25026
Cov: 25026 -> 25026
5854
Cov: 25026 -> 25026
Cov: 25026 -> 25026
5855
Cov: 25026 -> 25027
Cov: 25027 -> 25027
5856
Cov: 25027 -> 25027
Cov: 25027 -> 25027
5857
Cov: 25027 -> 25027
Cov: 25027 -> 25027
5858
Cov: 25027 -> 25028
Cov: 25028 -> 25028
5859
Cov: 25028 -> 25028
Cov: 25028 -> 25028
5860
Cov: 25028 -> 25028
Cov: 25028 -> 25028
5861
Cov: 25028 -> 25028
Cov: 25028 -> 25028
5862
Cov: 25028 -> 25028
Cov: 25028 -> 25028
5863
Cov: 25028 -> 25029
Cov: 25029 -> 25029
5864
Cov: 25029 -> 25029
Cov: 25029 -> 25029
5865
Cov: 25029 -> 25029
Cov: 25029 -> 25029
5866
Cov: 25029 -> 25029
Cov: 25029 -> 25029
5867
Cov: 25029 -> 25030
Cov: 25030 -> 25030
5868
Cov: 25030 -> 25030
Cov: 25030 -> 25030
5869
Cov: 25030 -> 25030
Cov: 25030 -> 25030
5870
Cov: 25030 -> 25030
Cov: 25030 -> 25030
5871
{"exception": "NotImplementedError", "msg": "Could not run 'aten::qscheme' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::qscheme' is only available for these backends: [QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nQuantizedCPU: registered at aten/src/ATen/RegisterQuantizedCPU.cpp:944 [kernel]\nQuantizedCUDA: registered at aten/src/ATen/RegisterQuantizedCUDA.cpp:459 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:153 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:498 [backend fallback]\nFunctionalize: registered at ../aten/src/ATen/FunctionalizeFallbackKernel.cpp:290 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:86 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradMTIA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_0.cpp:16725 [kernel]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:382 [backend fallback]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:249 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:710 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:203 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:161 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:494 [backend fallback]\nPreDispatch: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:165 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:157 [backend fallback]\n"}
5872
Cov: 25030 -> 25030
Cov: 25030 -> 25030
5873
Cov: 25030 -> 25030
Cov: 25030 -> 25030
5874
Cov: 25030 -> 25030
Cov: 25030 -> 25030
5875
{"exception": "NameError", "msg": "name 'optim' is not defined"}
5876
{"exception": "TypeError", "msg": "index_copy_() received an invalid combination of arguments - got (tensor=Tensor, index=Tensor, dim=int, ), but expected one of:\n * (int dim, Tensor index, Tensor source)\n      didn't match because some of the keywords were incorrect: tensor\n * (name dim, Tensor index, Tensor source)\n      didn't match because some of the keywords were incorrect: tensor\n"}
5877
Cov: 25030 -> 25031
Cov: 25031 -> 25031
5878
Cov: 25031 -> 25052
Cov: 25052 -> 25052
5879
{"exception": "RuntimeError", "msg": "torch.ormqr: other.shape[-2] must be equal to input.shape[-2]"}
5880
{"exception": "RuntimeError", "msg": "Expected nested_tensorlist[0].size() > 0 to be true, but got false.  (Could this error message be improved?  If so, please report an enhancement request to PyTorch.)"}
5881
Cov: 25052 -> 25053
Cov: 25053 -> 25053
5882
Cov: 25053 -> 25053
Cov: 25053 -> 25053
5883
Cov: 25053 -> 25053
Cov: 25053 -> 25053
5884
Cov: 25053 -> 25053
Cov: 25053 -> 25053
5885
Cov: 25053 -> 25053
Cov: 25053 -> 25053
5886
Cov: 25053 -> 25053
Cov: 25053 -> 25053
5887
Cov: 25053 -> 25053
Cov: 25053 -> 25053
5888
Cov: 25053 -> 25055
Cov: 25055 -> 25055
5889
Cov: 25055 -> 25055
Cov: 25055 -> 25055
5890
Cov: 25055 -> 25055
Cov: 25055 -> 25055
5891
Cov: 25055 -> 25055
Cov: 25055 -> 25055
5892
Cov: 25055 -> 25055
Cov: 25055 -> 25055
5893
Cov: 25055 -> 25055
Cov: 25055 -> 25055
5894
Cov: 25055 -> 25055
Cov: 25055 -> 25055
5895
Cov: 25055 -> 25055
Cov: 25055 -> 25055
5896
Cov: 25055 -> 25055
Cov: 25055 -> 25055
5897
Cov: 25055 -> 25055
Cov: 25055 -> 25055
5898
Cov: 25055 -> 25056
Cov: 25056 -> 25056
5899
Cov: 25056 -> 25056
Cov: 25056 -> 25056
5900
Cov: 25056 -> 25058
Cov: 25058 -> 25058
5901
Cov: 25058 -> 25058
Cov: 25058 -> 25058
5902
Cov: 25058 -> 25058
Cov: 25058 -> 25058
5903
Cov: 25058 -> 25058
Cov: 25058 -> 25058
5904
Cov: 25058 -> 25058
Cov: 25058 -> 25058
5905
Cov: 25058 -> 25058
Cov: 25058 -> 25058
5906
Cov: 25058 -> 25058
Cov: 25058 -> 25058
5907
Cov: 25058 -> 25058
Cov: 25058 -> 25058
5908
Cov: 25058 -> 25058
Cov: 25058 -> 25058
5909
Cov: 25058 -> 25058
Cov: 25058 -> 25058
5910
Cov: 25058 -> 25064
Cov: 25064 -> 25064
5911
Cov: 25064 -> 25065
Cov: 25065 -> 25065
5912
Cov: 25065 -> 25065
Cov: 25065 -> 25065
5913
Cov: 25065 -> 25065
Cov: 25065 -> 25065
5914
Cov: 25065 -> 25065
Cov: 25065 -> 25065
5915
Cov: 25065 -> 25065
Cov: 25065 -> 25065
5916
{"exception": "NameError", "msg": "name 'layer_norm' is not defined"}
5917
Cov: 25065 -> 25066
Cov: 25066 -> 25066
5918
Cov: 25066 -> 25066
Cov: 25066 -> 25066
5919
Cov: 25066 -> 25066
Cov: 25066 -> 25066
5920
Cov: 25066 -> 25067
Cov: 25067 -> 25067
5921
Cov: 25067 -> 25067
Cov: 25067 -> 25067
5922
Cov: 25067 -> 25067
Cov: 25067 -> 25067
5923
Cov: 25067 -> 25067
Cov: 25067 -> 25067
5924
{"exception": "_LinAlgError", "msg": "cholesky: The factorization could not be completed because the input is not positive-definite (the leading minor of order 1 is not positive-definite)."}
5925
Cov: 25067 -> 25067
Cov: 25067 -> 25067
5926
Cov: 25067 -> 25067
Cov: 25067 -> 25067
5927
{"exception": "RuntimeError", "msg": "scatter(): Expected self.dtype to be equal to src.dtype"}
5928
Cov: 25067 -> 25068
Cov: 25068 -> 25068
5929
Cov: 25068 -> 25068
Cov: 25068 -> 25068
5930
Cov: 25068 -> 25069
Cov: 25069 -> 25069
5931
{"exception": "RuntimeError", "msg": "element 0 of tensors does not require grad and does not have a grad_fn"}
5932
Cov: 25069 -> 25079
Cov: 25079 -> 25079
5933
Cov: 25079 -> 25079
Cov: 25079 -> 25079
5934
Cov: 25079 -> 25079
Cov: 25079 -> 25079
5935
Cov: 25079 -> 25079
Cov: 25079 -> 25079
5936
Cov: 25079 -> 25079
Cov: 25079 -> 25079
5937
Cov: 25079 -> 25079
Cov: 25079 -> 25079
5938
Cov: 25079 -> 25079
Cov: 25079 -> 25079
5939
Cov: 25079 -> 25079
Cov: 25079 -> 25079
5940
Cov: 25079 -> 25079
Cov: 25079 -> 25079
5941
Cov: 25079 -> 25079
Cov: 25079 -> 25079
5942
Cov: 25079 -> 25079
Cov: 25079 -> 25079
5943
Cov: 25079 -> 25079
Cov: 25079 -> 25079
5944
Cov: 25079 -> 25079
Cov: 25079 -> 25079
5945
Cov: 25079 -> 25079
Cov: 25079 -> 25079
5946
Cov: 25079 -> 25080
Cov: 25080 -> 25080
5947
Cov: 25080 -> 25080
Cov: 25080 -> 25080
5948
{"exception": "RuntimeError", "msg": "self and mat2 must have the same dtype, but got Float and Double"}
5949
Cov: 25080 -> 25080
Cov: 25080 -> 25080
5950
Cov: 25080 -> 25080
Cov: 25080 -> 25080
5951
Cov: 25080 -> 25080
Cov: 25080 -> 25080
5952
Cov: 25080 -> 25080
Cov: 25080 -> 25080
5953
Cov: 25080 -> 25080
Cov: 25080 -> 25080
5954
Cov: 25080 -> 25080
Cov: 25080 -> 25080
5955
Cov: 25080 -> 25080
Cov: 25080 -> 25080
5956
Cov: 25080 -> 25080
Cov: 25080 -> 25080
5957
Cov: 25080 -> 25080
Cov: 25080 -> 25080
5958
Cov: 25080 -> 25080
Cov: 25080 -> 25080
5959
Cov: 25080 -> 25080
Cov: 25080 -> 25080
5960
Cov: 25080 -> 25081
Cov: 25081 -> 25081
5961
Cov: 25081 -> 25081
Cov: 25081 -> 25081
5962
Cov: 25081 -> 25081
Cov: 25081 -> 25081
5963
Cov: 25081 -> 25081
Cov: 25081 -> 25081
5964
Cov: 25081 -> 25082
Cov: 25082 -> 25082
5965
Cov: 25082 -> 25082
Cov: 25082 -> 25082
5966
Cov: 25082 -> 25082
Cov: 25082 -> 25082
5967
Cov: 25082 -> 25082
Cov: 25082 -> 25082
5968
Cov: 25082 -> 25082
Cov: 25082 -> 25082
5969
Cov: 25082 -> 25083
Cov: 25083 -> 25083
5970
Cov: 25083 -> 25084
Cov: 25084 -> 25084
5971
Cov: 25084 -> 25084
Cov: 25084 -> 25084
5972
Cov: 25084 -> 25084
Cov: 25084 -> 25084
5973
Cov: 25084 -> 25085
Cov: 25085 -> 25085
5974
Cov: 25085 -> 25085
Cov: 25085 -> 25085
5975
Cov: 25085 -> 25085
Cov: 25085 -> 25085
5976
Cov: 25085 -> 25085
Cov: 25085 -> 25085
5977
Cov: 25085 -> 25109
Cov: 25109 -> 25109
5978
Cov: 25109 -> 25109
Cov: 25109 -> 25109
5979
Cov: 25109 -> 25109
Cov: 25109 -> 25109
5980
Cov: 25109 -> 25109
Cov: 25109 -> 25109
5981
Cov: 25109 -> 25109
Cov: 25109 -> 25109
5982
Cov: 25109 -> 25109
Cov: 25109 -> 25109
5983
Cov: 25109 -> 25110
Cov: 25110 -> 25110
5984
Cov: 25110 -> 25110
Cov: 25110 -> 25110
5985
{"exception": "RuntimeError", "msg": "For integral input tensors, argument alpha must not be a floating point number."}
5986
{"exception": "TypeError", "msg": "histogram() received an invalid combination of arguments - got (Tensor, bins=int), but expected one of:\n * (Tensor bins, *, Tensor weight, bool density)\n * (int bins, *, tuple of floats range, Tensor weight, bool density)\n"}
5987
Cov: 25110 -> 25111
Cov: 25111 -> 25111
5988
Cov: 25111 -> 25111
Cov: 25111 -> 25111
5989
Cov: 25111 -> 25111
Cov: 25111 -> 25111
5990
Cov: 25111 -> 25111
Cov: 25111 -> 25111
5991
Cov: 25111 -> 25111
Cov: 25111 -> 25111
5992
Cov: 25111 -> 25111
Cov: 25111 -> 25111
5993
Cov: 25111 -> 25111
Cov: 25111 -> 25111
5994
Cov: 25111 -> 25111
Cov: 25111 -> 25111
5995
Cov: 25111 -> 25111
Cov: 25111 -> 25111
5996
Cov: 25111 -> 25111
Cov: 25111 -> 25111
5997
Cov: 25111 -> 25111
Cov: 25111 -> 25111
5998
Cov: 25111 -> 25111
Cov: 25111 -> 25111
5999
Cov: 25111 -> 25112
Cov: 25112 -> 25112
6000
Cov: 25112 -> 25112
Cov: 25112 -> 25112
6001
Cov: 25112 -> 25112
Cov: 25112 -> 25112
6002
Cov: 25112 -> 25113
Cov: 25113 -> 25113
6003
Cov: 25113 -> 25114
Cov: 25114 -> 25114
6004
{"exception": "RuntimeError", "msg": "element 0 of tensors does not require grad and does not have a grad_fn"}
6005
Cov: 25114 -> 25115
Cov: 25115 -> 25115
6006
Cov: 25115 -> 25115
Cov: 25115 -> 25115
6007
Cov: 25115 -> 25115
Cov: 25115 -> 25115
6008
Cov: 25115 -> 25115
Cov: 25115 -> 25115
6009
Cov: 25115 -> 25115
Cov: 25115 -> 25115
6010
Cov: 25115 -> 25115
Cov: 25115 -> 25115
6011
Cov: 25115 -> 25115
Cov: 25115 -> 25115
6012
Cov: 25115 -> 25115
Cov: 25115 -> 25115
6013
Cov: 25115 -> 25115
Cov: 25115 -> 25115
6014
Cov: 25115 -> 25116
Cov: 25116 -> 25116
6015
{"exception": "TypeError", "msg": "index_copy_() received an invalid combination of arguments - got (tensor=Tensor, index=Tensor, dim=int, ), but expected one of:\n * (int dim, Tensor index, Tensor source)\n      didn't match because some of the keywords were incorrect: tensor\n * (name dim, Tensor index, Tensor source)\n      didn't match because some of the keywords were incorrect: tensor\n"}
6016
Cov: 25116 -> 25116
Cov: 25116 -> 25116
6017
Cov: 25116 -> 25121
Cov: 25121 -> 25121
6018
Cov: 25121 -> 25121
Cov: 25121 -> 25121
6019
Cov: 25121 -> 25121
Cov: 25121 -> 25121
6020
Cov: 25121 -> 25121
Cov: 25121 -> 25121
6021
Cov: 25121 -> 25121
Cov: 25121 -> 25121
6022
Cov: 25121 -> 25121
Cov: 25121 -> 25121
6023
Cov: 25121 -> 25121
Cov: 25121 -> 25121
6024
Cov: 25121 -> 25121
Cov: 25121 -> 25121
6025
Cov: 25121 -> 25121
Cov: 25121 -> 25121
6026
Cov: 25121 -> 25121
Cov: 25121 -> 25121
6027
Cov: 25121 -> 25121
Cov: 25121 -> 25121
6028
Cov: 25121 -> 25121
Cov: 25121 -> 25121
6029
Cov: 25121 -> 25121
Cov: 25121 -> 25121
6030
Cov: 25121 -> 25121
Cov: 25121 -> 25121
6031
Cov: 25121 -> 25121
Cov: 25121 -> 25121
6032
Cov: 25121 -> 25121
Cov: 25121 -> 25121
6033
Cov: 25121 -> 25122
Cov: 25122 -> 25122
6034
Cov: 25122 -> 25122
Cov: 25122 -> 25122
6035
Cov: 25122 -> 25122
Cov: 25122 -> 25122
6036
Cov: 25122 -> 25123
Cov: 25123 -> 25123
6037
Cov: 25123 -> 25123
Cov: 25123 -> 25123
6038
Cov: 25123 -> 25123
Cov: 25123 -> 25123
6039
Cov: 25123 -> 25123
Cov: 25123 -> 25123
6040
Cov: 25123 -> 25123
Cov: 25123 -> 25123
6041
Cov: 25123 -> 25123
Cov: 25123 -> 25123
6042
Cov: 25123 -> 25123
Cov: 25123 -> 25123
6043
Cov: 25123 -> 25124
Cov: 25124 -> 25124
6044
Cov: 25124 -> 25124
Cov: 25124 -> 25124
6045
Cov: 25124 -> 25124
Cov: 25124 -> 25124
6046
Cov: 25124 -> 25124
Cov: 25124 -> 25124
6047
Cov: 25124 -> 25124
Cov: 25124 -> 25124
6048
Cov: 25124 -> 25124
Cov: 25124 -> 25124
6049
Cov: 25124 -> 25124
Cov: 25124 -> 25124
6050
Cov: 25124 -> 25124
Cov: 25124 -> 25124
6051
Cov: 25124 -> 25124
Cov: 25124 -> 25124
6052
Cov: 25124 -> 25124
Cov: 25124 -> 25124
6053
{"exception": "TypeError", "msg": "copy_() received an invalid combination of arguments - got (), but expected (Tensor other, bool non_blocking)"}
6054
Cov: 25124 -> 25124
Cov: 25124 -> 25124
6055
Cov: 25124 -> 25124
Cov: 25124 -> 25124
6056
Cov: 25124 -> 25124
Cov: 25124 -> 25124
6057
Cov: 25124 -> 25124
Cov: 25124 -> 25124
6058
Cov: 25124 -> 25124
Cov: 25124 -> 25124
6059
Cov: 25124 -> 25124
Cov: 25124 -> 25124
6060
Cov: 25124 -> 25124
Cov: 25124 -> 25124
6061
Cov: 25124 -> 25124
Cov: 25124 -> 25124
6062
Cov: 25124 -> 25124
Cov: 25124 -> 25124
6063
Cov: 25124 -> 25124
Cov: 25124 -> 25124
6064
Cov: 25124 -> 25125
Cov: 25125 -> 25125
6065
Cov: 25125 -> 25125
Cov: 25125 -> 25125
6066
Cov: 25125 -> 25125
Cov: 25125 -> 25125
6067
Cov: 25125 -> 25126
Cov: 25126 -> 25126
6068
Cov: 25126 -> 25126
Cov: 25126 -> 25126
6069
Cov: 25126 -> 25126
Cov: 25126 -> 25126
6070
Cov: 25126 -> 25127
Cov: 25127 -> 25127
6071
Cov: 25127 -> 25127
Cov: 25127 -> 25127
6072
Cov: 25127 -> 25128
Cov: 25128 -> 25128
6073
Cov: 25128 -> 25128
Cov: 25128 -> 25128
6074
Cov: 25128 -> 25128
Cov: 25128 -> 25128
6075
Cov: 25128 -> 25128
Cov: 25128 -> 25128
6076
Cov: 25128 -> 25128
Cov: 25128 -> 25128
6077
Cov: 25128 -> 25128
Cov: 25128 -> 25128
6078
Cov: 25128 -> 25128
Cov: 25128 -> 25128
6079
Cov: 25128 -> 25129
Cov: 25129 -> 25129
6080
Cov: 25129 -> 25129
Cov: 25129 -> 25129
6081
Cov: 25129 -> 25130
Cov: 25130 -> 25130
6082
Cov: 25130 -> 25130
Cov: 25130 -> 25130
6083
{"exception": "RuntimeError", "msg": "t_() expects a tensor with <= 2 dimensions, but self is 4D"}
6084
Cov: 25130 -> 25130
Cov: 25130 -> 25130
6085
{"exception": "RuntimeError", "msg": "all: You passed a dimname (string) to this op in place of a dimension index but it does not yet support this behavior. Please pass a dimension index to work around this."}
6086
{"exception": "RuntimeError", "msg": "This function was deprecated since version 1.9 and is now removed. The default behavior has changed from using the upper triangular portion of the matrix by default to using the lower triangular portion.\n\nL, _ = torch.symeig(A, upper=upper) should be replaced with:\nL = torch.linalg.eigvalsh(A, UPLO='U' if upper else 'L')\n\nand\n\nL, V = torch.symeig(A, eigenvectors=True) should be replaced with:\nL, V = torch.linalg.eigh(A, UPLO='U' if upper else 'L')"}
6087
Cov: 25130 -> 25132
Cov: 25132 -> 25132
6088
Cov: 25132 -> 25132
Cov: 25132 -> 25132
6089
Cov: 25132 -> 25132
Cov: 25132 -> 25132
6090
Cov: 25132 -> 25132
Cov: 25132 -> 25132
6091
Cov: 25132 -> 25132
Cov: 25132 -> 25132
6092
Cov: 25132 -> 25133
Cov: 25133 -> 25133
6093
Cov: 25133 -> 25133
Cov: 25133 -> 25133
6094
Cov: 25133 -> 25133
Cov: 25133 -> 25133
6095
Cov: 25133 -> 25134
Cov: 25134 -> 25134
6096
{"exception": "RuntimeError", "msg": "unsupported operation: some elements of the input tensor and the written-to tensor refer to a single memory location. Please clone() the tensor before performing the operation."}
6097
Cov: 25134 -> 25134
Cov: 25134 -> 25134
6098
Cov: 25134 -> 25135
Cov: 25135 -> 25135
6099
Cov: 25135 -> 25135
Cov: 25135 -> 25135
6100
Cov: 25135 -> 25135
Cov: 25135 -> 25135
6101
{"exception": "TypeError", "msg": "'getset_descriptor' object is not callable"}
6102
Cov: 25135 -> 25135
Cov: 25135 -> 25135
6103
Cov: 25135 -> 25135
Cov: 25135 -> 25135
6104
Cov: 25135 -> 25136
Cov: 25136 -> 25136
6105
{"exception": "RuntimeError", "msg": "Index tensor must have the same number of dimensions as self tensor"}
6106
Cov: 25136 -> 25136
Cov: 25136 -> 25136
6107
Cov: 25136 -> 25136
Cov: 25136 -> 25136
6108
Cov: 25136 -> 25137
Cov: 25137 -> 25137
6109
Cov: 25137 -> 25137
Cov: 25137 -> 25137
6110
Cov: 25137 -> 25137
Cov: 25137 -> 25137
6111
Cov: 25137 -> 25137
Cov: 25137 -> 25137
6112
Cov: 25137 -> 25137
Cov: 25137 -> 25137
6113
Cov: 25137 -> 25137
Cov: 25137 -> 25137
6114
Cov: 25137 -> 25137
Cov: 25137 -> 25137
6115
Cov: 25137 -> 25137
Cov: 25137 -> 25137
6116
Cov: 25137 -> 25137
Cov: 25137 -> 25137
6117
Cov: 25137 -> 25137
Cov: 25137 -> 25137
6118
Cov: 25137 -> 25137
Cov: 25137 -> 25137
6119
Cov: 25137 -> 25137
Cov: 25137 -> 25137
6120
{"exception": "RuntimeError", "msg": "vector + matrix @ vector expected, got 2, 2, 1"}
6121
Cov: 25137 -> 25138
Cov: 25138 -> 25138
6122
Cov: 25138 -> 25138
Cov: 25138 -> 25138
6123
Cov: 25138 -> 25138
Cov: 25138 -> 25138
6124
Cov: 25138 -> 25139
Cov: 25139 -> 25139
6125
Cov: 25139 -> 25139
Cov: 25139 -> 25139
6126
Cov: 25139 -> 25139
Cov: 25139 -> 25139
6127
Cov: 25139 -> 25140
Cov: 25140 -> 25140
6128
Cov: 25140 -> 25140
Cov: 25140 -> 25140
6129
Cov: 25140 -> 25140
Cov: 25140 -> 25140
6130
Cov: 25140 -> 25140
Cov: 25140 -> 25140
6131
Cov: 25140 -> 25140
Cov: 25140 -> 25140
6132
Cov: 25140 -> 25140
Cov: 25140 -> 25140
6133
Cov: 25140 -> 25140
Cov: 25140 -> 25140
6134
Cov: 25140 -> 25140
Cov: 25140 -> 25140
6135
Cov: 25140 -> 25140
Cov: 25140 -> 25140
6136
Cov: 25140 -> 25140
Cov: 25140 -> 25140
6137
Cov: 25140 -> 25140
Cov: 25140 -> 25140
6138
Cov: 25140 -> 25140
Cov: 25140 -> 25140
6139
Cov: 25140 -> 25140
Cov: 25140 -> 25140
6140
Cov: 25140 -> 25143
Cov: 25143 -> 25143
6141
Cov: 25143 -> 25143
Cov: 25143 -> 25143
6142
Cov: 25143 -> 25143
Cov: 25143 -> 25143
6143
Cov: 25143 -> 25144
Cov: 25144 -> 25144
6144
Cov: 25144 -> 25144
Cov: 25144 -> 25144
6145
Cov: 25144 -> 25145
Cov: 25145 -> 25145
6146
Cov: 25145 -> 25145
Cov: 25145 -> 25145
6147
Cov: 25145 -> 25145
Cov: 25145 -> 25145
6148
Cov: 25145 -> 25145
Cov: 25145 -> 25145
6149
Cov: 25145 -> 25145
Cov: 25145 -> 25145
6150
Cov: 25145 -> 25145
Cov: 25145 -> 25145
6151
Cov: 25145 -> 25145
Cov: 25145 -> 25145
6152
Cov: 25145 -> 25145
Cov: 25145 -> 25145
6153
Cov: 25145 -> 25146
Cov: 25146 -> 25146
6154
Cov: 25146 -> 25146
Cov: 25146 -> 25146
6155
Cov: 25146 -> 25146
Cov: 25146 -> 25146
6156
Cov: 25146 -> 25146
Cov: 25146 -> 25146
6157
Cov: 25146 -> 25146
Cov: 25146 -> 25146
6158
Cov: 25146 -> 25146
Cov: 25146 -> 25146
6159
Cov: 25146 -> 25146
Cov: 25146 -> 25146
6160
Cov: 25146 -> 25146
Cov: 25146 -> 25146
6161
Cov: 25146 -> 25146
Cov: 25146 -> 25146
6162
Cov: 25146 -> 25146
Cov: 25146 -> 25146
6163
Cov: 25146 -> 25146
Cov: 25146 -> 25146
6164
{"exception": "RuntimeError", "msg": "element 0 of tensors does not require grad and does not have a grad_fn"}
6165
{"exception": "TypeError", "msg": "'getset_descriptor' object is not callable"}
6166
Cov: 25146 -> 25159
Cov: 25159 -> 25159
6167
Cov: 25159 -> 25159
Cov: 25159 -> 25159
6168
Cov: 25159 -> 25160
Cov: 25160 -> 25160
6169
Cov: 25160 -> 25160
Cov: 25160 -> 25160
6170
Cov: 25160 -> 25160
Cov: 25160 -> 25160
6171
Cov: 25160 -> 25160
Cov: 25160 -> 25160
6172
{"exception": "RuntimeError", "msg": "element 0 of tensors does not require grad and does not have a grad_fn"}
6173
Cov: 25160 -> 25178
Cov: 25178 -> 25178
6174
Cov: 25178 -> 25179
Cov: 25179 -> 25179
6175
Cov: 25179 -> 25179
Cov: 25179 -> 25179
6176
Cov: 25179 -> 25179
Cov: 25179 -> 25179
6177
Cov: 25179 -> 25179
Cov: 25179 -> 25179
6178
Cov: 25179 -> 25179
Cov: 25179 -> 25179
6179
Cov: 25179 -> 25179
Cov: 25179 -> 25179
6180
Cov: 25179 -> 25179
Cov: 25179 -> 25179
6181
{"exception": "NotImplementedError", "msg": "Could not run 'aten::qscheme' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::qscheme' is only available for these backends: [QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nQuantizedCPU: registered at aten/src/ATen/RegisterQuantizedCPU.cpp:944 [kernel]\nQuantizedCUDA: registered at aten/src/ATen/RegisterQuantizedCUDA.cpp:459 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:153 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:498 [backend fallback]\nFunctionalize: registered at ../aten/src/ATen/FunctionalizeFallbackKernel.cpp:290 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:86 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradMTIA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_0.cpp:16725 [kernel]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:382 [backend fallback]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:249 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:710 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:203 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:161 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:494 [backend fallback]\nPreDispatch: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:165 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:157 [backend fallback]\n"}
6182
Cov: 25179 -> 25179
Cov: 25179 -> 25179
6183
Cov: 25179 -> 25180
Cov: 25180 -> 25180
6184
Cov: 25180 -> 25180
Cov: 25180 -> 25180
6185
Cov: 25180 -> 25180
Cov: 25180 -> 25180
6186
Cov: 25180 -> 25180
Cov: 25180 -> 25180
6187
Cov: 25180 -> 25180
Cov: 25180 -> 25180
6188
Cov: 25180 -> 25180
Cov: 25180 -> 25180
6189
Cov: 25180 -> 25180
Cov: 25180 -> 25180
6190
Cov: 25180 -> 25180
Cov: 25180 -> 25180
6191
Cov: 25180 -> 25181
Cov: 25181 -> 25181
6192
Cov: 25181 -> 25181
Cov: 25181 -> 25181
6193
Cov: 25181 -> 25181
Cov: 25181 -> 25181
6194
Cov: 25181 -> 25181
Cov: 25181 -> 25181
6195
Cov: 25181 -> 25181
Cov: 25181 -> 25181
6196
Cov: 25181 -> 25181
Cov: 25181 -> 25181
6197
{"exception": "TypeError", "msg": "arctanh_() takes no keyword arguments"}
6198
Cov: 25181 -> 25182
Cov: 25182 -> 25182
6199
Cov: 25182 -> 25183
Cov: 25183 -> 25183
6200
Cov: 25183 -> 25183
Cov: 25183 -> 25183
6201
Cov: 25183 -> 25183
Cov: 25183 -> 25183
6202
Cov: 25183 -> 25184
Cov: 25184 -> 25184
6203
Cov: 25184 -> 25184
Cov: 25184 -> 25184
6204
Cov: 25184 -> 25184
Cov: 25184 -> 25184
6205
Cov: 25184 -> 25185
Cov: 25185 -> 25185
6206
Cov: 25185 -> 25186
Cov: 25186 -> 25186
6207
Cov: 25186 -> 25186
Cov: 25186 -> 25186
6208
Cov: 25186 -> 25187
Cov: 25187 -> 25187
6209
Cov: 25187 -> 25187
Cov: 25187 -> 25187
6210
{"exception": "NotImplementedError", "msg": "Could not run 'aten::q_per_channel_scales' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::q_per_channel_scales' is only available for these backends: [QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nQuantizedCPU: registered at aten/src/ATen/RegisterQuantizedCPU.cpp:944 [kernel]\nQuantizedCUDA: registered at aten/src/ATen/RegisterQuantizedCUDA.cpp:459 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:153 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:498 [backend fallback]\nFunctionalize: registered at ../aten/src/ATen/FunctionalizeFallbackKernel.cpp:290 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:86 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradMTIA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_3.cpp:14610 [kernel]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:382 [backend fallback]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:249 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:710 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:203 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:161 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:494 [backend fallback]\nPreDispatch: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:165 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:157 [backend fallback]\n"}
6211
Cov: 25187 -> 25187
Cov: 25187 -> 25187
6212
Cov: 25187 -> 25187
Cov: 25187 -> 25187
6213
Cov: 25187 -> 25187
Cov: 25187 -> 25187
6214
Cov: 25187 -> 25187
Cov: 25187 -> 25187
6215
{"exception": "RuntimeError", "msg": "index_copy_(): Source/destination tensor must have same slice shapes. Destination slice shape: 2 at dimension 1 and source slice shape: 3 at dimension 0."}
6216
Cov: 25187 -> 25187
Cov: 25187 -> 25187
6217
Cov: 25187 -> 25187
Cov: 25187 -> 25187
6218
Cov: 25187 -> 25187
Cov: 25187 -> 25187
6219
Cov: 25187 -> 25187
Cov: 25187 -> 25187
6220
Cov: 25187 -> 25187
Cov: 25187 -> 25187
6221
Cov: 25187 -> 25187
Cov: 25187 -> 25187
6222
Cov: 25187 -> 25187
Cov: 25187 -> 25187
6223
{"exception": "TypeError", "msg": "'getset_descriptor' object is not callable"}
6224
Cov: 25187 -> 25188
Cov: 25188 -> 25188
6225
Cov: 25188 -> 25188
Cov: 25188 -> 25188
6226
Cov: 25188 -> 25188
Cov: 25188 -> 25188
6227
Cov: 25188 -> 25189
Cov: 25189 -> 25189
6228
Cov: 25189 -> 25189
Cov: 25189 -> 25189
6229
Cov: 25189 -> 25189
Cov: 25189 -> 25189
6230
Cov: 25189 -> 25189
Cov: 25189 -> 25189
6231
{"exception": "TypeError", "msg": "addr_() missing 1 required positional arguments: \"vec2\""}
6232
Cov: 25189 -> 25190
Cov: 25190 -> 25190
6233
Cov: 25190 -> 25191
Cov: 25191 -> 25191
6234
Cov: 25191 -> 25191
Cov: 25191 -> 25191
6235
Cov: 25191 -> 25191
Cov: 25191 -> 25191
6236
Cov: 25191 -> 25191
Cov: 25191 -> 25191
6237
Cov: 25191 -> 25191
Cov: 25191 -> 25191
6238
Cov: 25191 -> 25191
Cov: 25191 -> 25191
6239
Cov: 25191 -> 25192
Cov: 25192 -> 25192
6240
{"exception": "NotImplementedError", "msg": "Could not run 'aten::q_per_channel_zero_points' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::q_per_channel_zero_points' is only available for these backends: [QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nQuantizedCPU: registered at aten/src/ATen/RegisterQuantizedCPU.cpp:944 [kernel]\nQuantizedCUDA: registered at aten/src/ATen/RegisterQuantizedCUDA.cpp:459 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:153 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:498 [backend fallback]\nFunctionalize: registered at ../aten/src/ATen/FunctionalizeFallbackKernel.cpp:290 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:86 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradMTIA: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_1.cpp:16002 [kernel]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:382 [backend fallback]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:249 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:710 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:203 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:161 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:494 [backend fallback]\nPreDispatch: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:165 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:157 [backend fallback]\n"}
6241
Cov: 25192 -> 25192
Cov: 25192 -> 25192
6242
Cov: 25192 -> 25192
Cov: 25192 -> 25192
6243
Cov: 25192 -> 25192
Cov: 25192 -> 25192
6244
Cov: 25192 -> 25192
Cov: 25192 -> 25192
6245
Cov: 25192 -> 25192
Cov: 25192 -> 25192
6246
Cov: 25192 -> 25193
Cov: 25193 -> 25193
6247
Cov: 25193 -> 25193
Cov: 25193 -> 25193
6248
Cov: 25193 -> 25194
Cov: 25194 -> 25194
6249
Cov: 25194 -> 25194
Cov: 25194 -> 25194
6250
Cov: 25194 -> 25195
Cov: 25195 -> 25195
6251
Cov: 25195 -> 25195
Cov: 25195 -> 25195
6252
Cov: 25195 -> 25195
Cov: 25195 -> 25195
6253
Cov: 25195 -> 25195
Cov: 25195 -> 25195
6254
Cov: 25195 -> 25195
Cov: 25195 -> 25195
6255
Cov: 25195 -> 25195
Cov: 25195 -> 25195
6256
Cov: 25195 -> 25195
Cov: 25195 -> 25195
6257
Cov: 25195 -> 25196
Cov: 25196 -> 25196
6258
Cov: 25196 -> 25196
Cov: 25196 -> 25196
6259
Cov: 25196 -> 25197
Cov: 25197 -> 25197
6260
Cov: 25197 -> 25197
Cov: 25197 -> 25197
6261
Cov: 25197 -> 25198
Cov: 25198 -> 25198
6262
Cov: 25198 -> 25198
Cov: 25198 -> 25198
6263
Cov: 25198 -> 25198
Cov: 25198 -> 25198
6264
Cov: 25198 -> 25198
Cov: 25198 -> 25198
6265
Cov: 25198 -> 25198
Cov: 25198 -> 25198
6266
Cov: 25198 -> 25199
Cov: 25199 -> 25199
6267
Cov: 25199 -> 25199
Cov: 25199 -> 25199
6268
Cov: 25199 -> 25199
Cov: 25199 -> 25199
6269
Cov: 25199 -> 25201
Cov: 25201 -> 25201
6270
Cov: 25201 -> 25202
Cov: 25202 -> 25202
6271
Cov: 25202 -> 25204
Cov: 25204 -> 25204
6272
Cov: 25204 -> 25204
Cov: 25204 -> 25204
6273
Cov: 25204 -> 25205
Cov: 25205 -> 25205
6274
Cov: 25205 -> 25205
Cov: 25205 -> 25205
6275
Cov: 25205 -> 25205
Cov: 25205 -> 25205
6276
Cov: 25205 -> 25206
Cov: 25206 -> 25206
6277
Cov: 25206 -> 25206
Cov: 25206 -> 25206
6278
Cov: 25206 -> 25206
Cov: 25206 -> 25206
6279
Cov: 25206 -> 25206
Cov: 25206 -> 25206
6280
Cov: 25206 -> 25206
Cov: 25206 -> 25206
6281
Cov: 25206 -> 25206
Cov: 25206 -> 25206
6282
Cov: 25206 -> 25206
Cov: 25206 -> 25206
6283
Cov: 25206 -> 25206
Cov: 25206 -> 25206
6284
Cov: 25206 -> 25206
Cov: 25206 -> 25206
6285
Cov: 25206 -> 25206
Cov: 25206 -> 25206
6286
Cov: 25206 -> 25206
Cov: 25206 -> 25206
6287
Cov: 25206 -> 25207
Cov: 25207 -> 25207
6288
Cov: 25207 -> 25207
Cov: 25207 -> 25207
6289
Cov: 25207 -> 25209
Cov: 25209 -> 25209
6290
Cov: 25209 -> 25209
Cov: 25209 -> 25209
6291
Cov: 25209 -> 25209
Cov: 25209 -> 25209
6292
Cov: 25209 -> 25209
Cov: 25209 -> 25209
6293
{"exception": "RuntimeError", "msg": "expand(torch.FloatTensor{[10, 3, 4]}, size=[3, 5]): the number of sizes provided (2) must be greater or equal to the number of dimensions in the tensor (3)"}
6294
Cov: 25209 -> 25209
Cov: 25209 -> 25209
6295
Cov: 25209 -> 25209
Cov: 25209 -> 25209
6296
Cov: 25209 -> 25209
Cov: 25209 -> 25209
6297
{"exception": "RuntimeError", "msg": "indices expected sparse coordinate tensor layout but got Strided"}
6298
Cov: 25209 -> 25209
Cov: 25209 -> 25209
6299
Cov: 25209 -> 25209
Cov: 25209 -> 25209
6300
Cov: 25209 -> 25209
Cov: 25209 -> 25209
6301
Cov: 25209 -> 25209
Cov: 25209 -> 25209
6302
Cov: 25209 -> 25210
Cov: 25210 -> 25210
6303
Cov: 25210 -> 25210
Cov: 25210 -> 25210
6304
Cov: 25210 -> 25210
Cov: 25210 -> 25210
6305
{"exception": "RuntimeError", "msg": "The expanded size of the tensor (5) must match the existing size (4) at non-singleton dimension 2.  Target sizes: [10, 3, 5].  Tensor sizes: [10, 3, 4]"}
6306
Cov: 25210 -> 25210
Cov: 25210 -> 25210
6307
Cov: 25210 -> 25211
Cov: 25211 -> 25211
6308
Cov: 25211 -> 25212
Cov: 25212 -> 25212
6309
Cov: 25212 -> 25212
Cov: 25212 -> 25212
6310
Cov: 25212 -> 25212
Cov: 25212 -> 25212
6311
Cov: 25212 -> 25212
Cov: 25212 -> 25212
6312
{"exception": "RuntimeError", "msg": "1D tensors expected, but got 2D and 2D tensors"}
6313
Cov: 25212 -> 25212
Cov: 25212 -> 25212
6314
Cov: 25212 -> 25212
Cov: 25212 -> 25212
6315
Cov: 25212 -> 25212
Cov: 25212 -> 25212
6316
Cov: 25212 -> 25212
Cov: 25212 -> 25212
6317
Cov: 25212 -> 25212
Cov: 25212 -> 25212
6318
Cov: 25212 -> 25212
Cov: 25212 -> 25212
6319
Cov: 25212 -> 25212
Cov: 25212 -> 25212
6320
Cov: 25212 -> 25212
Cov: 25212 -> 25212
6321
Cov: 25212 -> 25212
Cov: 25212 -> 25212
6322
Cov: 25212 -> 25212
Cov: 25212 -> 25212
6323
Cov: 25212 -> 25212
Cov: 25212 -> 25212
6324
Cov: 25212 -> 25212
Cov: 25212 -> 25212
6325
Cov: 25212 -> 25213
Cov: 25213 -> 25213
6326
Cov: 25213 -> 25214
Cov: 25214 -> 25214
6327
Cov: 25214 -> 25214
Cov: 25214 -> 25214
6328
Cov: 25214 -> 25214
Cov: 25214 -> 25214
6329
Cov: 25214 -> 25215
Cov: 25215 -> 25215
6330
Cov: 25215 -> 25215
Cov: 25215 -> 25215
6331
Cov: 25215 -> 25215
Cov: 25215 -> 25215
6332
Cov: 25215 -> 25215
Cov: 25215 -> 25215
6333
Cov: 25215 -> 25215
Cov: 25215 -> 25215
6334
Cov: 25215 -> 25215
Cov: 25215 -> 25215
6335
Cov: 25215 -> 25215
Cov: 25215 -> 25215
6336
Cov: 25215 -> 25215
Cov: 25215 -> 25215
6337
{"exception": "NameError", "msg": "name 'A' is not defined"}
6338
Cov: 25215 -> 25216
Cov: 25216 -> 25216
6339
Cov: 25216 -> 25216
Cov: 25216 -> 25216
6340
Cov: 25216 -> 25216
Cov: 25216 -> 25216
6341
Cov: 25216 -> 25216
Cov: 25216 -> 25216
6342
Cov: 25216 -> 25216
Cov: 25216 -> 25216
6343
Cov: 25216 -> 25216
Cov: 25216 -> 25216
6344
Cov: 25216 -> 25217
Cov: 25217 -> 25217
6345
Cov: 25217 -> 25218
Cov: 25218 -> 25218
6346
Cov: 25218 -> 25218
Cov: 25218 -> 25218
6347
Cov: 25218 -> 25218
Cov: 25218 -> 25218
6348
{"exception": "RuntimeError", "msg": "The expanded size of the tensor (3) must match the existing size (2) at non-singleton dimension 0.  Target sizes: [3, 3].  Tensor sizes: [2, 3]"}
6349
Cov: 25218 -> 25219
Cov: 25219 -> 25219
6350
Cov: 25219 -> 25219
Cov: 25219 -> 25219
6351
Cov: 25219 -> 25219
Cov: 25219 -> 25219
6352
Cov: 25219 -> 25219
Cov: 25219 -> 25219
6353
Cov: 25219 -> 25219
Cov: 25219 -> 25219
6354
{"exception": "TypeError", "msg": "random_() received an invalid combination of arguments - got (to=int, from_=int, ), but expected one of:\n * (*, torch.Generator generator)\n      didn't match because some of the keywords were incorrect: to, from_\n * (int from, int to, *, torch.Generator generator)\n * (int to, *, torch.Generator generator)\n"}
6355
Cov: 25219 -> 25223
Cov: 25223 -> 25223
6356
Cov: 25223 -> 25223
Cov: 25223 -> 25223
6357
Cov: 25223 -> 25223
Cov: 25223 -> 25223
6358
Cov: 25223 -> 25223
Cov: 25223 -> 25223
6359
Cov: 25223 -> 25223
Cov: 25223 -> 25223
6360
Cov: 25223 -> 25223
Cov: 25223 -> 25223
6361
Cov: 25223 -> 25223
Cov: 25223 -> 25223
6362
Cov: 25223 -> 25223
Cov: 25223 -> 25223
6363
Cov: 25223 -> 25223
Cov: 25223 -> 25223
6364
Cov: 25223 -> 25223
Cov: 25223 -> 25223
6365
Cov: 25223 -> 25223
Cov: 25223 -> 25223
6366
Cov: 25223 -> 25223
Cov: 25223 -> 25223
6367
Cov: 25223 -> 25223
Cov: 25223 -> 25223
6368
Cov: 25223 -> 25224
Cov: 25224 -> 25224
6369
Cov: 25224 -> 25225
Cov: 25225 -> 25225
6370
Cov: 25225 -> 25225
Cov: 25225 -> 25225
6371
Cov: 25225 -> 25225
Cov: 25225 -> 25225
6372
Cov: 25225 -> 25225
Cov: 25225 -> 25225
6373
Cov: 25225 -> 25225
Cov: 25225 -> 25225
6374
{"exception": "RuntimeError", "msg": "masked_fill_ only supports boolean masks, but got mask with dtype long int"}
6375
Cov: 25225 -> 25225
Cov: 25225 -> 25225
6376
Cov: 25225 -> 25225
Cov: 25225 -> 25225
6377
Cov: 25225 -> 25225
Cov: 25225 -> 25225
6378
Cov: 25225 -> 25225
Cov: 25225 -> 25225
6379
Cov: 25225 -> 25226
Cov: 25226 -> 25226
6380
Cov: 25226 -> 25227
Cov: 25227 -> 25227
6381
Cov: 25227 -> 25227
Cov: 25227 -> 25227
6382
Cov: 25227 -> 25227
Cov: 25227 -> 25227
6383
Cov: 25227 -> 25227
Cov: 25227 -> 25227
6384
Cov: 25227 -> 25227
Cov: 25227 -> 25227
6385
Cov: 25227 -> 25227
Cov: 25227 -> 25227
6386
Cov: 25227 -> 25227
Cov: 25227 -> 25227
6387
Cov: 25227 -> 25227
Cov: 25227 -> 25227
6388
{"exception": "RuntimeError", "msg": "element 0 of tensors does not require grad and does not have a grad_fn"}
6389
Cov: 25227 -> 25243
Cov: 25243 -> 25243
6390
Cov: 25243 -> 25243
Cov: 25243 -> 25243
6391
{"exception": "RuntimeError", "msg": "\"bitwise_not_cpu\" not implemented for 'Float'"}
6392
Cov: 25243 -> 25244
Cov: 25244 -> 25244
6393
Cov: 25244 -> 25244
Cov: 25244 -> 25244
6394
Cov: 25244 -> 25245
Cov: 25245 -> 25245
6395
Cov: 25245 -> 25245
Cov: 25245 -> 25245
6396
Cov: 25245 -> 25245
Cov: 25245 -> 25245
6397
Cov: 25245 -> 25245
Cov: 25245 -> 25245
6398
Cov: 25245 -> 25245
Cov: 25245 -> 25245
6399
Cov: 25245 -> 25245
Cov: 25245 -> 25245
6400
Cov: 25245 -> 25245
Cov: 25245 -> 25245
6401
{"exception": "RuntimeError", "msg": "tensor.sspaddmm(...) can only be called on sparse tensors"}
6402
Cov: 25245 -> 25245
Cov: 25245 -> 25245
6403
Cov: 25245 -> 25246
Cov: 25246 -> 25246
6404
Cov: 25246 -> 25273
Cov: 25273 -> 25273
6405
Cov: 25273 -> 25273
Cov: 25273 -> 25273
6406
Cov: 25273 -> 25273
Cov: 25273 -> 25273
6407
Cov: 25273 -> 25274
Cov: 25274 -> 25274
6408
{"exception": "RuntimeError", "msg": "element 0 of tensors does not require grad and does not have a grad_fn"}
6409
Cov: 25274 -> 25275
Cov: 25275 -> 25275
6410
Cov: 25275 -> 25275
Cov: 25275 -> 25275
6411
Cov: 25275 -> 25275
Cov: 25275 -> 25275
6412
Cov: 25275 -> 25275
Cov: 25275 -> 25275
6413
Cov: 25275 -> 25275
Cov: 25275 -> 25275
6414
Cov: 25275 -> 25276
Cov: 25276 -> 25276
6415
Cov: 25276 -> 25276
Cov: 25276 -> 25276
6416
Cov: 25276 -> 25276
Cov: 25276 -> 25276
6417
Cov: 25276 -> 25276
Cov: 25276 -> 25276
6418
Cov: 25276 -> 25276
Cov: 25276 -> 25276
6419
Cov: 25276 -> 25277
Cov: 25277 -> 25277
6420
Cov: 25277 -> 25278
Cov: 25278 -> 25278
6421
{"exception": "RuntimeError", "msg": "This function was deprecated since version 1.9 and is now removed. `torch.lstsq` is deprecated in favor of `torch.linalg.lstsq`.\n`torch.linalg.lstsq` has reversed arguments and does not return the QR decomposition in the returned tuple (although it returns other information about the problem).\n\nTo get the QR decomposition consider using `torch.linalg.qr`.\n\nThe returned solution in `torch.lstsq` stored the residuals of the solution in the last m - n columns of the returned value whenever m > n. In torch.linalg.lstsq, the residuals are in the field 'residuals' of the returned named tuple.\n\nThe unpacking of the solution, as in\nX, _ = torch.lstsq(B, A).solution[:A.size(1)]\nshould be replaced with:\nX = torch.linalg.lstsq(A, B).solution"}
6422
Cov: 25278 -> 25280
Cov: 25280 -> 25280
6423
Cov: 25280 -> 25282
Cov: 25282 -> 25282
6424
Cov: 25282 -> 25283
Cov: 25283 -> 25283
6425
Cov: 25283 -> 25283
Cov: 25283 -> 25283
6426
Cov: 25283 -> 25283
Cov: 25283 -> 25283
6427
Cov: 25283 -> 25283
Cov: 25283 -> 25283
6428
Cov: 25283 -> 25283
Cov: 25283 -> 25283
6429
Cov: 25283 -> 25283
Cov: 25283 -> 25283
6430
Cov: 25283 -> 25283
Cov: 25283 -> 25283
6431
Cov: 25283 -> 25283
Cov: 25283 -> 25283
6432
Cov: 25283 -> 25284
Cov: 25284 -> 25284
6433
Cov: 25284 -> 25284
Cov: 25284 -> 25284
6434
Cov: 25284 -> 25284
Cov: 25284 -> 25284
6435
Cov: 25284 -> 25284
Cov: 25284 -> 25284
6436
Cov: 25284 -> 25284
Cov: 25284 -> 25284
6437
Cov: 25284 -> 25284
Cov: 25284 -> 25284
6438
Cov: 25284 -> 25285
Cov: 25285 -> 25285
6439
Cov: 25285 -> 25285
Cov: 25285 -> 25285
6440
{"exception": "RuntimeError", "msg": "element 0 of tensors does not require grad and does not have a grad_fn"}
6441
Cov: 25285 -> 25290
Cov: 25290 -> 25290
6442
Cov: 25290 -> 25290
Cov: 25290 -> 25290
6443
Cov: 25290 -> 25290
Cov: 25290 -> 25290
6444
Cov: 25290 -> 25290
Cov: 25290 -> 25290
6445
{"exception": "TypeError", "msg": "'getset_descriptor' object is not callable"}
6446
Cov: 25290 -> 25290
Cov: 25290 -> 25290
6447
Cov: 25290 -> 25291
Cov: 25291 -> 25291
6448
Cov: 25291 -> 25291
Cov: 25291 -> 25291
6449
Cov: 25291 -> 25291
Cov: 25291 -> 25291
6450
{"exception": "RuntimeError", "msg": "tensor.sspaddmm(...) can only be called on sparse tensors"}
6451
Cov: 25291 -> 25291
Cov: 25291 -> 25291
6452
{"exception": "TypeError", "msg": "heaviside(): argument 'values' must be Tensor, not tuple"}
6453
Cov: 25291 -> 25291
Cov: 25291 -> 25291
6454
Cov: 25291 -> 25291
Cov: 25291 -> 25291
6455
Cov: 25291 -> 25291
Cov: 25291 -> 25291
6456
Cov: 25291 -> 25291
Cov: 25291 -> 25291
6457
Cov: 25291 -> 25292
Cov: 25292 -> 25292
6458
Cov: 25292 -> 25292
Cov: 25292 -> 25292
6459
Cov: 25292 -> 25292
Cov: 25292 -> 25292
6460
Cov: 25292 -> 25292
Cov: 25292 -> 25292
6461
Cov: 25292 -> 25292
Cov: 25292 -> 25292
6462
Cov: 25292 -> 25292
Cov: 25292 -> 25292
6463
Cov: 25292 -> 25292
Cov: 25292 -> 25292
6464
Cov: 25292 -> 25292
Cov: 25292 -> 25292
6465
Cov: 25292 -> 25292
Cov: 25292 -> 25292
6466
Cov: 25292 -> 25292
Cov: 25292 -> 25292
6467
Cov: 25292 -> 25292
Cov: 25292 -> 25292
6468
{"exception": "NameError", "msg": "name 'device' is not defined"}
6469
Cov: 25292 -> 25292
Cov: 25292 -> 25292
6470
Cov: 25292 -> 25293
Cov: 25293 -> 25293
6471
{"exception": "RuntimeError", "msg": "max_unpooling3d_forward_out does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True)'. You can turn off determinism just for this operation, or you can use the 'warn_only=True' option, if that's acceptable for your application. You can also file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation."}
6472
Cov: 25293 -> 25315
Cov: 25315 -> 25315
6473
Cov: 25315 -> 25316
Cov: 25316 -> 25316
6474
Cov: 25316 -> 25316
Cov: 25316 -> 25316
6475
Cov: 25316 -> 25316
Cov: 25316 -> 25316
6476
Cov: 25316 -> 25317
Cov: 25317 -> 25317
6477
Cov: 25317 -> 25318
Cov: 25318 -> 25318
6478
Cov: 25318 -> 25319
Cov: 25319 -> 25319
6479
Cov: 25319 -> 25319
Cov: 25319 -> 25319
6480
Cov: 25319 -> 25319
Cov: 25319 -> 25319
6481
Cov: 25319 -> 25319
Cov: 25319 -> 25319
6482
{"exception": "RuntimeError", "msg": "tensor.sspaddmm(...) can only be called on sparse tensors"}
6483
Cov: 25319 -> 25320
Cov: 25320 -> 25320
6484
{"exception": "TypeError", "msg": "'getset_descriptor' object is not callable"}
6485
Cov: 25320 -> 25321
Cov: 25321 -> 25321
6486
Cov: 25321 -> 25322
Cov: 25322 -> 25322
6487
Cov: 25322 -> 25322
Cov: 25322 -> 25322
6488
Cov: 25322 -> 25322
Cov: 25322 -> 25322
6489
Cov: 25322 -> 25323
Cov: 25323 -> 25323
6490
Cov: 25323 -> 25323
Cov: 25323 -> 25323
6491
{"exception": "RuntimeError", "msg": "result type Float can't be cast to the desired output type Long"}
6492
Cov: 25323 -> 25323
Cov: 25323 -> 25323
6493
Cov: 25323 -> 25323
Cov: 25323 -> 25323
6494
Cov: 25323 -> 25323
Cov: 25323 -> 25323
6495
Cov: 25323 -> 25323
Cov: 25323 -> 25323
6496
Cov: 25323 -> 25325
Cov: 25325 -> 25325
6497
Cov: 25325 -> 25325
Cov: 25325 -> 25325
6498
Cov: 25325 -> 25325
Cov: 25325 -> 25325
6499
Cov: 25325 -> 25326
Cov: 25326 -> 25326
6500
Cov: 25326 -> 25326
Cov: 25326 -> 25326
6501
Cov: 25326 -> 25326
Cov: 25326 -> 25326
6502
{"exception": "TypeError", "msg": "index_add_() missing 1 required positional arguments: \"source\""}
6503
Cov: 25326 -> 25326
Cov: 25326 -> 25326
6504
{"exception": "TypeError", "msg": "exponential_() got an unexpected keyword argument 'lam'"}
6505
Cov: 25326 -> 25326
Cov: 25326 -> 25326
6506
Cov: 25326 -> 25326
Cov: 25326 -> 25326
6507
Cov: 25326 -> 25326
Cov: 25326 -> 25326
6508
Cov: 25326 -> 25326
Cov: 25326 -> 25326
6509
{"exception": "RuntimeError", "msg": "This function was deprecated since version 1.9 and is now removed. `torch.linalg.eig` returns complex tensors of dtype `cfloat` or `cdouble` rather than real tensors mimicking complex tensors.\n\nL, _ = torch.eig(A) should be replaced with:\nL_complex = torch.linalg.eigvals(A)\n\nand\n\nL, V = torch.eig(A, eigenvectors=True) should be replaced with:\nL_complex, V_complex = torch.linalg.eig(A)"}
6510
Cov: 25326 -> 25327
Cov: 25327 -> 25327
6511
Cov: 25327 -> 25327
Cov: 25327 -> 25327
6512
Cov: 25327 -> 25328
Cov: 25328 -> 25328
6513
Cov: 25328 -> 25328
Cov: 25328 -> 25328
6514
Cov: 25328 -> 25328
Cov: 25328 -> 25328
6515
Cov: 25328 -> 25329
Cov: 25329 -> 25329
6516
Cov: 25329 -> 25329
Cov: 25329 -> 25329
6517
Cov: 25329 -> 25329
Cov: 25329 -> 25329
6518
Cov: 25329 -> 25329
Cov: 25329 -> 25329
6519
Cov: 25329 -> 25329
Cov: 25329 -> 25329
6520
{"exception": "RuntimeError", "msg": "\"bitwise_not_cpu\" not implemented for 'Float'"}
6521
Cov: 25329 -> 25329
Cov: 25329 -> 25329
6522
Cov: 25329 -> 25329
Cov: 25329 -> 25329
6523
Cov: 25329 -> 25329
Cov: 25329 -> 25329
6524
Cov: 25329 -> 25329
Cov: 25329 -> 25329
6525
Cov: 25329 -> 25329
Cov: 25329 -> 25329
6526
Cov: 25329 -> 25329
Cov: 25329 -> 25329
6527
Cov: 25329 -> 25329
Cov: 25329 -> 25329
6528
Cov: 25329 -> 25330
Cov: 25330 -> 25330
6529
{"exception": "RuntimeError", "msg": "Cauchy distribution is a continuous probability distribution. dtype must be a floating point but you specified long int"}
6530
Cov: 25330 -> 25330
Cov: 25330 -> 25330
6531
{"exception": "RuntimeError", "msg": "size mismatch, input: [2, 3], v1: [3], v2: [3]"}
6532
Cov: 25330 -> 25330
Cov: 25330 -> 25330
6533
Cov: 25330 -> 25330
Cov: 25330 -> 25330
6534
Cov: 25330 -> 25330
Cov: 25330 -> 25330
6535
Cov: 25330 -> 25334
Cov: 25334 -> 25334
6536
Cov: 25334 -> 25334
Cov: 25334 -> 25334
6537
Cov: 25334 -> 25334
Cov: 25334 -> 25334
6538
Cov: 25334 -> 25335
Cov: 25335 -> 25335
6539
Cov: 25335 -> 25335
Cov: 25335 -> 25335
6540
Cov: 25335 -> 25335
Cov: 25335 -> 25335
6541
Cov: 25335 -> 25335
Cov: 25335 -> 25335
6542
Cov: 25335 -> 25335
Cov: 25335 -> 25335
6543
Cov: 25335 -> 25335
Cov: 25335 -> 25335
6544
Cov: 25335 -> 25335
Cov: 25335 -> 25335
6545
Cov: 25335 -> 25335
Cov: 25335 -> 25335
6546
Cov: 25335 -> 25335
Cov: 25335 -> 25335
6547
Cov: 25335 -> 25335
Cov: 25335 -> 25335
6548
Cov: 25335 -> 25335
Cov: 25335 -> 25335
6549
Cov: 25335 -> 25335
Cov: 25335 -> 25335
6550
Cov: 25335 -> 25335
Cov: 25335 -> 25335
6551
Cov: 25335 -> 25335
Cov: 25335 -> 25335
6552
Cov: 25335 -> 25335
Cov: 25335 -> 25335
6553
Cov: 25335 -> 25336
Cov: 25336 -> 25336
6554
Cov: 25336 -> 25336
Cov: 25336 -> 25336
6555
Cov: 25336 -> 25336
Cov: 25336 -> 25336
6556
Cov: 25336 -> 25336
Cov: 25336 -> 25336
6557
Cov: 25336 -> 25336
Cov: 25336 -> 25336
6558
Cov: 25336 -> 25337
Cov: 25337 -> 25337
6559
Cov: 25337 -> 25337
Cov: 25337 -> 25337
6560
Cov: 25337 -> 25337
Cov: 25337 -> 25337
6561
Cov: 25337 -> 25337
Cov: 25337 -> 25337
6562
Cov: 25337 -> 25337
Cov: 25337 -> 25337
6563
Cov: 25337 -> 25338
Cov: 25338 -> 25338
6564
Cov: 25338 -> 25338
Cov: 25338 -> 25338
6565
Cov: 25338 -> 25338
Cov: 25338 -> 25338
6566
Cov: 25338 -> 25338
Cov: 25338 -> 25338
6567
Cov: 25338 -> 25338
Cov: 25338 -> 25338
6568
Cov: 25338 -> 25339
Cov: 25339 -> 25339
6569
Cov: 25339 -> 25339
Cov: 25339 -> 25339
6570
Cov: 25339 -> 25339
Cov: 25339 -> 25339
6571
Cov: 25339 -> 25350
Cov: 25350 -> 25350
6572
Cov: 25350 -> 25350
Cov: 25350 -> 25350
6573
Cov: 25350 -> 25350
Cov: 25350 -> 25350
6574
Cov: 25350 -> 25350
Cov: 25350 -> 25350
6575
Cov: 25350 -> 25350
Cov: 25350 -> 25350
6576
Cov: 25350 -> 25350
Cov: 25350 -> 25350
6577
Cov: 25350 -> 25350
Cov: 25350 -> 25350
6578
Cov: 25350 -> 25351
Cov: 25351 -> 25351
6579
Cov: 25351 -> 25351
Cov: 25351 -> 25351
6580
Cov: 25351 -> 25351
Cov: 25351 -> 25351
6581
Cov: 25351 -> 25351
Cov: 25351 -> 25351
6582
Cov: 25351 -> 25351
Cov: 25351 -> 25351
6583
Cov: 25351 -> 25352
Cov: 25352 -> 25352
6584
Cov: 25352 -> 25354
Cov: 25354 -> 25354
6585
Cov: 25354 -> 25354
Cov: 25354 -> 25354
6586
Cov: 25354 -> 25354
Cov: 25354 -> 25354
6587
Cov: 25354 -> 25354
Cov: 25354 -> 25354
6588
Cov: 25354 -> 25354
Cov: 25354 -> 25354
6589
Cov: 25354 -> 25355
Cov: 25355 -> 25355
6590
Cov: 25355 -> 25355
Cov: 25355 -> 25355
6591
Cov: 25355 -> 25356
Cov: 25356 -> 25356
6592
Cov: 25356 -> 25356
Cov: 25356 -> 25356
6593
Cov: 25356 -> 25356
Cov: 25356 -> 25356
6594
{"exception": "NotImplementedError", "msg": "Could not run 'aten::q_per_channel_axis' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::q_per_channel_axis' is only available for these backends: [QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nQuantizedCPU: registered at aten/src/ATen/RegisterQuantizedCPU.cpp:944 [kernel]\nQuantizedCUDA: registered at aten/src/ATen/RegisterQuantizedCUDA.cpp:459 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:153 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:498 [backend fallback]\nFunctionalize: registered at ../aten/src/ATen/FunctionalizeFallbackKernel.cpp:290 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:86 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradMTIA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_4.cpp:13056 [kernel]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:382 [backend fallback]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:249 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:710 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:203 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:161 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:494 [backend fallback]\nPreDispatch: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:165 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:157 [backend fallback]\n"}
6595
Cov: 25356 -> 25356
Cov: 25356 -> 25356
6596
Cov: 25356 -> 25356
Cov: 25356 -> 25356
6597
Cov: 25356 -> 25357
Cov: 25357 -> 25357
6598
Cov: 25357 -> 25357
Cov: 25357 -> 25357
6599
Cov: 25357 -> 25358
Cov: 25358 -> 25358
6600
Cov: 25358 -> 25359
Cov: 25359 -> 25359
6601
Cov: 25359 -> 25360
Cov: 25360 -> 25360
6602
Cov: 25360 -> 25361
Cov: 25361 -> 25361
6603
Cov: 25361 -> 25361
Cov: 25361 -> 25361
6604
Cov: 25361 -> 25361
Cov: 25361 -> 25361
6605
Cov: 25361 -> 25361
Cov: 25361 -> 25361
6606
Cov: 25361 -> 25362
Cov: 25362 -> 25362
6607
Cov: 25362 -> 25362
Cov: 25362 -> 25362
6608
Cov: 25362 -> 25362
Cov: 25362 -> 25362
6609
Cov: 25362 -> 25362
Cov: 25362 -> 25362
6610
Cov: 25362 -> 25362
Cov: 25362 -> 25362
6611
Cov: 25362 -> 25362
Cov: 25362 -> 25362
6612
Cov: 25362 -> 25363
Cov: 25363 -> 25363
6613
Cov: 25363 -> 25492
Cov: 25492 -> 25492
6614
Cov: 25492 -> 25492
Cov: 25492 -> 25492
6615
Cov: 25492 -> 25492
Cov: 25492 -> 25492
6616
Cov: 25492 -> 25493
Cov: 25493 -> 25493
6617
Cov: 25493 -> 25493
Cov: 25493 -> 25493
6618
{"exception": "RuntimeError", "msg": "\"lshift_cpu\" not implemented for 'Float'"}
6619
Cov: 25493 -> 25493
Cov: 25493 -> 25493
6620
Cov: 25493 -> 25493
Cov: 25493 -> 25493
6621
Cov: 25493 -> 25494
Cov: 25494 -> 25494
6622
Cov: 25494 -> 25494
Cov: 25494 -> 25494
6623
Cov: 25494 -> 25494
Cov: 25494 -> 25494
6624
Cov: 25494 -> 25494
Cov: 25494 -> 25494
6625
Cov: 25494 -> 25494
Cov: 25494 -> 25494
6626
Cov: 25494 -> 25494
Cov: 25494 -> 25494
6627
Cov: 25494 -> 25494
Cov: 25494 -> 25494
6628
Cov: 25494 -> 25494
Cov: 25494 -> 25494
6629
Cov: 25494 -> 25494
Cov: 25494 -> 25494
6630
Cov: 25494 -> 25494
Cov: 25494 -> 25494
6631
Cov: 25494 -> 25494
Cov: 25494 -> 25494
6632
Cov: 25494 -> 25499
Cov: 25499 -> 25499
6633
Cov: 25499 -> 25499
Cov: 25499 -> 25499
6634
Cov: 25499 -> 25499
Cov: 25499 -> 25499
6635
Cov: 25499 -> 25500
Cov: 25500 -> 25500
6636
Cov: 25500 -> 25500
Cov: 25500 -> 25500
6637
Cov: 25500 -> 25501
Cov: 25501 -> 25501
6638
Cov: 25501 -> 25569
Cov: 25569 -> 25569
6639
{"exception": "RuntimeError", "msg": "a leaf Variable that requires grad is being used in an in-place operation."}
6640
Cov: 25569 -> 25569
Cov: 25569 -> 25569
6641
{"exception": "RuntimeError", "msg": "Expected nested_tensorlist[0].size() > 0 to be true, but got false.  (Could this error message be improved?  If so, please report an enhancement request to PyTorch.)"}
6642
Cov: 25569 -> 25569
Cov: 25569 -> 25569
6643
Cov: 25569 -> 25569
Cov: 25569 -> 25569
6644
{"exception": "RuntimeError", "msg": "\"hypot_cpu\" not implemented for 'Long'"}
6645
Cov: 25569 -> 25569
Cov: 25569 -> 25569
6646
Cov: 25569 -> 25569
Cov: 25569 -> 25569
6647
Cov: 25569 -> 25569
Cov: 25569 -> 25569
6648
Cov: 25569 -> 25569
Cov: 25569 -> 25569
6649
Cov: 25569 -> 25570
Cov: 25570 -> 25570
6650
Cov: 25570 -> 25570
Cov: 25570 -> 25570
6651
Cov: 25570 -> 25571
Cov: 25571 -> 25571
6652
Cov: 25571 -> 25571
Cov: 25571 -> 25571
6653
Cov: 25571 -> 25571
Cov: 25571 -> 25571
6654
{"exception": "NameError", "msg": "name 'transforms' is not defined"}
6655
Cov: 25571 -> 25573
Cov: 25573 -> 25573
6656
Cov: 25573 -> 25574
Cov: 25574 -> 25574
6657
Cov: 25574 -> 25574
Cov: 25574 -> 25574
6658
Cov: 25574 -> 25574
Cov: 25574 -> 25574
6659
Cov: 25574 -> 25574
Cov: 25574 -> 25574
6660
Cov: 25574 -> 25574
Cov: 25574 -> 25574
6661
Cov: 25574 -> 25574
Cov: 25574 -> 25574
6662
Cov: 25574 -> 25575
Cov: 25575 -> 25575
6663
Cov: 25575 -> 25575
Cov: 25575 -> 25575
6664
Cov: 25575 -> 25576
Cov: 25576 -> 25576
6665
Cov: 25576 -> 25576
Cov: 25576 -> 25576
6666
{"exception": "RuntimeError", "msg": "\"rshift_cpu\" not implemented for 'Float'"}
6667
Cov: 25576 -> 25577
Cov: 25577 -> 25577
6668
Cov: 25577 -> 25577
Cov: 25577 -> 25577
6669
Cov: 25577 -> 25578
Cov: 25578 -> 25578
6670
Cov: 25578 -> 25579
Cov: 25579 -> 25579
6671
Cov: 25579 -> 25580
Cov: 25580 -> 25580
6672
Cov: 25580 -> 25580
Cov: 25580 -> 25580
6673
Cov: 25580 -> 25580
Cov: 25580 -> 25580
6674
Cov: 25580 -> 25580
Cov: 25580 -> 25580
6675
Cov: 25580 -> 25581
Cov: 25581 -> 25581
6676
Cov: 25581 -> 25581
Cov: 25581 -> 25581
6677
Cov: 25581 -> 25581
Cov: 25581 -> 25581
6678
Cov: 25581 -> 25582
Cov: 25582 -> 25582
6679
Cov: 25582 -> 25583
Cov: 25583 -> 25583
6680
Cov: 25583 -> 25583
Cov: 25583 -> 25583
6681
Cov: 25583 -> 25584
Cov: 25584 -> 25584
6682
Cov: 25584 -> 25584
Cov: 25584 -> 25584
6683
Cov: 25584 -> 25584
Cov: 25584 -> 25584
6684
Cov: 25584 -> 25584
Cov: 25584 -> 25584
6685
Cov: 25584 -> 25584
Cov: 25584 -> 25584
6686
Cov: 25584 -> 25584
Cov: 25584 -> 25584
6687
Cov: 25584 -> 25584
Cov: 25584 -> 25584
6688
Cov: 25584 -> 25584
Cov: 25584 -> 25584
6689
Cov: 25584 -> 25584
Cov: 25584 -> 25584
6690
Cov: 25584 -> 25585
Cov: 25585 -> 25585
6691
Cov: 25585 -> 25586
Cov: 25586 -> 25586
6692
Cov: 25586 -> 25586
Cov: 25586 -> 25586
6693
Cov: 25586 -> 25586
Cov: 25586 -> 25586
6694
Cov: 25586 -> 25586
Cov: 25586 -> 25586
6695
Cov: 25586 -> 25586
Cov: 25586 -> 25586
6696
Cov: 25586 -> 25586
Cov: 25586 -> 25586
6697
Cov: 25586 -> 25587
Cov: 25587 -> 25587
6698
Cov: 25587 -> 25587
Cov: 25587 -> 25587
6699
Cov: 25587 -> 25588
Cov: 25588 -> 25588
6700
Cov: 25588 -> 25588
Cov: 25588 -> 25588
6701
Cov: 25588 -> 25588
Cov: 25588 -> 25588
6702
Cov: 25588 -> 25588
Cov: 25588 -> 25588
6703
Cov: 25588 -> 25588
Cov: 25588 -> 25588
6704
Cov: 25588 -> 25588
Cov: 25588 -> 25588
6705
Cov: 25588 -> 25588
Cov: 25588 -> 25588
6706
Cov: 25588 -> 25588
Cov: 25588 -> 25588
6707
Cov: 25588 -> 25588
Cov: 25588 -> 25588
6708
Cov: 25588 -> 25588
Cov: 25588 -> 25588
6709
Cov: 25588 -> 25588
Cov: 25588 -> 25588
6710
Cov: 25588 -> 25588
Cov: 25588 -> 25588
6711
Cov: 25588 -> 25588
Cov: 25588 -> 25588
6712
Cov: 25588 -> 25588
Cov: 25588 -> 25588
6713
Cov: 25588 -> 25588
Cov: 25588 -> 25588
6714
Cov: 25588 -> 25589
Cov: 25589 -> 25589
6715
Cov: 25589 -> 25589
Cov: 25589 -> 25589
6716
Cov: 25589 -> 25589
Cov: 25589 -> 25589
6717
Cov: 25589 -> 25589
Cov: 25589 -> 25589
6718
Cov: 25589 -> 25589
Cov: 25589 -> 25589
6719
Cov: 25589 -> 25590
Cov: 25590 -> 25590
6720
Cov: 25590 -> 25619
Cov: 25619 -> 25619
6721
Cov: 25619 -> 25619
Cov: 25619 -> 25619
6722
Cov: 25619 -> 25625
Cov: 25625 -> 25625
6723
Cov: 25625 -> 25625
Cov: 25625 -> 25625
6724
Cov: 25625 -> 25625
Cov: 25625 -> 25625
6725
Cov: 25625 -> 25627
Cov: 25627 -> 25627
6726
Cov: 25627 -> 25627
Cov: 25627 -> 25627
6727
Cov: 25627 -> 25627
Cov: 25627 -> 25627
6728
Cov: 25627 -> 25627
Cov: 25627 -> 25627
6729
Cov: 25627 -> 25627
Cov: 25627 -> 25627
6730
Cov: 25627 -> 25627
Cov: 25627 -> 25627
6731
Cov: 25627 -> 25628
Cov: 25628 -> 25628
6732
Cov: 25628 -> 25628
Cov: 25628 -> 25628
6733
Cov: 25628 -> 25629
Cov: 25629 -> 25629
6734
Cov: 25629 -> 25630
Cov: 25630 -> 25630
6735
Cov: 25630 -> 25630
Cov: 25630 -> 25630
6736
Cov: 25630 -> 25630
Cov: 25630 -> 25630
6737
Cov: 25630 -> 25630
Cov: 25630 -> 25630
6738
Cov: 25630 -> 25631
Cov: 25631 -> 25631
6739
Cov: 25631 -> 25632
Cov: 25632 -> 25632
6740
Cov: 25632 -> 25632
Cov: 25632 -> 25632
6741
Cov: 25632 -> 25632
Cov: 25632 -> 25632
6742
Cov: 25632 -> 25632
Cov: 25632 -> 25632
6743
Cov: 25632 -> 25633
Cov: 25633 -> 25633
6744
Cov: 25633 -> 25633
Cov: 25633 -> 25633
6745
Cov: 25633 -> 25634
Cov: 25634 -> 25634
6746
Cov: 25634 -> 25634
Cov: 25634 -> 25634
6747
Cov: 25634 -> 25634
Cov: 25634 -> 25634
6748
Cov: 25634 -> 25634
Cov: 25634 -> 25634
6749
Cov: 25634 -> 25634
Cov: 25634 -> 25634
6750
Cov: 25634 -> 25634
Cov: 25634 -> 25634
6751
Cov: 25634 -> 25635
Cov: 25635 -> 25635
6752
Cov: 25635 -> 25636
Cov: 25636 -> 25636
6753
Cov: 25636 -> 25636
Cov: 25636 -> 25636
6754
Cov: 25636 -> 25636
Cov: 25636 -> 25636
6755
Cov: 25636 -> 25637
Cov: 25637 -> 25637
6756
Cov: 25637 -> 25637
Cov: 25637 -> 25637
6757
{"exception": "TypeError", "msg": "lcm(): argument 'other' must be Tensor, not int"}
6758
Cov: 25637 -> 25637
Cov: 25637 -> 25637
6759
Cov: 25637 -> 25637
Cov: 25637 -> 25637
6760
{"exception": "RuntimeError", "msg": "torch.linalg.householder_product: input.shape[-1] must be greater than or equal to tau.shape[-1]"}
6761
Cov: 25637 -> 25638
Cov: 25638 -> 25638
6762
Cov: 25638 -> 25638
Cov: 25638 -> 25638
6763
Cov: 25638 -> 25638
Cov: 25638 -> 25638
6764
Cov: 25638 -> 25638
Cov: 25638 -> 25638
6765
Cov: 25638 -> 25638
Cov: 25638 -> 25638
6766
Cov: 25638 -> 25638
Cov: 25638 -> 25638
6767
Cov: 25638 -> 25638
Cov: 25638 -> 25638
6768
Cov: 25638 -> 25638
Cov: 25638 -> 25638
6769
Cov: 25638 -> 25638
Cov: 25638 -> 25638
6770
Cov: 25638 -> 25639
Cov: 25639 -> 25639
6771
Cov: 25639 -> 25639
Cov: 25639 -> 25639
6772
Cov: 25639 -> 25639
Cov: 25639 -> 25639
6773
Cov: 25639 -> 25640
Cov: 25640 -> 25640
6774
Cov: 25640 -> 25640
Cov: 25640 -> 25640
6775
Cov: 25640 -> 25641
Cov: 25641 -> 25641
6776
Cov: 25641 -> 25641
Cov: 25641 -> 25641
6777
Cov: 25641 -> 25641
Cov: 25641 -> 25641
6778
Cov: 25641 -> 25641
Cov: 25641 -> 25641
6779
Cov: 25641 -> 25641
Cov: 25641 -> 25641
6780
Cov: 25641 -> 25641
Cov: 25641 -> 25641
6781
Cov: 25641 -> 25641
Cov: 25641 -> 25641
6782
Cov: 25641 -> 25641
Cov: 25641 -> 25641
6783
Cov: 25641 -> 25645
Cov: 25645 -> 25645
6784
Cov: 25645 -> 25645
Cov: 25645 -> 25645
6785
Cov: 25645 -> 25646
Cov: 25646 -> 25646
6786
{"exception": "RuntimeError", "msg": "tensor.sspaddmm(...) can only be called on sparse tensors"}
6787
Cov: 25646 -> 25646
Cov: 25646 -> 25646
6788
Cov: 25646 -> 25646
Cov: 25646 -> 25646
6789
Cov: 25646 -> 25646
Cov: 25646 -> 25646
6790
Cov: 25646 -> 25646
Cov: 25646 -> 25646
6791
Cov: 25646 -> 25646
Cov: 25646 -> 25646
6792
Cov: 25646 -> 25647
Cov: 25647 -> 25647
6793
Cov: 25647 -> 25648
Cov: 25648 -> 25648
6794
{"exception": "RuntimeError", "msg": "Please look up dimensions by name, got: name = None."}
6795
Cov: 25648 -> 25648
Cov: 25648 -> 25648
6796
Cov: 25648 -> 25648
Cov: 25648 -> 25648
6797
Cov: 25648 -> 25649
Cov: 25649 -> 25649
6798
Cov: 25649 -> 25649
Cov: 25649 -> 25649
6799
Cov: 25649 -> 25650
Cov: 25650 -> 25650
6800
Cov: 25650 -> 25650
Cov: 25650 -> 25650
6801
Cov: 25650 -> 25650
Cov: 25650 -> 25650
6802
Cov: 25650 -> 25650
Cov: 25650 -> 25650
6803
Cov: 25650 -> 25650
Cov: 25650 -> 25650
6804
Cov: 25650 -> 25650
Cov: 25650 -> 25650
6805
Cov: 25650 -> 25650
Cov: 25650 -> 25650
6806
Cov: 25650 -> 25650
Cov: 25650 -> 25650
6807
Cov: 25650 -> 25650
Cov: 25650 -> 25650
6808
Cov: 25650 -> 25651
Cov: 25651 -> 25651
6809
Cov: 25651 -> 25651
Cov: 25651 -> 25651
6810
Cov: 25651 -> 25651
Cov: 25651 -> 25651
6811
Cov: 25651 -> 25651
Cov: 25651 -> 25651
6812
Cov: 25651 -> 25652
Cov: 25652 -> 25652
6813
Cov: 25652 -> 25654
Cov: 25654 -> 25654
6814
Cov: 25654 -> 25654
Cov: 25654 -> 25654
6815
Cov: 25654 -> 25654
Cov: 25654 -> 25654
6816
Cov: 25654 -> 25654
Cov: 25654 -> 25654
6817
Cov: 25654 -> 25654
Cov: 25654 -> 25654
6818
Cov: 25654 -> 25654
Cov: 25654 -> 25654
6819
Cov: 25654 -> 25654
Cov: 25654 -> 25654
6820
Cov: 25654 -> 25654
Cov: 25654 -> 25654
6821
Cov: 25654 -> 25655
Cov: 25655 -> 25655
6822
Cov: 25655 -> 25789
Cov: 25789 -> 25789
6823
Cov: 25789 -> 25789
Cov: 25789 -> 25789
6824
Cov: 25789 -> 25789
Cov: 25789 -> 25789
6825
{"exception": "NotImplementedError", "msg": "Could not run 'aten::q_scale' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::q_scale' is only available for these backends: [QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nQuantizedCPU: registered at aten/src/ATen/RegisterQuantizedCPU.cpp:944 [kernel]\nQuantizedCUDA: registered at aten/src/ATen/RegisterQuantizedCUDA.cpp:459 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:153 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:498 [backend fallback]\nFunctionalize: registered at ../aten/src/ATen/FunctionalizeFallbackKernel.cpp:290 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:86 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradMTIA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_4.cpp:13056 [kernel]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:382 [backend fallback]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:249 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:710 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:203 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:161 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:494 [backend fallback]\nPreDispatch: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:165 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:157 [backend fallback]\n"}
6826
Cov: 25789 -> 25789
Cov: 25789 -> 25789
6827
Cov: 25789 -> 25790
Cov: 25790 -> 25790
6828
Cov: 25790 -> 25790
Cov: 25790 -> 25790
6829
{"exception": "RuntimeError", "msg": "addr: Expected 1-D argument vec1, but got 2-D"}
6830
Cov: 25790 -> 25790
Cov: 25790 -> 25790
6831
Cov: 25790 -> 25790
Cov: 25790 -> 25790
6832
Cov: 25790 -> 25790
Cov: 25790 -> 25790
6833
Cov: 25790 -> 25792
Cov: 25792 -> 25792
6834
Cov: 25792 -> 25792
Cov: 25792 -> 25792
6835
{"exception": "RuntimeError", "msg": "expand(torch.DoubleTensor{[10, 3, 4]}, size=[3, 5]): the number of sizes provided (2) must be greater or equal to the number of dimensions in the tensor (3)"}
6836
Cov: 25792 -> 25792
Cov: 25792 -> 25792
6837
Cov: 25792 -> 25793
Cov: 25793 -> 25793
6838
Cov: 25793 -> 25794
Cov: 25794 -> 25794
6839
Cov: 25794 -> 25794
Cov: 25794 -> 25794
6840
Cov: 25794 -> 25794
Cov: 25794 -> 25794
6841
Cov: 25794 -> 25794
Cov: 25794 -> 25794
6842
Cov: 25794 -> 25794
Cov: 25794 -> 25794
6843
Cov: 25794 -> 25794
Cov: 25794 -> 25794
6844
Cov: 25794 -> 25795
Cov: 25795 -> 25795
6845
Cov: 25795 -> 25819
Cov: 25819 -> 25819
6846
Cov: 25819 -> 25819
Cov: 25819 -> 25819
6847
Cov: 25819 -> 25821
Cov: 25821 -> 25821
6848
{"exception": "RuntimeError", "msg": "linalg.det: A must be batches of square matrices, but they are 3 by 4 matrices"}
6849
Cov: 25821 -> 25821
Cov: 25821 -> 25821
6850
Cov: 25821 -> 25822
Cov: 25822 -> 25822
6851
Cov: 25822 -> 25822
Cov: 25822 -> 25822
6852
Cov: 25822 -> 25822
Cov: 25822 -> 25822
6853
Cov: 25822 -> 25823
Cov: 25823 -> 25823
6854
Cov: 25823 -> 25823
Cov: 25823 -> 25823
6855
Cov: 25823 -> 25823
Cov: 25823 -> 25823
6856
Cov: 25823 -> 25823
Cov: 25823 -> 25823
6857
Cov: 25823 -> 25823
Cov: 25823 -> 25823
6858
Cov: 25823 -> 25823
Cov: 25823 -> 25823
6859
Cov: 25823 -> 25823
Cov: 25823 -> 25823
6860
{"exception": "RuntimeError", "msg": "a leaf Variable that requires grad is being used in an in-place operation."}
6861
Cov: 25823 -> 25823
Cov: 25823 -> 25823
6862
Cov: 25823 -> 25823
Cov: 25823 -> 25823
6863
Cov: 25823 -> 25823
Cov: 25823 -> 25823
6864
Cov: 25823 -> 25823
Cov: 25823 -> 25823
6865
Cov: 25823 -> 25823
Cov: 25823 -> 25823
6866
Cov: 25823 -> 25823
Cov: 25823 -> 25823
6867
Cov: 25823 -> 25824
Cov: 25824 -> 25824
6868
Cov: 25824 -> 25824
Cov: 25824 -> 25824
6869
Cov: 25824 -> 25824
Cov: 25824 -> 25824
6870
Cov: 25824 -> 25825
Cov: 25825 -> 25825
6871
{"exception": "RuntimeError", "msg": "scatter(): Expected self.dtype to be equal to src.dtype"}
6872
Cov: 25825 -> 25830
Cov: 25830 -> 25830
6873
Cov: 25830 -> 25830
Cov: 25830 -> 25830
6874
{"exception": "RuntimeError", "msg": "index_add_(): self (Double) and source (Long) must have the same scalar type"}
6875
Cov: 25830 -> 25830
Cov: 25830 -> 25830
6876
Cov: 25830 -> 25830
Cov: 25830 -> 25830
6877
{"exception": "TypeError", "msg": "random_() missing 2 required positional argument: \"from\", \"to\""}
6878
Cov: 25830 -> 25831
Cov: 25831 -> 25831
6879
Cov: 25831 -> 25831
Cov: 25831 -> 25831
6880
{"exception": "RuntimeError", "msg": "self and mat2 must have the same dtype, but got Float and Double"}
6881
Cov: 25831 -> 25832
Cov: 25832 -> 25832
6882
Cov: 25832 -> 25832
Cov: 25832 -> 25832
6883
Cov: 25832 -> 25833
Cov: 25833 -> 25833
6884
Cov: 25833 -> 25833
Cov: 25833 -> 25833
6885
Cov: 25833 -> 25833
Cov: 25833 -> 25833
6886
Cov: 25833 -> 25833
Cov: 25833 -> 25833
6887
Cov: 25833 -> 25833
Cov: 25833 -> 25833
6888
Cov: 25833 -> 25833
Cov: 25833 -> 25833
6889
Cov: 25833 -> 25833
Cov: 25833 -> 25833
6890
Cov: 25833 -> 25833
Cov: 25833 -> 25833
6891
Cov: 25833 -> 25834
Cov: 25834 -> 25834
6892
Cov: 25834 -> 25834
Cov: 25834 -> 25834
6893
Cov: 25834 -> 25834
Cov: 25834 -> 25834
6894
Cov: 25834 -> 25835
Cov: 25835 -> 25835
6895
Cov: 25835 -> 25836
Cov: 25836 -> 25836
6896
Cov: 25836 -> 25836
Cov: 25836 -> 25836
6897
{"exception": "TypeError", "msg": "copy_() received an invalid combination of arguments - got (), but expected (Tensor other, bool non_blocking)"}
6898
Cov: 25836 -> 25838
Cov: 25838 -> 25838
6899
Cov: 25838 -> 25838
Cov: 25838 -> 25838
6900
Cov: 25838 -> 25838
Cov: 25838 -> 25838
6901
Cov: 25838 -> 25838
Cov: 25838 -> 25838
6902
Cov: 25838 -> 25838
Cov: 25838 -> 25838
6903
Cov: 25838 -> 25838
Cov: 25838 -> 25838
6904
Cov: 25838 -> 25839
Cov: 25839 -> 25839
6905
Cov: 25839 -> 25839
Cov: 25839 -> 25839
6906
Cov: 25839 -> 25839
Cov: 25839 -> 25839
6907
Cov: 25839 -> 25839
Cov: 25839 -> 25839
6908
Cov: 25839 -> 25839
Cov: 25839 -> 25839
6909
Cov: 25839 -> 25839
Cov: 25839 -> 25839
6910
Cov: 25839 -> 25839
Cov: 25839 -> 25839
6911
Cov: 25839 -> 25839
Cov: 25839 -> 25839
6912
Cov: 25839 -> 25839
Cov: 25839 -> 25839
6913
Cov: 25839 -> 25840
Cov: 25840 -> 25840
6914
Cov: 25840 -> 25840
Cov: 25840 -> 25840
6915
Cov: 25840 -> 25840
Cov: 25840 -> 25840
6916
Cov: 25840 -> 25840
Cov: 25840 -> 25840
6917
Cov: 25840 -> 25840
Cov: 25840 -> 25840
6918
Cov: 25840 -> 25842
Cov: 25842 -> 25842
6919
Cov: 25842 -> 25942
Cov: 25942 -> 25942
6920
Cov: 25942 -> 25943
Cov: 25943 -> 25943
6921
Cov: 25943 -> 25943
Cov: 25943 -> 25943
6922
Cov: 25943 -> 25943
Cov: 25943 -> 25943
6923
Cov: 25943 -> 25943
Cov: 25943 -> 25943
6924
Cov: 25943 -> 25943
Cov: 25943 -> 25943
6925
Cov: 25943 -> 25944
Cov: 25944 -> 25944
6926
Cov: 25944 -> 25944
Cov: 25944 -> 25944
6927
{"exception": "RuntimeError", "msg": "expand(torch.DoubleTensor{[10, 3, 4]}, size=[3, 5]): the number of sizes provided (2) must be greater or equal to the number of dimensions in the tensor (3)"}
6928
Cov: 25944 -> 25944
Cov: 25944 -> 25944
6929
Cov: 25944 -> 25944
Cov: 25944 -> 25944
6930
{"exception": "TypeError", "msg": "heaviside(): argument 'values' must be Tensor, not NoneType"}
6931
Cov: 25944 -> 25944
Cov: 25944 -> 25944
6932
Cov: 25944 -> 25944
Cov: 25944 -> 25944
6933
Cov: 25944 -> 25944
Cov: 25944 -> 25944
6934
Cov: 25944 -> 25944
Cov: 25944 -> 25944
6935
Cov: 25944 -> 25944
Cov: 25944 -> 25944
6936
Cov: 25944 -> 25945
Cov: 25945 -> 25945
6937
Cov: 25945 -> 25945
Cov: 25945 -> 25945
6938
Cov: 25945 -> 25945
Cov: 25945 -> 25945
6939
Cov: 25945 -> 25945
Cov: 25945 -> 25945
6940
Cov: 25945 -> 25945
Cov: 25945 -> 25945
6941
Cov: 25945 -> 25946
Cov: 25946 -> 25946
6942
Cov: 25946 -> 25946
Cov: 25946 -> 25946
6943
Cov: 25946 -> 25947
Cov: 25947 -> 25947
6944
Cov: 25947 -> 25948
Cov: 25948 -> 25948
6945
Cov: 25948 -> 25948
Cov: 25948 -> 25948
6946
Cov: 25948 -> 25948
Cov: 25948 -> 25948
6947
Cov: 25948 -> 25948
Cov: 25948 -> 25948
6948
{"exception": "TypeError", "msg": "'getset_descriptor' object is not callable"}
6949
Cov: 25948 -> 25948
Cov: 25948 -> 25948
6950
Cov: 25948 -> 25948
Cov: 25948 -> 25948
6951
Cov: 25948 -> 25948
Cov: 25948 -> 25948
6952
Cov: 25948 -> 25948
Cov: 25948 -> 25948
6953
Cov: 25948 -> 25948
Cov: 25948 -> 25948
6954
Cov: 25948 -> 25949
Cov: 25949 -> 25949
6955
Cov: 25949 -> 25950
Cov: 25950 -> 25950
6956
Cov: 25950 -> 25950
Cov: 25950 -> 25950
6957
Cov: 25950 -> 25950
Cov: 25950 -> 25950
6958
Cov: 25950 -> 25950
Cov: 25950 -> 25950
6959
Cov: 25950 -> 25950
Cov: 25950 -> 25950
6960
{"exception": "RuntimeError", "msg": "Index tensor must have the same number of dimensions as self tensor"}
6961
Cov: 25950 -> 25950
Cov: 25950 -> 25950
6962
Cov: 25950 -> 25950
Cov: 25950 -> 25950
6963
Cov: 25950 -> 25950
Cov: 25950 -> 25950
6964
Cov: 25950 -> 25950
Cov: 25950 -> 25950
6965
Cov: 25950 -> 25951
Cov: 25951 -> 25951
6966
Cov: 25951 -> 25951
Cov: 25951 -> 25951
6967
Cov: 25951 -> 25951
Cov: 25951 -> 25951
6968
Cov: 25951 -> 25951
Cov: 25951 -> 25951
6969
{"exception": "RuntimeError", "msg": "scatter(): Expected self.dtype to be equal to src.dtype"}
6970
Cov: 25951 -> 25951
Cov: 25951 -> 25951
6971
Cov: 25951 -> 25951
Cov: 25951 -> 25951
6972
Cov: 25951 -> 25951
Cov: 25951 -> 25951
6973
Cov: 25951 -> 25951
Cov: 25951 -> 25951
6974
Cov: 25951 -> 25951
Cov: 25951 -> 25951
6975
Cov: 25951 -> 25951
Cov: 25951 -> 25951
6976
Cov: 25951 -> 25951
Cov: 25951 -> 25951
6977
Cov: 25951 -> 25952
Cov: 25952 -> 25952
6978
Cov: 25952 -> 25953
Cov: 25953 -> 25953
6979
Cov: 25953 -> 25953
Cov: 25953 -> 25953
6980
Cov: 25953 -> 25954
Cov: 25954 -> 25954
6981
Cov: 25954 -> 25954
Cov: 25954 -> 25954
6982
Cov: 25954 -> 25958
Cov: 25958 -> 25958
6983
Cov: 25958 -> 25958
Cov: 25958 -> 25958
6984
Cov: 25958 -> 25958
Cov: 25958 -> 25958
6985
Cov: 25958 -> 25958
Cov: 25958 -> 25958
6986
Cov: 25958 -> 25959
Cov: 25959 -> 25959
6987
Cov: 25959 -> 25959
Cov: 25959 -> 25959
6988
Cov: 25959 -> 25959
Cov: 25959 -> 25959
6989
{"exception": "TypeError", "msg": "set_() received an invalid combination of arguments - got (stride=NoneType, size=NoneType, storage_offset=int, source=NoneType, ), but expected one of:\n * ()\n * (torch.Storage source)\n * (torch.Storage source, int storage_offset, tuple of ints size, tuple of ints stride)\n      didn't match because some of the arguments have invalid types: (\u001b[31;1msource=NoneType\u001b[0m, \u001b[32;1mstorage_offset=int\u001b[0m, \u001b[31;1msize=NoneType\u001b[0m, \u001b[31;1mstride=NoneType\u001b[0m, )\n * (Tensor source)\n * (Tensor source, int storage_offset, tuple of ints size, tuple of ints stride)\n      didn't match because some of the arguments have invalid types: (\u001b[31;1msource=NoneType\u001b[0m, \u001b[32;1mstorage_offset=int\u001b[0m, \u001b[31;1msize=NoneType\u001b[0m, \u001b[31;1mstride=NoneType\u001b[0m, )\n"}
6990
Cov: 25959 -> 25960
Cov: 25960 -> 25960
6991
Cov: 25960 -> 25960
Cov: 25960 -> 25960
6992
Cov: 25960 -> 25960
Cov: 25960 -> 25960
6993
Cov: 25960 -> 25960
Cov: 25960 -> 25960
6994
{"exception": "RuntimeError", "msg": "masked_fill_ only supports boolean masks, but got mask with dtype unsigned char"}
6995
Cov: 25960 -> 25960
Cov: 25960 -> 25960
6996
Cov: 25960 -> 25961
Cov: 25961 -> 25961
6997
Cov: 25961 -> 25961
Cov: 25961 -> 25961
6998
Cov: 25961 -> 25961
Cov: 25961 -> 25961
6999
Cov: 25961 -> 25961
Cov: 25961 -> 25961
7000
Cov: 25961 -> 25961
Cov: 25961 -> 25961
7001
Cov: 25961 -> 25962
Cov: 25962 -> 25962
7002
Cov: 25962 -> 25962
Cov: 25962 -> 25962
7003
Cov: 25962 -> 25962
Cov: 25962 -> 25962
7004
Cov: 25962 -> 25963
Cov: 25963 -> 25963
7005
Cov: 25963 -> 25964
Cov: 25964 -> 25964
7006
{"exception": "TypeError", "msg": "copy_() missing 1 required positional arguments: \"other\""}
7007
Cov: 25964 -> 25965
Cov: 25965 -> 25965
7008
Cov: 25965 -> 25965
Cov: 25965 -> 25965
7009
Cov: 25965 -> 25965
Cov: 25965 -> 25965
7010
Cov: 25965 -> 25965
Cov: 25965 -> 25965
7011
Cov: 25965 -> 25965
Cov: 25965 -> 25965
7012
Cov: 25965 -> 25965
Cov: 25965 -> 25965
7013
{"exception": "NotImplementedError", "msg": "Could not run 'aten::int_repr' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::int_repr' is only available for these backends: [QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nQuantizedCPU: registered at aten/src/ATen/RegisterQuantizedCPU.cpp:944 [kernel]\nQuantizedCUDA: registered at aten/src/ATen/RegisterQuantizedCUDA.cpp:459 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:153 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:498 [backend fallback]\nFunctionalize: registered at ../aten/src/ATen/FunctionalizeFallbackKernel.cpp:290 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:86 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradMTIA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_0.cpp:16725 [kernel]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:382 [backend fallback]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:249 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:710 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:203 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:161 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:494 [backend fallback]\nPreDispatch: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:165 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:157 [backend fallback]\n"}
7014
Cov: 25965 -> 25966
Cov: 25966 -> 25966
7015
Cov: 25966 -> 25966
Cov: 25966 -> 25966
7016
Cov: 25966 -> 25966
Cov: 25966 -> 25966
7017
Cov: 25966 -> 25966
Cov: 25966 -> 25966
7018
{"exception": "RuntimeError", "msg": "linalg.slogdet: A must be batches of square matrices, but they are 3 by 4 matrices"}
7019
Cov: 25966 -> 25966
Cov: 25966 -> 25966
7020
Cov: 25966 -> 25967
Cov: 25967 -> 25967
7021
Cov: 25967 -> 25967
Cov: 25967 -> 25967
7022
Cov: 25967 -> 25967
Cov: 25967 -> 25967
7023
Cov: 25967 -> 25967
Cov: 25967 -> 25967
7024
Cov: 25967 -> 25967
Cov: 25967 -> 25967
7025
Cov: 25967 -> 25968
Cov: 25968 -> 25968
7026
Cov: 25968 -> 25968
Cov: 25968 -> 25968
7027
Cov: 25968 -> 25969
Cov: 25969 -> 25969
7028
Cov: 25969 -> 25969
Cov: 25969 -> 25969
7029
Cov: 25969 -> 25969
Cov: 25969 -> 25969
7030
Cov: 25969 -> 25969
Cov: 25969 -> 25969
7031
Cov: 25969 -> 25969
Cov: 25969 -> 25969
7032
Cov: 25969 -> 25969
Cov: 25969 -> 25969
7033
Cov: 25969 -> 25970
Cov: 25970 -> 25970
7034
Cov: 25970 -> 25971
Cov: 25971 -> 25971
7035
Cov: 25971 -> 25971
Cov: 25971 -> 25971
7036
Cov: 25971 -> 25971
Cov: 25971 -> 25971
7037
Cov: 25971 -> 25971
Cov: 25971 -> 25971
7038
Cov: 25971 -> 25972
Cov: 25972 -> 25972
7039
Cov: 25972 -> 25972
Cov: 25972 -> 25972
7040
Cov: 25972 -> 25972
Cov: 25972 -> 25972
7041
{"exception": "TypeError", "msg": "set_() received an invalid combination of arguments - got (stride=NoneType, size=NoneType, storage_offset=int, source=NoneType, ), but expected one of:\n * ()\n * (torch.Storage source)\n * (torch.Storage source, int storage_offset, tuple of ints size, tuple of ints stride)\n      didn't match because some of the arguments have invalid types: (\u001b[31;1msource=NoneType\u001b[0m, \u001b[32;1mstorage_offset=int\u001b[0m, \u001b[31;1msize=NoneType\u001b[0m, \u001b[31;1mstride=NoneType\u001b[0m, )\n * (Tensor source)\n * (Tensor source, int storage_offset, tuple of ints size, tuple of ints stride)\n      didn't match because some of the arguments have invalid types: (\u001b[31;1msource=NoneType\u001b[0m, \u001b[32;1mstorage_offset=int\u001b[0m, \u001b[31;1msize=NoneType\u001b[0m, \u001b[31;1mstride=NoneType\u001b[0m, )\n"}
7042
{"exception": "TypeError", "msg": "'getset_descriptor' object is not callable"}
7043
Cov: 25972 -> 25972
Cov: 25972 -> 25972
7044
{"exception": "RuntimeError", "msg": "For integral input tensors, argument alpha must not be a floating point number."}
7045
Cov: 25972 -> 25973
Cov: 25973 -> 25973
7046
Cov: 25973 -> 25974
Cov: 25974 -> 25974
7047
{"exception": "RuntimeError", "msg": "a Tensor with 150528 elements cannot be converted to Scalar"}
7048
{"exception": "TypeError", "msg": "index_add_() missing 1 required positional arguments: \"source\""}
7049
Cov: 25974 -> 25974
Cov: 25974 -> 25974
7050
Cov: 25974 -> 25974
Cov: 25974 -> 25974
7051
Cov: 25974 -> 25974
Cov: 25974 -> 25974
7052
Cov: 25974 -> 25975
Cov: 25975 -> 25975
7053
Cov: 25975 -> 25975
Cov: 25975 -> 25975
7054
Cov: 25975 -> 25976
Cov: 25976 -> 25976
7055
Cov: 25976 -> 25976
Cov: 25976 -> 25976
7056
{"exception": "RuntimeError", "msg": "self and mat2 must have the same dtype, but got Float and Double"}
7057
Cov: 25976 -> 25976
Cov: 25976 -> 25976
7058
Cov: 25976 -> 25976
Cov: 25976 -> 25976
7059
Cov: 25976 -> 25977
Cov: 25977 -> 25977
7060
Cov: 25977 -> 25977
Cov: 25977 -> 25977
7061
Cov: 25977 -> 25977
Cov: 25977 -> 25977
7062
Cov: 25977 -> 25980
Cov: 25980 -> 25980
7063
Cov: 25980 -> 25981
Cov: 25981 -> 25981
7064
Cov: 25981 -> 25981
Cov: 25981 -> 25981
7065
Cov: 25981 -> 25982
Cov: 25982 -> 25982
7066
Cov: 25982 -> 25982
Cov: 25982 -> 25982
7067
Cov: 25982 -> 25982
Cov: 25982 -> 25982
7068
Cov: 25982 -> 25983
Cov: 25983 -> 25983
7069
{"exception": "RuntimeError", "msg": "masked_fill_ only supports a 0-dimensional value tensor, but got tensor with 1 dimension(s)."}
7070
{"exception": "RuntimeError", "msg": "This function was deprecated since version 1.9 and is now removed. `torch.solve` is deprecated in favor of `torch.linalg.solve`. `torch.linalg.solve` has its arguments reversed and does not return the LU factorization.\n\nTo get the LU factorization see `torch.lu`, which can be used with `torch.lu_solve` or `torch.lu_unpack`.\nX = torch.solve(B, A).solution should be replaced with:\nX = torch.linalg.solve(A, B)"}
7071
Cov: 25983 -> 25984
Cov: 25984 -> 25984
7072
Cov: 25984 -> 25984
Cov: 25984 -> 25984
7073
Cov: 25984 -> 25984
Cov: 25984 -> 25984
7074
Cov: 25984 -> 25984
Cov: 25984 -> 25984
7075
Cov: 25984 -> 25984
Cov: 25984 -> 25984
7076
Cov: 25984 -> 25984
Cov: 25984 -> 25984
7077
Cov: 25984 -> 25985
Cov: 25985 -> 25985
7078
Cov: 25985 -> 25985
Cov: 25985 -> 25985
7079
Cov: 25985 -> 25985
Cov: 25985 -> 25985
7080
Cov: 25985 -> 25986
Cov: 25986 -> 25986
7081
{"exception": "RuntimeError", "msg": "scatter(): Expected self.dtype to be equal to src.dtype"}
7082
Cov: 25986 -> 25986
Cov: 25986 -> 25986
7083
Cov: 25986 -> 25986
Cov: 25986 -> 25986
7084
Cov: 25986 -> 25986
Cov: 25986 -> 25986
7085
Cov: 25986 -> 25987
Cov: 25987 -> 25987
7086
Cov: 25987 -> 25987
Cov: 25987 -> 25987
7087
Cov: 25987 -> 25987
Cov: 25987 -> 25987
7088
Cov: 25987 -> 25987
Cov: 25987 -> 25987
7089
Cov: 25987 -> 25987
Cov: 25987 -> 25987
7090
Cov: 25987 -> 26010
Cov: 26010 -> 26010
7091
Cov: 26010 -> 26010
Cov: 26010 -> 26010
7092
Cov: 26010 -> 26010
Cov: 26010 -> 26010
7093
Cov: 26010 -> 26010
Cov: 26010 -> 26010
7094
{"exception": "RuntimeError", "msg": "The expanded size of the tensor (3) must match the existing size (2) at non-singleton dimension 0.  Target sizes: [3, 3].  Tensor sizes: [2, 3]"}
7095
Cov: 26010 -> 26010
Cov: 26010 -> 26010
7096
Cov: 26010 -> 26010
Cov: 26010 -> 26010
7097
Cov: 26010 -> 26011
Cov: 26011 -> 26011
7098
Cov: 26011 -> 26011
Cov: 26011 -> 26011
7099
Cov: 26011 -> 26011
Cov: 26011 -> 26011
7100
Cov: 26011 -> 26012
Cov: 26012 -> 26012
7101
Cov: 26012 -> 26012
Cov: 26012 -> 26012
7102
Cov: 26012 -> 26012
Cov: 26012 -> 26012
7103
Cov: 26012 -> 26013
Cov: 26013 -> 26013
7104
Cov: 26013 -> 26013
Cov: 26013 -> 26013
7105
Cov: 26013 -> 26013
Cov: 26013 -> 26013
7106
Cov: 26013 -> 26013
Cov: 26013 -> 26013
7107
Cov: 26013 -> 26013
Cov: 26013 -> 26013
7108
Cov: 26013 -> 26013
Cov: 26013 -> 26013
7109
Cov: 26013 -> 26013
Cov: 26013 -> 26013
7110
Cov: 26013 -> 26013
Cov: 26013 -> 26013
7111
Cov: 26013 -> 26013
Cov: 26013 -> 26013
7112
Cov: 26013 -> 26013
Cov: 26013 -> 26013
7113
Cov: 26013 -> 26013
Cov: 26013 -> 26013
7114
Cov: 26013 -> 26013
Cov: 26013 -> 26013
7115
Cov: 26013 -> 26013
Cov: 26013 -> 26013
7116
Cov: 26013 -> 26013
Cov: 26013 -> 26013
7117
Cov: 26013 -> 26013
Cov: 26013 -> 26013
7118
Cov: 26013 -> 26013
Cov: 26013 -> 26013
7119
Cov: 26013 -> 26013
Cov: 26013 -> 26013
7120
Cov: 26013 -> 26014
Cov: 26014 -> 26014
7121
Cov: 26014 -> 26014
Cov: 26014 -> 26014
7122
Cov: 26014 -> 26014
Cov: 26014 -> 26014
7123
Cov: 26014 -> 26014
Cov: 26014 -> 26014
7124
Cov: 26014 -> 26014
Cov: 26014 -> 26014
7125
Cov: 26014 -> 26014
Cov: 26014 -> 26014
7126
Cov: 26014 -> 26014
Cov: 26014 -> 26014
7127
Cov: 26014 -> 26014
Cov: 26014 -> 26014
7128
Cov: 26014 -> 26014
Cov: 26014 -> 26014
7129
Cov: 26014 -> 26014
Cov: 26014 -> 26014
7130
Cov: 26014 -> 26014
Cov: 26014 -> 26014
7131
Cov: 26014 -> 26014
Cov: 26014 -> 26014
7132
Cov: 26014 -> 26014
Cov: 26014 -> 26014
7133
Cov: 26014 -> 26014
Cov: 26014 -> 26014
7134
{"exception": "NotImplementedError", "msg": "Could not run 'aten::q_per_channel_scales' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::q_per_channel_scales' is only available for these backends: [QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nQuantizedCPU: registered at aten/src/ATen/RegisterQuantizedCPU.cpp:944 [kernel]\nQuantizedCUDA: registered at aten/src/ATen/RegisterQuantizedCUDA.cpp:459 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:153 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:498 [backend fallback]\nFunctionalize: registered at ../aten/src/ATen/FunctionalizeFallbackKernel.cpp:290 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:86 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradMTIA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_3.cpp:14610 [kernel]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:382 [backend fallback]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:249 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:710 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:203 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:161 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:494 [backend fallback]\nPreDispatch: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:165 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:157 [backend fallback]\n"}
7135
Cov: 26014 -> 26014
Cov: 26014 -> 26014
7136
Cov: 26014 -> 26015
Cov: 26015 -> 26015
7137
Cov: 26015 -> 26016
Cov: 26016 -> 26016
7138
{"exception": "_LinAlgError", "msg": "cholesky: The factorization could not be completed because the input is not positive-definite (the leading minor of order 1 is not positive-definite)."}
7139
Cov: 26016 -> 26018
Cov: 26018 -> 26018
7140
Cov: 26018 -> 26019
Cov: 26019 -> 26019
7141
Cov: 26019 -> 26020
Cov: 26020 -> 26020
7142
Cov: 26020 -> 26020
Cov: 26020 -> 26020
7143
Cov: 26020 -> 26021
Cov: 26021 -> 26021
7144
{"exception": "TypeError", "msg": "'getset_descriptor' object is not callable"}
7145
Cov: 26021 -> 26021
Cov: 26021 -> 26021
7146
Cov: 26021 -> 26021
Cov: 26021 -> 26021
7147
Cov: 26021 -> 26022
Cov: 26022 -> 26022
7148
Cov: 26022 -> 26022
Cov: 26022 -> 26022
7149
Cov: 26022 -> 26022
Cov: 26022 -> 26022
7150
Cov: 26022 -> 26022
Cov: 26022 -> 26022
7151
Cov: 26022 -> 26022
Cov: 26022 -> 26022
7152
{"exception": "TypeError", "msg": "'getset_descriptor' object is not callable"}
7153
Cov: 26022 -> 26022
Cov: 26022 -> 26022
7154
Cov: 26022 -> 26022
Cov: 26022 -> 26022
7155
Cov: 26022 -> 26023
Cov: 26023 -> 26023
7156
Cov: 26023 -> 26023
Cov: 26023 -> 26023
7157
Cov: 26023 -> 26024
Cov: 26024 -> 26024
7158
Cov: 26024 -> 26024
Cov: 26024 -> 26024
7159
Cov: 26024 -> 26024
Cov: 26024 -> 26024
7160
Cov: 26024 -> 26024
Cov: 26024 -> 26024
7161
Cov: 26024 -> 26024
Cov: 26024 -> 26024
7162
Cov: 26024 -> 26024
Cov: 26024 -> 26024
7163
Cov: 26024 -> 26026
Cov: 26026 -> 26026
7164
Cov: 26026 -> 26026
Cov: 26026 -> 26026
7165
Cov: 26026 -> 26026
Cov: 26026 -> 26026
7166
Cov: 26026 -> 26026
Cov: 26026 -> 26026
7167
Cov: 26026 -> 26026
Cov: 26026 -> 26026
7168
Cov: 26026 -> 26027
Cov: 26027 -> 26027
7169
Cov: 26027 -> 26027
Cov: 26027 -> 26027
7170
{"exception": "RuntimeError", "msg": "scatter(): Expected self.dtype to be equal to src.dtype"}
7171
Cov: 26027 -> 26027
Cov: 26027 -> 26027
7172
Cov: 26027 -> 26027
Cov: 26027 -> 26027
7173
Cov: 26027 -> 26041
Cov: 26041 -> 26041
7174
{"exception": "RuntimeError", "msg": "masked_select: expected BoolTensor for mask"}
7175
Cov: 26041 -> 26042
Cov: 26042 -> 26042
7176
{"exception": "RuntimeError", "msg": "the base given to float_power_ has dtype Float but the operation's result requires dtype Double"}
7177
{"exception": "RuntimeError", "msg": "any: You passed a dimname (string) to this op in place of a dimension index but it does not yet support this behavior. Please pass a dimension index to work around this."}
7178
Cov: 26042 -> 26042
Cov: 26042 -> 26042
7179
{"exception": "TypeError", "msg": "new_empty() missing 1 required positional arguments: \"size\""}
7180
Cov: 26042 -> 26042
Cov: 26042 -> 26042
7181
Cov: 26042 -> 26043
Cov: 26043 -> 26043
7182
Cov: 26043 -> 26044
Cov: 26044 -> 26044
7183
Cov: 26044 -> 26044
Cov: 26044 -> 26044
7184
Cov: 26044 -> 26044
Cov: 26044 -> 26044
7185
Cov: 26044 -> 26045
Cov: 26045 -> 26045
7186
Cov: 26045 -> 26045
Cov: 26045 -> 26045
7187
Cov: 26045 -> 26045
Cov: 26045 -> 26045
7188
Cov: 26045 -> 26045
Cov: 26045 -> 26045
7189
Cov: 26045 -> 26046
Cov: 26046 -> 26046
7190
Cov: 26046 -> 26046
Cov: 26046 -> 26046
7191
Cov: 26046 -> 26051
Cov: 26051 -> 26051
7192
Cov: 26051 -> 26051
Cov: 26051 -> 26051
7193
Cov: 26051 -> 26051
Cov: 26051 -> 26051
7194
Cov: 26051 -> 26051
Cov: 26051 -> 26051
7195
Cov: 26051 -> 26051
Cov: 26051 -> 26051
7196
{"exception": "RuntimeError", "msg": "The expanded size of the tensor (3) must match the existing size (2) at non-singleton dimension 0.  Target sizes: [3, 3].  Tensor sizes: [2, 3]"}
7197
Cov: 26051 -> 26051
Cov: 26051 -> 26051
7198
Cov: 26051 -> 26051
Cov: 26051 -> 26051
7199
Cov: 26051 -> 26051
Cov: 26051 -> 26051
7200
Cov: 26051 -> 26051
Cov: 26051 -> 26051
7201
Cov: 26051 -> 26051
Cov: 26051 -> 26051
7202
Cov: 26051 -> 26051
Cov: 26051 -> 26051
7203
Cov: 26051 -> 26051
Cov: 26051 -> 26051
7204
Cov: 26051 -> 26193
Cov: 26193 -> 26193
7205
Cov: 26193 -> 26224
Cov: 26224 -> 26224
7206
Cov: 26224 -> 26226
Cov: 26226 -> 26226
7207
Cov: 26226 -> 26226
Cov: 26226 -> 26226
7208
Cov: 26226 -> 26226
Cov: 26226 -> 26226
7209
{"exception": "RuntimeError", "msg": "linalg.vector_norm: Expected a floating point or complex tensor as input. Got Long"}
7210
Cov: 26226 -> 26226
Cov: 26226 -> 26226
7211
Cov: 26226 -> 26227
Cov: 26227 -> 26227
7212
Cov: 26227 -> 26227
Cov: 26227 -> 26227
7213
Cov: 26227 -> 26227
Cov: 26227 -> 26227
7214
Cov: 26227 -> 26227
Cov: 26227 -> 26227
7215
Cov: 26227 -> 26227
Cov: 26227 -> 26227
7216
Cov: 26227 -> 26227
Cov: 26227 -> 26227
7217
Cov: 26227 -> 26229
Cov: 26229 -> 26229
7218
{"exception": "RuntimeError", "msg": "Creating a Tensor subclass from a class that does not inherit from Tensor is not possible. Make sure your class inherits from Tensor."}
7219
Cov: 26229 -> 26229
Cov: 26229 -> 26229
7220
Cov: 26229 -> 26229
Cov: 26229 -> 26229
7221
Cov: 26229 -> 26229
Cov: 26229 -> 26229
7222
Cov: 26229 -> 26233
Cov: 26233 -> 26233
7223
Cov: 26233 -> 26233
Cov: 26233 -> 26233
7224
{"exception": "NotImplementedError", "msg": "Could not run 'aten::q_per_channel_scales' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::q_per_channel_scales' is only available for these backends: [QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nQuantizedCPU: registered at aten/src/ATen/RegisterQuantizedCPU.cpp:944 [kernel]\nQuantizedCUDA: registered at aten/src/ATen/RegisterQuantizedCUDA.cpp:459 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:153 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:498 [backend fallback]\nFunctionalize: registered at ../aten/src/ATen/FunctionalizeFallbackKernel.cpp:290 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:86 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradMTIA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_3.cpp:14610 [kernel]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:382 [backend fallback]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:249 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:710 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:203 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:161 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:494 [backend fallback]\nPreDispatch: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:165 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:157 [backend fallback]\n"}
7225
Cov: 26233 -> 26234
Cov: 26234 -> 26234
7226
Cov: 26234 -> 26234
Cov: 26234 -> 26234
7227
Cov: 26234 -> 26234
Cov: 26234 -> 26234
7228
Cov: 26234 -> 26234
Cov: 26234 -> 26234
7229
Cov: 26234 -> 26234
Cov: 26234 -> 26234
7230
Cov: 26234 -> 26234
Cov: 26234 -> 26234
7231
Cov: 26234 -> 26234
Cov: 26234 -> 26234
7232
Cov: 26234 -> 26234
Cov: 26234 -> 26234
7233
Cov: 26234 -> 26234
Cov: 26234 -> 26234
7234
Cov: 26234 -> 26234
Cov: 26234 -> 26234
7235
Cov: 26234 -> 26234
Cov: 26234 -> 26234
7236
Cov: 26234 -> 26235
Cov: 26235 -> 26235
7237
Cov: 26235 -> 26235
Cov: 26235 -> 26235
7238
Cov: 26235 -> 26235
Cov: 26235 -> 26235
7239
Cov: 26235 -> 26235
Cov: 26235 -> 26235
7240
Cov: 26235 -> 26235
Cov: 26235 -> 26235
7241
Cov: 26235 -> 26235
Cov: 26235 -> 26235
7242
Cov: 26235 -> 26235
Cov: 26235 -> 26235
7243
Cov: 26235 -> 26235
Cov: 26235 -> 26235
7244
Cov: 26235 -> 26236
Cov: 26236 -> 26236
7245
{"exception": "TypeError", "msg": "arctanh_() takes no keyword arguments"}
7246
Cov: 26236 -> 26236
Cov: 26236 -> 26236
7247
Cov: 26236 -> 26237
Cov: 26237 -> 26237
7248
Cov: 26237 -> 26237
Cov: 26237 -> 26237
7249
Cov: 26237 -> 26238
Cov: 26238 -> 26238
7250
Cov: 26238 -> 26239
Cov: 26239 -> 26239
7251
{"exception": "RuntimeError", "msg": "Index tensor must have the same number of dimensions as self tensor"}
7252
Cov: 26239 -> 26239
Cov: 26239 -> 26239
7253
Cov: 26239 -> 26239
Cov: 26239 -> 26239
7254
Cov: 26239 -> 26240
Cov: 26240 -> 26240
7255
Cov: 26240 -> 26240
Cov: 26240 -> 26240
7256
Cov: 26240 -> 26240
Cov: 26240 -> 26240
7257
Cov: 26240 -> 26240
Cov: 26240 -> 26240
7258
Cov: 26240 -> 26240
Cov: 26240 -> 26240
7259
Cov: 26240 -> 26240
Cov: 26240 -> 26240
7260
Cov: 26240 -> 26240
Cov: 26240 -> 26240
7261
Cov: 26240 -> 26241
Cov: 26241 -> 26241
7262
Cov: 26241 -> 26241
Cov: 26241 -> 26241
7263
Cov: 26241 -> 26241
Cov: 26241 -> 26241
7264
Cov: 26241 -> 26242
Cov: 26242 -> 26242
7265
Cov: 26242 -> 26242
Cov: 26242 -> 26242
7266
Cov: 26242 -> 26242
Cov: 26242 -> 26242
7267
Cov: 26242 -> 26243
Cov: 26243 -> 26243
7268
Cov: 26243 -> 26243
Cov: 26243 -> 26243
7269
Cov: 26243 -> 26244
Cov: 26244 -> 26244
7270
Cov: 26244 -> 26244
Cov: 26244 -> 26244
7271
Cov: 26244 -> 26244
Cov: 26244 -> 26244
7272
{"exception": "RuntimeError", "msg": "scatter(): Expected self.dtype to be equal to src.dtype"}
7273
Cov: 26244 -> 26244
Cov: 26244 -> 26244
7274
Cov: 26244 -> 26244
Cov: 26244 -> 26244
7275
Cov: 26244 -> 26246
Cov: 26246 -> 26246
7276
Cov: 26246 -> 26246
Cov: 26246 -> 26246
7277
Cov: 26246 -> 26246
Cov: 26246 -> 26246
7278
Cov: 26246 -> 26246
Cov: 26246 -> 26246
7279
{"exception": "TypeError", "msg": "'getset_descriptor' object is not callable"}
7280
{"exception": "TypeError", "msg": "ldexp_(): argument 'other' (position 1) must be Tensor, not int"}
7281
Cov: 26246 -> 26246
Cov: 26246 -> 26246
7282
Cov: 26246 -> 26246
Cov: 26246 -> 26246
7283
Cov: 26246 -> 26246
Cov: 26246 -> 26246
7284
Cov: 26246 -> 26246
Cov: 26246 -> 26246
7285
Cov: 26246 -> 26246
Cov: 26246 -> 26246
7286
Cov: 26246 -> 26246
Cov: 26246 -> 26246
7287
Cov: 26246 -> 26246
Cov: 26246 -> 26246
7288
Cov: 26246 -> 26246
Cov: 26246 -> 26246
7289
Cov: 26246 -> 26246
Cov: 26246 -> 26246
7290
Cov: 26246 -> 26246
Cov: 26246 -> 26246
7291
Cov: 26246 -> 26246
Cov: 26246 -> 26246
7292
Cov: 26246 -> 26246
Cov: 26246 -> 26246
7293
Cov: 26246 -> 26246
Cov: 26246 -> 26246
7294
Cov: 26246 -> 26246
Cov: 26246 -> 26246
7295
Cov: 26246 -> 26246
Cov: 26246 -> 26246
7296
Cov: 26246 -> 26247
Cov: 26247 -> 26247
7297
{"exception": "RuntimeError", "msg": "The function 'soft_margin_loss' is not differentiable with respect to argument 'target'. This input cannot have requires_grad True."}
7298
Cov: 26247 -> 26248
Cov: 26248 -> 26248
7299
Cov: 26248 -> 26248
Cov: 26248 -> 26248
7300
{"exception": "RuntimeError", "msg": "vector + matrix @ vector expected, got 3, 2, 1"}
7301
Cov: 26248 -> 26248
Cov: 26248 -> 26248
7302
Cov: 26248 -> 26248
Cov: 26248 -> 26248
7303
Cov: 26248 -> 26248
Cov: 26248 -> 26248
7304
Cov: 26248 -> 26249
Cov: 26249 -> 26249
7305
Cov: 26249 -> 26249
Cov: 26249 -> 26249
7306
{"exception": "RuntimeError", "msg": "Bad in-place call: input tensor size [2, 3] and output tensor size [3, 5] should match"}
7307
Cov: 26249 -> 26249
Cov: 26249 -> 26249
7308
Cov: 26249 -> 26249
Cov: 26249 -> 26249
7309
{"exception": "RuntimeError", "msg": "The expanded size of the tensor (3) must match the existing size (2) at non-singleton dimension 1.  Target sizes: [2, 3, 3].  Tensor sizes: [2, 3]"}
7310
Cov: 26249 -> 26249
Cov: 26249 -> 26249
7311
{"exception": "RuntimeError", "msg": "result type Double can't be cast to the desired output type Long"}
7312
Cov: 26249 -> 26250
Cov: 26250 -> 26250
7313
Cov: 26250 -> 26252
Cov: 26252 -> 26252
7314
Cov: 26252 -> 26252
Cov: 26252 -> 26252
7315
Cov: 26252 -> 26252
Cov: 26252 -> 26252
7316
Cov: 26252 -> 26253
Cov: 26253 -> 26253
7317
Cov: 26253 -> 26253
Cov: 26253 -> 26253
7318
Cov: 26253 -> 26253
Cov: 26253 -> 26253
7319
Cov: 26253 -> 26254
Cov: 26254 -> 26254
7320
Cov: 26254 -> 26254
Cov: 26254 -> 26254
7321
Cov: 26254 -> 26254
Cov: 26254 -> 26254
7322
Cov: 26254 -> 26254
Cov: 26254 -> 26254
7323
Cov: 26254 -> 26254
Cov: 26254 -> 26254
7324
Cov: 26254 -> 26255
Cov: 26255 -> 26255
7325
Cov: 26255 -> 26256
Cov: 26256 -> 26256
7326
Cov: 26256 -> 26256
Cov: 26256 -> 26256
7327
Cov: 26256 -> 26256
Cov: 26256 -> 26256
7328
Cov: 26256 -> 26256
Cov: 26256 -> 26256
7329
Cov: 26256 -> 26256
Cov: 26256 -> 26256
7330
Cov: 26256 -> 26257
Cov: 26257 -> 26257
7331
Cov: 26257 -> 26257
Cov: 26257 -> 26257
7332
Cov: 26257 -> 26257
Cov: 26257 -> 26257
7333
{"exception": "RuntimeError", "msg": "size mismatch, input: [2, 3], v1: [3], v2: [3]"}
7334
Cov: 26257 -> 26257
Cov: 26257 -> 26257
7335
Cov: 26257 -> 26257
Cov: 26257 -> 26257
7336
Cov: 26257 -> 26257
Cov: 26257 -> 26257
7337
Cov: 26257 -> 26257
Cov: 26257 -> 26257
7338
Cov: 26257 -> 26257
Cov: 26257 -> 26257
7339
Cov: 26257 -> 26257
Cov: 26257 -> 26257
7340
Cov: 26257 -> 26257
Cov: 26257 -> 26257
7341
Cov: 26257 -> 26257
Cov: 26257 -> 26257
7342
Cov: 26257 -> 26257
Cov: 26257 -> 26257
7343
Cov: 26257 -> 26257
Cov: 26257 -> 26257
7344
Cov: 26257 -> 26258
Cov: 26258 -> 26258
7345
Cov: 26258 -> 26258
Cov: 26258 -> 26258
7346
Cov: 26258 -> 26258
Cov: 26258 -> 26258
7347
Cov: 26258 -> 26258
Cov: 26258 -> 26258
7348
Cov: 26258 -> 26258
Cov: 26258 -> 26258
7349
Cov: 26258 -> 26258
Cov: 26258 -> 26258
7350
Cov: 26258 -> 26258
Cov: 26258 -> 26258
7351
Cov: 26258 -> 26259
Cov: 26259 -> 26259
7352
Cov: 26259 -> 26259
Cov: 26259 -> 26259
7353
Cov: 26259 -> 26259
Cov: 26259 -> 26259
7354
Cov: 26259 -> 26259
Cov: 26259 -> 26259
7355
Cov: 26259 -> 26259
Cov: 26259 -> 26259
7356
Cov: 26259 -> 26259
Cov: 26259 -> 26259
7357
Cov: 26259 -> 26259
Cov: 26259 -> 26259
7358
Cov: 26259 -> 26259
Cov: 26259 -> 26259
7359
Cov: 26259 -> 26259
Cov: 26259 -> 26259
7360
Cov: 26259 -> 26259
Cov: 26259 -> 26259
7361
Cov: 26259 -> 26259
Cov: 26259 -> 26259
7362
Cov: 26259 -> 26259
Cov: 26259 -> 26259
7363
Cov: 26259 -> 26261
Cov: 26261 -> 26261
7364
Cov: 26261 -> 26261
Cov: 26261 -> 26261
7365
Cov: 26261 -> 26261
Cov: 26261 -> 26261
7366
Cov: 26261 -> 26261
Cov: 26261 -> 26261
7367
Cov: 26261 -> 26261
Cov: 26261 -> 26261
7368
Cov: 26261 -> 26262
Cov: 26262 -> 26262
7369
Cov: 26262 -> 26262
Cov: 26262 -> 26262
7370
Cov: 26262 -> 26262
Cov: 26262 -> 26262
7371
{"exception": "RuntimeError", "msg": "This function was deprecated since version 1.9 and is now removed. `torch.linalg.eig` returns complex tensors of dtype `cfloat` or `cdouble` rather than real tensors mimicking complex tensors.\n\nL, _ = torch.eig(A) should be replaced with:\nL_complex = torch.linalg.eigvals(A)\n\nand\n\nL, V = torch.eig(A, eigenvectors=True) should be replaced with:\nL_complex, V_complex = torch.linalg.eig(A)"}
7372
Cov: 26262 -> 26263
Cov: 26263 -> 26263
7373
Cov: 26263 -> 26264
Cov: 26264 -> 26264
7374
Cov: 26264 -> 26264
Cov: 26264 -> 26264
7375
Cov: 26264 -> 26264
Cov: 26264 -> 26264
7376
Cov: 26264 -> 26264
Cov: 26264 -> 26264
7377
Cov: 26264 -> 26264
Cov: 26264 -> 26264
7378
Cov: 26264 -> 26264
Cov: 26264 -> 26264
7379
Cov: 26264 -> 26264
Cov: 26264 -> 26264
7380
Cov: 26264 -> 26265
Cov: 26265 -> 26265
7381
Cov: 26265 -> 26265
Cov: 26265 -> 26265
7382
Cov: 26265 -> 26265
Cov: 26265 -> 26265
7383
Cov: 26265 -> 26265
Cov: 26265 -> 26265
7384
Cov: 26265 -> 26605
Cov: 26605 -> 26605
7385
Cov: 26605 -> 26605
Cov: 26605 -> 26605
7386
Cov: 26605 -> 26605
Cov: 26605 -> 26605
7387
Cov: 26605 -> 26606
Cov: 26606 -> 26606
7388
Cov: 26606 -> 26606
Cov: 26606 -> 26606
7389
Cov: 26606 -> 26606
Cov: 26606 -> 26606
7390
Cov: 26606 -> 26606
Cov: 26606 -> 26606
7391
Cov: 26606 -> 26606
Cov: 26606 -> 26606
7392
Cov: 26606 -> 26606
Cov: 26606 -> 26606
7393
Cov: 26606 -> 26606
Cov: 26606 -> 26606
7394
{"exception": "TypeError", "msg": "addbmm() missing 1 required positional arguments: \"batch2\""}
7395
Cov: 26606 -> 26606
Cov: 26606 -> 26606
7396
Cov: 26606 -> 26606
Cov: 26606 -> 26606
7397
Cov: 26606 -> 26607
Cov: 26607 -> 26607
7398
{"exception": "TypeError", "msg": "heaviside(): argument 'values' (position 1) must be Tensor, not int"}
7399
{"exception": "TypeError", "msg": "'getset_descriptor' object is not callable"}
7400
Cov: 26607 -> 26608
Cov: 26608 -> 26608
7401
{"exception": "RuntimeError", "msg": "max_unpooling3d_forward_out does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True)'. You can turn off determinism just for this operation, or you can use the 'warn_only=True' option, if that's acceptable for your application. You can also file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation."}
7402
Cov: 26608 -> 26613
Cov: 26613 -> 26613
7403
Cov: 26613 -> 26613
Cov: 26613 -> 26613
7404
Cov: 26613 -> 26614
Cov: 26614 -> 26614
7405
Cov: 26614 -> 26614
Cov: 26614 -> 26614
7406
Cov: 26614 -> 26614
Cov: 26614 -> 26614
7407
Cov: 26614 -> 26614
Cov: 26614 -> 26614
7408
Cov: 26614 -> 26614
Cov: 26614 -> 26614
7409
Cov: 26614 -> 26614
Cov: 26614 -> 26614
7410
Cov: 26614 -> 26614
Cov: 26614 -> 26614
7411
Cov: 26614 -> 26614
Cov: 26614 -> 26614
7412
Cov: 26614 -> 26615
Cov: 26615 -> 26615
7413
Cov: 26615 -> 26615
Cov: 26615 -> 26615
7414
Cov: 26615 -> 26624
Cov: 26624 -> 26624
7415
Cov: 26624 -> 26624
Cov: 26624 -> 26624
7416
Cov: 26624 -> 26624
Cov: 26624 -> 26624
7417
Cov: 26624 -> 26624
Cov: 26624 -> 26624
7418
Cov: 26624 -> 26624
Cov: 26624 -> 26624
7419
Cov: 26624 -> 26624
Cov: 26624 -> 26624
7420
Cov: 26624 -> 26624
Cov: 26624 -> 26624
7421
Cov: 26624 -> 26624
Cov: 26624 -> 26624
7422
{"exception": "RuntimeError", "msg": "setStorage: sizes [4, 3], strides [2, 2], storage offset 0, and itemsize 8 requiring a storage size of 88 are out of bounds for storage of size 80"}
7423
Cov: 26624 -> 26624
Cov: 26624 -> 26624
7424
Cov: 26624 -> 26625
Cov: 26625 -> 26625
7425
Cov: 26625 -> 26625
Cov: 26625 -> 26625
7426
Cov: 26625 -> 26625
Cov: 26625 -> 26625
7427
Cov: 26625 -> 26626
Cov: 26626 -> 26626
7428
Cov: 26626 -> 26626
Cov: 26626 -> 26626
7429
Cov: 26626 -> 26626
Cov: 26626 -> 26626
7430
Cov: 26626 -> 26626
Cov: 26626 -> 26626
7431
Cov: 26626 -> 26626
Cov: 26626 -> 26626
7432
Cov: 26626 -> 26626
Cov: 26626 -> 26626
7433
{"exception": "TypeError", "msg": "'getset_descriptor' object is not callable"}
7434
Cov: 26626 -> 26626
Cov: 26626 -> 26626
7435
Cov: 26626 -> 26626
Cov: 26626 -> 26626
7436
Cov: 26626 -> 26626
Cov: 26626 -> 26626
7437
Cov: 26626 -> 26627
Cov: 26627 -> 26627
7438
Cov: 26627 -> 26627
Cov: 26627 -> 26627
7439
Cov: 26627 -> 26627
Cov: 26627 -> 26627
7440
Cov: 26627 -> 26627
Cov: 26627 -> 26627
7441
Cov: 26627 -> 26627
Cov: 26627 -> 26627
7442
Cov: 26627 -> 26628
Cov: 26628 -> 26628
7443
Cov: 26628 -> 26629
Cov: 26629 -> 26629
7444
Cov: 26629 -> 26629
Cov: 26629 -> 26629
7445
Cov: 26629 -> 26629
Cov: 26629 -> 26629
7446
Cov: 26629 -> 26629
Cov: 26629 -> 26629
7447
{"exception": "NotImplementedError", "msg": "Could not run 'aten::qscheme' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::qscheme' is only available for these backends: [QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nQuantizedCPU: registered at aten/src/ATen/RegisterQuantizedCPU.cpp:944 [kernel]\nQuantizedCUDA: registered at aten/src/ATen/RegisterQuantizedCUDA.cpp:459 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:153 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:498 [backend fallback]\nFunctionalize: registered at ../aten/src/ATen/FunctionalizeFallbackKernel.cpp:290 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:86 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradMTIA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_0.cpp:16725 [kernel]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:382 [backend fallback]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:249 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:710 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:203 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:161 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:494 [backend fallback]\nPreDispatch: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:165 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:157 [backend fallback]\n"}
7448
Cov: 26629 -> 26630
Cov: 26630 -> 26630
7449
Cov: 26630 -> 26630
Cov: 26630 -> 26630
7450
{"exception": "NotImplementedError", "msg": "Could not run 'aten::q_per_channel_zero_points' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::q_per_channel_zero_points' is only available for these backends: [QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nQuantizedCPU: registered at aten/src/ATen/RegisterQuantizedCPU.cpp:944 [kernel]\nQuantizedCUDA: registered at aten/src/ATen/RegisterQuantizedCUDA.cpp:459 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:153 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:498 [backend fallback]\nFunctionalize: registered at ../aten/src/ATen/FunctionalizeFallbackKernel.cpp:290 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:86 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradMTIA: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_1.cpp:16002 [kernel]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:382 [backend fallback]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:249 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:710 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:203 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:161 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:494 [backend fallback]\nPreDispatch: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:165 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:157 [backend fallback]\n"}
7451
Cov: 26630 -> 26630
Cov: 26630 -> 26630
7452
Cov: 26630 -> 26630
Cov: 26630 -> 26630
7453
Cov: 26630 -> 26630
Cov: 26630 -> 26630
7454
{"exception": "TypeError", "msg": "'getset_descriptor' object is not callable"}
7455
Cov: 26630 -> 26631
Cov: 26631 -> 26631
7456
Cov: 26631 -> 26631
Cov: 26631 -> 26631
7457
Cov: 26631 -> 26631
Cov: 26631 -> 26631
7458
Cov: 26631 -> 26631
Cov: 26631 -> 26631
7459
Cov: 26631 -> 26632
Cov: 26632 -> 26632
7460
Cov: 26632 -> 26632
Cov: 26632 -> 26632
7461
Cov: 26632 -> 26632
Cov: 26632 -> 26632
7462
Cov: 26632 -> 26632
Cov: 26632 -> 26632
7463
{"exception": "RuntimeError", "msg": "Index tensor must have the same number of dimensions as self tensor"}
7464
Cov: 26632 -> 26632
Cov: 26632 -> 26632
7465
Cov: 26632 -> 26632
Cov: 26632 -> 26632
7466
Cov: 26632 -> 26632
Cov: 26632 -> 26632
7467
Cov: 26632 -> 26633
Cov: 26633 -> 26633
7468
Cov: 26633 -> 26633
Cov: 26633 -> 26633
7469
Cov: 26633 -> 26633
Cov: 26633 -> 26633
7470
Cov: 26633 -> 26633
Cov: 26633 -> 26633
7471
Cov: 26633 -> 26633
Cov: 26633 -> 26633
7472
Cov: 26633 -> 26633
Cov: 26633 -> 26633
7473
Cov: 26633 -> 26634
Cov: 26634 -> 26634
7474
Cov: 26634 -> 26634
Cov: 26634 -> 26634
7475
Cov: 26634 -> 26634
Cov: 26634 -> 26634
7476
Cov: 26634 -> 26634
Cov: 26634 -> 26634
7477
Cov: 26634 -> 26634
Cov: 26634 -> 26634
7478
Cov: 26634 -> 26634
Cov: 26634 -> 26634
7479
{"exception": "RuntimeError", "msg": "masked_scatter_ only supports boolean masks, but got mask with dtype Long"}
7480
Cov: 26634 -> 26634
Cov: 26634 -> 26634
7481
Cov: 26634 -> 26634
Cov: 26634 -> 26634
7482
Cov: 26634 -> 26634
Cov: 26634 -> 26634
7483
{"exception": "RuntimeError", "msg": "The expanded size of the tensor (5) must match the existing size (4) at non-singleton dimension 2.  Target sizes: [10, 3, 5].  Tensor sizes: [10, 3, 4]"}
7484
{"exception": "TypeError", "msg": "mul_() missing 1 required positional arguments: \"other\""}
7485
Cov: 26634 -> 26634
Cov: 26634 -> 26634
7486
Cov: 26634 -> 26634
Cov: 26634 -> 26634
7487
Cov: 26634 -> 26634
Cov: 26634 -> 26634
7488
Cov: 26634 -> 26634
Cov: 26634 -> 26634
7489
Cov: 26634 -> 26634
Cov: 26634 -> 26634
7490
Cov: 26634 -> 26634
Cov: 26634 -> 26634
7491
Cov: 26634 -> 26634
Cov: 26634 -> 26634
7492
Cov: 26634 -> 26634
Cov: 26634 -> 26634
7493
Cov: 26634 -> 26634
Cov: 26634 -> 26634
7494
Cov: 26634 -> 26634
Cov: 26634 -> 26634
7495
Cov: 26634 -> 26634
Cov: 26634 -> 26634
7496
Cov: 26634 -> 26634
Cov: 26634 -> 26634
7497
Cov: 26634 -> 26634
Cov: 26634 -> 26634
7498
{"exception": "NotImplementedError", "msg": "Could not run 'aten::q_scale' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::q_scale' is only available for these backends: [QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nQuantizedCPU: registered at aten/src/ATen/RegisterQuantizedCPU.cpp:944 [kernel]\nQuantizedCUDA: registered at aten/src/ATen/RegisterQuantizedCUDA.cpp:459 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:153 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:498 [backend fallback]\nFunctionalize: registered at ../aten/src/ATen/FunctionalizeFallbackKernel.cpp:290 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:86 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradMTIA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_4.cpp:13056 [kernel]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:382 [backend fallback]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:249 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:710 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:203 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:161 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:494 [backend fallback]\nPreDispatch: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:165 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:157 [backend fallback]\n"}
7499
Cov: 26634 -> 26634
Cov: 26634 -> 26634
7500
Cov: 26634 -> 26634
Cov: 26634 -> 26634
7501
Cov: 26634 -> 26634
Cov: 26634 -> 26634
7502
Cov: 26634 -> 26634
Cov: 26634 -> 26634
7503
Cov: 26634 -> 26634
Cov: 26634 -> 26634
7504
{"exception": "TypeError", "msg": "dequantize() takes no keyword arguments"}
7505
Cov: 26634 -> 26634
Cov: 26634 -> 26634
7506
Cov: 26634 -> 26634
Cov: 26634 -> 26634
7507
Cov: 26634 -> 26634
Cov: 26634 -> 26634
7508
Cov: 26634 -> 26634
Cov: 26634 -> 26634
7509
Cov: 26634 -> 26635
Cov: 26635 -> 26635
7510
Cov: 26635 -> 26635
Cov: 26635 -> 26635
7511
Cov: 26635 -> 26635
Cov: 26635 -> 26635
7512
Cov: 26635 -> 26635
Cov: 26635 -> 26635
7513
Cov: 26635 -> 26636
Cov: 26636 -> 26636
7514
Cov: 26636 -> 26636
Cov: 26636 -> 26636
7515
Cov: 26636 -> 26636
Cov: 26636 -> 26636
7516
Cov: 26636 -> 26637
Cov: 26637 -> 26637
7517
Cov: 26637 -> 26637
Cov: 26637 -> 26637
7518
Cov: 26637 -> 26637
Cov: 26637 -> 26637
7519
{"exception": "RuntimeError", "msg": "bincount only supports 1-d non-negative integral inputs."}
7520
Cov: 26637 -> 26637
Cov: 26637 -> 26637
7521
Cov: 26637 -> 26638
Cov: 26638 -> 26638
7522
Cov: 26638 -> 26639
Cov: 26639 -> 26639
7523
Cov: 26639 -> 26639
Cov: 26639 -> 26639
7524
Cov: 26639 -> 26639
Cov: 26639 -> 26639
7525
Cov: 26639 -> 26639
Cov: 26639 -> 26639
7526
Cov: 26639 -> 26639
Cov: 26639 -> 26639
7527
Cov: 26639 -> 26639
Cov: 26639 -> 26639
7528
Cov: 26639 -> 26639
Cov: 26639 -> 26639
7529
Cov: 26639 -> 26639
Cov: 26639 -> 26639
7530
Cov: 26639 -> 26639
Cov: 26639 -> 26639
7531
Cov: 26639 -> 26639
Cov: 26639 -> 26639
7532
Cov: 26639 -> 26640
Cov: 26640 -> 26640
7533
Cov: 26640 -> 26640
Cov: 26640 -> 26640
7534
Cov: 26640 -> 26640
Cov: 26640 -> 26640
7535
Cov: 26640 -> 26640
Cov: 26640 -> 26640
7536
Cov: 26640 -> 26640
Cov: 26640 -> 26640
7537
Cov: 26640 -> 26641
Cov: 26641 -> 26641
7538
Cov: 26641 -> 26641
Cov: 26641 -> 26641
7539
Cov: 26641 -> 26641
Cov: 26641 -> 26641
7540
Cov: 26641 -> 26641
Cov: 26641 -> 26641
7541
Cov: 26641 -> 26641
Cov: 26641 -> 26641
7542
Cov: 26641 -> 26642
Cov: 26642 -> 26642
7543
Cov: 26642 -> 26642
Cov: 26642 -> 26642
7544
Cov: 26642 -> 26642
Cov: 26642 -> 26642
7545
Cov: 26642 -> 26642
Cov: 26642 -> 26642
7546
Cov: 26642 -> 26643
Cov: 26643 -> 26643
7547
Cov: 26643 -> 26643
Cov: 26643 -> 26643
7548
Cov: 26643 -> 26643
Cov: 26643 -> 26643
7549
Cov: 26643 -> 26643
Cov: 26643 -> 26643
7550
Cov: 26643 -> 26643
Cov: 26643 -> 26643
7551
Cov: 26643 -> 26645
Cov: 26645 -> 26645
7552
Cov: 26645 -> 26645
Cov: 26645 -> 26645
7553
Cov: 26645 -> 26646
Cov: 26646 -> 26646
7554
Cov: 26646 -> 26646
Cov: 26646 -> 26646
7555
{"exception": "RuntimeError", "msg": "scatter(): Expected self.dtype to be equal to src.dtype"}
7556
Cov: 26646 -> 26647
Cov: 26647 -> 26647
7557
Cov: 26647 -> 26648
Cov: 26648 -> 26648
7558
Cov: 26648 -> 26648
Cov: 26648 -> 26648
7559
Cov: 26648 -> 26648
Cov: 26648 -> 26648
7560
Cov: 26648 -> 26648
Cov: 26648 -> 26648
7561
Cov: 26648 -> 26648
Cov: 26648 -> 26648
7562
Cov: 26648 -> 26649
Cov: 26649 -> 26649
7563
Cov: 26649 -> 26649
Cov: 26649 -> 26649
7564
Cov: 26649 -> 26649
Cov: 26649 -> 26649
7565
Cov: 26649 -> 26649
Cov: 26649 -> 26649
7566
Cov: 26649 -> 26649
Cov: 26649 -> 26649
7567
Cov: 26649 -> 26649
Cov: 26649 -> 26649
7568
Cov: 26649 -> 26649
Cov: 26649 -> 26649
7569
{"exception": "TypeError", "msg": "random_() missing 2 required positional argument: \"from\", \"to\""}
7570
Cov: 26649 -> 26650
Cov: 26650 -> 26650
7571
Cov: 26650 -> 26650
Cov: 26650 -> 26650
7572
Cov: 26650 -> 26650
Cov: 26650 -> 26650
7573
Cov: 26650 -> 26650
Cov: 26650 -> 26650
7574
{"exception": "TypeError", "msg": "sum_to_size() got an unexpected keyword argument 'out'"}
7575
Cov: 26650 -> 26650
Cov: 26650 -> 26650
7576
Cov: 26650 -> 26650
Cov: 26650 -> 26650
7577
Cov: 26650 -> 26651
Cov: 26651 -> 26651
7578
Cov: 26651 -> 26651
Cov: 26651 -> 26651
7579
Cov: 26651 -> 26651
Cov: 26651 -> 26651
7580
Cov: 26651 -> 26651
Cov: 26651 -> 26651
7581
Cov: 26651 -> 26651
Cov: 26651 -> 26651
7582
Cov: 26651 -> 26651
Cov: 26651 -> 26651
7583
Cov: 26651 -> 26651
Cov: 26651 -> 26651
7584
Cov: 26651 -> 26651
Cov: 26651 -> 26651
7585
Cov: 26651 -> 26651
Cov: 26651 -> 26651
7586
Cov: 26651 -> 26651
Cov: 26651 -> 26651
7587
{"exception": "TypeError", "msg": "arctanh_() takes no arguments (1 given)"}
7588
{"exception": "_LinAlgError", "msg": "linalg.inv: The diagonal element 3 is zero, the inversion could not be completed because the input matrix is singular."}
7589
{"exception": "RuntimeError", "msg": "outer: Expected 1-D argument self, but got 2-D"}
7590
Cov: 26651 -> 26651
Cov: 26651 -> 26651
7591
Cov: 26651 -> 26651
Cov: 26651 -> 26651
7592
Cov: 26651 -> 26657
Cov: 26657 -> 26657
7593
Cov: 26657 -> 26658
Cov: 26658 -> 26658
7594
Cov: 26658 -> 26659
Cov: 26659 -> 26659
7595
Cov: 26659 -> 26659
Cov: 26659 -> 26659
7596
Cov: 26659 -> 26659
Cov: 26659 -> 26659
7597
Cov: 26659 -> 26659
Cov: 26659 -> 26659
7598
Cov: 26659 -> 26659
Cov: 26659 -> 26659
7599
Cov: 26659 -> 26659
Cov: 26659 -> 26659
7600
Cov: 26659 -> 26659
Cov: 26659 -> 26659
7601
Cov: 26659 -> 26659
Cov: 26659 -> 26659
7602
Cov: 26659 -> 26661
Cov: 26661 -> 26661
7603
Cov: 26661 -> 26697
Cov: 26697 -> 26697
7604
Cov: 26697 -> 26697
Cov: 26697 -> 26697
7605
{"exception": "RuntimeError", "msg": "a Tensor with 9 elements cannot be converted to Scalar"}
7606
Cov: 26697 -> 26697
Cov: 26697 -> 26697
7607
{"exception": "RuntimeError", "msg": "the base given to float_power_ has dtype Float but the operation's result requires dtype Double"}
7608
Cov: 26697 -> 26697
Cov: 26697 -> 26697
7609
Cov: 26697 -> 26697
Cov: 26697 -> 26697
7610
Cov: 26697 -> 26698
Cov: 26698 -> 26698
7611
Cov: 26698 -> 26698
Cov: 26698 -> 26698
7612
Cov: 26698 -> 26699
Cov: 26699 -> 26699
7613
Cov: 26699 -> 26699
Cov: 26699 -> 26699
7614
Cov: 26699 -> 26699
Cov: 26699 -> 26699
7615
Cov: 26699 -> 26700
Cov: 26700 -> 26700
7616
{"exception": "_LinAlgError", "msg": "linalg.cholesky: The factorization could not be completed because the input is not positive-definite (the leading minor of order 3 is not positive-definite)."}
7617
{"exception": "NotImplementedError", "msg": "Could not run 'aten::q_per_channel_zero_points' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::q_per_channel_zero_points' is only available for these backends: [QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nQuantizedCPU: registered at aten/src/ATen/RegisterQuantizedCPU.cpp:944 [kernel]\nQuantizedCUDA: registered at aten/src/ATen/RegisterQuantizedCUDA.cpp:459 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:153 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:498 [backend fallback]\nFunctionalize: registered at ../aten/src/ATen/FunctionalizeFallbackKernel.cpp:290 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:86 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradMTIA: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_1.cpp:16002 [kernel]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:382 [backend fallback]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:249 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:710 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:203 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:161 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:494 [backend fallback]\nPreDispatch: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:165 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:157 [backend fallback]\n"}
7618
Cov: 26700 -> 26700
Cov: 26700 -> 26700
7619
Cov: 26700 -> 26702
Cov: 26702 -> 26702
7620
Cov: 26702 -> 26702
Cov: 26702 -> 26702
7621
Cov: 26702 -> 26702
Cov: 26702 -> 26702
7622
Cov: 26702 -> 26702
Cov: 26702 -> 26702
7623
Cov: 26702 -> 26703
Cov: 26703 -> 26703
7624
Cov: 26703 -> 26703
Cov: 26703 -> 26703
7625
Cov: 26703 -> 26704
Cov: 26704 -> 26704
7626
Cov: 26704 -> 26704
Cov: 26704 -> 26704
7627
{"exception": "RuntimeError", "msg": "outer: Expected 1-D argument self, but got 2-D"}
7628
Cov: 26704 -> 26705
Cov: 26705 -> 26705
7629
Cov: 26705 -> 26706
Cov: 26706 -> 26706
7630
Cov: 26706 -> 26707
Cov: 26707 -> 26707
7631
Cov: 26707 -> 26707
Cov: 26707 -> 26707
7632
Cov: 26707 -> 26707
Cov: 26707 -> 26707
7633
Cov: 26707 -> 26707
Cov: 26707 -> 26707
7634
Cov: 26707 -> 26707
Cov: 26707 -> 26707
7635
Cov: 26707 -> 26707
Cov: 26707 -> 26707
7636
Cov: 26707 -> 26707
Cov: 26707 -> 26707
7637
Cov: 26707 -> 26707
Cov: 26707 -> 26707
7638
Cov: 26707 -> 26707
Cov: 26707 -> 26707
7639
Cov: 26707 -> 26707
Cov: 26707 -> 26707
7640
Cov: 26707 -> 26707
Cov: 26707 -> 26707
7641
Cov: 26707 -> 26707
Cov: 26707 -> 26707
7642
Cov: 26707 -> 26707
Cov: 26707 -> 26707
7643
Cov: 26707 -> 26707
Cov: 26707 -> 26707
7644
Cov: 26707 -> 26708
Cov: 26708 -> 26708
7645
Cov: 26708 -> 26708
Cov: 26708 -> 26708
7646
Cov: 26708 -> 26708
Cov: 26708 -> 26708
7647
Cov: 26708 -> 26708
Cov: 26708 -> 26708
7648
Cov: 26708 -> 26708
Cov: 26708 -> 26708
7649
Cov: 26708 -> 26713
Cov: 26713 -> 26713
7650
Cov: 26713 -> 26713
Cov: 26713 -> 26713
7651
Cov: 26713 -> 26713
Cov: 26713 -> 26713
7652
{"exception": "RuntimeError", "msg": "batch1 must be a 3D tensor"}
7653
Cov: 26713 -> 26713
Cov: 26713 -> 26713
7654
{"exception": "RuntimeError", "msg": "tensor.sspaddmm(...) can only be called on sparse tensors"}
7655
Cov: 26713 -> 26799
Cov: 26799 -> 26799
7656
{"exception": "_LinAlgError", "msg": "torch.linalg.solve: The solver failed because the input matrix is singular."}
7657
Cov: 26799 -> 26799
Cov: 26799 -> 26799
7658
Cov: 26799 -> 26799
Cov: 26799 -> 26799
7659
Cov: 26799 -> 26799
Cov: 26799 -> 26799
7660
Cov: 26799 -> 26799
Cov: 26799 -> 26799
7661
Cov: 26799 -> 26799
Cov: 26799 -> 26799
7662
Cov: 26799 -> 26799
Cov: 26799 -> 26799
7663
Cov: 26799 -> 26799
Cov: 26799 -> 26799
7664
Cov: 26799 -> 26799
Cov: 26799 -> 26799
7665
Cov: 26799 -> 26799
Cov: 26799 -> 26799
7666
Cov: 26799 -> 26799
Cov: 26799 -> 26799
7667
Cov: 26799 -> 26799
Cov: 26799 -> 26799
7668
Cov: 26799 -> 26799
Cov: 26799 -> 26799
7669
Cov: 26799 -> 26799
Cov: 26799 -> 26799
7670
Cov: 26799 -> 26799
Cov: 26799 -> 26799
7671
Cov: 26799 -> 26800
Cov: 26800 -> 26800
7672
Cov: 26800 -> 26800
Cov: 26800 -> 26800
7673
Cov: 26800 -> 26800
Cov: 26800 -> 26800
7674
Cov: 26800 -> 26800
Cov: 26800 -> 26800
7675
Cov: 26800 -> 26800
Cov: 26800 -> 26800
7676
Cov: 26800 -> 26800
Cov: 26800 -> 26800
7677
Cov: 26800 -> 26801
Cov: 26801 -> 26801
7678
Cov: 26801 -> 26801
Cov: 26801 -> 26801
7679
Cov: 26801 -> 26801
Cov: 26801 -> 26801
7680
{"exception": "NotImplementedError", "msg": "Could not run 'aten::q_zero_point' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::q_zero_point' is only available for these backends: [QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nQuantizedCPU: registered at aten/src/ATen/RegisterQuantizedCPU.cpp:944 [kernel]\nQuantizedCUDA: registered at aten/src/ATen/RegisterQuantizedCUDA.cpp:459 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:153 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:498 [backend fallback]\nFunctionalize: registered at ../aten/src/ATen/FunctionalizeFallbackKernel.cpp:290 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:86 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradMTIA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_3.cpp:14610 [kernel]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:382 [backend fallback]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:249 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:710 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:203 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:161 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:494 [backend fallback]\nPreDispatch: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:165 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:157 [backend fallback]\n"}
7681
Cov: 26801 -> 26801
Cov: 26801 -> 26801
7682
Cov: 26801 -> 26801
Cov: 26801 -> 26801
7683
Cov: 26801 -> 26801
Cov: 26801 -> 26801
7684
Cov: 26801 -> 26801
Cov: 26801 -> 26801
7685
Cov: 26801 -> 26801
Cov: 26801 -> 26801
7686
Cov: 26801 -> 26801
Cov: 26801 -> 26801
7687
Cov: 26801 -> 26802
Cov: 26802 -> 26802
7688
Cov: 26802 -> 26802
Cov: 26802 -> 26802
7689
Cov: 26802 -> 26802
Cov: 26802 -> 26802
7690
Cov: 26802 -> 26802
Cov: 26802 -> 26802
7691
Cov: 26802 -> 26803
Cov: 26803 -> 26803
7692
Cov: 26803 -> 26803
Cov: 26803 -> 26803
7693
Cov: 26803 -> 26803
Cov: 26803 -> 26803
7694
Cov: 26803 -> 26804
Cov: 26804 -> 26804
7695
Cov: 26804 -> 26804
Cov: 26804 -> 26804
7696
Cov: 26804 -> 26806
Cov: 26806 -> 26806
7697
{"exception": "TypeError", "msg": "type_as(): argument 'other' (position 1) must be Tensor, not torch.tensortype"}
7698
Cov: 26806 -> 26810
Cov: 26810 -> 26810
7699
Cov: 26810 -> 26811
Cov: 26811 -> 26811
7700
Cov: 26811 -> 26811
Cov: 26811 -> 26811
7701
Cov: 26811 -> 26811
Cov: 26811 -> 26811
7702
Cov: 26811 -> 26811
Cov: 26811 -> 26811
7703
{"exception": "RuntimeError", "msg": "Please look up dimensions by name, got: name = None."}
7704
Cov: 26811 -> 26811
Cov: 26811 -> 26811
7705
Cov: 26811 -> 26812
Cov: 26812 -> 26812
7706
Cov: 26812 -> 26812
Cov: 26812 -> 26812
7707
Cov: 26812 -> 26813
Cov: 26813 -> 26813
7708
Cov: 26813 -> 26813
Cov: 26813 -> 26813
7709
Cov: 26813 -> 26813
Cov: 26813 -> 26813
7710
Cov: 26813 -> 26814
Cov: 26814 -> 26814
7711
Cov: 26814 -> 26815
Cov: 26815 -> 26815
7712
Cov: 26815 -> 26815
Cov: 26815 -> 26815
7713
Cov: 26815 -> 26815
Cov: 26815 -> 26815
7714
Cov: 26815 -> 26815
Cov: 26815 -> 26815
7715
Cov: 26815 -> 26815
Cov: 26815 -> 26815
7716
Cov: 26815 -> 26815
Cov: 26815 -> 26815
7717
{"exception": "RuntimeError", "msg": "mat1 and mat2 must have the same dtype, but got Float and Double"}
7718
Cov: 26815 -> 26817
Cov: 26817 -> 26817
7719
Cov: 26817 -> 26817
Cov: 26817 -> 26817
7720
Cov: 26817 -> 26818
Cov: 26818 -> 26818
7721
Cov: 26818 -> 26818
Cov: 26818 -> 26818
7722
Cov: 26818 -> 26818
Cov: 26818 -> 26818
7723
Cov: 26818 -> 26818
Cov: 26818 -> 26818
7724
{"exception": "TypeError", "msg": "'getset_descriptor' object is not callable"}
7725
Cov: 26818 -> 26818
Cov: 26818 -> 26818
7726
Cov: 26818 -> 26818
Cov: 26818 -> 26818
7727
Cov: 26818 -> 26834
Cov: 26834 -> 26834
7728
{"exception": "TypeError", "msg": "index_add_() missing 1 required positional arguments: \"source\""}
7729
Cov: 26834 -> 26834
Cov: 26834 -> 26834
7730
Cov: 26834 -> 26834
Cov: 26834 -> 26834
7731
{"exception": "NotImplementedError", "msg": "Could not run 'aten::q_per_channel_scales' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::q_per_channel_scales' is only available for these backends: [QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nQuantizedCPU: registered at aten/src/ATen/RegisterQuantizedCPU.cpp:944 [kernel]\nQuantizedCUDA: registered at aten/src/ATen/RegisterQuantizedCUDA.cpp:459 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:153 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:498 [backend fallback]\nFunctionalize: registered at ../aten/src/ATen/FunctionalizeFallbackKernel.cpp:290 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:86 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradMTIA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_3.cpp:14610 [kernel]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:382 [backend fallback]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:249 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:710 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:203 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:161 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:494 [backend fallback]\nPreDispatch: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:165 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:157 [backend fallback]\n"}
7732
Cov: 26834 -> 26834
Cov: 26834 -> 26834
7733
Cov: 26834 -> 26834
Cov: 26834 -> 26834
7734
Cov: 26834 -> 26834
Cov: 26834 -> 26834
7735
Cov: 26834 -> 26834
Cov: 26834 -> 26834
7736
Cov: 26834 -> 26834
Cov: 26834 -> 26834
7737
Cov: 26834 -> 26834
Cov: 26834 -> 26834
7738
Cov: 26834 -> 26834
Cov: 26834 -> 26834
7739
Cov: 26834 -> 26834
Cov: 26834 -> 26834
7740
{"exception": "RuntimeError", "msg": "any: You passed a dimname (string) to this op in place of a dimension index but it does not yet support this behavior. Please pass a dimension index to work around this."}
7741
Cov: 26834 -> 26835
Cov: 26835 -> 26835
7742
Cov: 26835 -> 26835
Cov: 26835 -> 26835
7743
Cov: 26835 -> 26836
Cov: 26836 -> 26836
7744
Cov: 26836 -> 26836
Cov: 26836 -> 26836
7745
Cov: 26836 -> 26837
Cov: 26837 -> 26837
7746
{"exception": "RuntimeError", "msg": "expand(torch.DoubleTensor{[10, 3, 4]}, size=[3, 5]): the number of sizes provided (2) must be greater or equal to the number of dimensions in the tensor (3)"}
7747
Cov: 26837 -> 26837
Cov: 26837 -> 26837
7748
Cov: 26837 -> 26837
Cov: 26837 -> 26837
7749
Cov: 26837 -> 26837
Cov: 26837 -> 26837
7750
Cov: 26837 -> 26839
Cov: 26839 -> 26839
7751
Cov: 26839 -> 26839
Cov: 26839 -> 26839
7752
Cov: 26839 -> 26839
Cov: 26839 -> 26839
7753
Cov: 26839 -> 26839
Cov: 26839 -> 26839
7754
Cov: 26839 -> 26839
Cov: 26839 -> 26839
7755
{"exception": "TypeError", "msg": "'getset_descriptor' object is not callable"}
7756
Cov: 26839 -> 26839
Cov: 26839 -> 26839
7757
Cov: 26839 -> 26839
Cov: 26839 -> 26839
7758
Cov: 26839 -> 26839
Cov: 26839 -> 26839
7759
Cov: 26839 -> 26851
Cov: 26851 -> 26851
7760
Cov: 26851 -> 26851
Cov: 26851 -> 26851
7761
Cov: 26851 -> 26851
Cov: 26851 -> 26851
7762
Cov: 26851 -> 26852
Cov: 26852 -> 26852
7763
Cov: 26852 -> 26852
Cov: 26852 -> 26852
7764
Cov: 26852 -> 26853
Cov: 26853 -> 26853
7765
Cov: 26853 -> 26853
Cov: 26853 -> 26853
7766
Cov: 26853 -> 26853
Cov: 26853 -> 26853
7767
Cov: 26853 -> 26853
Cov: 26853 -> 26853
7768
Cov: 26853 -> 26853
Cov: 26853 -> 26853
7769
Cov: 26853 -> 26854
Cov: 26854 -> 26854
7770
Cov: 26854 -> 26855
Cov: 26855 -> 26855
7771
Cov: 26855 -> 26855
Cov: 26855 -> 26855
7772
Cov: 26855 -> 26855
Cov: 26855 -> 26855
7773
Cov: 26855 -> 26855
Cov: 26855 -> 26855
7774
Cov: 26855 -> 26855
Cov: 26855 -> 26855
7775
Cov: 26855 -> 26856
Cov: 26856 -> 26856
7776
Cov: 26856 -> 26856
Cov: 26856 -> 26856
7777
Cov: 26856 -> 26856
Cov: 26856 -> 26856
7778
{"exception": "RuntimeError", "msg": "index_add_(): Number of indices (2) should be equal to source.size(dim): (3), for dim: 0"}
7779
{"exception": "RuntimeError", "msg": "This function was deprecated since version 1.9 and is now removed. The default behavior has changed from using the upper triangular portion of the matrix by default to using the lower triangular portion.\n\nL, _ = torch.symeig(A, upper=upper) should be replaced with:\nL = torch.linalg.eigvalsh(A, UPLO='U' if upper else 'L')\n\nand\n\nL, V = torch.symeig(A, eigenvectors=True) should be replaced with:\nL, V = torch.linalg.eigh(A, UPLO='U' if upper else 'L')"}
7780
Cov: 26856 -> 26856
Cov: 26856 -> 26856
7781
Cov: 26856 -> 26858
Cov: 26858 -> 26858
7782
Cov: 26858 -> 26858
Cov: 26858 -> 26858
7783
Cov: 26858 -> 26858
Cov: 26858 -> 26858
7784
Cov: 26858 -> 26858
Cov: 26858 -> 26858
7785
Cov: 26858 -> 26858
Cov: 26858 -> 26858
7786
Cov: 26858 -> 26858
Cov: 26858 -> 26858
7787
Cov: 26858 -> 26858
Cov: 26858 -> 26858
7788
Cov: 26858 -> 26858
Cov: 26858 -> 26858
7789
Cov: 26858 -> 26858
Cov: 26858 -> 26858
7790
Cov: 26858 -> 26858
Cov: 26858 -> 26858
7791
Cov: 26858 -> 26858
Cov: 26858 -> 26858
7792
Cov: 26858 -> 26858
Cov: 26858 -> 26858
7793
Cov: 26858 -> 26858
Cov: 26858 -> 26858
7794
Cov: 26858 -> 26859
Cov: 26859 -> 26859
7795
Cov: 26859 -> 26860
Cov: 26860 -> 26860
7796
Cov: 26860 -> 26861
Cov: 26861 -> 26861
7797
Cov: 26861 -> 26861
Cov: 26861 -> 26861
7798
Cov: 26861 -> 26861
Cov: 26861 -> 26861
7799
Cov: 26861 -> 26861
Cov: 26861 -> 26861
7800
Cov: 26861 -> 26861
Cov: 26861 -> 26861
7801
Cov: 26861 -> 26861
Cov: 26861 -> 26861
7802
Cov: 26861 -> 26863
Cov: 26863 -> 26863
7803
Cov: 26863 -> 26863
Cov: 26863 -> 26863
7804
Cov: 26863 -> 26863
Cov: 26863 -> 26863
7805
Cov: 26863 -> 26863
Cov: 26863 -> 26863
7806
Cov: 26863 -> 26863
Cov: 26863 -> 26863
7807
Cov: 26863 -> 26863
Cov: 26863 -> 26863
7808
Cov: 26863 -> 26863
Cov: 26863 -> 26863
7809
Cov: 26863 -> 26863
Cov: 26863 -> 26863
7810
Cov: 26863 -> 26863
Cov: 26863 -> 26863
7811
Cov: 26863 -> 26863
Cov: 26863 -> 26863
7812
Cov: 26863 -> 26863
Cov: 26863 -> 26863
7813
Cov: 26863 -> 26863
Cov: 26863 -> 26863
7814
Cov: 26863 -> 26863
Cov: 26863 -> 26863
7815
Cov: 26863 -> 26863
Cov: 26863 -> 26863
7816
Cov: 26863 -> 26863
Cov: 26863 -> 26863
7817
Cov: 26863 -> 26863
Cov: 26863 -> 26863
7818
Cov: 26863 -> 26863
Cov: 26863 -> 26863
7819
Cov: 26863 -> 26863
Cov: 26863 -> 26863
7820
Cov: 26863 -> 26863
Cov: 26863 -> 26863
7821
Cov: 26863 -> 26863
Cov: 26863 -> 26863
7822
Cov: 26863 -> 26863
Cov: 26863 -> 26863
7823
Cov: 26863 -> 26864
Cov: 26864 -> 26864
7824
Cov: 26864 -> 26865
Cov: 26865 -> 26865
7825
Cov: 26865 -> 26865
Cov: 26865 -> 26865
7826
Cov: 26865 -> 26865
Cov: 26865 -> 26865
7827
Cov: 26865 -> 26866
Cov: 26866 -> 26866
7828
Cov: 26866 -> 26866
Cov: 26866 -> 26866
7829
Cov: 26866 -> 26866
Cov: 26866 -> 26866
7830
Cov: 26866 -> 26866
Cov: 26866 -> 26866
7831
{"exception": "RuntimeError", "msg": "Bad in-place call: input tensor dtype double and output tensor dtype float should match"}
7832
Cov: 26866 -> 26866
Cov: 26866 -> 26866
7833
Cov: 26866 -> 26866
Cov: 26866 -> 26866
7834
Cov: 26866 -> 26867
Cov: 26867 -> 26867
7835
Cov: 26867 -> 26867
Cov: 26867 -> 26867
7836
Cov: 26867 -> 26867
Cov: 26867 -> 26867
7837
{"exception": "NameError", "msg": "name 'math' is not defined"}
7838
Cov: 26867 -> 26867
Cov: 26867 -> 26867
7839
Cov: 26867 -> 26868
Cov: 26868 -> 26868
7840
Cov: 26868 -> 26868
Cov: 26868 -> 26868
7841
{"exception": "NameError", "msg": "name 'tau' is not defined"}
7842
Cov: 26868 -> 26868
Cov: 26868 -> 26868
7843
Cov: 26868 -> 26868
Cov: 26868 -> 26868
7844
Cov: 26868 -> 26868
Cov: 26868 -> 26868
7845
Cov: 26868 -> 26869
Cov: 26869 -> 26869
7846
{"exception": "RuntimeError", "msg": "Argument #4: Padding size should be less than the corresponding input dimension, but got: padding (5, 5) at dimension 2 of input [1, 10, 3]"}
7847
Cov: 26869 -> 26883
Cov: 26883 -> 26883
7848
{"exception": "RuntimeError", "msg": "\"gcd_cpu\" not implemented for 'Double'"}
7849
Cov: 26883 -> 26884
Cov: 26884 -> 26884
7850
Cov: 26884 -> 26885
Cov: 26885 -> 26885
7851
Cov: 26885 -> 26886
Cov: 26886 -> 26886
7852
Cov: 26886 -> 26886
Cov: 26886 -> 26886
7853
Cov: 26886 -> 26886
Cov: 26886 -> 26886
7854
Cov: 26886 -> 26886
Cov: 26886 -> 26886
7855
Cov: 26886 -> 26886
Cov: 26886 -> 26886
7856
Cov: 26886 -> 26886
Cov: 26886 -> 26886
7857
Cov: 26886 -> 26887
Cov: 26887 -> 26887
7858
Cov: 26887 -> 26887
Cov: 26887 -> 26887
7859
Cov: 26887 -> 26887
Cov: 26887 -> 26887
7860
Cov: 26887 -> 26887
Cov: 26887 -> 26887
7861
Cov: 26887 -> 26888
Cov: 26888 -> 26888
7862
Cov: 26888 -> 26888
Cov: 26888 -> 26888
7863
Cov: 26888 -> 26888
Cov: 26888 -> 26888
7864
Cov: 26888 -> 26888
Cov: 26888 -> 26888
7865
Cov: 26888 -> 26888
Cov: 26888 -> 26888
7866
Cov: 26888 -> 26888
Cov: 26888 -> 26888
7867
Cov: 26888 -> 26890
Cov: 26890 -> 26890
7868
Cov: 26890 -> 26890
Cov: 26890 -> 26890
7869
Cov: 26890 -> 26890
Cov: 26890 -> 26890
7870
Cov: 26890 -> 26890
Cov: 26890 -> 26890
7871
Cov: 26890 -> 26890
Cov: 26890 -> 26890
7872
Cov: 26890 -> 26890
Cov: 26890 -> 26890
7873
Cov: 26890 -> 26890
Cov: 26890 -> 26890
7874
Cov: 26890 -> 26890
Cov: 26890 -> 26890
7875
Cov: 26890 -> 26890
Cov: 26890 -> 26890
7876
Cov: 26890 -> 26890
Cov: 26890 -> 26890
7877
Cov: 26890 -> 26892
Cov: 26892 -> 26892
7878
Cov: 26892 -> 26892
Cov: 26892 -> 26892
7879
Cov: 26892 -> 26894
Cov: 26894 -> 26894
7880
Cov: 26894 -> 26894
Cov: 26894 -> 26894
7881
Cov: 26894 -> 26894
Cov: 26894 -> 26894
7882
Cov: 26894 -> 26894
Cov: 26894 -> 26894
7883
Cov: 26894 -> 26894
Cov: 26894 -> 26894
7884
Cov: 26894 -> 26895
Cov: 26895 -> 26895
7885
Cov: 26895 -> 26895
Cov: 26895 -> 26895
7886
Cov: 26895 -> 26896
Cov: 26896 -> 26896
7887
Cov: 26896 -> 26896
Cov: 26896 -> 26896
7888
Cov: 26896 -> 26896
Cov: 26896 -> 26896
7889
{"exception": "TypeError", "msg": "'getset_descriptor' object is not callable"}
7890
Cov: 26896 -> 26897
Cov: 26897 -> 26897
7891
Cov: 26897 -> 26897
Cov: 26897 -> 26897
7892
Cov: 26897 -> 26897
Cov: 26897 -> 26897
7893
Cov: 26897 -> 26897
Cov: 26897 -> 26897
7894
Cov: 26897 -> 26897
Cov: 26897 -> 26897
7895
Cov: 26897 -> 26897
Cov: 26897 -> 26897
7896
{"exception": "NotImplementedError", "msg": "Could not run 'aten::q_per_channel_axis' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::q_per_channel_axis' is only available for these backends: [QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nQuantizedCPU: registered at aten/src/ATen/RegisterQuantizedCPU.cpp:944 [kernel]\nQuantizedCUDA: registered at aten/src/ATen/RegisterQuantizedCUDA.cpp:459 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:153 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:498 [backend fallback]\nFunctionalize: registered at ../aten/src/ATen/FunctionalizeFallbackKernel.cpp:290 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:86 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradMTIA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_4.cpp:13056 [kernel]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:382 [backend fallback]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:249 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:710 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:203 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:161 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:494 [backend fallback]\nPreDispatch: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:165 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:157 [backend fallback]\n"}
7897
Cov: 26897 -> 26897
Cov: 26897 -> 26897
7898
Cov: 26897 -> 26897
Cov: 26897 -> 26897
7899
Cov: 26897 -> 26897
Cov: 26897 -> 26897
7900
Cov: 26897 -> 26897
Cov: 26897 -> 26897
7901
Cov: 26897 -> 26897
Cov: 26897 -> 26897
7902
{"exception": "NameError", "msg": "name 'F' is not defined"}
7903
Cov: 26897 -> 26899
Cov: 26899 -> 26899
7904
{"exception": "RuntimeError", "msg": "Creating a Tensor subclass from a class that does not inherit from Tensor is not possible. Make sure your class inherits from Tensor."}
7905
Cov: 26899 -> 26899
Cov: 26899 -> 26899
7906
Cov: 26899 -> 26899
Cov: 26899 -> 26899
7907
Cov: 26899 -> 26899
Cov: 26899 -> 26899
7908
Cov: 26899 -> 26899
Cov: 26899 -> 26899
7909
Cov: 26899 -> 26899
Cov: 26899 -> 26899
7910
Cov: 26899 -> 26900
Cov: 26900 -> 26900
7911
Cov: 26900 -> 26900
Cov: 26900 -> 26900
7912
Cov: 26900 -> 26901
Cov: 26901 -> 26901
7913
Cov: 26901 -> 26901
Cov: 26901 -> 26901
7914
Cov: 26901 -> 26901
Cov: 26901 -> 26901
7915
{"exception": "RuntimeError", "msg": "This function was deprecated since version 1.9 and is now removed. `torch.lstsq` is deprecated in favor of `torch.linalg.lstsq`.\n`torch.linalg.lstsq` has reversed arguments and does not return the QR decomposition in the returned tuple (although it returns other information about the problem).\n\nTo get the QR decomposition consider using `torch.linalg.qr`.\n\nThe returned solution in `torch.lstsq` stored the residuals of the solution in the last m - n columns of the returned value whenever m > n. In torch.linalg.lstsq, the residuals are in the field 'residuals' of the returned named tuple.\n\nThe unpacking of the solution, as in\nX, _ = torch.lstsq(B, A).solution[:A.size(1)]\nshould be replaced with:\nX = torch.linalg.lstsq(A, B).solution"}
7916
Cov: 26901 -> 26903
Cov: 26903 -> 26903
7917
Cov: 26903 -> 26903
Cov: 26903 -> 26903
7918
Cov: 26903 -> 26904
Cov: 26904 -> 26904
7919
{"exception": "RuntimeError", "msg": "a leaf Variable that requires grad is being used in an in-place operation."}
7920
Cov: 26904 -> 26904
Cov: 26904 -> 26904
7921
Cov: 26904 -> 26904
Cov: 26904 -> 26904
7922
Cov: 26904 -> 26905
Cov: 26905 -> 26905
7923
Cov: 26905 -> 26905
Cov: 26905 -> 26905
7924
Cov: 26905 -> 26905
Cov: 26905 -> 26905
7925
Cov: 26905 -> 26906
Cov: 26906 -> 26906
7926
{"exception": "RuntimeError", "msg": "addr: Expected 1-D argument vec1, but got 2-D"}
7927
Cov: 26906 -> 26906
Cov: 26906 -> 26906
7928
Cov: 26906 -> 26906
Cov: 26906 -> 26906
7929
Cov: 26906 -> 26906
Cov: 26906 -> 26906
7930
{"exception": "RuntimeError", "msg": "linalg.det: Expected a floating point or complex tensor as input. Got Long"}
7931
Cov: 26906 -> 26906
Cov: 26906 -> 26906
7932
Cov: 26906 -> 26906
Cov: 26906 -> 26906
7933
Cov: 26906 -> 26907
Cov: 26907 -> 26907
7934
Cov: 26907 -> 26907
Cov: 26907 -> 26907
7935
{"exception": "TypeError", "msg": "'getset_descriptor' object is not callable"}
7936
Cov: 26907 -> 26907
Cov: 26907 -> 26907
7937
Cov: 26907 -> 26907
Cov: 26907 -> 26907
7938
Cov: 26907 -> 26908
Cov: 26908 -> 26908
7939
Cov: 26908 -> 26909
Cov: 26909 -> 26909
7940
Cov: 26909 -> 26909
Cov: 26909 -> 26909
7941
Cov: 26909 -> 26909
Cov: 26909 -> 26909
7942
Cov: 26909 -> 26910
Cov: 26910 -> 26910
7943
Cov: 26910 -> 26910
Cov: 26910 -> 26910
7944
Cov: 26910 -> 26910
Cov: 26910 -> 26910
7945
Cov: 26910 -> 26910
Cov: 26910 -> 26910
7946
{"exception": "RuntimeError", "msg": "Bad in-place call: input tensor size [2, 3] and output tensor size [3, 3] should match"}
7947
Cov: 26910 -> 26910
Cov: 26910 -> 26910
7948
Cov: 26910 -> 26910
Cov: 26910 -> 26910
7949
Cov: 26910 -> 26910
Cov: 26910 -> 26910
7950
Cov: 26910 -> 26911
Cov: 26911 -> 26911
7951
Cov: 26911 -> 26911
Cov: 26911 -> 26911
7952
Cov: 26911 -> 26911
Cov: 26911 -> 26911
7953
Cov: 26911 -> 26911
Cov: 26911 -> 26911
7954
Cov: 26911 -> 26911
Cov: 26911 -> 26911
7955
Cov: 26911 -> 26911
Cov: 26911 -> 26911
7956
Cov: 26911 -> 26914
Cov: 26914 -> 26914
7957
Cov: 26914 -> 26928
Cov: 26928 -> 26928
7958
Cov: 26928 -> 26929
Cov: 26929 -> 26929
7959
Cov: 26929 -> 26930
Cov: 26930 -> 26930
7960
Cov: 26930 -> 26930
Cov: 26930 -> 26930
7961
Cov: 26930 -> 26930
Cov: 26930 -> 26930
7962
Cov: 26930 -> 26930
Cov: 26930 -> 26930
7963
Cov: 26930 -> 26931
Cov: 26931 -> 26931
7964
Cov: 26931 -> 26931
Cov: 26931 -> 26931
7965
Cov: 26931 -> 26931
Cov: 26931 -> 26931
7966
Cov: 26931 -> 26932
Cov: 26932 -> 26932
7967
{"exception": "TypeError", "msg": "'getset_descriptor' object is not callable"}
7968
Cov: 26932 -> 26932
Cov: 26932 -> 26932
7969
Cov: 26932 -> 26932
Cov: 26932 -> 26932
7970
Cov: 26932 -> 26932
Cov: 26932 -> 26932
7971
Cov: 26932 -> 26933
Cov: 26933 -> 26933
7972
Cov: 26933 -> 26933
Cov: 26933 -> 26933
7973
Cov: 26933 -> 26933
Cov: 26933 -> 26933
7974
Cov: 26933 -> 26933
Cov: 26933 -> 26933
7975
Cov: 26933 -> 26934
Cov: 26934 -> 26934
7976
Cov: 26934 -> 26934
Cov: 26934 -> 26934
7977
Cov: 26934 -> 26935
Cov: 26935 -> 26935
7978
Cov: 26935 -> 26935
Cov: 26935 -> 26935
7979
Cov: 26935 -> 26935
Cov: 26935 -> 26935
7980
Cov: 26935 -> 26936
Cov: 26936 -> 26936
7981
Cov: 26936 -> 26936
Cov: 26936 -> 26936
7982
Cov: 26936 -> 26936
Cov: 26936 -> 26936
7983
Cov: 26936 -> 26936
Cov: 26936 -> 26936
7984
{"exception": "RuntimeError", "msg": "This function was deprecated since version 1.9 and is now removed. `torch.linalg.eig` returns complex tensors of dtype `cfloat` or `cdouble` rather than real tensors mimicking complex tensors.\n\nL, _ = torch.eig(A) should be replaced with:\nL_complex = torch.linalg.eigvals(A)\n\nand\n\nL, V = torch.eig(A, eigenvectors=True) should be replaced with:\nL_complex, V_complex = torch.linalg.eig(A)"}
7985
Cov: 26936 -> 26937
Cov: 26937 -> 26937
7986
Cov: 26937 -> 26981
Cov: 26981 -> 26981
7987
Cov: 26981 -> 26981
Cov: 26981 -> 26981
7988
Cov: 26981 -> 26981
Cov: 26981 -> 26981
7989
Cov: 26981 -> 26981
Cov: 26981 -> 26981
7990
Cov: 26981 -> 26981
Cov: 26981 -> 26981
7991
Cov: 26981 -> 26981
Cov: 26981 -> 26981
7992
Cov: 26981 -> 26982
Cov: 26982 -> 26982
7993
Cov: 26982 -> 26982
Cov: 26982 -> 26982
7994
Cov: 26982 -> 26983
Cov: 26983 -> 26983
7995
Cov: 26983 -> 26984
Cov: 26984 -> 26984
7996
Cov: 26984 -> 27005
Cov: 27005 -> 27005
7997
Cov: 27005 -> 27005
Cov: 27005 -> 27005
7998
Cov: 27005 -> 27005
Cov: 27005 -> 27005
7999
Cov: 27005 -> 27005
Cov: 27005 -> 27005
8000
Cov: 27005 -> 27053
Cov: 27053 -> 27053
8001
Cov: 27053 -> 27053
Cov: 27053 -> 27053
8002
Cov: 27053 -> 27053
Cov: 27053 -> 27053
8003
Cov: 27053 -> 27053
Cov: 27053 -> 27053
8004
Cov: 27053 -> 27053
Cov: 27053 -> 27053
8005
Cov: 27053 -> 27054
Cov: 27054 -> 27054
8006
Cov: 27054 -> 27055
Cov: 27055 -> 27055
8007
Cov: 27055 -> 27055
Cov: 27055 -> 27055
8008
Cov: 27055 -> 27055
Cov: 27055 -> 27055
8009
Cov: 27055 -> 27055
Cov: 27055 -> 27055
8010
Cov: 27055 -> 27055
Cov: 27055 -> 27055
8011
Cov: 27055 -> 27055
Cov: 27055 -> 27055
8012
Cov: 27055 -> 27055
Cov: 27055 -> 27055
8013
Cov: 27055 -> 27056
Cov: 27056 -> 27056
8014
Cov: 27056 -> 27057
Cov: 27057 -> 27057
8015
{"exception": "TypeError", "msg": "'getset_descriptor' object is not callable"}
8016
Cov: 27057 -> 27057
Cov: 27057 -> 27057
8017
Cov: 27057 -> 27057
Cov: 27057 -> 27057
8018
Cov: 27057 -> 27057
Cov: 27057 -> 27057
8019
{"exception": "RuntimeError", "msg": "the base given to float_power_ has dtype Float but the operation's result requires dtype Double"}
8020
Cov: 27057 -> 27058
Cov: 27058 -> 27058
8021
Cov: 27058 -> 27058
Cov: 27058 -> 27058
8022
Cov: 27058 -> 27058
Cov: 27058 -> 27058
8023
Cov: 27058 -> 27058
Cov: 27058 -> 27058
8024
Cov: 27058 -> 27058
Cov: 27058 -> 27058
8025
Cov: 27058 -> 27058
Cov: 27058 -> 27058
8026
Cov: 27058 -> 27058
Cov: 27058 -> 27058
8027
Cov: 27058 -> 27059
Cov: 27059 -> 27059
8028
Cov: 27059 -> 27060
Cov: 27060 -> 27060
8029
Cov: 27060 -> 27061
Cov: 27061 -> 27061
8030
{"exception": "RuntimeError", "msg": "torch.ormqr: Expected other to have the same number of dimensions as input, but got other.ndim equal to 2 and input.ndim is equal to 3"}
8031
Cov: 27061 -> 27062
Cov: 27062 -> 27062
8032
{"exception": "RuntimeError", "msg": "The expanded size of the tensor (4) must match the existing size (3) at non-singleton dimension 1.  Target sizes: [2, 4].  Tensor sizes: [2, 3]"}
8033
Cov: 27062 -> 27062
Cov: 27062 -> 27062
8034
Cov: 27062 -> 27062
Cov: 27062 -> 27062
8035
Cov: 27062 -> 27063
Cov: 27063 -> 27063
8036
Cov: 27063 -> 27068
Cov: 27068 -> 27068
8037
Cov: 27068 -> 27069
Cov: 27069 -> 27069
8038
Cov: 27069 -> 27069
Cov: 27069 -> 27069
8039
Cov: 27069 -> 27070
Cov: 27070 -> 27070
8040
Cov: 27070 -> 27070
Cov: 27070 -> 27070
8041
Cov: 27070 -> 27071
Cov: 27071 -> 27071
8042
Cov: 27071 -> 27071
Cov: 27071 -> 27071
8043
Cov: 27071 -> 27071
Cov: 27071 -> 27071
8044
Cov: 27071 -> 27071
Cov: 27071 -> 27071
8045
Cov: 27071 -> 27071
Cov: 27071 -> 27071
8046
Cov: 27071 -> 27071
Cov: 27071 -> 27071
8047
Cov: 27071 -> 27071
Cov: 27071 -> 27071
8048
Cov: 27071 -> 27071
Cov: 27071 -> 27071
8049
Cov: 27071 -> 27072
Cov: 27072 -> 27072
8050
Cov: 27072 -> 27072
Cov: 27072 -> 27072
8051
Cov: 27072 -> 27072
Cov: 27072 -> 27072
8052
{"exception": "RuntimeError", "msg": "The expanded size of the tensor (3) must match the existing size (4) at non-singleton dimension 1.  Target sizes: [4, 3].  Tensor sizes: [3, 4]"}
8053
Cov: 27072 -> 27072
Cov: 27072 -> 27072
8054
Cov: 27072 -> 27072
Cov: 27072 -> 27072
8055
Cov: 27072 -> 27073
Cov: 27073 -> 27073
8056
{"exception": "TypeError", "msg": "'getset_descriptor' object is not callable"}
8057
{"exception": "RuntimeError", "msg": "This function was deprecated since version 1.9 and is now removed.\nPlease use the `torch.linalg.matrix_rank` function instead. The parameter 'symmetric' was renamed in `torch.linalg.matrix_rank()` to 'hermitian'."}
8058
Cov: 27073 -> 27075
Cov: 27075 -> 27075
8059
Cov: 27075 -> 27075
Cov: 27075 -> 27075
8060
Cov: 27075 -> 27075
Cov: 27075 -> 27075
8061
Cov: 27075 -> 27075
Cov: 27075 -> 27075
8062
Cov: 27075 -> 27075
Cov: 27075 -> 27075
8063
Cov: 27075 -> 27076
Cov: 27076 -> 27076
8064
Cov: 27076 -> 27076
Cov: 27076 -> 27076
8065
{"exception": "IndexError", "msg": "Dimension out of range (expected to be in range of [-2, 1], but got 2)"}
8066
Cov: 27076 -> 27076
Cov: 27076 -> 27076
8067
Cov: 27076 -> 27076
Cov: 27076 -> 27076
8068
Cov: 27076 -> 27077
Cov: 27077 -> 27077
8069
Cov: 27077 -> 27077
Cov: 27077 -> 27077
8070
Cov: 27077 -> 27077
Cov: 27077 -> 27077
8071
Cov: 27077 -> 27078
Cov: 27078 -> 27078
8072
Cov: 27078 -> 27078
Cov: 27078 -> 27078
8073
Cov: 27078 -> 27078
Cov: 27078 -> 27078
8074
Cov: 27078 -> 27078
Cov: 27078 -> 27078
8075
Cov: 27078 -> 27078
Cov: 27078 -> 27078
8076
Cov: 27078 -> 27078
Cov: 27078 -> 27078
8077
Cov: 27078 -> 27079
Cov: 27079 -> 27079
8078
Cov: 27079 -> 27079
Cov: 27079 -> 27079
8079
Cov: 27079 -> 27080
Cov: 27080 -> 27080
8080
{"exception": "TypeError", "msg": "atanh_() takes no arguments (1 given)"}
8081
Cov: 27080 -> 27080
Cov: 27080 -> 27080
8082
Cov: 27080 -> 27081
Cov: 27081 -> 27081
8083
Cov: 27081 -> 27082
Cov: 27082 -> 27082
8084
Cov: 27082 -> 27082
Cov: 27082 -> 27082
8085
Cov: 27082 -> 27082
Cov: 27082 -> 27082
8086
Cov: 27082 -> 27086
Cov: 27086 -> 27086
8087
Cov: 27086 -> 27086
Cov: 27086 -> 27086
8088
{"exception": "TypeError", "msg": "roll(): argument 'shifts' (position 1) must be tuple of ints, not Tensor"}
8089
Cov: 27086 -> 27086
Cov: 27086 -> 27086
8090
Cov: 27086 -> 27087
Cov: 27087 -> 27087
8091
Cov: 27087 -> 27087
Cov: 27087 -> 27087
8092
Cov: 27087 -> 27096
Cov: 27096 -> 27096
8093
Cov: 27096 -> 27096
Cov: 27096 -> 27096
8094
Cov: 27096 -> 27096
Cov: 27096 -> 27096
8095
Cov: 27096 -> 27096
Cov: 27096 -> 27096
8096
Cov: 27096 -> 27096
Cov: 27096 -> 27096
8097
{"exception": "RuntimeError", "msg": "linalg.inv: A must be batches of square matrices, but they are 2 by 3 matrices"}
8098
Cov: 27096 -> 27097
Cov: 27097 -> 27097
8099
Cov: 27097 -> 27097
Cov: 27097 -> 27097
8100
Cov: 27097 -> 27097
Cov: 27097 -> 27097
8101
Cov: 27097 -> 27097
Cov: 27097 -> 27097
8102
Cov: 27097 -> 27098
Cov: 27098 -> 27098
8103
Cov: 27098 -> 27098
Cov: 27098 -> 27098
8104
Cov: 27098 -> 27098
Cov: 27098 -> 27098
8105
Cov: 27098 -> 27098
Cov: 27098 -> 27098
8106
Cov: 27098 -> 27098
Cov: 27098 -> 27098
8107
Cov: 27098 -> 27098
Cov: 27098 -> 27098
8108
Cov: 27098 -> 27098
Cov: 27098 -> 27098
8109
{"exception": "TypeError", "msg": "set_() received an invalid combination of arguments - got (stride=NoneType, size=NoneType, storage_offset=int, source=NoneType, ), but expected one of:\n * ()\n * (torch.Storage source)\n * (torch.Storage source, int storage_offset, tuple of ints size, tuple of ints stride)\n      didn't match because some of the arguments have invalid types: (\u001b[31;1msource=NoneType\u001b[0m, \u001b[32;1mstorage_offset=int\u001b[0m, \u001b[31;1msize=NoneType\u001b[0m, \u001b[31;1mstride=NoneType\u001b[0m, )\n * (Tensor source)\n * (Tensor source, int storage_offset, tuple of ints size, tuple of ints stride)\n      didn't match because some of the arguments have invalid types: (\u001b[31;1msource=NoneType\u001b[0m, \u001b[32;1mstorage_offset=int\u001b[0m, \u001b[31;1msize=NoneType\u001b[0m, \u001b[31;1mstride=NoneType\u001b[0m, )\n"}
8110
Cov: 27098 -> 27098
Cov: 27098 -> 27098
8111
Cov: 27098 -> 27098
Cov: 27098 -> 27098
8112
Cov: 27098 -> 27098
Cov: 27098 -> 27098
8113
Cov: 27098 -> 27098
Cov: 27098 -> 27098
8114
Cov: 27098 -> 27099
Cov: 27099 -> 27099
8115
Cov: 27099 -> 27100
Cov: 27100 -> 27100
8116
Cov: 27100 -> 27100
Cov: 27100 -> 27100
8117
Cov: 27100 -> 27100
Cov: 27100 -> 27100
8118
Cov: 27100 -> 27101
Cov: 27101 -> 27101
8119
Cov: 27101 -> 27101
Cov: 27101 -> 27101
8120
Cov: 27101 -> 27101
Cov: 27101 -> 27101
8121
Cov: 27101 -> 27101
Cov: 27101 -> 27101
8122
Cov: 27101 -> 27102
Cov: 27102 -> 27102
8123
Cov: 27102 -> 27102
Cov: 27102 -> 27102
8124
Cov: 27102 -> 27102
Cov: 27102 -> 27102
8125
Cov: 27102 -> 27102
Cov: 27102 -> 27102
8126
Cov: 27102 -> 27102
Cov: 27102 -> 27102
8127
{"exception": "RuntimeError", "msg": "tensor.sspaddmm(...) can only be called on sparse tensors"}
8128
Cov: 27102 -> 27102
Cov: 27102 -> 27102
8129
{"exception": "RuntimeError", "msg": "tensor.sspaddmm(...) can only be called on sparse tensors"}
8130
Cov: 27102 -> 27102
Cov: 27102 -> 27102
8131
Cov: 27102 -> 27102
Cov: 27102 -> 27102
8132
Cov: 27102 -> 27102
Cov: 27102 -> 27102
8133
Cov: 27102 -> 27102
Cov: 27102 -> 27102
8134
Cov: 27102 -> 27102
Cov: 27102 -> 27102
8135
Cov: 27102 -> 27102
Cov: 27102 -> 27102
8136
Cov: 27102 -> 27102
Cov: 27102 -> 27102
8137
Cov: 27102 -> 27102
Cov: 27102 -> 27102
8138
{"exception": "RuntimeError", "msg": "torch.ormqr: other.shape[-2] must be greater than or equal to tau.shape[-1]"}
8139
Cov: 27102 -> 27102
Cov: 27102 -> 27102
8140
Cov: 27102 -> 27102
Cov: 27102 -> 27102
8141
Cov: 27102 -> 27102
Cov: 27102 -> 27102
8142
Cov: 27102 -> 27102
Cov: 27102 -> 27102
8143
Cov: 27102 -> 27104
Cov: 27104 -> 27104
8144
Cov: 27104 -> 27104
Cov: 27104 -> 27104
8145
{"exception": "RuntimeError", "msg": "The expanded size of the tensor (5) must match the existing size (4) at non-singleton dimension 2.  Target sizes: [10, 3, 5].  Tensor sizes: [10, 3, 4]"}
8146
Cov: 27104 -> 27104
Cov: 27104 -> 27104
8147
Cov: 27104 -> 27104
Cov: 27104 -> 27104
8148
{"exception": "RuntimeError", "msg": "max_unpooling2d_forward_out does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True)'. You can turn off determinism just for this operation, or you can use the 'warn_only=True' option, if that's acceptable for your application. You can also file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation."}
8149
Cov: 27104 -> 27104
Cov: 27104 -> 27104
8150
Cov: 27104 -> 27104
Cov: 27104 -> 27104
8151
Cov: 27104 -> 27104
Cov: 27104 -> 27104
8152
Cov: 27104 -> 27105
Cov: 27105 -> 27105
8153
Cov: 27105 -> 27107
Cov: 27107 -> 27107
8154
{"exception": "RuntimeError", "msg": "torch.dsplit requires a tensor with at least 3 dimension, but got a tensor with 2 dimensions!"}
8155
Cov: 27107 -> 27107
Cov: 27107 -> 27107
8156
Cov: 27107 -> 27109
Cov: 27109 -> 27109
8157
{"exception": "TypeError", "msg": "addbmm_() missing 1 required positional arguments: \"batch2\""}
8158
Cov: 27109 -> 27111
Cov: 27111 -> 27111
8159
Cov: 27111 -> 27111
Cov: 27111 -> 27111
8160
Cov: 27111 -> 27111
Cov: 27111 -> 27111
8161
Cov: 27111 -> 27111
Cov: 27111 -> 27111
8162
Cov: 27111 -> 27111
Cov: 27111 -> 27111
8163
Cov: 27111 -> 27111
Cov: 27111 -> 27111
8164
Cov: 27111 -> 27111
Cov: 27111 -> 27111
8165
Cov: 27111 -> 27112
Cov: 27112 -> 27112
8166
Cov: 27112 -> 27112
Cov: 27112 -> 27112
8167
Cov: 27112 -> 27112
Cov: 27112 -> 27112
8168
Cov: 27112 -> 27112
Cov: 27112 -> 27112
8169
Cov: 27112 -> 27112
Cov: 27112 -> 27112
8170
{"exception": "TypeError", "msg": "arctanh_() takes no arguments (1 given)"}
8171
Cov: 27112 -> 27113
Cov: 27113 -> 27113
8172
Cov: 27113 -> 27113
Cov: 27113 -> 27113
8173
Cov: 27113 -> 27113
Cov: 27113 -> 27113
8174
Cov: 27113 -> 27113
Cov: 27113 -> 27113
8175
Cov: 27113 -> 27113
Cov: 27113 -> 27113
8176
Cov: 27113 -> 27113
Cov: 27113 -> 27113
8177
Cov: 27113 -> 27113
Cov: 27113 -> 27113
8178
Cov: 27113 -> 27114
Cov: 27114 -> 27114
8179
Cov: 27114 -> 27114
Cov: 27114 -> 27114
8180
Cov: 27114 -> 27114
Cov: 27114 -> 27114
8181
{"exception": "_LinAlgError", "msg": "cholesky: The factorization could not be completed because the input is not positive-definite (the leading minor of order 2 is not positive-definite)."}
8182
Cov: 27114 -> 27114
Cov: 27114 -> 27114
8183
Cov: 27114 -> 27114
Cov: 27114 -> 27114
8184
Cov: 27114 -> 27115
Cov: 27115 -> 27115
8185
Cov: 27115 -> 27116
Cov: 27116 -> 27116
8186
Cov: 27116 -> 27116
Cov: 27116 -> 27116
8187
Cov: 27116 -> 27116
Cov: 27116 -> 27116
8188
Cov: 27116 -> 27116
Cov: 27116 -> 27116
8189
Cov: 27116 -> 27116
Cov: 27116 -> 27116
8190
Cov: 27116 -> 27116
Cov: 27116 -> 27116
8191
Cov: 27116 -> 27116
Cov: 27116 -> 27116
8192
Cov: 27116 -> 27116
Cov: 27116 -> 27116
8193
Cov: 27116 -> 27116
Cov: 27116 -> 27116
8194
Cov: 27116 -> 27116
Cov: 27116 -> 27116
8195
{"exception": "TypeError", "msg": "'getset_descriptor' object is not callable"}
8196
Cov: 27116 -> 27116
Cov: 27116 -> 27116
8197
Cov: 27116 -> 27117
Cov: 27117 -> 27117
8198
{"exception": "RuntimeError", "msg": "result type Double can't be cast to the desired output type Long"}
8199
Cov: 27117 -> 27117
Cov: 27117 -> 27117
8200
Cov: 27117 -> 27117
Cov: 27117 -> 27117
8201
Cov: 27117 -> 27117
Cov: 27117 -> 27117
8202
Cov: 27117 -> 27118
Cov: 27118 -> 27118
8203
Cov: 27118 -> 27118
Cov: 27118 -> 27118
8204
Cov: 27118 -> 27118
Cov: 27118 -> 27118
8205
Cov: 27118 -> 27118
Cov: 27118 -> 27118
8206
Cov: 27118 -> 27118
Cov: 27118 -> 27118
8207
Cov: 27118 -> 27118
Cov: 27118 -> 27118
8208
{"exception": "TypeError", "msg": "'getset_descriptor' object is not callable"}
8209
Cov: 27118 -> 27119
Cov: 27119 -> 27119
8210
Cov: 27119 -> 27119
Cov: 27119 -> 27119
8211
{"exception": "TypeError", "msg": "random_() received an invalid combination of arguments - got (to=int, from_=int, ), but expected one of:\n * (*, torch.Generator generator)\n      didn't match because some of the keywords were incorrect: to, from_\n * (int from, int to, *, torch.Generator generator)\n * (int to, *, torch.Generator generator)\n"}
8212
Cov: 27119 -> 27119
Cov: 27119 -> 27119
8213
Cov: 27119 -> 27120
Cov: 27120 -> 27120
8214
Cov: 27120 -> 27120
Cov: 27120 -> 27120
8215
Cov: 27120 -> 27121
Cov: 27121 -> 27121
8216
Cov: 27121 -> 27122
Cov: 27122 -> 27122
8217
Cov: 27122 -> 27122
Cov: 27122 -> 27122
8218
Cov: 27122 -> 27123
Cov: 27123 -> 27123
8219
Cov: 27123 -> 27123
Cov: 27123 -> 27123
8220
Cov: 27123 -> 27124
Cov: 27124 -> 27124
8221
Cov: 27124 -> 27124
Cov: 27124 -> 27124
8222
Cov: 27124 -> 27124
Cov: 27124 -> 27124
8223
Cov: 27124 -> 27124
Cov: 27124 -> 27124
8224
Cov: 27124 -> 27125
Cov: 27125 -> 27125
8225
Cov: 27125 -> 27125
Cov: 27125 -> 27125
8226
Cov: 27125 -> 27126
Cov: 27126 -> 27126
8227
Cov: 27126 -> 27126
Cov: 27126 -> 27126
8228
Cov: 27126 -> 27126
Cov: 27126 -> 27126
8229
Cov: 27126 -> 27126
Cov: 27126 -> 27126
8230
{"exception": "RuntimeError", "msg": "istft requires a complex-valued input tensor matching the output from stft with return_complex=True."}
8231
Cov: 27126 -> 27127
Cov: 27127 -> 27127
8232
Cov: 27127 -> 27127
Cov: 27127 -> 27127
8233
Cov: 27127 -> 27128
Cov: 27128 -> 27128
8234
Cov: 27128 -> 27128
Cov: 27128 -> 27128
8235
Cov: 27128 -> 27128
Cov: 27128 -> 27128
8236
Cov: 27128 -> 27128
Cov: 27128 -> 27128
8237
Cov: 27128 -> 27128
Cov: 27128 -> 27128
8238
Cov: 27128 -> 27129
Cov: 27129 -> 27129
8239
Cov: 27129 -> 27129
Cov: 27129 -> 27129
8240
Cov: 27129 -> 27129
Cov: 27129 -> 27129
8241
Cov: 27129 -> 27129
Cov: 27129 -> 27129
8242
Cov: 27129 -> 27129
Cov: 27129 -> 27129
8243
Cov: 27129 -> 27129
Cov: 27129 -> 27129
8244
{"exception": "RuntimeError", "msg": "\"lshift_cpu\" not implemented for 'Float'"}
8245
Cov: 27129 -> 27142
Cov: 27142 -> 27142
8246
Cov: 27142 -> 27143
Cov: 27143 -> 27143
8247
Cov: 27143 -> 27144
Cov: 27144 -> 27144
8248
Cov: 27144 -> 27145
Cov: 27145 -> 27145
8249
Cov: 27145 -> 27146
Cov: 27146 -> 27146
8250
Cov: 27146 -> 27158
Cov: 27158 -> 27158
8251
Cov: 27158 -> 27158
Cov: 27158 -> 27158
8252
Cov: 27158 -> 27158
Cov: 27158 -> 27158
8253
Cov: 27158 -> 27158
Cov: 27158 -> 27158
8254
{"exception": "RuntimeError", "msg": "required rank 4 tensor to use channels_last format"}
8255
Cov: 27158 -> 27158
Cov: 27158 -> 27158
8256
Cov: 27158 -> 27158
Cov: 27158 -> 27158
8257
Cov: 27158 -> 27158
Cov: 27158 -> 27158
8258
Cov: 27158 -> 27158
Cov: 27158 -> 27158
8259
Cov: 27158 -> 27158
Cov: 27158 -> 27158
8260
Cov: 27158 -> 27159
Cov: 27159 -> 27159
8261
{"exception": "RuntimeError", "msg": "the base given to float_power_ has dtype Float but the operation's result requires dtype Double"}
8262
Cov: 27159 -> 27160
Cov: 27160 -> 27160
8263
Cov: 27160 -> 27160
Cov: 27160 -> 27160
8264
Cov: 27160 -> 27160
Cov: 27160 -> 27160
8265
Cov: 27160 -> 27160
Cov: 27160 -> 27160
8266
Cov: 27160 -> 27160
Cov: 27160 -> 27160
8267
Cov: 27160 -> 27160
Cov: 27160 -> 27160
8268
Cov: 27160 -> 27160
Cov: 27160 -> 27160
8269
Cov: 27160 -> 27160
Cov: 27160 -> 27160
8270
Cov: 27160 -> 27161
Cov: 27161 -> 27161
8271
Cov: 27161 -> 27162
Cov: 27162 -> 27162
8272
Cov: 27162 -> 27162
Cov: 27162 -> 27162
8273
Cov: 27162 -> 27162
Cov: 27162 -> 27162
8274
Cov: 27162 -> 27162
Cov: 27162 -> 27162
8275
Cov: 27162 -> 27163
Cov: 27163 -> 27163
8276
Cov: 27163 -> 27163
Cov: 27163 -> 27163
8277
{"exception": "NotImplementedError", "msg": "Could not run 'aten::q_per_channel_scales' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::q_per_channel_scales' is only available for these backends: [QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nQuantizedCPU: registered at aten/src/ATen/RegisterQuantizedCPU.cpp:944 [kernel]\nQuantizedCUDA: registered at aten/src/ATen/RegisterQuantizedCUDA.cpp:459 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:153 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:498 [backend fallback]\nFunctionalize: registered at ../aten/src/ATen/FunctionalizeFallbackKernel.cpp:290 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:86 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradMTIA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_3.cpp:14610 [kernel]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:382 [backend fallback]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:249 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:710 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:203 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:161 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:494 [backend fallback]\nPreDispatch: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:165 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:157 [backend fallback]\n"}
8278
Cov: 27163 -> 27163
Cov: 27163 -> 27163
8279
Cov: 27163 -> 27164
Cov: 27164 -> 27164
8280
Cov: 27164 -> 27164
Cov: 27164 -> 27164
8281
Cov: 27164 -> 27164
Cov: 27164 -> 27164
8282
Cov: 27164 -> 27164
Cov: 27164 -> 27164
8283
Cov: 27164 -> 27164
Cov: 27164 -> 27164
8284
Cov: 27164 -> 27164
Cov: 27164 -> 27164
8285
Cov: 27164 -> 27164
Cov: 27164 -> 27164
8286
Cov: 27164 -> 27164
Cov: 27164 -> 27164
8287
Cov: 27164 -> 27164
Cov: 27164 -> 27164
8288
Cov: 27164 -> 27164
Cov: 27164 -> 27164
8289
Cov: 27164 -> 27165
Cov: 27165 -> 27165
8290
Cov: 27165 -> 27165
Cov: 27165 -> 27165
8291
Cov: 27165 -> 27167
Cov: 27167 -> 27167
8292
{"exception": "NameError", "msg": "name 'Linear' is not defined"}
8293
Cov: 27167 -> 27167
Cov: 27167 -> 27167
8294
Cov: 27167 -> 27168
Cov: 27168 -> 27168
8295
Cov: 27168 -> 27168
Cov: 27168 -> 27168
8296
Cov: 27168 -> 27168
Cov: 27168 -> 27168
8297
Cov: 27168 -> 27168
Cov: 27168 -> 27168
8298
{"exception": "RuntimeError", "msg": "1D tensors expected, but got 2D and 2D tensors"}
8299
Cov: 27168 -> 27168
Cov: 27168 -> 27168
8300
Cov: 27168 -> 27169
Cov: 27169 -> 27169
8301
Cov: 27169 -> 27171
Cov: 27171 -> 27171
8302
Cov: 27171 -> 27171
Cov: 27171 -> 27171
8303
Cov: 27171 -> 27171
Cov: 27171 -> 27171
8304
{"exception": "RuntimeError", "msg": "logdet: A must be batches of square matrices, but they are 4 by 5 matrices"}
8305
Cov: 27171 -> 27172
Cov: 27172 -> 27172
8306
Cov: 27172 -> 27172
Cov: 27172 -> 27172
8307
Cov: 27172 -> 27172
Cov: 27172 -> 27172
8308
Cov: 27172 -> 27172
Cov: 27172 -> 27172
8309
Cov: 27172 -> 27172
Cov: 27172 -> 27172
8310
{"exception": "NotImplementedError", "msg": "Could not run 'aten::int_repr' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::int_repr' is only available for these backends: [QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nQuantizedCPU: registered at aten/src/ATen/RegisterQuantizedCPU.cpp:944 [kernel]\nQuantizedCUDA: registered at aten/src/ATen/RegisterQuantizedCUDA.cpp:459 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:153 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:498 [backend fallback]\nFunctionalize: registered at ../aten/src/ATen/FunctionalizeFallbackKernel.cpp:290 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:86 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradMTIA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_0.cpp:16725 [kernel]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:382 [backend fallback]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:249 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:710 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:203 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:161 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:494 [backend fallback]\nPreDispatch: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:165 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:157 [backend fallback]\n"}
8311
Cov: 27172 -> 27172
Cov: 27172 -> 27172
8312
Cov: 27172 -> 27172
Cov: 27172 -> 27172
8313
Cov: 27172 -> 27172
Cov: 27172 -> 27172
8314
Cov: 27172 -> 27172
Cov: 27172 -> 27172
8315
Cov: 27172 -> 27173
Cov: 27173 -> 27173
8316
Cov: 27173 -> 27175
Cov: 27175 -> 27175
8317
Cov: 27175 -> 27175
Cov: 27175 -> 27175
8318
Cov: 27175 -> 27176
Cov: 27176 -> 27176
8319
{"exception": "NotImplementedError", "msg": "Could not run 'aten::q_per_channel_zero_points' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::q_per_channel_zero_points' is only available for these backends: [QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nQuantizedCPU: registered at aten/src/ATen/RegisterQuantizedCPU.cpp:944 [kernel]\nQuantizedCUDA: registered at aten/src/ATen/RegisterQuantizedCUDA.cpp:459 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:153 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:498 [backend fallback]\nFunctionalize: registered at ../aten/src/ATen/FunctionalizeFallbackKernel.cpp:290 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:86 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradMTIA: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_1.cpp:16002 [kernel]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:382 [backend fallback]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:249 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:710 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:203 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:161 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:494 [backend fallback]\nPreDispatch: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:165 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:157 [backend fallback]\n"}
8320
Cov: 27176 -> 27177
Cov: 27177 -> 27177
8321
Cov: 27177 -> 27177
Cov: 27177 -> 27177
8322
Cov: 27177 -> 27177
Cov: 27177 -> 27177
8323
Cov: 27177 -> 27177
Cov: 27177 -> 27177
8324
Cov: 27177 -> 27178
Cov: 27178 -> 27178
8325
Cov: 27178 -> 27178
Cov: 27178 -> 27178
8326
Cov: 27178 -> 27179
Cov: 27179 -> 27179
8327
Cov: 27179 -> 27179
Cov: 27179 -> 27179
8328
Cov: 27179 -> 27179
Cov: 27179 -> 27179
8329
Cov: 27179 -> 27179
Cov: 27179 -> 27179
8330
Cov: 27179 -> 27179
Cov: 27179 -> 27179
8331
Cov: 27179 -> 27179
Cov: 27179 -> 27179
8332
Cov: 27179 -> 27179
Cov: 27179 -> 27179
8333
{"exception": "NotImplementedError", "msg": "Could not run 'aten::q_per_channel_axis' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::q_per_channel_axis' is only available for these backends: [QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nQuantizedCPU: registered at aten/src/ATen/RegisterQuantizedCPU.cpp:944 [kernel]\nQuantizedCUDA: registered at aten/src/ATen/RegisterQuantizedCUDA.cpp:459 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:153 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:498 [backend fallback]\nFunctionalize: registered at ../aten/src/ATen/FunctionalizeFallbackKernel.cpp:290 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:86 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradMTIA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_4.cpp:13056 [kernel]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:382 [backend fallback]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:249 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:710 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:203 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:161 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:494 [backend fallback]\nPreDispatch: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:165 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:157 [backend fallback]\n"}
8334
Cov: 27179 -> 27179
Cov: 27179 -> 27179
8335
Cov: 27179 -> 27179
Cov: 27179 -> 27179
8336
Cov: 27179 -> 27179
Cov: 27179 -> 27179
8337
{"exception": "RuntimeError", "msg": "1D tensors expected, but got 2D and 2D tensors"}
8338
Cov: 27179 -> 27249
Cov: 27249 -> 27249
8339
Cov: 27249 -> 27250
Cov: 27250 -> 27250
8340
Cov: 27250 -> 27251
Cov: 27251 -> 27251
8341
Cov: 27251 -> 27251
Cov: 27251 -> 27251
8342
Cov: 27251 -> 27251
Cov: 27251 -> 27251
8343
Cov: 27251 -> 27251
Cov: 27251 -> 27251
8344
Cov: 27251 -> 27251
Cov: 27251 -> 27251
8345
Cov: 27251 -> 27252
Cov: 27252 -> 27252
8346
Cov: 27252 -> 27252
Cov: 27252 -> 27252
8347
Cov: 27252 -> 27254
Cov: 27254 -> 27254
8348
Cov: 27254 -> 27254
Cov: 27254 -> 27254
8349
Cov: 27254 -> 27254
Cov: 27254 -> 27254
8350
{"exception": "RuntimeError", "msg": "\"triangular_solve_cpu\" not implemented for 'Long'"}
8351
Cov: 27254 -> 27254
Cov: 27254 -> 27254
8352
Cov: 27254 -> 27254
Cov: 27254 -> 27254
8353
Cov: 27254 -> 27255
Cov: 27255 -> 27255
8354
Cov: 27255 -> 27256
Cov: 27256 -> 27256
8355
{"exception": "RuntimeError", "msg": "tensor.sspaddmm(...) can only be called on sparse tensors"}
8356
Cov: 27256 -> 27257
Cov: 27257 -> 27257
8357
Cov: 27257 -> 27258
Cov: 27258 -> 27258
8358
Cov: 27258 -> 27258
Cov: 27258 -> 27258
8359
Cov: 27258 -> 27259
Cov: 27259 -> 27259
8360
Cov: 27259 -> 27259
Cov: 27259 -> 27259
8361
Cov: 27259 -> 27259
Cov: 27259 -> 27259
8362
Cov: 27259 -> 27259
Cov: 27259 -> 27259
8363
Cov: 27259 -> 27260
Cov: 27260 -> 27260
8364
Cov: 27260 -> 27261
Cov: 27261 -> 27261
8365
Cov: 27261 -> 27261
Cov: 27261 -> 27261
8366
Cov: 27261 -> 27261
Cov: 27261 -> 27261
8367
Cov: 27261 -> 27261
Cov: 27261 -> 27261
8368
Cov: 27261 -> 27262
Cov: 27262 -> 27262
8369
Cov: 27262 -> 27262
Cov: 27262 -> 27262
8370
Cov: 27262 -> 27262
Cov: 27262 -> 27262
8371
Cov: 27262 -> 27263
Cov: 27263 -> 27263
8372
Cov: 27263 -> 27263
Cov: 27263 -> 27263
8373
Cov: 27263 -> 27263
Cov: 27263 -> 27263
8374
Cov: 27263 -> 27263
Cov: 27263 -> 27263
8375
Cov: 27263 -> 27263
Cov: 27263 -> 27263
8376
Cov: 27263 -> 27263
Cov: 27263 -> 27263
8377
Cov: 27263 -> 27264
Cov: 27264 -> 27264
8378
Cov: 27264 -> 27264
Cov: 27264 -> 27264
8379
Cov: 27264 -> 27270
Cov: 27270 -> 27270
8380
Cov: 27270 -> 27270
Cov: 27270 -> 27270
8381
Cov: 27270 -> 27271
Cov: 27271 -> 27271
8382
Cov: 27271 -> 27271
Cov: 27271 -> 27271
8383
Cov: 27271 -> 27272
Cov: 27272 -> 27272
8384
{"exception": "RuntimeError", "msg": "Error: cannot set number of interop threads after parallel work has started or set_num_interop_threads called"}
8385
Cov: 27272 -> 27272
Cov: 27272 -> 27272
8386
Cov: 27272 -> 27273
Cov: 27273 -> 27273
8387
Cov: 27273 -> 27274
Cov: 27274 -> 27274
8388
Cov: 27274 -> 27274
Cov: 27274 -> 27274
8389
Cov: 27274 -> 27274
Cov: 27274 -> 27274
8390
Cov: 27274 -> 27274
Cov: 27274 -> 27274
8391
Cov: 27274 -> 27274
Cov: 27274 -> 27274
8392
Cov: 27274 -> 27274
Cov: 27274 -> 27274
8393
Cov: 27274 -> 27274
Cov: 27274 -> 27274
8394
Cov: 27274 -> 27274
Cov: 27274 -> 27274
8395
Cov: 27274 -> 27274
Cov: 27274 -> 27274
8396
Cov: 27274 -> 27274
Cov: 27274 -> 27274
8397
Cov: 27274 -> 27274
Cov: 27274 -> 27274
8398
Cov: 27274 -> 27274
Cov: 27274 -> 27274
8399
Cov: 27274 -> 27275
Cov: 27275 -> 27275
8400
Cov: 27275 -> 27275
Cov: 27275 -> 27275
8401
Cov: 27275 -> 27277
Cov: 27277 -> 27277
8402
{"exception": "RuntimeError", "msg": "vector + matrix @ vector expected, got 2, 2, 1"}
8403
Cov: 27277 -> 27278
Cov: 27278 -> 27278
8404
Cov: 27278 -> 27278
Cov: 27278 -> 27278
8405
Cov: 27278 -> 27279
Cov: 27279 -> 27279
8406
Cov: 27279 -> 27279
Cov: 27279 -> 27279
8407
Cov: 27279 -> 27279
Cov: 27279 -> 27279
8408
Cov: 27279 -> 27280
Cov: 27280 -> 27280
8409
Cov: 27280 -> 27283
Cov: 27283 -> 27283
8410
Cov: 27283 -> 27283
Cov: 27283 -> 27283
8411
Cov: 27283 -> 27283
Cov: 27283 -> 27283
8412
Cov: 27283 -> 27283
Cov: 27283 -> 27283
8413
Cov: 27283 -> 27283
Cov: 27283 -> 27283
8414
Cov: 27283 -> 27284
Cov: 27284 -> 27284
8415
Cov: 27284 -> 27284
Cov: 27284 -> 27284
8416
Cov: 27284 -> 27284
Cov: 27284 -> 27284
8417
{"exception": "NotImplementedError", "msg": "Could not run 'aten::int_repr' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::int_repr' is only available for these backends: [QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nQuantizedCPU: registered at aten/src/ATen/RegisterQuantizedCPU.cpp:944 [kernel]\nQuantizedCUDA: registered at aten/src/ATen/RegisterQuantizedCUDA.cpp:459 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:153 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:498 [backend fallback]\nFunctionalize: registered at ../aten/src/ATen/FunctionalizeFallbackKernel.cpp:290 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:86 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradMTIA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_0.cpp:16725 [kernel]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:382 [backend fallback]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:249 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:710 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:203 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:161 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:494 [backend fallback]\nPreDispatch: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:165 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:157 [backend fallback]\n"}
8418
Cov: 27284 -> 27285
Cov: 27285 -> 27285
8419
Cov: 27285 -> 27285
Cov: 27285 -> 27285
8420
Cov: 27285 -> 27285
Cov: 27285 -> 27285
8421
Cov: 27285 -> 27285
Cov: 27285 -> 27285
8422
Cov: 27285 -> 27285
Cov: 27285 -> 27285
8423
Cov: 27285 -> 27285
Cov: 27285 -> 27285
8424
Cov: 27285 -> 27285
Cov: 27285 -> 27285
8425
Cov: 27285 -> 27285
Cov: 27285 -> 27285
8426
{"exception": "TypeError", "msg": "arctanh_() takes no arguments (1 given)"}
8427
Cov: 27285 -> 27285
Cov: 27285 -> 27285
8428
Cov: 27285 -> 27285
Cov: 27285 -> 27285
8429
Cov: 27285 -> 27285
Cov: 27285 -> 27285
8430
Cov: 27285 -> 27285
Cov: 27285 -> 27285
8431
Cov: 27285 -> 27285
Cov: 27285 -> 27285
8432
Cov: 27285 -> 27286
Cov: 27286 -> 27286
8433
Cov: 27286 -> 27287
Cov: 27287 -> 27287
8434
Cov: 27287 -> 27288
Cov: 27288 -> 27288
8435
Cov: 27288 -> 27289
Cov: 27289 -> 27289
8436
Cov: 27289 -> 27289
Cov: 27289 -> 27289
8437
{"exception": "NotImplementedError", "msg": "Could not run 'aten::q_per_channel_axis' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::q_per_channel_axis' is only available for these backends: [QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nQuantizedCPU: registered at aten/src/ATen/RegisterQuantizedCPU.cpp:944 [kernel]\nQuantizedCUDA: registered at aten/src/ATen/RegisterQuantizedCUDA.cpp:459 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:153 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:498 [backend fallback]\nFunctionalize: registered at ../aten/src/ATen/FunctionalizeFallbackKernel.cpp:290 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:86 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradMTIA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_4.cpp:13056 [kernel]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:382 [backend fallback]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:249 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:710 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:203 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:161 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:494 [backend fallback]\nPreDispatch: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:165 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:157 [backend fallback]\n"}
8438
Cov: 27289 -> 27289
Cov: 27289 -> 27289
8439
Cov: 27289 -> 27289
Cov: 27289 -> 27289
8440
Cov: 27289 -> 27289
Cov: 27289 -> 27289
8441
Cov: 27289 -> 27289
Cov: 27289 -> 27289
8442
{"exception": "RuntimeError", "msg": "The expanded size of the tensor (5) must match the existing size (4) at non-singleton dimension 2.  Target sizes: [10, 3, 5].  Tensor sizes: [10, 3, 4]"}
8443
Cov: 27289 -> 27289
Cov: 27289 -> 27289
8444
Cov: 27289 -> 27289
Cov: 27289 -> 27289
8445
Cov: 27289 -> 27289
Cov: 27289 -> 27289
8446
Cov: 27289 -> 27289
Cov: 27289 -> 27289
8447
Cov: 27289 -> 27290
Cov: 27290 -> 27290
8448
Cov: 27290 -> 27290
Cov: 27290 -> 27290
8449
Cov: 27290 -> 27290
Cov: 27290 -> 27290
8450
Cov: 27290 -> 27290
Cov: 27290 -> 27290
8451
Cov: 27290 -> 27290
Cov: 27290 -> 27290
8452
Cov: 27290 -> 27290
Cov: 27290 -> 27290
8453
Cov: 27290 -> 27290
Cov: 27290 -> 27290
8454
Cov: 27290 -> 27290
Cov: 27290 -> 27290
8455
Cov: 27290 -> 27291
Cov: 27291 -> 27291
8456
Cov: 27291 -> 27291
Cov: 27291 -> 27291
8457
Cov: 27291 -> 27291
Cov: 27291 -> 27291
8458
Cov: 27291 -> 27292
Cov: 27292 -> 27292
8459
Cov: 27292 -> 27292
Cov: 27292 -> 27292
8460
Cov: 27292 -> 27292
Cov: 27292 -> 27292
8461
Cov: 27292 -> 27292
Cov: 27292 -> 27292
8462
Cov: 27292 -> 27292
Cov: 27292 -> 27292
8463
Cov: 27292 -> 27292
Cov: 27292 -> 27292
8464
Cov: 27292 -> 27292
Cov: 27292 -> 27292
8465
Cov: 27292 -> 27292
Cov: 27292 -> 27292
8466
Cov: 27292 -> 27292
Cov: 27292 -> 27292
8467
Cov: 27292 -> 27306
Cov: 27306 -> 27306
8468
Cov: 27306 -> 27306
Cov: 27306 -> 27306
8469
Cov: 27306 -> 27306
Cov: 27306 -> 27306
8470
Cov: 27306 -> 27306
Cov: 27306 -> 27306
8471
Cov: 27306 -> 27306
Cov: 27306 -> 27306
8472
Cov: 27306 -> 27306
Cov: 27306 -> 27306
8473
Cov: 27306 -> 27306
Cov: 27306 -> 27306
8474
Cov: 27306 -> 27306
Cov: 27306 -> 27306
8475
Cov: 27306 -> 27306
Cov: 27306 -> 27306
8476
Cov: 27306 -> 27306
Cov: 27306 -> 27306
8477
Cov: 27306 -> 27306
Cov: 27306 -> 27306
8478
Cov: 27306 -> 27306
Cov: 27306 -> 27306
8479
{"exception": "RuntimeError", "msg": "This function was deprecated since version 1.9 and is now removed. `torch.solve` is deprecated in favor of `torch.linalg.solve`. `torch.linalg.solve` has its arguments reversed and does not return the LU factorization.\n\nTo get the LU factorization see `torch.lu`, which can be used with `torch.lu_solve` or `torch.lu_unpack`.\nX = torch.solve(B, A).solution should be replaced with:\nX = torch.linalg.solve(A, B)"}
8480
Cov: 27306 -> 27307
Cov: 27307 -> 27307
8481
Cov: 27307 -> 27307
Cov: 27307 -> 27307
8482
Cov: 27307 -> 27307
Cov: 27307 -> 27307
8483
{"exception": "TypeError", "msg": "'getset_descriptor' object is not callable"}
8484
Cov: 27307 -> 27307
Cov: 27307 -> 27307
8485
Cov: 27307 -> 27307
Cov: 27307 -> 27307
8486
Cov: 27307 -> 27308
Cov: 27308 -> 27308
8487
Cov: 27308 -> 27308
Cov: 27308 -> 27308
8488
{"exception": "RuntimeError", "msg": "the base given to float_power_ has dtype Float but the operation's result requires dtype Double"}
8489
Cov: 27308 -> 27308
Cov: 27308 -> 27308
8490
Cov: 27308 -> 27308
Cov: 27308 -> 27308
8491
Cov: 27308 -> 27308
Cov: 27308 -> 27308
8492
Cov: 27308 -> 27308
Cov: 27308 -> 27308
8493
Cov: 27308 -> 27308
Cov: 27308 -> 27308
8494
Cov: 27308 -> 27309
Cov: 27309 -> 27309
8495
Cov: 27309 -> 27310
Cov: 27310 -> 27310
8496
Cov: 27310 -> 27311
Cov: 27311 -> 27311
8497
Cov: 27311 -> 27311
Cov: 27311 -> 27311
8498
Cov: 27311 -> 27311
Cov: 27311 -> 27311
8499
{"exception": "TypeError", "msg": "'getset_descriptor' object is not callable"}
8500
Cov: 27311 -> 27311
Cov: 27311 -> 27311
8501
Cov: 27311 -> 27311
Cov: 27311 -> 27311
8502
{"exception": "NotImplementedError", "msg": "Could not run 'aten::q_per_channel_axis' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::q_per_channel_axis' is only available for these backends: [QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nQuantizedCPU: registered at aten/src/ATen/RegisterQuantizedCPU.cpp:944 [kernel]\nQuantizedCUDA: registered at aten/src/ATen/RegisterQuantizedCUDA.cpp:459 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:153 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:498 [backend fallback]\nFunctionalize: registered at ../aten/src/ATen/FunctionalizeFallbackKernel.cpp:290 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:86 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradMTIA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_4.cpp:13056 [kernel]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:382 [backend fallback]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:249 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:710 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:203 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:161 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:494 [backend fallback]\nPreDispatch: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:165 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:157 [backend fallback]\n"}
8503
{"exception": "TypeError", "msg": "nextafter_(): argument 'other' must be Tensor, not float"}
8504
Cov: 27311 -> 27311
Cov: 27311 -> 27311
8505
Cov: 27311 -> 27311
Cov: 27311 -> 27311
8506
Cov: 27311 -> 27311
Cov: 27311 -> 27311
8507
Cov: 27311 -> 27311
Cov: 27311 -> 27311
8508
Cov: 27311 -> 27312
Cov: 27312 -> 27312
8509
Cov: 27312 -> 27312
Cov: 27312 -> 27312
8510
Cov: 27312 -> 27312
Cov: 27312 -> 27312
8511
Cov: 27312 -> 27312
Cov: 27312 -> 27312
8512
Cov: 27312 -> 27313
Cov: 27313 -> 27313
8513
Cov: 27313 -> 27313
Cov: 27313 -> 27313
8514
Cov: 27313 -> 27313
Cov: 27313 -> 27313
8515
Cov: 27313 -> 27313
Cov: 27313 -> 27313
8516
Cov: 27313 -> 27314
Cov: 27314 -> 27314
8517
Cov: 27314 -> 27314
Cov: 27314 -> 27314
8518
Cov: 27314 -> 27314
Cov: 27314 -> 27314
8519
Cov: 27314 -> 27314
Cov: 27314 -> 27314
8520
{"exception": "RuntimeError", "msg": "1D tensors expected, but got 2D and 2D tensors"}
8521
Cov: 27314 -> 27314
Cov: 27314 -> 27314
8522
Cov: 27314 -> 27315
Cov: 27315 -> 27315
8523
Cov: 27315 -> 27315
Cov: 27315 -> 27315
8524
Cov: 27315 -> 27315
Cov: 27315 -> 27315
8525
Cov: 27315 -> 27315
Cov: 27315 -> 27315
8526
Cov: 27315 -> 27315
Cov: 27315 -> 27315
8527
Cov: 27315 -> 27316
Cov: 27316 -> 27316
8528
Cov: 27316 -> 27316
Cov: 27316 -> 27316
8529
Cov: 27316 -> 27316
Cov: 27316 -> 27316
8530
Cov: 27316 -> 27317
Cov: 27317 -> 27317
8531
Cov: 27317 -> 27318
Cov: 27318 -> 27318
8532
Cov: 27318 -> 27318
Cov: 27318 -> 27318
8533
Cov: 27318 -> 27318
Cov: 27318 -> 27318
8534
Cov: 27318 -> 27318
Cov: 27318 -> 27318
8535
Cov: 27318 -> 27318
Cov: 27318 -> 27318
8536
Cov: 27318 -> 27318
Cov: 27318 -> 27318
8537
Cov: 27318 -> 27318
Cov: 27318 -> 27318
8538
{"exception": "NameError", "msg": "name 'other' is not defined"}
8539
Cov: 27318 -> 27318
Cov: 27318 -> 27318
8540
Cov: 27318 -> 27318
Cov: 27318 -> 27318
8541
Cov: 27318 -> 27323
Cov: 27323 -> 27323
8542
Cov: 27323 -> 27326
Cov: 27326 -> 27326
8543
Cov: 27326 -> 27326
Cov: 27326 -> 27326
8544
Cov: 27326 -> 27326
Cov: 27326 -> 27326
8545
{"exception": "RuntimeError", "msg": "This function was deprecated since version 1.9 and is now removed.\nPlease use the `torch.linalg.matrix_rank` function instead. The parameter 'symmetric' was renamed in `torch.linalg.matrix_rank()` to 'hermitian'."}
8546
Cov: 27326 -> 27328
Cov: 27328 -> 27328
8547
Cov: 27328 -> 27328
Cov: 27328 -> 27328
8548
Cov: 27328 -> 27328
Cov: 27328 -> 27328
8549
{"exception": "RuntimeError", "msg": "Index tensor must have the same number of dimensions as self tensor"}
8550
Cov: 27328 -> 27328
Cov: 27328 -> 27328
8551
Cov: 27328 -> 27329
Cov: 27329 -> 27329
8552
Cov: 27329 -> 27329
Cov: 27329 -> 27329
8553
Cov: 27329 -> 27329
Cov: 27329 -> 27329
8554
Cov: 27329 -> 27329
Cov: 27329 -> 27329
8555
Cov: 27329 -> 27329
Cov: 27329 -> 27329
8556
Cov: 27329 -> 27329
Cov: 27329 -> 27329
8557
Cov: 27329 -> 27329
Cov: 27329 -> 27329
8558
{"exception": "RuntimeError", "msg": "The size of tensor a (3) must match the size of tensor b (2) at non-singleton dimension 0"}
8559
Cov: 27329 -> 27330
Cov: 27330 -> 27330
8560
Cov: 27330 -> 27330
Cov: 27330 -> 27330
8561
Cov: 27330 -> 27330
Cov: 27330 -> 27330
8562
Cov: 27330 -> 27330
Cov: 27330 -> 27330
8563
Cov: 27330 -> 27330
Cov: 27330 -> 27330
8564
Cov: 27330 -> 27330
Cov: 27330 -> 27330
8565
Cov: 27330 -> 27330
Cov: 27330 -> 27330
8566
{"exception": "NameError", "msg": "name 'math' is not defined"}
8567
Cov: 27330 -> 27330
Cov: 27330 -> 27330
8568
Cov: 27330 -> 27330
Cov: 27330 -> 27330
8569
Cov: 27330 -> 27331
Cov: 27331 -> 27331
8570
Cov: 27331 -> 27331
Cov: 27331 -> 27331
8571
Cov: 27331 -> 27331
Cov: 27331 -> 27331
8572
Cov: 27331 -> 27331
Cov: 27331 -> 27331
8573
Cov: 27331 -> 27331
Cov: 27331 -> 27331
8574
Cov: 27331 -> 27331
Cov: 27331 -> 27331
8575
Cov: 27331 -> 27331
Cov: 27331 -> 27331
8576
Cov: 27331 -> 27332
Cov: 27332 -> 27332
8577
Cov: 27332 -> 27332
Cov: 27332 -> 27332
8578
Cov: 27332 -> 27332
Cov: 27332 -> 27332
8579
Cov: 27332 -> 27332
Cov: 27332 -> 27332
8580
Cov: 27332 -> 27332
Cov: 27332 -> 27332
8581
Cov: 27332 -> 27332
Cov: 27332 -> 27332
8582
Cov: 27332 -> 27332
Cov: 27332 -> 27332
8583
Cov: 27332 -> 27332
Cov: 27332 -> 27332
8584
Cov: 27332 -> 27332
Cov: 27332 -> 27332
8585
Cov: 27332 -> 27332
Cov: 27332 -> 27332
8586
Cov: 27332 -> 27333
Cov: 27333 -> 27333
8587
{"exception": "NameError", "msg": "name 't' is not defined"}
8588
Cov: 27333 -> 27333
Cov: 27333 -> 27333
8589
Cov: 27333 -> 27333
Cov: 27333 -> 27333
8590
Cov: 27333 -> 27333
Cov: 27333 -> 27333
8591
Cov: 27333 -> 27333
Cov: 27333 -> 27333
8592
Cov: 27333 -> 27333
Cov: 27333 -> 27333
8593
Cov: 27333 -> 27333
Cov: 27333 -> 27333
8594
Cov: 27333 -> 27333
Cov: 27333 -> 27333
8595
Cov: 27333 -> 27334
Cov: 27334 -> 27334
8596
Cov: 27334 -> 27334
Cov: 27334 -> 27334
8597
Cov: 27334 -> 27334
Cov: 27334 -> 27334
8598
Cov: 27334 -> 27335
Cov: 27335 -> 27335
8599
Cov: 27335 -> 27335
Cov: 27335 -> 27335
8600
Cov: 27335 -> 27335
Cov: 27335 -> 27335
8601
Cov: 27335 -> 27335
Cov: 27335 -> 27335
8602
Cov: 27335 -> 27335
Cov: 27335 -> 27335
8603
Cov: 27335 -> 27335
Cov: 27335 -> 27335
8604
Cov: 27335 -> 27335
Cov: 27335 -> 27335
8605
Cov: 27335 -> 27335
Cov: 27335 -> 27335
8606
Cov: 27335 -> 27335
Cov: 27335 -> 27335
8607
Cov: 27335 -> 27335
Cov: 27335 -> 27335
8608
Cov: 27335 -> 27335
Cov: 27335 -> 27335
8609
Cov: 27335 -> 27335
Cov: 27335 -> 27335
8610
Cov: 27335 -> 27335
Cov: 27335 -> 27335
8611
Cov: 27335 -> 27336
Cov: 27336 -> 27336
8612
Cov: 27336 -> 27337
Cov: 27337 -> 27337
8613
Cov: 27337 -> 27337
Cov: 27337 -> 27337
8614
Cov: 27337 -> 27337
Cov: 27337 -> 27337
8615
Cov: 27337 -> 27337
Cov: 27337 -> 27337
8616
Cov: 27337 -> 27338
Cov: 27338 -> 27338
8617
Cov: 27338 -> 27338
Cov: 27338 -> 27338
8618
Cov: 27338 -> 27338
Cov: 27338 -> 27338
8619
Cov: 27338 -> 27339
Cov: 27339 -> 27339
8620
Cov: 27339 -> 27339
Cov: 27339 -> 27339
8621
Cov: 27339 -> 27340
Cov: 27340 -> 27340
8622
Cov: 27340 -> 27340
Cov: 27340 -> 27340
8623
{"exception": "TypeError", "msg": "'getset_descriptor' object is not callable"}
8624
Cov: 27340 -> 27342
Cov: 27342 -> 27342
8625
Cov: 27342 -> 27342
Cov: 27342 -> 27342
8626
Cov: 27342 -> 27343
Cov: 27343 -> 27343
8627
Cov: 27343 -> 27343
Cov: 27343 -> 27343
8628
Cov: 27343 -> 27344
Cov: 27344 -> 27344
8629
Cov: 27344 -> 27345
Cov: 27345 -> 27345
8630
Cov: 27345 -> 27345
Cov: 27345 -> 27345
8631
Cov: 27345 -> 27345
Cov: 27345 -> 27345
8632
Cov: 27345 -> 27345
Cov: 27345 -> 27345
8633
Cov: 27345 -> 27346
Cov: 27346 -> 27346
8634
{"exception": "RuntimeError", "msg": "masked_select: expected BoolTensor for mask"}
8635
Cov: 27346 -> 27346
Cov: 27346 -> 27346
8636
Cov: 27346 -> 27346
Cov: 27346 -> 27346
8637
Cov: 27346 -> 27346
Cov: 27346 -> 27346
8638
Cov: 27346 -> 27346
Cov: 27346 -> 27346
8639
Cov: 27346 -> 27347
Cov: 27347 -> 27347
8640
Cov: 27347 -> 27348
Cov: 27348 -> 27348
8641
Cov: 27348 -> 27348
Cov: 27348 -> 27348
8642
{"exception": "NotImplementedError", "msg": "Could not run 'aten::q_scale' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::q_scale' is only available for these backends: [QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nQuantizedCPU: registered at aten/src/ATen/RegisterQuantizedCPU.cpp:944 [kernel]\nQuantizedCUDA: registered at aten/src/ATen/RegisterQuantizedCUDA.cpp:459 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:153 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:498 [backend fallback]\nFunctionalize: registered at ../aten/src/ATen/FunctionalizeFallbackKernel.cpp:290 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:86 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradMTIA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_4.cpp:13056 [kernel]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:382 [backend fallback]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:249 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:710 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:203 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:161 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:494 [backend fallback]\nPreDispatch: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:165 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:157 [backend fallback]\n"}
8643
Cov: 27348 -> 27348
Cov: 27348 -> 27348
8644
Cov: 27348 -> 27348
Cov: 27348 -> 27348
8645
Cov: 27348 -> 27348
Cov: 27348 -> 27348
8646
Cov: 27348 -> 27348
Cov: 27348 -> 27348
8647
Cov: 27348 -> 27348
Cov: 27348 -> 27348
8648
Cov: 27348 -> 27348
Cov: 27348 -> 27348
8649
Cov: 27348 -> 27348
Cov: 27348 -> 27348
8650
Cov: 27348 -> 27348
Cov: 27348 -> 27348
8651
Cov: 27348 -> 27348
Cov: 27348 -> 27348
8652
Cov: 27348 -> 27348
Cov: 27348 -> 27348
8653
Cov: 27348 -> 27349
Cov: 27349 -> 27349
8654
Cov: 27349 -> 27349
Cov: 27349 -> 27349
8655
Cov: 27349 -> 27350
Cov: 27350 -> 27350
8656
Cov: 27350 -> 27350
Cov: 27350 -> 27350
8657
Cov: 27350 -> 27350
Cov: 27350 -> 27350
8658
Cov: 27350 -> 27358
Cov: 27358 -> 27358
8659
Cov: 27358 -> 27358
Cov: 27358 -> 27358
8660
Cov: 27358 -> 27358
Cov: 27358 -> 27358
8661
Cov: 27358 -> 27359
Cov: 27359 -> 27359
8662
Cov: 27359 -> 27359
Cov: 27359 -> 27359
8663
Cov: 27359 -> 27359
Cov: 27359 -> 27359
8664
Cov: 27359 -> 27359
Cov: 27359 -> 27359
8665
Cov: 27359 -> 27371
Cov: 27371 -> 27371
8666
Cov: 27371 -> 27371
Cov: 27371 -> 27371
8667
Cov: 27371 -> 27372
Cov: 27372 -> 27372
8668
Cov: 27372 -> 27372
Cov: 27372 -> 27372
8669
Cov: 27372 -> 27373
Cov: 27373 -> 27373
8670
Cov: 27373 -> 27373
Cov: 27373 -> 27373
8671
Cov: 27373 -> 27373
Cov: 27373 -> 27373
8672
Cov: 27373 -> 27373
Cov: 27373 -> 27373
8673
Cov: 27373 -> 27374
Cov: 27374 -> 27374
8674
Cov: 27374 -> 27374
Cov: 27374 -> 27374
8675
Cov: 27374 -> 27374
Cov: 27374 -> 27374
8676
Cov: 27374 -> 27374
Cov: 27374 -> 27374
8677
Cov: 27374 -> 27374
Cov: 27374 -> 27374
8678
Cov: 27374 -> 27374
Cov: 27374 -> 27374
8679
Cov: 27374 -> 27374
Cov: 27374 -> 27374
8680
Cov: 27374 -> 27374
Cov: 27374 -> 27374
8681
{"exception": "RuntimeError", "msg": "istft requires a complex-valued input tensor matching the output from stft with return_complex=True."}
8682
Cov: 27374 -> 27375
Cov: 27375 -> 27375
8683
Cov: 27375 -> 27375
Cov: 27375 -> 27375
8684
Cov: 27375 -> 27375
Cov: 27375 -> 27375
8685
Cov: 27375 -> 27375
Cov: 27375 -> 27375
8686
Cov: 27375 -> 27376
Cov: 27376 -> 27376
8687
Cov: 27376 -> 27377
Cov: 27377 -> 27377
8688
Cov: 27377 -> 27378
Cov: 27378 -> 27378
8689
Cov: 27378 -> 27378
Cov: 27378 -> 27378
8690
{"exception": "RuntimeError", "msg": "result type Float can't be cast to the desired output type Long"}
8691
Cov: 27378 -> 27378
Cov: 27378 -> 27378
8692
Cov: 27378 -> 27378
Cov: 27378 -> 27378
8693
Cov: 27378 -> 27378
Cov: 27378 -> 27378
8694
Cov: 27378 -> 27378
Cov: 27378 -> 27378
8695
Cov: 27378 -> 27378
Cov: 27378 -> 27378
8696
Cov: 27378 -> 27378
Cov: 27378 -> 27378
8697
Cov: 27378 -> 27378
Cov: 27378 -> 27378
8698
Cov: 27378 -> 27378
Cov: 27378 -> 27378
8699
{"exception": "NotImplementedError", "msg": "Could not run 'aten::q_scale' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::q_scale' is only available for these backends: [QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nQuantizedCPU: registered at aten/src/ATen/RegisterQuantizedCPU.cpp:944 [kernel]\nQuantizedCUDA: registered at aten/src/ATen/RegisterQuantizedCUDA.cpp:459 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:153 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:498 [backend fallback]\nFunctionalize: registered at ../aten/src/ATen/FunctionalizeFallbackKernel.cpp:290 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:86 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradMTIA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_4.cpp:13056 [kernel]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:382 [backend fallback]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:249 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:710 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:203 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:161 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:494 [backend fallback]\nPreDispatch: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:165 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:157 [backend fallback]\n"}
8700
Cov: 27378 -> 27378
Cov: 27378 -> 27378
8701
Cov: 27378 -> 27378
Cov: 27378 -> 27378
8702
Cov: 27378 -> 27378
Cov: 27378 -> 27378
8703
Cov: 27378 -> 27378
Cov: 27378 -> 27378
8704
Cov: 27378 -> 27378
Cov: 27378 -> 27378
8705
Cov: 27378 -> 27378
Cov: 27378 -> 27378
8706
Cov: 27378 -> 27378
Cov: 27378 -> 27378
8707
Cov: 27378 -> 27379
Cov: 27379 -> 27379
8708
Cov: 27379 -> 27380
Cov: 27380 -> 27380
8709
Cov: 27380 -> 27380
Cov: 27380 -> 27380
8710
Cov: 27380 -> 27381
Cov: 27381 -> 27381
8711
Cov: 27381 -> 27382
Cov: 27382 -> 27382
8712
{"exception": "RuntimeError", "msg": "This function was deprecated since version 1.9 and is now removed. `torch.linalg.eig` returns complex tensors of dtype `cfloat` or `cdouble` rather than real tensors mimicking complex tensors.\n\nL, _ = torch.eig(A) should be replaced with:\nL_complex = torch.linalg.eigvals(A)\n\nand\n\nL, V = torch.eig(A, eigenvectors=True) should be replaced with:\nL_complex, V_complex = torch.linalg.eig(A)"}
8713
Cov: 27382 -> 27385
Cov: 27385 -> 27385
8714
Cov: 27385 -> 27385
Cov: 27385 -> 27385
8715
Cov: 27385 -> 27385
Cov: 27385 -> 27385
8716
Cov: 27385 -> 27385
Cov: 27385 -> 27385
8717
Cov: 27385 -> 27385
Cov: 27385 -> 27385
8718
Cov: 27385 -> 27386
Cov: 27386 -> 27386
8719
Cov: 27386 -> 27386
Cov: 27386 -> 27386
8720
Cov: 27386 -> 27387
Cov: 27387 -> 27387
8721
Cov: 27387 -> 27388
Cov: 27388 -> 27388
8722
Cov: 27388 -> 27388
Cov: 27388 -> 27388
8723
Cov: 27388 -> 27388
Cov: 27388 -> 27388
8724
Cov: 27388 -> 27389
Cov: 27389 -> 27389
8725
Cov: 27389 -> 27389
Cov: 27389 -> 27389
8726
Cov: 27389 -> 27389
Cov: 27389 -> 27389
8727
Cov: 27389 -> 27389
Cov: 27389 -> 27389
8728
Cov: 27389 -> 27390
Cov: 27390 -> 27390
8729
Cov: 27390 -> 27390
Cov: 27390 -> 27390
8730
Cov: 27390 -> 27390
Cov: 27390 -> 27390
8731
Cov: 27390 -> 27390
Cov: 27390 -> 27390
8732
Cov: 27390 -> 27390
Cov: 27390 -> 27390
8733
Cov: 27390 -> 27390
Cov: 27390 -> 27390
8734
Cov: 27390 -> 27390
Cov: 27390 -> 27390
8735
Cov: 27390 -> 27390
Cov: 27390 -> 27390
8736
Cov: 27390 -> 27390
Cov: 27390 -> 27390
8737
Cov: 27390 -> 27391
Cov: 27391 -> 27391
8738
Cov: 27391 -> 27391
Cov: 27391 -> 27391
8739
Cov: 27391 -> 27391
Cov: 27391 -> 27391
8740
Cov: 27391 -> 27391
Cov: 27391 -> 27391
8741
Cov: 27391 -> 27391
Cov: 27391 -> 27391
8742
Cov: 27391 -> 27391
Cov: 27391 -> 27391
8743
Cov: 27391 -> 27391
Cov: 27391 -> 27391
8744
Cov: 27391 -> 27391
Cov: 27391 -> 27391
8745
{"exception": "TypeError", "msg": "type_as(): argument 'other' (position 1) must be Tensor, not torch.tensortype"}
8746
Cov: 27391 -> 27392
Cov: 27392 -> 27392
8747
Cov: 27392 -> 27392
Cov: 27392 -> 27392
8748
Cov: 27392 -> 27392
Cov: 27392 -> 27392
8749
Cov: 27392 -> 27392
Cov: 27392 -> 27392
8750
Cov: 27392 -> 27392
Cov: 27392 -> 27392
8751
Cov: 27392 -> 27392
Cov: 27392 -> 27392
8752
Cov: 27392 -> 27392
Cov: 27392 -> 27392
8753
{"exception": "RuntimeError", "msg": "\"lerp_kernel_scalar\" not implemented for 'Long'"}
8754
Cov: 27392 -> 27392
Cov: 27392 -> 27392
8755
Cov: 27392 -> 27392
Cov: 27392 -> 27392
8756
Cov: 27392 -> 27392
Cov: 27392 -> 27392
8757
Cov: 27392 -> 27393
Cov: 27393 -> 27393
8758
Cov: 27393 -> 27393
Cov: 27393 -> 27393
8759
Cov: 27393 -> 27394
Cov: 27394 -> 27394
8760
Cov: 27394 -> 27394
Cov: 27394 -> 27394
8761
Cov: 27394 -> 27394
Cov: 27394 -> 27394
8762
Cov: 27394 -> 27394
Cov: 27394 -> 27394
8763
Cov: 27394 -> 27394
Cov: 27394 -> 27394
8764
Cov: 27394 -> 27394
Cov: 27394 -> 27394
8765
{"exception": "TypeError", "msg": "'getset_descriptor' object is not callable"}
8766
Cov: 27394 -> 27394
Cov: 27394 -> 27394
8767
{"exception": "TypeError", "msg": "index_add_() missing 1 required positional arguments: \"source\""}
8768
Cov: 27394 -> 27394
Cov: 27394 -> 27394
8769
Cov: 27394 -> 27394
Cov: 27394 -> 27394
8770
Cov: 27394 -> 27394
Cov: 27394 -> 27394
8771
{"exception": "NotImplementedError", "msg": "Could not run 'aten::qscheme' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::qscheme' is only available for these backends: [QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nQuantizedCPU: registered at aten/src/ATen/RegisterQuantizedCPU.cpp:944 [kernel]\nQuantizedCUDA: registered at aten/src/ATen/RegisterQuantizedCUDA.cpp:459 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:153 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:498 [backend fallback]\nFunctionalize: registered at ../aten/src/ATen/FunctionalizeFallbackKernel.cpp:290 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:86 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradMTIA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_0.cpp:16725 [kernel]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:382 [backend fallback]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:249 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:710 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:203 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:161 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:494 [backend fallback]\nPreDispatch: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:165 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:157 [backend fallback]\n"}
8772
Cov: 27394 -> 27395
Cov: 27395 -> 27395
8773
Cov: 27395 -> 27396
Cov: 27396 -> 27396
8774
Cov: 27396 -> 27396
Cov: 27396 -> 27396
8775
{"exception": "TypeError", "msg": "geometric_(): argument 'p' (position 1) must be float, not Tensor"}
8776
Cov: 27396 -> 27397
Cov: 27397 -> 27397
8777
Cov: 27397 -> 27398
Cov: 27398 -> 27398
8778
{"exception": "TypeError", "msg": "to_sparse() received an invalid combination of arguments - got (sparse_dims=int, ), but expected one of:\n * (*, torch.layout layout, tuple of ints blocksize, int dense_dim)\n * (int sparse_dim)\n      didn't match because some of the keywords were incorrect: sparse_dims\n"}
8779
Cov: 27398 -> 27398
Cov: 27398 -> 27398
8780
Cov: 27398 -> 27398
Cov: 27398 -> 27398
8781
Cov: 27398 -> 27399
Cov: 27399 -> 27399
8782
Cov: 27399 -> 27399
Cov: 27399 -> 27399
8783
Cov: 27399 -> 27399
Cov: 27399 -> 27399
8784
Cov: 27399 -> 27400
Cov: 27400 -> 27400
8785
Cov: 27400 -> 27400
Cov: 27400 -> 27400
8786
Cov: 27400 -> 27400
Cov: 27400 -> 27400
8787
Cov: 27400 -> 27400
Cov: 27400 -> 27400
8788
{"exception": "RuntimeError", "msg": "This function was deprecated since version 1.9 and is now removed. `torch.lstsq` is deprecated in favor of `torch.linalg.lstsq`.\n`torch.linalg.lstsq` has reversed arguments and does not return the QR decomposition in the returned tuple (although it returns other information about the problem).\n\nTo get the QR decomposition consider using `torch.linalg.qr`.\n\nThe returned solution in `torch.lstsq` stored the residuals of the solution in the last m - n columns of the returned value whenever m > n. In torch.linalg.lstsq, the residuals are in the field 'residuals' of the returned named tuple.\n\nThe unpacking of the solution, as in\nX, _ = torch.lstsq(B, A).solution[:A.size(1)]\nshould be replaced with:\nX = torch.linalg.lstsq(A, B).solution"}
8789
Cov: 27400 -> 27400
Cov: 27400 -> 27400
8790
Cov: 27400 -> 27400
Cov: 27400 -> 27400
8791
Cov: 27400 -> 27401
Cov: 27401 -> 27401
8792
Cov: 27401 -> 27401
Cov: 27401 -> 27401
8793
Cov: 27401 -> 27401
Cov: 27401 -> 27401
8794
Cov: 27401 -> 27401
Cov: 27401 -> 27401
8795
{"exception": "_LinAlgError", "msg": "torch.linalg.solve: The solver failed because the input matrix is singular."}
8796
{"exception": "RuntimeError", "msg": "scatter(): Expected self.dtype to be equal to src.dtype"}
8797
Cov: 27401 -> 27402
Cov: 27402 -> 27402
8798
Cov: 27402 -> 27403
Cov: 27403 -> 27403
8799
Cov: 27403 -> 27404
Cov: 27404 -> 27404
8800
Cov: 27404 -> 27404
Cov: 27404 -> 27404
8801
Cov: 27404 -> 27404
Cov: 27404 -> 27404
8802
Cov: 27404 -> 27404
Cov: 27404 -> 27404
8803
Cov: 27404 -> 27404
Cov: 27404 -> 27404
8804
Cov: 27404 -> 27404
Cov: 27404 -> 27404
8805
Cov: 27404 -> 27404
Cov: 27404 -> 27404
8806
Cov: 27404 -> 27457
Cov: 27457 -> 27457
8807
Cov: 27457 -> 27457
Cov: 27457 -> 27457
8808
Cov: 27457 -> 27457
Cov: 27457 -> 27457
8809
Cov: 27457 -> 27457
Cov: 27457 -> 27457
8810
Cov: 27457 -> 27457
Cov: 27457 -> 27457
8811
Cov: 27457 -> 27458
Cov: 27458 -> 27458
8812
Cov: 27458 -> 27458
Cov: 27458 -> 27458
8813
Cov: 27458 -> 27458
Cov: 27458 -> 27458
8814
Cov: 27458 -> 27458
Cov: 27458 -> 27458
8815
Cov: 27458 -> 27458
Cov: 27458 -> 27458
8816
Cov: 27458 -> 27458
Cov: 27458 -> 27458
8817
Cov: 27458 -> 27459
Cov: 27459 -> 27459
8818
Cov: 27459 -> 27460
Cov: 27460 -> 27460
8819
Cov: 27460 -> 27460
Cov: 27460 -> 27460
8820
Cov: 27460 -> 27460
Cov: 27460 -> 27460
8821
Cov: 27460 -> 27461
Cov: 27461 -> 27461
8822
Cov: 27461 -> 27461
Cov: 27461 -> 27461
8823
Cov: 27461 -> 27462
Cov: 27462 -> 27462
8824
Cov: 27462 -> 27462
Cov: 27462 -> 27462
8825
Cov: 27462 -> 27463
Cov: 27463 -> 27463
8826
Cov: 27463 -> 27463
Cov: 27463 -> 27463
8827
Cov: 27463 -> 27463
Cov: 27463 -> 27463
8828
Cov: 27463 -> 27463
Cov: 27463 -> 27463
8829
Cov: 27463 -> 27463
Cov: 27463 -> 27463
8830
Cov: 27463 -> 27464
Cov: 27464 -> 27464
8831
Cov: 27464 -> 27464
Cov: 27464 -> 27464
8832
{"exception": "TypeError", "msg": "'getset_descriptor' object is not callable"}
8833
{"exception": "RuntimeError", "msg": "scatter(): Expected self.dtype to be equal to src.dtype"}
8834
Cov: 27464 -> 27464
Cov: 27464 -> 27464
8835
{"exception": "RuntimeError", "msg": "a leaf Variable that requires grad is being used in an in-place operation."}
8836
Cov: 27464 -> 27464
Cov: 27464 -> 27464
8837
Cov: 27464 -> 27464
Cov: 27464 -> 27464
8838
Cov: 27464 -> 27464
Cov: 27464 -> 27464
8839
{"exception": "RuntimeError", "msg": "masked_fill_ only supports boolean masks, but got mask with dtype unsigned char"}
8840
Cov: 27464 -> 27464
Cov: 27464 -> 27464
8841
Cov: 27464 -> 27465
Cov: 27465 -> 27465
8842
Cov: 27465 -> 27466
Cov: 27466 -> 27466
8843
Cov: 27466 -> 27466
Cov: 27466 -> 27466
8844
Cov: 27466 -> 27466
Cov: 27466 -> 27466
8845
Cov: 27466 -> 27466
Cov: 27466 -> 27466
8846
Cov: 27466 -> 27466
Cov: 27466 -> 27466
8847
Cov: 27466 -> 27467
Cov: 27467 -> 27467
8848
Cov: 27467 -> 27467
Cov: 27467 -> 27467
8849
Cov: 27467 -> 27467
Cov: 27467 -> 27467
8850
Cov: 27467 -> 27467
Cov: 27467 -> 27467
8851
Cov: 27467 -> 27467
Cov: 27467 -> 27467
8852
Cov: 27467 -> 27468
Cov: 27468 -> 27468
8853
Cov: 27468 -> 27468
Cov: 27468 -> 27468
8854
Cov: 27468 -> 27469
Cov: 27469 -> 27469
8855
Cov: 27469 -> 27469
Cov: 27469 -> 27469
8856
Cov: 27469 -> 27469
Cov: 27469 -> 27469
8857
Cov: 27469 -> 27470
Cov: 27470 -> 27470
8858
Cov: 27470 -> 27470
Cov: 27470 -> 27470
8859
Cov: 27470 -> 27470
Cov: 27470 -> 27470
8860
Cov: 27470 -> 27471
Cov: 27471 -> 27471
8861
Cov: 27471 -> 27472
Cov: 27472 -> 27472
8862
Cov: 27472 -> 27472
Cov: 27472 -> 27472
8863
Cov: 27472 -> 27472
Cov: 27472 -> 27472
8864
Cov: 27472 -> 27472
Cov: 27472 -> 27472
8865
Cov: 27472 -> 27472
Cov: 27472 -> 27472
8866
Cov: 27472 -> 27472
Cov: 27472 -> 27472
8867
Cov: 27472 -> 27472
Cov: 27472 -> 27472
8868
Cov: 27472 -> 27473
Cov: 27473 -> 27473
8869
Cov: 27473 -> 27473
Cov: 27473 -> 27473
8870
Cov: 27473 -> 27473
Cov: 27473 -> 27473
8871
Cov: 27473 -> 27473
Cov: 27473 -> 27473
8872
Cov: 27473 -> 27473
Cov: 27473 -> 27473
8873
Cov: 27473 -> 27473
Cov: 27473 -> 27473
8874
{"exception": "RuntimeError", "msg": "vector + matrix @ vector expected, got 2, 2, 1"}
8875
Cov: 27473 -> 27473
Cov: 27473 -> 27473
8876
Cov: 27473 -> 27476
Cov: 27476 -> 27476
8877
Cov: 27476 -> 27476
Cov: 27476 -> 27476
8878
Cov: 27476 -> 27476
Cov: 27476 -> 27476
8879
Cov: 27476 -> 27476
Cov: 27476 -> 27476
8880
Cov: 27476 -> 27476
Cov: 27476 -> 27476
8881
Cov: 27476 -> 27476
Cov: 27476 -> 27476
8882
Cov: 27476 -> 27476
Cov: 27476 -> 27476
8883
Cov: 27476 -> 27476
Cov: 27476 -> 27476
8884
Cov: 27476 -> 27476
Cov: 27476 -> 27476
8885
Cov: 27476 -> 27477
Cov: 27477 -> 27477
8886
Cov: 27477 -> 27477
Cov: 27477 -> 27477
8887
{"exception": "RuntimeError", "msg": "Please look up dimensions by name, got: name = None."}
8888
Cov: 27477 -> 27477
Cov: 27477 -> 27477
8889
Cov: 27477 -> 27477
Cov: 27477 -> 27477
8890
Cov: 27477 -> 27477
Cov: 27477 -> 27477
8891
{"exception": "RuntimeError", "msg": "all: You passed a dimname (string) to this op in place of a dimension index but it does not yet support this behavior. Please pass a dimension index to work around this."}
8892
Cov: 27477 -> 27478
Cov: 27478 -> 27478
8893
Cov: 27478 -> 27478
Cov: 27478 -> 27478
8894
Cov: 27478 -> 27478
Cov: 27478 -> 27478
8895
Cov: 27478 -> 27478
Cov: 27478 -> 27478
8896
{"exception": "_LinAlgError", "msg": "linalg.inv: The diagonal element 3 is zero, the inversion could not be completed because the input matrix is singular."}
8897
Cov: 27478 -> 27479
Cov: 27479 -> 27479
8898
Cov: 27479 -> 27479
Cov: 27479 -> 27479
8899
Cov: 27479 -> 27483
Cov: 27483 -> 27483
8900
Cov: 27483 -> 27483
Cov: 27483 -> 27483
8901
Cov: 27483 -> 27483
Cov: 27483 -> 27483
8902
Cov: 27483 -> 27483
Cov: 27483 -> 27483
8903
Cov: 27483 -> 27483
Cov: 27483 -> 27483
8904
Cov: 27483 -> 27483
Cov: 27483 -> 27483
8905
Cov: 27483 -> 27483
Cov: 27483 -> 27483
8906
Cov: 27483 -> 27483
Cov: 27483 -> 27483
8907
Cov: 27483 -> 27483
Cov: 27483 -> 27483
8908
Cov: 27483 -> 27485
Cov: 27485 -> 27485
8909
Cov: 27485 -> 27486
Cov: 27486 -> 27486
8910
Cov: 27486 -> 27486
Cov: 27486 -> 27486
8911
Cov: 27486 -> 27486
Cov: 27486 -> 27486
8912
Cov: 27486 -> 27486
Cov: 27486 -> 27486
8913
Cov: 27486 -> 27486
Cov: 27486 -> 27486
8914
{"exception": "RuntimeError", "msg": "the base given to float_power_ has dtype Float but the operation's result requires dtype Double"}
8915
Cov: 27486 -> 27486
Cov: 27486 -> 27486
8916
Cov: 27486 -> 27486
Cov: 27486 -> 27486
8917
Cov: 27486 -> 27486
Cov: 27486 -> 27486
8918
Cov: 27486 -> 27486
Cov: 27486 -> 27486
8919
Cov: 27486 -> 27486
Cov: 27486 -> 27486
8920
Cov: 27486 -> 27487
Cov: 27487 -> 27487
8921
Cov: 27487 -> 27488
Cov: 27488 -> 27488
8922
Cov: 27488 -> 27489
Cov: 27489 -> 27489
8923
Cov: 27489 -> 27489
Cov: 27489 -> 27489
8924
Cov: 27489 -> 27489
Cov: 27489 -> 27489
8925
Cov: 27489 -> 27490
Cov: 27490 -> 27490
8926
Cov: 27490 -> 27490
Cov: 27490 -> 27490
8927
Cov: 27490 -> 27490
Cov: 27490 -> 27490
8928
Cov: 27490 -> 27490
Cov: 27490 -> 27490
8929
Cov: 27490 -> 27490
Cov: 27490 -> 27490
8930
{"exception": "RuntimeError", "msg": "size {[2, 3]} is not expandable to size {[3, 4]}."}
8931
Cov: 27490 -> 27490
Cov: 27490 -> 27490
8932
Cov: 27490 -> 27490
Cov: 27490 -> 27490
8933
Cov: 27490 -> 27491
Cov: 27491 -> 27491
8934
Cov: 27491 -> 27491
Cov: 27491 -> 27491
8935
Cov: 27491 -> 27491
Cov: 27491 -> 27491
8936
Cov: 27491 -> 27491
Cov: 27491 -> 27491
8937
{"exception": "RuntimeError", "msg": "\"rshift_cpu\" not implemented for 'Double'"}
8938
Cov: 27491 -> 27491
Cov: 27491 -> 27491
8939
Cov: 27491 -> 27492
Cov: 27492 -> 27492
8940
Cov: 27492 -> 27493
Cov: 27493 -> 27493
8941
Cov: 27493 -> 27493
Cov: 27493 -> 27493
8942
Cov: 27493 -> 27493
Cov: 27493 -> 27493
8943
Cov: 27493 -> 27493
Cov: 27493 -> 27493
8944
Cov: 27493 -> 27493
Cov: 27493 -> 27493
8945
Cov: 27493 -> 27494
Cov: 27494 -> 27494
8946
Cov: 27494 -> 27494
Cov: 27494 -> 27494
8947
Cov: 27494 -> 27494
Cov: 27494 -> 27494
8948
Cov: 27494 -> 27494
Cov: 27494 -> 27494
8949
Cov: 27494 -> 27494
Cov: 27494 -> 27494
8950
Cov: 27494 -> 27495
Cov: 27495 -> 27495
8951
Cov: 27495 -> 27495
Cov: 27495 -> 27495
8952
Cov: 27495 -> 27495
Cov: 27495 -> 27495
8953
Cov: 27495 -> 27495
Cov: 27495 -> 27495
8954
Cov: 27495 -> 27495
Cov: 27495 -> 27495
8955
Cov: 27495 -> 27496
Cov: 27496 -> 27496
8956
Cov: 27496 -> 27496
Cov: 27496 -> 27496
8957
Cov: 27496 -> 27496
Cov: 27496 -> 27496
8958
Cov: 27496 -> 27496
Cov: 27496 -> 27496
8959
Cov: 27496 -> 27496
Cov: 27496 -> 27496
8960
Cov: 27496 -> 27496
Cov: 27496 -> 27496
8961
Cov: 27496 -> 27497
Cov: 27497 -> 27497
8962
Cov: 27497 -> 27497
Cov: 27497 -> 27497
8963
Cov: 27497 -> 27497
Cov: 27497 -> 27497
8964
Cov: 27497 -> 27497
Cov: 27497 -> 27497
8965
Cov: 27497 -> 27497
Cov: 27497 -> 27497
8966
{"exception": "RuntimeError", "msg": "mat1 and mat2 shapes cannot be multiplied (2x3 and 2x3)"}
8967
Cov: 27497 -> 27497
Cov: 27497 -> 27497
8968
{"exception": "NotImplementedError", "msg": "Could not run 'aten::sparse_mask' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::sparse_mask' is only available for these backends: [SparseCPU, SparseCUDA, SparseCsrCPU, SparseCsrCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nSparseCPU: registered at aten/src/ATen/RegisterSparseCPU.cpp:1387 [kernel]\nSparseCUDA: registered at aten/src/ATen/RegisterSparseCUDA.cpp:1573 [kernel]\nSparseCsrCPU: registered at aten/src/ATen/RegisterSparseCsrCPU.cpp:1135 [kernel]\nSparseCsrCUDA: registered at aten/src/ATen/RegisterSparseCsrCUDA.cpp:1276 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:153 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:498 [backend fallback]\nFunctionalize: registered at ../aten/src/ATen/FunctionalizeFallbackKernel.cpp:290 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:86 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradMTIA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_0.cpp:16725 [kernel]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:382 [backend fallback]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:249 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:710 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:203 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:161 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:494 [backend fallback]\nPreDispatch: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:165 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:157 [backend fallback]\n"}
8969
Cov: 27497 -> 27497
Cov: 27497 -> 27497
8970
Cov: 27497 -> 27497
Cov: 27497 -> 27497
8971
Cov: 27497 -> 27498
Cov: 27498 -> 27498
8972
Cov: 27498 -> 27498
Cov: 27498 -> 27498
8973
Cov: 27498 -> 27499
Cov: 27499 -> 27499
8974
Cov: 27499 -> 27499
Cov: 27499 -> 27499
8975
{"exception": "RuntimeError", "msg": "nanmean(): expected input to have floating point or complex dtype but got Long"}
8976
Cov: 27499 -> 27499
Cov: 27499 -> 27499
8977
Cov: 27499 -> 27499
Cov: 27499 -> 27499
8978
Cov: 27499 -> 27499
Cov: 27499 -> 27499
8979
Cov: 27499 -> 27499
Cov: 27499 -> 27499
8980
Cov: 27499 -> 27499
Cov: 27499 -> 27499
8981
Cov: 27499 -> 27500
Cov: 27500 -> 27500
8982
Cov: 27500 -> 27500
Cov: 27500 -> 27500
8983
Cov: 27500 -> 27500
Cov: 27500 -> 27500
8984
Cov: 27500 -> 27500
Cov: 27500 -> 27500
8985
Cov: 27500 -> 27500
Cov: 27500 -> 27500
8986
Cov: 27500 -> 27500
Cov: 27500 -> 27500
8987
Cov: 27500 -> 27500
Cov: 27500 -> 27500
8988
Cov: 27500 -> 27500
Cov: 27500 -> 27500
8989
Cov: 27500 -> 27502
Cov: 27502 -> 27502
8990
Cov: 27502 -> 27502
Cov: 27502 -> 27502
8991
Cov: 27502 -> 27502
Cov: 27502 -> 27502
8992
Cov: 27502 -> 27502
Cov: 27502 -> 27502
8993
Cov: 27502 -> 27502
Cov: 27502 -> 27502
8994
Cov: 27502 -> 27502
Cov: 27502 -> 27502
8995
Cov: 27502 -> 27502
Cov: 27502 -> 27502
8996
Cov: 27502 -> 27502
Cov: 27502 -> 27502
8997
Cov: 27502 -> 27502
Cov: 27502 -> 27502
8998
Cov: 27502 -> 27503
Cov: 27503 -> 27503
8999
Cov: 27503 -> 27503
Cov: 27503 -> 27503
9000
Cov: 27503 -> 27503
Cov: 27503 -> 27503
9001
Cov: 27503 -> 27509
Cov: 27509 -> 27509
9002
Cov: 27509 -> 27509
Cov: 27509 -> 27509
9003
Cov: 27509 -> 27509
Cov: 27509 -> 27509
9004
Cov: 27509 -> 27509
Cov: 27509 -> 27509
9005
Cov: 27509 -> 27510
Cov: 27510 -> 27510
9006
Cov: 27510 -> 27510
Cov: 27510 -> 27510
9007
{"exception": "NotImplementedError", "msg": "Could not run 'aten::q_scale' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::q_scale' is only available for these backends: [QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nQuantizedCPU: registered at aten/src/ATen/RegisterQuantizedCPU.cpp:944 [kernel]\nQuantizedCUDA: registered at aten/src/ATen/RegisterQuantizedCUDA.cpp:459 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:153 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:498 [backend fallback]\nFunctionalize: registered at ../aten/src/ATen/FunctionalizeFallbackKernel.cpp:290 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:86 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradMTIA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_4.cpp:13056 [kernel]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:382 [backend fallback]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:249 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:710 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:203 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:161 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:494 [backend fallback]\nPreDispatch: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:165 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:157 [backend fallback]\n"}
9008
Cov: 27510 -> 27510
Cov: 27510 -> 27510
9009
Cov: 27510 -> 27510
Cov: 27510 -> 27510
9010
Cov: 27510 -> 27511
Cov: 27511 -> 27511
9011
Cov: 27511 -> 27511
Cov: 27511 -> 27511
9012
Cov: 27511 -> 27511
Cov: 27511 -> 27511
9013
Cov: 27511 -> 27511
Cov: 27511 -> 27511
9014
Cov: 27511 -> 27511
Cov: 27511 -> 27511
9015
Cov: 27511 -> 27511
Cov: 27511 -> 27511
9016
Cov: 27511 -> 27511
Cov: 27511 -> 27511
9017
Cov: 27511 -> 27511
Cov: 27511 -> 27511
9018
Cov: 27511 -> 27511
Cov: 27511 -> 27511
9019
Cov: 27511 -> 27511
Cov: 27511 -> 27511
9020
Cov: 27511 -> 27511
Cov: 27511 -> 27511
9021
Cov: 27511 -> 27512
Cov: 27512 -> 27512
9022
Cov: 27512 -> 27512
Cov: 27512 -> 27512
9023
Cov: 27512 -> 27512
Cov: 27512 -> 27512
9024
Cov: 27512 -> 27512
Cov: 27512 -> 27512
9025
Cov: 27512 -> 27512
Cov: 27512 -> 27512
9026
Cov: 27512 -> 27513
Cov: 27513 -> 27513
9027
Cov: 27513 -> 27532
Cov: 27532 -> 27532
9028
Cov: 27532 -> 27532
Cov: 27532 -> 27532
9029
Cov: 27532 -> 27535
Cov: 27535 -> 27535
9030
Cov: 27535 -> 27535
Cov: 27535 -> 27535
9031
Cov: 27535 -> 27536
Cov: 27536 -> 27536
9032
Cov: 27536 -> 27536
Cov: 27536 -> 27536
9033
Cov: 27536 -> 27537
Cov: 27537 -> 27537
9034
Cov: 27537 -> 27537
Cov: 27537 -> 27537
9035
Cov: 27537 -> 27537
Cov: 27537 -> 27537
9036
Cov: 27537 -> 27537
Cov: 27537 -> 27537
9037
Cov: 27537 -> 27537
Cov: 27537 -> 27537
9038
Cov: 27537 -> 27537
Cov: 27537 -> 27537
9039
Cov: 27537 -> 27537
Cov: 27537 -> 27537
9040
Cov: 27537 -> 27537
Cov: 27537 -> 27537
9041
Cov: 27537 -> 27537
Cov: 27537 -> 27537
9042
Cov: 27537 -> 27537
Cov: 27537 -> 27537
9043
Cov: 27537 -> 27537
Cov: 27537 -> 27537
9044
Cov: 27537 -> 27537
Cov: 27537 -> 27537
9045
Cov: 27537 -> 27538
Cov: 27538 -> 27538
9046
{"exception": "TypeError", "msg": "type_as(): argument 'other' (position 1) must be Tensor, not torch.tensortype"}
9047
Cov: 27538 -> 27539
Cov: 27539 -> 27539
9048
Cov: 27539 -> 27542
Cov: 27542 -> 27542
9049
Cov: 27542 -> 27543
Cov: 27543 -> 27543
9050
Cov: 27543 -> 27543
Cov: 27543 -> 27543
9051
Cov: 27543 -> 27543
Cov: 27543 -> 27543
9052
Cov: 27543 -> 27543
Cov: 27543 -> 27543
9053
Cov: 27543 -> 27543
Cov: 27543 -> 27543
9054
{"exception": "RuntimeError", "msg": "scatter(): Expected self.dtype to be equal to src.dtype"}
9055
Cov: 27543 -> 27543
Cov: 27543 -> 27543
9056
Cov: 27543 -> 27543
Cov: 27543 -> 27543
9057
Cov: 27543 -> 27544
Cov: 27544 -> 27544
9058
{"exception": "RuntimeError", "msg": "This function was deprecated since version 1.9 and is now removed. `torch.lstsq` is deprecated in favor of `torch.linalg.lstsq`.\n`torch.linalg.lstsq` has reversed arguments and does not return the QR decomposition in the returned tuple (although it returns other information about the problem).\n\nTo get the QR decomposition consider using `torch.linalg.qr`.\n\nThe returned solution in `torch.lstsq` stored the residuals of the solution in the last m - n columns of the returned value whenever m > n. In torch.linalg.lstsq, the residuals are in the field 'residuals' of the returned named tuple.\n\nThe unpacking of the solution, as in\nX, _ = torch.lstsq(B, A).solution[:A.size(1)]\nshould be replaced with:\nX = torch.linalg.lstsq(A, B).solution"}
9059
Cov: 27544 -> 27547
Cov: 27547 -> 27547
9060
Cov: 27547 -> 27548
Cov: 27548 -> 27548
9061
Cov: 27548 -> 27549
Cov: 27549 -> 27549
9062
Cov: 27549 -> 27549
Cov: 27549 -> 27549
9063
Cov: 27549 -> 27549
Cov: 27549 -> 27549
9064
Cov: 27549 -> 27552
Cov: 27552 -> 27552
9065
Cov: 27552 -> 27552
Cov: 27552 -> 27552
9066
Cov: 27552 -> 27552
Cov: 27552 -> 27552
9067
Cov: 27552 -> 27552
Cov: 27552 -> 27552
9068
Cov: 27552 -> 27552
Cov: 27552 -> 27552
9069
Cov: 27552 -> 27552
Cov: 27552 -> 27552
9070
Cov: 27552 -> 27554
Cov: 27554 -> 27554
9071
Cov: 27554 -> 27554
Cov: 27554 -> 27554
9072
Cov: 27554 -> 27554
Cov: 27554 -> 27554
9073
Cov: 27554 -> 27554
Cov: 27554 -> 27554
9074
Cov: 27554 -> 27555
Cov: 27555 -> 27555
9075
Cov: 27555 -> 27555
Cov: 27555 -> 27555
9076
Cov: 27555 -> 27555
Cov: 27555 -> 27555
9077
Cov: 27555 -> 27555
Cov: 27555 -> 27555
9078
Cov: 27555 -> 27556
Cov: 27556 -> 27556
9079
Cov: 27556 -> 27556
Cov: 27556 -> 27556
9080
Cov: 27556 -> 27559
Cov: 27559 -> 27559
9081
Cov: 27559 -> 27562
Cov: 27562 -> 27562
9082
Cov: 27562 -> 27563
Cov: 27563 -> 27563
9083
{"exception": "RuntimeError", "msg": "Expected size for first two dimensions of batch2 tensor to be: [2, 4] but got: [2, 3]."}
9084
Cov: 27563 -> 27563
Cov: 27563 -> 27563
9085
Cov: 27563 -> 27563
Cov: 27563 -> 27563
9086
Cov: 27563 -> 27564
Cov: 27564 -> 27564
9087
Cov: 27564 -> 27565
Cov: 27565 -> 27565
9088
Cov: 27565 -> 27565
Cov: 27565 -> 27565
9089
Cov: 27565 -> 27565
Cov: 27565 -> 27565
9090
Cov: 27565 -> 27565
Cov: 27565 -> 27565
9091
Cov: 27565 -> 27565
Cov: 27565 -> 27565
9092
Cov: 27565 -> 27565
Cov: 27565 -> 27565
9093
Cov: 27565 -> 27568
Cov: 27568 -> 27568
9094
Cov: 27568 -> 27568
Cov: 27568 -> 27568
9095
Cov: 27568 -> 27568
Cov: 27568 -> 27568
9096
Cov: 27568 -> 27568
Cov: 27568 -> 27568
9097
Cov: 27568 -> 27568
Cov: 27568 -> 27568
9098
Cov: 27568 -> 27568
Cov: 27568 -> 27568
9099
Cov: 27568 -> 27568
Cov: 27568 -> 27568
9100
{"exception": "RuntimeError", "msg": "\"bitwise_and_cpu\" not implemented for 'Double'"}
9101
Cov: 27568 -> 27569
Cov: 27569 -> 27569
9102
Cov: 27569 -> 27569
Cov: 27569 -> 27569
9103
Cov: 27569 -> 27569
Cov: 27569 -> 27569
9104
Cov: 27569 -> 27570
Cov: 27570 -> 27570
9105
Cov: 27570 -> 27571
Cov: 27571 -> 27571
9106
Cov: 27571 -> 27572
Cov: 27572 -> 27572
9107
Cov: 27572 -> 27573
Cov: 27573 -> 27573
9108
Cov: 27573 -> 27574
Cov: 27574 -> 27574
9109
Cov: 27574 -> 27574
Cov: 27574 -> 27574
9110
Cov: 27574 -> 27574
Cov: 27574 -> 27574
9111
Cov: 27574 -> 27574
Cov: 27574 -> 27574
9112
Cov: 27574 -> 27574
Cov: 27574 -> 27574
9113
Cov: 27574 -> 27574
Cov: 27574 -> 27574
9114
Cov: 27574 -> 27575
Cov: 27575 -> 27575
9115
Cov: 27575 -> 27577
Cov: 27577 -> 27577
9116
Cov: 27577 -> 27577
Cov: 27577 -> 27577
9117
Cov: 27577 -> 27577
Cov: 27577 -> 27577
9118
{"exception": "TypeError", "msg": "'getset_descriptor' object is not callable"}
9119
Cov: 27577 -> 27578
Cov: 27578 -> 27578
9120
Cov: 27578 -> 27578
Cov: 27578 -> 27578
9121
Cov: 27578 -> 27581
Cov: 27581 -> 27581
9122
Cov: 27581 -> 27583
Cov: 27583 -> 27583
9123
Cov: 27583 -> 27583
Cov: 27583 -> 27583
9124
Cov: 27583 -> 27583
Cov: 27583 -> 27583
9125
Cov: 27583 -> 27583
Cov: 27583 -> 27583
9126
Cov: 27583 -> 27583
Cov: 27583 -> 27583
9127
{"exception": "RuntimeError", "msg": "This function was deprecated since version 1.9 and is now removed. `torch.lstsq` is deprecated in favor of `torch.linalg.lstsq`.\n`torch.linalg.lstsq` has reversed arguments and does not return the QR decomposition in the returned tuple (although it returns other information about the problem).\n\nTo get the QR decomposition consider using `torch.linalg.qr`.\n\nThe returned solution in `torch.lstsq` stored the residuals of the solution in the last m - n columns of the returned value whenever m > n. In torch.linalg.lstsq, the residuals are in the field 'residuals' of the returned named tuple.\n\nThe unpacking of the solution, as in\nX, _ = torch.lstsq(B, A).solution[:A.size(1)]\nshould be replaced with:\nX = torch.linalg.lstsq(A, B).solution"}
9128
Cov: 27583 -> 27584
Cov: 27584 -> 27584
9129
Cov: 27584 -> 27584
Cov: 27584 -> 27584
9130
Cov: 27584 -> 27584
Cov: 27584 -> 27584
9131
Cov: 27584 -> 27584
Cov: 27584 -> 27584
9132
Cov: 27584 -> 27584
Cov: 27584 -> 27584
9133
Cov: 27584 -> 27584
Cov: 27584 -> 27584
9134
Cov: 27584 -> 27584
Cov: 27584 -> 27584
9135
Cov: 27584 -> 27584
Cov: 27584 -> 27584
9136
Cov: 27584 -> 27584
Cov: 27584 -> 27584
9137
Cov: 27584 -> 27584
Cov: 27584 -> 27584
9138
Cov: 27584 -> 27584
Cov: 27584 -> 27584
9139
Cov: 27584 -> 27584
Cov: 27584 -> 27584
9140
Cov: 27584 -> 27584
Cov: 27584 -> 27584
9141
Cov: 27584 -> 27584
Cov: 27584 -> 27584
9142
Cov: 27584 -> 27584
Cov: 27584 -> 27584
9143
Cov: 27584 -> 27584
Cov: 27584 -> 27584
9144
Cov: 27584 -> 27584
Cov: 27584 -> 27584
9145
Cov: 27584 -> 27585
Cov: 27585 -> 27585
9146
{"exception": "RuntimeError", "msg": "The expanded size of the tensor (4) must match the existing size (3) at non-singleton dimension 2.  Target sizes: [2, 3, 4].  Tensor sizes: [2, 3]"}
9147
Cov: 27585 -> 27585
Cov: 27585 -> 27585
9148
Cov: 27585 -> 27585
Cov: 27585 -> 27585
9149
Cov: 27585 -> 27586
Cov: 27586 -> 27586
9150
Cov: 27586 -> 27586
Cov: 27586 -> 27586
9151
Cov: 27586 -> 27586
Cov: 27586 -> 27586
9152
Cov: 27586 -> 27587
Cov: 27587 -> 27587
9153
Cov: 27587 -> 27587
Cov: 27587 -> 27587
9154
Cov: 27587 -> 27587
Cov: 27587 -> 27587
9155
Cov: 27587 -> 27587
Cov: 27587 -> 27587
9156
Cov: 27587 -> 27587
Cov: 27587 -> 27587
9157
Cov: 27587 -> 27587
Cov: 27587 -> 27587
9158
Cov: 27587 -> 27588
Cov: 27588 -> 27588
9159
Cov: 27588 -> 27588
Cov: 27588 -> 27588
9160
Cov: 27588 -> 27588
Cov: 27588 -> 27588
9161
Cov: 27588 -> 27588
Cov: 27588 -> 27588
9162
Cov: 27588 -> 27588
Cov: 27588 -> 27588
9163
Cov: 27588 -> 27590
Cov: 27590 -> 27590
9164
Cov: 27590 -> 27590
Cov: 27590 -> 27590
9165
Cov: 27590 -> 27590
Cov: 27590 -> 27590
9166
{"exception": "RuntimeError", "msg": "any: You passed a dimname (string) to this op in place of a dimension index but it does not yet support this behavior. Please pass a dimension index to work around this."}
9167
Cov: 27590 -> 27590
Cov: 27590 -> 27590
9168
Cov: 27590 -> 27591
Cov: 27591 -> 27591
9169
Cov: 27591 -> 27591
Cov: 27591 -> 27591
9170
Cov: 27591 -> 27591
Cov: 27591 -> 27591
9171
Cov: 27591 -> 27591
Cov: 27591 -> 27591
9172
Cov: 27591 -> 27591
Cov: 27591 -> 27591
9173
Cov: 27591 -> 27591
Cov: 27591 -> 27591
9174
Cov: 27591 -> 27591
Cov: 27591 -> 27591
9175
Cov: 27591 -> 27591
Cov: 27591 -> 27591
9176
Cov: 27591 -> 27591
Cov: 27591 -> 27591
9177
Cov: 27591 -> 27591
Cov: 27591 -> 27591
9178
Cov: 27591 -> 27591
Cov: 27591 -> 27591
9179
Cov: 27591 -> 27591
Cov: 27591 -> 27591
9180
{"exception": "TypeError", "msg": "heaviside(): argument 'values' must be Tensor, not tuple"}
9181
Cov: 27591 -> 27591
Cov: 27591 -> 27591
9182
Cov: 27591 -> 27591
Cov: 27591 -> 27591
9183
Cov: 27591 -> 27592
Cov: 27592 -> 27592
9184
Cov: 27592 -> 27593
Cov: 27593 -> 27593
9185
Cov: 27593 -> 27593
Cov: 27593 -> 27593
9186
Cov: 27593 -> 27593
Cov: 27593 -> 27593
9187
Cov: 27593 -> 27593
Cov: 27593 -> 27593
9188
Cov: 27593 -> 27594
Cov: 27594 -> 27594
9189
Cov: 27594 -> 27594
Cov: 27594 -> 27594
9190
Cov: 27594 -> 27595
Cov: 27595 -> 27595
9191
Cov: 27595 -> 27595
Cov: 27595 -> 27595
9192
{"exception": "TypeError", "msg": "'getset_descriptor' object is not callable"}
9193
Cov: 27595 -> 27595
Cov: 27595 -> 27595
9194
Cov: 27595 -> 27595
Cov: 27595 -> 27595
9195
Cov: 27595 -> 27595
Cov: 27595 -> 27595
9196
Cov: 27595 -> 27595
Cov: 27595 -> 27595
9197
Cov: 27595 -> 27595
Cov: 27595 -> 27595
9198
Cov: 27595 -> 27595
Cov: 27595 -> 27595
9199
Cov: 27595 -> 27595
Cov: 27595 -> 27595
9200
Cov: 27595 -> 27595
Cov: 27595 -> 27595
9201
Cov: 27595 -> 27595
Cov: 27595 -> 27595
9202
Cov: 27595 -> 27595
Cov: 27595 -> 27595
9203
Cov: 27595 -> 27595
Cov: 27595 -> 27595
9204
Cov: 27595 -> 27595
Cov: 27595 -> 27595
9205
Cov: 27595 -> 27596
Cov: 27596 -> 27596
9206
Cov: 27596 -> 27596
Cov: 27596 -> 27596
9207
Cov: 27596 -> 27596
Cov: 27596 -> 27596
9208
Cov: 27596 -> 27597
Cov: 27597 -> 27597
9209
Cov: 27597 -> 27597
Cov: 27597 -> 27597
9210
Cov: 27597 -> 27597
Cov: 27597 -> 27597
9211
Cov: 27597 -> 27598
Cov: 27598 -> 27598
9212
{"exception": "RuntimeError", "msg": "any: You passed a dimname (string) to this op in place of a dimension index but it does not yet support this behavior. Please pass a dimension index to work around this."}
9213
Cov: 27598 -> 27598
Cov: 27598 -> 27598
9214
Cov: 27598 -> 27598
Cov: 27598 -> 27598
9215
Cov: 27598 -> 27598
Cov: 27598 -> 27598
9216
Cov: 27598 -> 27599
Cov: 27599 -> 27599
9217
Cov: 27599 -> 27599
Cov: 27599 -> 27599
9218
Cov: 27599 -> 27599
Cov: 27599 -> 27599
9219
Cov: 27599 -> 27599
Cov: 27599 -> 27599
9220
Cov: 27599 -> 27599
Cov: 27599 -> 27599
9221
Cov: 27599 -> 27600
Cov: 27600 -> 27600
9222
Cov: 27600 -> 27600
Cov: 27600 -> 27600
9223
Cov: 27600 -> 27600
Cov: 27600 -> 27600
9224
Cov: 27600 -> 27602
Cov: 27602 -> 27602
9225
Cov: 27602 -> 27602
Cov: 27602 -> 27602
9226
Cov: 27602 -> 27602
Cov: 27602 -> 27602
9227
Cov: 27602 -> 27602
Cov: 27602 -> 27602
9228
Cov: 27602 -> 27603
Cov: 27603 -> 27603
9229
Cov: 27603 -> 27603
Cov: 27603 -> 27603
9230
Cov: 27603 -> 27604
Cov: 27604 -> 27604
9231
Cov: 27604 -> 27604
Cov: 27604 -> 27604
9232
{"exception": "RuntimeError", "msg": "masked_scatter_ only supports boolean masks, but got mask with dtype Byte"}
9233
Cov: 27604 -> 27604
Cov: 27604 -> 27604
9234
Cov: 27604 -> 27604
Cov: 27604 -> 27604
9235
Cov: 27604 -> 27604
Cov: 27604 -> 27604
9236
Cov: 27604 -> 27604
Cov: 27604 -> 27604
9237
{"exception": "TypeError", "msg": "addbmm() missing 1 required positional arguments: \"batch2\""}
9238
Cov: 27604 -> 27604
Cov: 27604 -> 27604
9239
Cov: 27604 -> 27604
Cov: 27604 -> 27604
9240
Cov: 27604 -> 27604
Cov: 27604 -> 27604
9241
Cov: 27604 -> 27604
Cov: 27604 -> 27604
9242
Cov: 27604 -> 27604
Cov: 27604 -> 27604
9243
{"exception": "NotImplementedError", "msg": "Could not run 'aten::q_scale' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::q_scale' is only available for these backends: [QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nQuantizedCPU: registered at aten/src/ATen/RegisterQuantizedCPU.cpp:944 [kernel]\nQuantizedCUDA: registered at aten/src/ATen/RegisterQuantizedCUDA.cpp:459 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:153 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:498 [backend fallback]\nFunctionalize: registered at ../aten/src/ATen/FunctionalizeFallbackKernel.cpp:290 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:86 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradMTIA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_4.cpp:13056 [kernel]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:382 [backend fallback]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:249 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:710 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:203 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:161 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:494 [backend fallback]\nPreDispatch: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:165 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:157 [backend fallback]\n"}
9244
Cov: 27604 -> 27604
Cov: 27604 -> 27604
9245
Cov: 27604 -> 27604
Cov: 27604 -> 27604
9246
Cov: 27604 -> 27607
Cov: 27607 -> 27607
9247
Cov: 27607 -> 27607
Cov: 27607 -> 27607
9248
Cov: 27607 -> 27607
Cov: 27607 -> 27607
9249
{"exception": "RuntimeError", "msg": "scatter(): Expected self.dtype to be equal to src.dtype"}
9250
Cov: 27607 -> 27607
Cov: 27607 -> 27607
9251
Cov: 27607 -> 27607
Cov: 27607 -> 27607
9252
Cov: 27607 -> 27607
Cov: 27607 -> 27607
9253
Cov: 27607 -> 27607
Cov: 27607 -> 27607
9254
Cov: 27607 -> 27607
Cov: 27607 -> 27607
9255
Cov: 27607 -> 27607
Cov: 27607 -> 27607
9256
Cov: 27607 -> 27607
Cov: 27607 -> 27607
9257
Cov: 27607 -> 27611
Cov: 27611 -> 27611
9258
Cov: 27611 -> 27611
Cov: 27611 -> 27611
9259
Cov: 27611 -> 27611
Cov: 27611 -> 27611
9260
Cov: 27611 -> 27643
Cov: 27643 -> 27643
9261
Cov: 27643 -> 27643
Cov: 27643 -> 27643
9262
Cov: 27643 -> 27646
Cov: 27646 -> 27646
9263
Cov: 27646 -> 27646
Cov: 27646 -> 27646
9264
Cov: 27646 -> 27646
Cov: 27646 -> 27646
9265
Cov: 27646 -> 27649
Cov: 27649 -> 27649
9266
Cov: 27649 -> 27650
Cov: 27650 -> 27650
9267
Cov: 27650 -> 27650
Cov: 27650 -> 27650
9268
Cov: 27650 -> 27650
Cov: 27650 -> 27650
9269
{"exception": "TypeError", "msg": "baddbmm() missing 1 required positional arguments: \"batch2\""}
9270
Cov: 27650 -> 27650
Cov: 27650 -> 27650
9271
Cov: 27650 -> 27650
Cov: 27650 -> 27650
9272
Cov: 27650 -> 27650
Cov: 27650 -> 27650
9273
Cov: 27650 -> 27651
Cov: 27651 -> 27651
9274
Cov: 27651 -> 27652
Cov: 27652 -> 27652
9275
Cov: 27652 -> 27652
Cov: 27652 -> 27652
9276
Cov: 27652 -> 27652
Cov: 27652 -> 27652
9277
Cov: 27652 -> 27652
Cov: 27652 -> 27652
9278
Cov: 27652 -> 27652
Cov: 27652 -> 27652
9279
Cov: 27652 -> 27652
Cov: 27652 -> 27652
9280
Cov: 27652 -> 27652
Cov: 27652 -> 27652
9281
Cov: 27652 -> 27652
Cov: 27652 -> 27652
9282
Cov: 27652 -> 27652
Cov: 27652 -> 27652
9283
Cov: 27652 -> 27652
Cov: 27652 -> 27652
9284
Cov: 27652 -> 27652
Cov: 27652 -> 27652
9285
Cov: 27652 -> 27652
Cov: 27652 -> 27652
9286
Cov: 27652 -> 27653
Cov: 27653 -> 27653
9287
Cov: 27653 -> 27653
Cov: 27653 -> 27653
9288
Cov: 27653 -> 27654
Cov: 27654 -> 27654
9289
Cov: 27654 -> 27657
Cov: 27657 -> 27657
9290
Cov: 27657 -> 27657
Cov: 27657 -> 27657
9291
Cov: 27657 -> 27657
Cov: 27657 -> 27657
9292
Cov: 27657 -> 27657
Cov: 27657 -> 27657
9293
Cov: 27657 -> 27657
Cov: 27657 -> 27657
9294
Cov: 27657 -> 27657
Cov: 27657 -> 27657
9295
{"exception": "RuntimeError", "msg": "index_add_(): Number of indices (2) should be equal to source.size(dim): (4), for dim: 1"}
9296
Cov: 27657 -> 27657
Cov: 27657 -> 27657
9297
Cov: 27657 -> 27657
Cov: 27657 -> 27657
9298
Cov: 27657 -> 27659
Cov: 27659 -> 27659
9299
Cov: 27659 -> 27662
Cov: 27662 -> 27662
9300
Cov: 27662 -> 27662
Cov: 27662 -> 27662
9301
Cov: 27662 -> 27662
Cov: 27662 -> 27662
9302
Cov: 27662 -> 27662
Cov: 27662 -> 27662
9303
Cov: 27662 -> 27662
Cov: 27662 -> 27662
9304
Cov: 27662 -> 27662
Cov: 27662 -> 27662
9305
{"exception": "TypeError", "msg": "bernoulli() received an invalid combination of arguments - got (out=Tensor, ), but expected one of:\n * (*, torch.Generator generator)\n * (float p, *, torch.Generator generator)\n"}
9306
Cov: 27662 -> 27662
Cov: 27662 -> 27662
9307
Cov: 27662 -> 27662
Cov: 27662 -> 27662
9308
Cov: 27662 -> 27665
Cov: 27665 -> 27665
9309
{"exception": "RuntimeError", "msg": "Expected nested_tensorlist[0].size() > 0 to be true, but got false.  (Could this error message be improved?  If so, please report an enhancement request to PyTorch.)"}
9310
{"exception": "RuntimeError", "msg": "torch.dsplit requires a tensor with at least 3 dimension, but got a tensor with 2 dimensions!"}
9311
{"exception": "RuntimeError", "msg": "mat1 and mat2 must have the same dtype, but got Float and Double"}
9312
Cov: 27665 -> 27675
Cov: 27675 -> 27675
9313
Cov: 27675 -> 27675
Cov: 27675 -> 27675
9314
Cov: 27675 -> 27675
Cov: 27675 -> 27675
9315
Cov: 27675 -> 27675
Cov: 27675 -> 27675
9316
Cov: 27675 -> 27675
Cov: 27675 -> 27675
9317
Cov: 27675 -> 27675
Cov: 27675 -> 27675
9318
{"exception": "RuntimeError", "msg": "masked_scatter_ only supports boolean masks, but got mask with dtype Long"}
9319
{"exception": "NotImplementedError", "msg": "Could not run 'aten::q_per_channel_scales' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::q_per_channel_scales' is only available for these backends: [QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nQuantizedCPU: registered at aten/src/ATen/RegisterQuantizedCPU.cpp:944 [kernel]\nQuantizedCUDA: registered at aten/src/ATen/RegisterQuantizedCUDA.cpp:459 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:153 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:498 [backend fallback]\nFunctionalize: registered at ../aten/src/ATen/FunctionalizeFallbackKernel.cpp:290 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:86 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradMTIA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:18233 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_3.cpp:14610 [kernel]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:382 [backend fallback]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:249 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:710 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:203 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:161 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:494 [backend fallback]\nPreDispatch: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:165 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:157 [backend fallback]\n"}
9320
Cov: 27675 -> 27676
Cov: 27676 -> 27676
9321
Cov: 27676 -> 27676
Cov: 27676 -> 27676
9322
Cov: 27676 -> 27676
Cov: 27676 -> 27676
9323
Cov: 27676 -> 27676
Cov: 27676 -> 27676
9324
Cov: 27676 -> 27676
Cov: 27676 -> 27676
9325
Cov: 27676 -> 27678
Cov: 27678 -> 27678
9326
Cov: 27678 -> 27680
Cov: 27680 -> 27680
9327
Cov: 27680 -> 27680
Cov: 27680 -> 27680
9328
Cov: 27680 -> 27680
Cov: 27680 -> 27680
9329
Cov: 27680 -> 27681
Cov: 27681 -> 27681
9330
Cov: 27681 -> 27682
Cov: 27682 -> 27682
9331
Cov: 27682 -> 27682
Cov: 27682 -> 27682
9332
Cov: 27682 -> 27682
Cov: 27682 -> 27682
9333
Cov: 27682 -> 27682
Cov: 27682 -> 27682
9334
Cov: 27682 -> 27682
Cov: 27682 -> 27682
9335
Cov: 27682 -> 27682
Cov: 27682 -> 27682
9336
Cov: 27682 -> 27682
Cov: 27682 -> 27682
9337
Cov: 27682 -> 27682
Cov: 27682 -> 27682
9338
Cov: 27682 -> 27682
Cov: 27682 -> 27682
9339
Cov: 27682 -> 27684
Cov: 27684 -> 27684
9340
Cov: 27684 -> 27684
Cov: 27684 -> 27684
9341
Cov: 27684 -> 27687
Cov: 27687 -> 27687
9342
Cov: 27687 -> 27688
Cov: 27688 -> 27688
9343
Cov: 27688 -> 27689
Cov: 27689 -> 27689
9344
Cov: 27689 -> 27689
Cov: 27689 -> 27689
9345
Cov: 27689 -> 27692
Cov: 27692 -> 27692
9346
Cov: 27692 -> 27692
Cov: 27692 -> 27692
9347
Cov: 27692 -> 27692
Cov: 27692 -> 27692
9348
Cov: 27692 -> 27693
Cov: 27693 -> 27693
9349
Cov: 27693 -> 27693
Cov: 27693 -> 27693
9350
{"exception": "RuntimeError", "msg": "masked_scatter_ only supports boolean masks, but got mask with dtype Byte"}
9351
Cov: 27693 -> 27693
Cov: 27693 -> 27693
9352
Cov: 27693 -> 27693
Cov: 27693 -> 27693
9353
Cov: 27693 -> 27693
Cov: 27693 -> 27693
9354
Cov: 27693 -> 27694
Cov: 27694 -> 27694
9355
Cov: 27694 -> 27694
Cov: 27694 -> 27694
9356
Cov: 27694 -> 27694
Cov: 27694 -> 27694
9357
Cov: 27694 -> 27695
Cov: 27695 -> 27695
9358
{"exception": "TypeError", "msg": "'getset_descriptor' object is not callable"}
9359
Cov: 27695 -> 27695
Cov: 27695 -> 27695
9360
Cov: 27695 -> 27695
Cov: 27695 -> 27695
9361
Cov: 27695 -> 27695
Cov: 27695 -> 27695
9362
{"exception": "RuntimeError", "msg": "\"bitwise_xor_cpu\" not implemented for 'Double'"}
9363
Cov: 27695 -> 27695
Cov: 27695 -> 27695
9364
Cov: 27695 -> 27696
Cov: 27696 -> 27696
9365
Cov: 27696 -> 27697
Cov: 27697 -> 27697
9366
Cov: 27697 -> 27697
Cov: 27697 -> 27697
9367
Cov: 27697 -> 27697
Cov: 27697 -> 27697
9368
Cov: 27697 -> 27697
Cov: 27697 -> 27697
9369
Cov: 27697 -> 27697
Cov: 27697 -> 27697
9370
Cov: 27697 -> 27746
Cov: 27746 -> 27746
9371
Cov: 27746 -> 27747
Cov: 27747 -> 27747
9372
Cov: 27747 -> 27747
Cov: 27747 -> 27747
9373
Cov: 27747 -> 27747
Cov: 27747 -> 27747
9374
Cov: 27747 -> 27748
Cov: 27748 -> 27748
9375
Cov: 27748 -> 27748
Cov: 27748 -> 27748
9376
Cov: 27748 -> 27748
Cov: 27748 -> 27748
9377
Cov: 27748 -> 27748
Cov: 27748 -> 27748
9378
Cov: 27748 -> 27748
Cov: 27748 -> 27748
9379
Cov: 27748 -> 27748
Cov: 27748 -> 27748
9380
{"exception": "RuntimeError", "msg": "outer: Expected 1-D argument self, but got 2-D"}
9381
Cov: 27748 -> 27748
Cov: 27748 -> 27748
9382
Cov: 27748 -> 27748
Cov: 27748 -> 27748
9383
Cov: 27748 -> 27748
Cov: 27748 -> 27748
9384
Cov: 27748 -> 27748
Cov: 27748 -> 27748
9385
Cov: 27748 -> 27748
Cov: 27748 -> 27748
9386
Cov: 27748 -> 27748
Cov: 27748 -> 27748
9387
Cov: 27748 -> 27748
Cov: 27748 -> 27748
9388
Cov: 27748 -> 27748
Cov: 27748 -> 27748
9389
Cov: 27748 -> 27748
Cov: 27748 -> 27748
9390
Cov: 27748 -> 27748
Cov: 27748 -> 27748
9391
Cov: 27748 -> 27748
Cov: 27748 -> 27748
9392
Cov: 27748 -> 27748
Cov: 27748 -> 27748
9393
Cov: 27748 -> 27749
Cov: 27749 -> 27749
9394
Cov: 27749 -> 27749
Cov: 27749 -> 27749
9395
Cov: 27749 -> 27749
Cov: 27749 -> 27749
9396
Cov: 27749 -> 27749
Cov: 27749 -> 27749
9397
Cov: 27749 -> 27749
Cov: 27749 -> 27749
9398
Cov: 27749 -> 27749
Cov: 27749 -> 27749
9399
Cov: 27749 -> 27749
Cov: 27749 -> 27749
9400
Cov: 27749 -> 27749
Cov: 27749 -> 27749
9401
Cov: 27749 -> 27749
Cov: 27749 -> 27749
9402
Cov: 27749 -> 27749
Cov: 27749 -> 27749
9403
Cov: 27749 -> 27749
Cov: 27749 -> 27749
9404
Cov: 27749 -> 27749
Cov: 27749 -> 27749
9405
Cov: 27749 -> 27750
Cov: 27750 -> 27750
9406
{"exception": "IndexError", "msg": "index_copy_(): When source and destination are not scalars, their dimensionality must match. Source dimensionality (2), destination dimensionality (3)"}
9407
Cov: 27750 -> 27750
Cov: 27750 -> 27750
9408
Cov: 27750 -> 27750
Cov: 27750 -> 27750
9409
Cov: 27750 -> 27750
Cov: 27750 -> 27750
9410
Cov: 27750 -> 27751
Cov: 27751 -> 27751
9411
Cov: 27751 -> 27751
Cov: 27751 -> 27751
9412
Cov: 27751 -> 27751
Cov: 27751 -> 27751
9413
Cov: 27751 -> 27751
Cov: 27751 -> 27751
9414
Cov: 27751 -> 27752
Cov: 27752 -> 27752
9415
Cov: 27752 -> 27753
Cov: 27753 -> 27753
9416
Cov: 27753 -> 27754
Cov: 27754 -> 27754
9417
Cov: 27754 -> 27754
Cov: 27754 -> 27754
9418
Cov: 27754 -> 27754
Cov: 27754 -> 27754
9419
Cov: 27754 -> 27754
Cov: 27754 -> 27754
9420
Cov: 27754 -> 27754
Cov: 27754 -> 27754
9421
Cov: 27754 -> 27754
Cov: 27754 -> 27754
9422
{"exception": "TypeError", "msg": "new_full(): argument 'size' (position 1) must be tuple of ints, not int"}
9423
Cov: 27754 -> 27754
Cov: 27754 -> 27754
9424
Cov: 27754 -> 27755
Cov: 27755 -> 27755
9425
Cov: 27755 -> 27756
Cov: 27756 -> 27756
9426
Cov: 27756 -> 27756
Cov: 27756 -> 27756
9427
Cov: 27756 -> 27756
Cov: 27756 -> 27756
9428
Cov: 27756 -> 27757
Cov: 27757 -> 27757
9429
Cov: 27757 -> 27757
Cov: 27757 -> 27757
9430
Cov: 27757 -> 27757
Cov: 27757 -> 27757
9431
Cov: 27757 -> 27757
Cov: 27757 -> 27757
9432
Cov: 27757 -> 27757
Cov: 27757 -> 27757
9433
Cov: 27757 -> 27757
Cov: 27757 -> 27757
9434
Cov: 27757 -> 27758
Cov: 27758 -> 27758
9435
Cov: 27758 -> 27758
Cov: 27758 -> 27758
9436
{"exception": "RuntimeError", "msg": "masked_scatter_ only supports boolean masks, but got mask with dtype Byte"}
9437
Cov: 27758 -> 27758
Cov: 27758 -> 27758
9438
Cov: 27758 -> 27759
Cov: 27759 -> 27759
9439
Cov: 27759 -> 27759
Cov: 27759 -> 27759
9440
{"exception": "TypeError", "msg": "'getset_descriptor' object is not callable"}
9441
Cov: 27759 -> 27759
Cov: 27759 -> 27759
9442
Cov: 27759 -> 27760
Cov: 27760 -> 27760
9443
Cov: 27760 -> 27760
Cov: 27760 -> 27760
9444
Cov: 27760 -> 27761
Cov: 27761 -> 27761
9445
{"exception": "_LinAlgError", "msg": "torch.linalg.solve: The solver failed because the input matrix is singular."}
9446
Cov: 27761 -> 27761
Cov: 27761 -> 27761
9447
Cov: 27761 -> 27762
Cov: 27762 -> 27762
9448
Cov: 27762 -> 27762
Cov: 27762 -> 27762
9449
Cov: 27762 -> 27762
Cov: 27762 -> 27762
9450
Cov: 27762 -> 27762
Cov: 27762 -> 27762
9451
Cov: 27762 -> 27762
Cov: 27762 -> 27762
9452
{"exception": "RuntimeError", "msg": "Creating a Tensor subclass from a class that does not inherit from Tensor is not possible. Make sure your class inherits from Tensor."}
9453
Cov: 27762 -> 27762
Cov: 27762 -> 27762
9454
{"exception": "NameError", "msg": "name 'math' is not defined"}
9455
Cov: 27762 -> 27764
Cov: 27764 -> 27764
9456
Cov: 27764 -> 27764
Cov: 27764 -> 27764
9457
{"exception": "RuntimeError", "msg": "1D tensors expected, but got 2D and 2D tensors"}
9458
Cov: 27764 -> 27764
Cov: 27764 -> 27764
9459
Cov: 27764 -> 27764
Cov: 27764 -> 27764
9460
Cov: 27764 -> 27765
Cov: 27765 -> 27765
9461
Cov: 27765 -> 27765
Cov: 27765 -> 27765
9462
Cov: 27765 -> 27765
Cov: 27765 -> 27765
9463
Cov: 27765 -> 27765
Cov: 27765 -> 27765
9464
Cov: 27765 -> 27765
Cov: 27765 -> 27765
9465
Cov: 27765 -> 27768
Cov: 27768 -> 27768
9466
Cov: 27768 -> 27768
Cov: 27768 -> 27768
9467
Cov: 27768 -> 27768
Cov: 27768 -> 27768
9468
Cov: 27768 -> 27768
Cov: 27768 -> 27768
9469
{"exception": "RuntimeError", "msg": "torch.dsplit requires a tensor with at least 3 dimension, but got a tensor with 2 dimensions!"}
9470
Cov: 27768 -> 27768
Cov: 27768 -> 27768
9471
Cov: 27768 -> 27769
Cov: 27769 -> 27769
9472
Cov: 27769 -> 27769
Cov: 27769 -> 27769
9473
Cov: 27769 -> 27770
Cov: 27770 -> 27770
9474
Cov: 27770 -> 27771
Cov: 27771 -> 27771
9475
Cov: 27771 -> 27771
Cov: 27771 -> 27771
9476
Cov: 27771 -> 27774
Cov: 27774 -> 27774
9477
Cov: 27774 -> 27774
Cov: 27774 -> 27774
9478
Cov: 27774 -> 27774
Cov: 27774 -> 27774
9479
Cov: 27774 -> 27774
Cov: 27774 -> 27774
9480
Cov: 27774 -> 27774
Cov: 27774 -> 27774
9481
Cov: 27774 -> 27774
Cov: 27774 -> 27774
9482
Cov: 27774 -> 27774
Cov: 27774 -> 27774
9483
Cov: 27774 -> 27775
Cov: 27775 -> 27775
9484
Cov: 27775 -> 27775
Cov: 27775 -> 27775
9485
Cov: 27775 -> 27775
Cov: 27775 -> 27775
9486
{"exception": "RuntimeError", "msg": "the base given to float_power_ has dtype Float but the operation's result requires dtype Double"}
9487
Cov: 27775 -> 27775
Cov: 27775 -> 27775
9488
Cov: 27775 -> 27775
Cov: 27775 -> 27775
9489
Cov: 27775 -> 27775
Cov: 27775 -> 27775
9490
Cov: 27775 -> 27775
Cov: 27775 -> 27775
9491
Cov: 27775 -> 27775
Cov: 27775 -> 27775
9492
{"exception": "RuntimeError", "msg": "norm(): input dtype should be either floating point or complex. Got Long instead."}
9493
Cov: 27775 -> 27778
Cov: 27778 -> 27778
9494
Cov: 27778 -> 27778
Cov: 27778 -> 27778
9495
Cov: 27778 -> 27778
Cov: 27778 -> 27778
9496
Cov: 27778 -> 27778
Cov: 27778 -> 27778
9497
Cov: 27778 -> 27778
Cov: 27778 -> 27778
9498
Cov: 27778 -> 27778
Cov: 27778 -> 27778
9499
Cov: 27778 -> 27778
Cov: 27778 -> 27778
9500
Cov: 27778 -> 27778
Cov: 27778 -> 27778
9501
Cov: 27778 -> 27778
Cov: 27778 -> 27778
9502
Cov: 27778 -> 27778
Cov: 27778 -> 27778
9503
Cov: 27778 -> 27778
Cov: 27778 -> 27778
9504
Cov: 27778 -> 27778
Cov: 27778 -> 27778
9505
Cov: 27778 -> 27779
Cov: 27779 -> 27779
9506
Cov: 27779 -> 27779
Cov: 27779 -> 27779
9507
{"exception": "RuntimeError", "msg": "masked_select: expected BoolTensor for mask"}
9508
Cov: 27779 -> 27779
Cov: 27779 -> 27779
9509
Cov: 27779 -> 27779
Cov: 27779 -> 27779
9510
Cov: 27779 -> 27779
Cov: 27779 -> 27779
9511
Cov: 27779 -> 27779
Cov: 27779 -> 27779
9512
Cov: 27779 -> 27779
Cov: 27779 -> 27779
9513
Cov: 27779 -> 27779
Cov: 27779 -> 27779
9514
Cov: 27779 -> 27779
Cov: 27779 -> 27779
9515
Cov: 27779 -> 27779
Cov: 27779 -> 27779
9516
Cov: 27779 -> 27797
Cov: 27797 -> 27797
9517
Cov: 27797 -> 27798
Cov: 27798 -> 27798
9518
Cov: 27798 -> 27798
Cov: 27798 -> 27798
9519
Cov: 27798 -> 27801
Cov: 27801 -> 27801
9520
Cov: 27801 -> 27801
Cov: 27801 -> 27801
9521
Cov: 27801 -> 27801
Cov: 27801 -> 27801
9522
Cov: 27801 -> 27802
Cov: 27802 -> 27802
9523
{"exception": "RuntimeError", "msg": "Expected nested_tensorlist[0].size() > 0 to be true, but got false.  (Could this error message be improved?  If so, please report an enhancement request to PyTorch.)"}
9524
Cov: 27802 -> 27805
Cov: 27805 -> 27805
9525
Cov: 27805 -> 27806
Cov: 27806 -> 27806
9526
Cov: 27806 -> 27806
Cov: 27806 -> 27806
9527
Cov: 27806 -> 27806
Cov: 27806 -> 27806
9528
Cov: 27806 -> 27806
Cov: 27806 -> 27806
9529
Cov: 27806 -> 27806
Cov: 27806 -> 27806
9530
Cov: 27806 -> 27806
Cov: 27806 -> 27806
9531
Cov: 27806 -> 27806
Cov: 27806 -> 27806
9532
Cov: 27806 -> 27807
Cov: 27807 -> 27807
9533
Cov: 27807 -> 27808
Cov: 27808 -> 27808
9534
Cov: 27808 -> 27809
Cov: 27809 -> 27809
9535
Cov: 27809 -> 27809
Cov: 27809 -> 27809
9536
Cov: 27809 -> 27810
Cov: 27810 -> 27810
9537
Cov: 27810 -> 27813
Cov: 27813 -> 27813
9538
Cov: 27813 -> 27813
Cov: 27813 -> 27813
9539
Cov: 27813 -> 27813
Cov: 27813 -> 27813
9540
Cov: 27813 -> 27813
Cov: 27813 -> 27813
9541
Cov: 27813 -> 27814
Cov: 27814 -> 27814
9542
Cov: 27814 -> 27814
Cov: 27814 -> 27814
9543
Cov: 27814 -> 27814
Cov: 27814 -> 27814
9544
Cov: 27814 -> 27814
Cov: 27814 -> 27814
9545
Cov: 27814 -> 27814
Cov: 27814 -> 27814
9546
Cov: 27814 -> 27814
Cov: 27814 -> 27814
9547
Cov: 27814 -> 27814
Cov: 27814 -> 27814
9548
Cov: 27814 -> 27814
Cov: 27814 -> 27814
9549
Cov: 27814 -> 27815
Cov: 27815 -> 27815
9550
{"exception": "NotImplementedError", "msg": "Could not run 'aten::sparse_mask' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::sparse_mask' is only available for these backends: [SparseCPU, SparseCUDA, SparseCsrCPU, SparseCsrCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nSparseCPU: registered at aten/src/ATen/RegisterSparseCPU.cpp:1387 [kernel]\nSparseCUDA: registered at aten/src/ATen/RegisterSparseCUDA.cpp:1573 [kernel]\nSparseCsrCPU: registered at aten/src/ATen/RegisterSparseCsrCPU.cpp:1135 [kernel]\nSparseCsrCUDA: registered at aten/src/ATen/RegisterSparseCsrCUDA.cpp:1276 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:153 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:498 [backend fallback]\nFunctionalize: registered at ../aten/src/ATen/FunctionalizeFallbackKernel.cpp:290 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:86 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradMTIA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_0.cpp:16725 [kernel]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:382 [backend fallback]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:249 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:710 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:203 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:161 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:494 [backend fallback]\nPreDispatch: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:165 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:157 [backend fallback]\n"}
9551
Cov: 27815 -> 27815
Cov: 27815 -> 27815
9552
Cov: 27815 -> 27815
Cov: 27815 -> 27815
9553
Cov: 27815 -> 27816
Cov: 27816 -> 27816
9554
Cov: 27816 -> 27816
Cov: 27816 -> 27816
9555
Cov: 27816 -> 27816
Cov: 27816 -> 27816
9556
Cov: 27816 -> 27817
Cov: 27817 -> 27817
9557
Cov: 27817 -> 27817
Cov: 27817 -> 27817
9558
Cov: 27817 -> 27817
Cov: 27817 -> 27817
9559
Cov: 27817 -> 27818
Cov: 27818 -> 27818
9560
Cov: 27818 -> 27818
Cov: 27818 -> 27818
9561
Cov: 27818 -> 27818
Cov: 27818 -> 27818
9562
Cov: 27818 -> 27819
Cov: 27819 -> 27819
9563
Cov: 27819 -> 27819
Cov: 27819 -> 27819
9564
Cov: 27819 -> 27819
Cov: 27819 -> 27819
9565
Cov: 27819 -> 27822
Cov: 27822 -> 27822
9566
Cov: 27822 -> 27823
Cov: 27823 -> 27823
9567
Cov: 27823 -> 27823
Cov: 27823 -> 27823
9568
Cov: 27823 -> 27823
Cov: 27823 -> 27823
9569
Cov: 27823 -> 27823
Cov: 27823 -> 27823
9570
Cov: 27823 -> 27823
Cov: 27823 -> 27823
9571
Cov: 27823 -> 27823
Cov: 27823 -> 27823
9572
Cov: 27823 -> 27823
Cov: 27823 -> 27823
9573
Cov: 27823 -> 27823
Cov: 27823 -> 27823
9574
Cov: 27823 -> 27823
Cov: 27823 -> 27823
9575
Cov: 27823 -> 27823
Cov: 27823 -> 27823
9576
Cov: 27823 -> 27824
Cov: 27824 -> 27824
9577
{"exception": "RuntimeError", "msg": "The expanded size of the tensor (5) must match the existing size (3) at non-singleton dimension 1.  Target sizes: [3, 5].  Tensor sizes: [2, 3]"}
9578
Cov: 27824 -> 27824
Cov: 27824 -> 27824
9579
Cov: 27824 -> 27824
Cov: 27824 -> 27824
9580
Cov: 27824 -> 27824
Cov: 27824 -> 27824
9581
Cov: 27824 -> 27824
Cov: 27824 -> 27824
9582
Cov: 27824 -> 27824
Cov: 27824 -> 27824
9583
Cov: 27824 -> 27824
Cov: 27824 -> 27824
9584
{"exception": "RuntimeError", "msg": "size {[2, 3]} is not expandable to size {[3, 4, 5]}."}
9585
{"exception": "TypeError", "msg": "'getset_descriptor' object is not callable"}
9586
{"exception": "RuntimeError", "msg": "Bad in-place call: input tensor size [10, 20] and output tensor size [20, 40] should match"}
9587
Cov: 27824 -> 27825
Cov: 27825 -> 27825
9588
Cov: 27825 -> 27825
Cov: 27825 -> 27825
9589
Cov: 27825 -> 27825
Cov: 27825 -> 27825
9590
Cov: 27825 -> 27825
Cov: 27825 -> 27825
9591
Cov: 27825 -> 27829
Cov: 27829 -> 27829
9592
{"exception": "RuntimeError", "msg": "The size of tensor a (3) must match the size of tensor b (2) at non-singleton dimension 0"}
9593
Cov: 27829 -> 27829
Cov: 27829 -> 27829
9594
Cov: 27829 -> 27829
Cov: 27829 -> 27829
9595
{"exception": "RuntimeError", "msg": "indices expected sparse coordinate tensor layout but got Strided"}
9596
Cov: 27829 -> 27829
Cov: 27829 -> 27829
9597
Cov: 27829 -> 27829
Cov: 27829 -> 27829
9598
{"exception": "TypeError", "msg": "'getset_descriptor' object is not callable"}
9599
Cov: 27829 -> 27829
Cov: 27829 -> 27829
9600
Cov: 27829 -> 27829
Cov: 27829 -> 27829
9601
Cov: 27829 -> 27829
Cov: 27829 -> 27829
9602
{"exception": "RuntimeError", "msg": "This function was deprecated since version 1.9 and is now removed. `torch.lstsq` is deprecated in favor of `torch.linalg.lstsq`.\n`torch.linalg.lstsq` has reversed arguments and does not return the QR decomposition in the returned tuple (although it returns other information about the problem).\n\nTo get the QR decomposition consider using `torch.linalg.qr`.\n\nThe returned solution in `torch.lstsq` stored the residuals of the solution in the last m - n columns of the returned value whenever m > n. In torch.linalg.lstsq, the residuals are in the field 'residuals' of the returned named tuple.\n\nThe unpacking of the solution, as in\nX, _ = torch.lstsq(B, A).solution[:A.size(1)]\nshould be replaced with:\nX = torch.linalg.lstsq(A, B).solution"}
9603
Cov: 27829 -> 27830
Cov: 27830 -> 27830
9604
Cov: 27830 -> 27830
Cov: 27830 -> 27830
9605
Cov: 27830 -> 27830
Cov: 27830 -> 27830
9606
Cov: 27830 -> 27830
Cov: 27830 -> 27830
9607
Cov: 27830 -> 27830
Cov: 27830 -> 27830
9608
Cov: 27830 -> 27830
Cov: 27830 -> 27830
9609
Cov: 27830 -> 27831
Cov: 27831 -> 27831
9610
Cov: 27831 -> 27831
Cov: 27831 -> 27831
9611
Cov: 27831 -> 27832
Cov: 27832 -> 27832
9612
Cov: 27832 -> 27832
Cov: 27832 -> 27832
9613
Cov: 27832 -> 27832
Cov: 27832 -> 27832
9614
Cov: 27832 -> 27832
Cov: 27832 -> 27832
9615
Cov: 27832 -> 27832
Cov: 27832 -> 27832
9616
Cov: 27832 -> 27832
Cov: 27832 -> 27832
9617
Cov: 27832 -> 27832
Cov: 27832 -> 27832
9618
Cov: 27832 -> 27832
Cov: 27832 -> 27832
9619
{"exception": "RuntimeError", "msg": "linalg.slogdet: A must be batches of square matrices, but they are 3 by 4 matrices"}
9620
Cov: 27832 -> 27832
Cov: 27832 -> 27832
9621
{"exception": "RuntimeError", "msg": "Number of elements of source < number of ones in mask"}
9622
Cov: 27832 -> 27832
Cov: 27832 -> 27832
9623
Cov: 27832 -> 27832
Cov: 27832 -> 27832
9624
Cov: 27832 -> 27832
Cov: 27832 -> 27832
9625
Cov: 27832 -> 27835
Cov: 27835 -> 27835
9626
Cov: 27835 -> 27835
Cov: 27835 -> 27835
9627
Cov: 27835 -> 27835
Cov: 27835 -> 27835
9628
Cov: 27835 -> 27835
Cov: 27835 -> 27835
9629
Cov: 27835 -> 27835
Cov: 27835 -> 27835
9630
Cov: 27835 -> 27835
Cov: 27835 -> 27835
9631
Cov: 27835 -> 27835
Cov: 27835 -> 27835
9632
Cov: 27835 -> 27835
Cov: 27835 -> 27835
9633
Cov: 27835 -> 27835
Cov: 27835 -> 27835
9634
Cov: 27835 -> 27836
Cov: 27836 -> 27836
9635
Cov: 27836 -> 27836
Cov: 27836 -> 27836
9636
Cov: 27836 -> 27837
Cov: 27837 -> 27837
9637
Cov: 27837 -> 27837
Cov: 27837 -> 27837
9638
Cov: 27837 -> 27837
Cov: 27837 -> 27837
9639
Cov: 27837 -> 27837
Cov: 27837 -> 27837
9640
Cov: 27837 -> 27839
Cov: 27839 -> 27839
9641
Cov: 27839 -> 27840
Cov: 27840 -> 27840
9642
{"exception": "TypeError", "msg": "random_() missing 2 required positional argument: \"from\", \"to\""}
9643
Cov: 27840 -> 27840
Cov: 27840 -> 27840
9644
Cov: 27840 -> 27841
Cov: 27841 -> 27841
9645
Cov: 27841 -> 27841
Cov: 27841 -> 27841
9646
Cov: 27841 -> 27844
Cov: 27844 -> 27844
9647
{"exception": "RuntimeError", "msg": "The expanded size of the tensor (3) must match the existing size (5) at non-singleton dimension 1.  Target sizes: [5, 3].  Tensor sizes: [3, 5]"}
9648
Cov: 27844 -> 27845
Cov: 27845 -> 27845
9649
Cov: 27845 -> 27845
Cov: 27845 -> 27845
9650
Cov: 27845 -> 27845
Cov: 27845 -> 27845
9651
{"exception": "RuntimeError", "msg": "addr: Expected 1-D argument vec1, but got 2-D"}
9652
{"exception": "RuntimeError", "msg": "Creating a Tensor subclass from a class that does not inherit from Tensor is not possible. Make sure your class inherits from Tensor."}
9653
{"exception": "RuntimeError", "msg": "The size of tensor a (3) must match the size of tensor b (2) at non-singleton dimension 1"}
9654
Cov: 27845 -> 27845
Cov: 27845 -> 27845
9655
Cov: 27845 -> 27845
Cov: 27845 -> 27845
9656
Cov: 27845 -> 27845
Cov: 27845 -> 27845
9657
Cov: 27845 -> 27846
Cov: 27846 -> 27846
9658
Cov: 27846 -> 27846
Cov: 27846 -> 27846
9659
Cov: 27846 -> 27848
Cov: 27848 -> 27848
9660
Cov: 27848 -> 27848
Cov: 27848 -> 27848
9661
Cov: 27848 -> 27848
Cov: 27848 -> 27848
9662
Cov: 27848 -> 27848
Cov: 27848 -> 27848
9663
Cov: 27848 -> 27849
Cov: 27849 -> 27849
9664
{"exception": "RuntimeError", "msg": "This function was deprecated since version 1.9 and is now removed.\nPlease use the `torch.linalg.matrix_rank` function instead. The parameter 'symmetric' was renamed in `torch.linalg.matrix_rank()` to 'hermitian'."}
9665
Cov: 27849 -> 27850
Cov: 27850 -> 27850
9666
Cov: 27850 -> 27850
Cov: 27850 -> 27850
9667
Cov: 27850 -> 27850
Cov: 27850 -> 27850
9668
Cov: 27850 -> 27850
Cov: 27850 -> 27850
9669
{"exception": "RuntimeError", "msg": "expand(torch.FloatTensor{[10, 3, 4]}, size=[3, 5]): the number of sizes provided (2) must be greater or equal to the number of dimensions in the tensor (3)"}
9670
Cov: 27850 -> 27850
Cov: 27850 -> 27850
9671
Cov: 27850 -> 27851
Cov: 27851 -> 27851
9672
Cov: 27851 -> 27851
Cov: 27851 -> 27851
9673
Cov: 27851 -> 27853
Cov: 27853 -> 27853
9674
Cov: 27853 -> 27853
Cov: 27853 -> 27853
9675
Cov: 27853 -> 27853
Cov: 27853 -> 27853
9676
{"exception": "RuntimeError", "msg": "scatter(): Expected self.dtype to be equal to src.dtype"}
9677
Cov: 27853 -> 27856
Cov: 27856 -> 27856
9678
Cov: 27856 -> 27858
Cov: 27858 -> 27858
9679
Cov: 27858 -> 27858
Cov: 27858 -> 27858
9680
Cov: 27858 -> 27861
Cov: 27861 -> 27861
9681
Cov: 27861 -> 27861
Cov: 27861 -> 27861
9682
Cov: 27861 -> 27861
Cov: 27861 -> 27861
9683
Cov: 27861 -> 27861
Cov: 27861 -> 27861
9684
Cov: 27861 -> 27862
Cov: 27862 -> 27862
9685
Cov: 27862 -> 27863
Cov: 27863 -> 27863
9686
Cov: 27863 -> 27863
Cov: 27863 -> 27863
9687
Cov: 27863 -> 27864
Cov: 27864 -> 27864
9688
Cov: 27864 -> 27864
Cov: 27864 -> 27864
9689
Cov: 27864 -> 27864
Cov: 27864 -> 27864
9690
Cov: 27864 -> 27864
Cov: 27864 -> 27864
9691
Cov: 27864 -> 27864
Cov: 27864 -> 27864
9692
Cov: 27864 -> 27865
Cov: 27865 -> 27865
9693
Cov: 27865 -> 27866
Cov: 27866 -> 27866
9694
Cov: 27866 -> 27867
Cov: 27867 -> 27867
9695
Cov: 27867 -> 27872
Cov: 27872 -> 27872
9696
Cov: 27872 -> 27872
Cov: 27872 -> 27872
9697
Cov: 27872 -> 27872
Cov: 27872 -> 27872
9698
Cov: 27872 -> 27872
Cov: 27872 -> 27872
9699
Cov: 27872 -> 27872
Cov: 27872 -> 27872
9700
Cov: 27872 -> 27872
Cov: 27872 -> 27872
9701
Cov: 27872 -> 27872
Cov: 27872 -> 27872
9702
Cov: 27872 -> 27872
Cov: 27872 -> 27872
9703
Cov: 27872 -> 27873
Cov: 27873 -> 27873
9704
Cov: 27873 -> 27873
Cov: 27873 -> 27873
9705
{"exception": "TypeError", "msg": "'getset_descriptor' object is not callable"}
9706
{"exception": "NotImplementedError", "msg": "Could not run 'aten::sparse_mask' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::sparse_mask' is only available for these backends: [SparseCPU, SparseCUDA, SparseCsrCPU, SparseCsrCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nSparseCPU: registered at aten/src/ATen/RegisterSparseCPU.cpp:1387 [kernel]\nSparseCUDA: registered at aten/src/ATen/RegisterSparseCUDA.cpp:1573 [kernel]\nSparseCsrCPU: registered at aten/src/ATen/RegisterSparseCsrCPU.cpp:1135 [kernel]\nSparseCsrCUDA: registered at aten/src/ATen/RegisterSparseCsrCUDA.cpp:1276 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:153 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:498 [backend fallback]\nFunctionalize: registered at ../aten/src/ATen/FunctionalizeFallbackKernel.cpp:290 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:86 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradMTIA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_0.cpp:16725 [kernel]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:382 [backend fallback]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:249 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:710 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:203 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:161 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:494 [backend fallback]\nPreDispatch: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:165 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:157 [backend fallback]\n"}
9707
Cov: 27873 -> 27873
Cov: 27873 -> 27873
9708
Cov: 27873 -> 27873
Cov: 27873 -> 27873
9709
Cov: 27873 -> 27904
Cov: 27904 -> 27904
9710
Cov: 27904 -> 27905
Cov: 27905 -> 27905
9711
Cov: 27905 -> 27905
Cov: 27905 -> 27905
9712
Cov: 27905 -> 27905
Cov: 27905 -> 27905
9713
Cov: 27905 -> 27905
Cov: 27905 -> 27905
9714
Cov: 27905 -> 27926
Cov: 27926 -> 27926
9715
Cov: 27926 -> 27927
Cov: 27927 -> 27927
9716
Cov: 27927 -> 27928
Cov: 27928 -> 27928
9717
Cov: 27928 -> 27928
Cov: 27928 -> 27928
9718
Cov: 27928 -> 27929
Cov: 27929 -> 27929
9719
Cov: 27929 -> 27929
Cov: 27929 -> 27929
9720
Cov: 27929 -> 27929
Cov: 27929 -> 27929
9721
Cov: 27929 -> 27929
Cov: 27929 -> 27929
9722
Cov: 27929 -> 27929
Cov: 27929 -> 27929
9723
Cov: 27929 -> 27929
Cov: 27929 -> 27929
9724
Cov: 27929 -> 27929
Cov: 27929 -> 27929
9725
Cov: 27929 -> 27929
Cov: 27929 -> 27929
9726
Cov: 27929 -> 27929
Cov: 27929 -> 27929
9727
Cov: 27929 -> 27929
Cov: 27929 -> 27929
9728
Cov: 27929 -> 27930
Cov: 27930 -> 27930
9729
Cov: 27930 -> 27934
Cov: 27934 -> 27934
9730
Cov: 27934 -> 27934
Cov: 27934 -> 27934
9731
Cov: 27934 -> 27935
Cov: 27935 -> 27935
9732
Cov: 27935 -> 27935
Cov: 27935 -> 27935
9733
Cov: 27935 -> 27935
Cov: 27935 -> 27935
9734
Cov: 27935 -> 27935
Cov: 27935 -> 27935
9735
Cov: 27935 -> 27935
Cov: 27935 -> 27935
9736
Cov: 27935 -> 27935
Cov: 27935 -> 27935
9737
Cov: 27935 -> 27936
Cov: 27936 -> 27936
9738
Cov: 27936 -> 27936
Cov: 27936 -> 27936
9739
Cov: 27936 -> 27936
Cov: 27936 -> 27936
9740
Cov: 27936 -> 27936
Cov: 27936 -> 27936
9741
Cov: 27936 -> 27936
Cov: 27936 -> 27936
9742
{"exception": "RuntimeError", "msg": "istft requires a complex-valued input tensor matching the output from stft with return_complex=True."}
9743
Cov: 27936 -> 27936
Cov: 27936 -> 27936
9744
Cov: 27936 -> 27936
Cov: 27936 -> 27936
9745
Cov: 27936 -> 27936
Cov: 27936 -> 27936
9746
Cov: 27936 -> 27936
Cov: 27936 -> 27936
9747
Cov: 27936 -> 27936
Cov: 27936 -> 27936
9748
Cov: 27936 -> 27936
Cov: 27936 -> 27936
9749
Cov: 27936 -> 27936
Cov: 27936 -> 27936
9750
Cov: 27936 -> 27936
Cov: 27936 -> 27936
9751
Cov: 27936 -> 27936
Cov: 27936 -> 27936
9752
Cov: 27936 -> 27937
Cov: 27937 -> 27937
9753
Cov: 27937 -> 27937
Cov: 27937 -> 27937
9754
Cov: 27937 -> 27994
Cov: 27994 -> 27994
9755
Cov: 27994 -> 27995
Cov: 27995 -> 27995
9756
Cov: 27995 -> 27995
Cov: 27995 -> 27995
9757
Cov: 27995 -> 27995
Cov: 27995 -> 27995
9758
Cov: 27995 -> 27995
Cov: 27995 -> 27995
9759
Cov: 27995 -> 27995
Cov: 27995 -> 27995
9760
Cov: 27995 -> 27995
Cov: 27995 -> 27995
9761
Cov: 27995 -> 27996
Cov: 27996 -> 27996
9762
Cov: 27996 -> 27999
Cov: 27999 -> 27999
9763
Cov: 27999 -> 27999
Cov: 27999 -> 27999
9764
Cov: 27999 -> 27999
Cov: 27999 -> 27999
9765
Cov: 27999 -> 27999
Cov: 27999 -> 27999
9766
Cov: 27999 -> 27999
Cov: 27999 -> 27999
9767
Cov: 27999 -> 27999
Cov: 27999 -> 27999
9768
Cov: 27999 -> 27999
Cov: 27999 -> 27999
9769
Cov: 27999 -> 27999
Cov: 27999 -> 27999
9770
Cov: 27999 -> 27999
Cov: 27999 -> 27999
9771
Cov: 27999 -> 27999
Cov: 27999 -> 27999
9772
Cov: 27999 -> 27999
Cov: 27999 -> 27999
9773
Cov: 27999 -> 28001
Cov: 28001 -> 28001
9774
Cov: 28001 -> 28001
Cov: 28001 -> 28001
9775
Cov: 28001 -> 28001
Cov: 28001 -> 28001
9776
Cov: 28001 -> 28001
Cov: 28001 -> 28001
9777
Cov: 28001 -> 28001
Cov: 28001 -> 28001
9778
Cov: 28001 -> 28001
Cov: 28001 -> 28001
9779
Cov: 28001 -> 28001
Cov: 28001 -> 28001
9780
Cov: 28001 -> 28002
Cov: 28002 -> 28002
9781
Cov: 28002 -> 28002
Cov: 28002 -> 28002
9782
Cov: 28002 -> 28002
Cov: 28002 -> 28002
9783
Cov: 28002 -> 28002
Cov: 28002 -> 28002
9784
Cov: 28002 -> 28002
Cov: 28002 -> 28002
9785
Cov: 28002 -> 28002
Cov: 28002 -> 28002
9786
Cov: 28002 -> 28002
Cov: 28002 -> 28002
9787
Cov: 28002 -> 28002
Cov: 28002 -> 28002
9788
Cov: 28002 -> 28002
Cov: 28002 -> 28002
9789
Cov: 28002 -> 28003
Cov: 28003 -> 28003
9790
Cov: 28003 -> 28003
Cov: 28003 -> 28003
9791
Cov: 28003 -> 28003
Cov: 28003 -> 28003
9792
Cov: 28003 -> 28003
Cov: 28003 -> 28003
9793
Cov: 28003 -> 28003
Cov: 28003 -> 28003
9794
Cov: 28003 -> 28003
Cov: 28003 -> 28003
9795
Cov: 28003 -> 28302
Cov: 28302 -> 28302
9796
Cov: 28302 -> 28302
Cov: 28302 -> 28302
9797
{"exception": "RuntimeError", "msg": "Bad in-place call: input tensor size [2, 3] and output tensor size [2, 4] should match"}
9798
Cov: 28302 -> 28303
Cov: 28303 -> 28303
9799
Cov: 28303 -> 28303
Cov: 28303 -> 28303
9800
Cov: 28303 -> 28303
Cov: 28303 -> 28303
9801
Cov: 28303 -> 28303
Cov: 28303 -> 28303
9802
Cov: 28303 -> 28303
Cov: 28303 -> 28303
9803
Cov: 28303 -> 28304
Cov: 28304 -> 28304
9804
{"exception": "TypeError", "msg": "addbmm() received an invalid combination of arguments - got (Tensor), but expected (Tensor batch1, Tensor batch2, *, Number beta, Number alpha)"}
9805
Cov: 28304 -> 28304
Cov: 28304 -> 28304
9806
Cov: 28304 -> 28305
Cov: 28305 -> 28305
9807
Cov: 28305 -> 28305
Cov: 28305 -> 28305
9808
{"exception": "RuntimeError", "msg": "istft requires a complex-valued input tensor matching the output from stft with return_complex=True."}
9809
Cov: 28305 -> 28306
Cov: 28306 -> 28306
9810
Cov: 28306 -> 28309
Cov: 28309 -> 28309
9811
Cov: 28309 -> 28309
Cov: 28309 -> 28309
9812
Cov: 28309 -> 28309
Cov: 28309 -> 28309
9813
{"exception": "RuntimeError", "msg": "masked_scatter_ only supports boolean masks, but got mask with dtype Byte"}
9814
Cov: 28309 -> 28311
Cov: 28311 -> 28311
9815
Cov: 28311 -> 28311
Cov: 28311 -> 28311
9816
Cov: 28311 -> 28311
Cov: 28311 -> 28311
9817
Cov: 28311 -> 28311
Cov: 28311 -> 28311
9818
{"exception": "TypeError", "msg": "'Tensor' object is not callable"}
9819
Cov: 28311 -> 28317
Cov: 28317 -> 28317
9820
Cov: 28317 -> 28317
Cov: 28317 -> 28317
9821
Cov: 28317 -> 28317
Cov: 28317 -> 28317
9822
Cov: 28317 -> 28317
Cov: 28317 -> 28317
9823
Cov: 28317 -> 28317
Cov: 28317 -> 28317
9824
Cov: 28317 -> 28317
Cov: 28317 -> 28317
9825
Cov: 28317 -> 28319
Cov: 28319 -> 28319
9826
Cov: 28319 -> 28319
Cov: 28319 -> 28319
9827
Cov: 28319 -> 28319
Cov: 28319 -> 28319
9828
Cov: 28319 -> 28319
Cov: 28319 -> 28319
9829
Cov: 28319 -> 28319
Cov: 28319 -> 28319
9830
Cov: 28319 -> 28320
Cov: 28320 -> 28320
9831
Cov: 28320 -> 28320
Cov: 28320 -> 28320
9832
Cov: 28320 -> 28320
Cov: 28320 -> 28320
9833
Cov: 28320 -> 28320
Cov: 28320 -> 28320
9834
Cov: 28320 -> 28322
Cov: 28322 -> 28322
9835
Cov: 28322 -> 28322
Cov: 28322 -> 28322
9836
Cov: 28322 -> 28322
Cov: 28322 -> 28322
9837
Cov: 28322 -> 28322
Cov: 28322 -> 28322
9838
Cov: 28322 -> 28322
Cov: 28322 -> 28322
9839
Cov: 28322 -> 28322
Cov: 28322 -> 28322
9840
Cov: 28322 -> 28322
Cov: 28322 -> 28322
9841
Cov: 28322 -> 28322
Cov: 28322 -> 28322
9842
Cov: 28322 -> 28322
Cov: 28322 -> 28322
9843
Cov: 28322 -> 28323
Cov: 28323 -> 28323
9844
Cov: 28323 -> 28323
Cov: 28323 -> 28323
9845
Cov: 28323 -> 28323
Cov: 28323 -> 28323
9846
Cov: 28323 -> 28323
Cov: 28323 -> 28323
9847
Cov: 28323 -> 28323
Cov: 28323 -> 28323
9848
Cov: 28323 -> 28323
Cov: 28323 -> 28323
9849
Cov: 28323 -> 28323
Cov: 28323 -> 28323
9850
Cov: 28323 -> 28323
Cov: 28323 -> 28323
9851
Cov: 28323 -> 28323
Cov: 28323 -> 28323
9852
Cov: 28323 -> 28323
Cov: 28323 -> 28323
9853
Cov: 28323 -> 28324
Cov: 28324 -> 28324
9854
Cov: 28324 -> 28324
Cov: 28324 -> 28324
9855
Cov: 28324 -> 28325
Cov: 28325 -> 28325
9856
Cov: 28325 -> 28325
Cov: 28325 -> 28325
9857
Cov: 28325 -> 28325
Cov: 28325 -> 28325
9858
Cov: 28325 -> 28325
Cov: 28325 -> 28325
9859
Cov: 28325 -> 28326
Cov: 28326 -> 28326
9860
{"exception": "TypeError", "msg": "set_() received an invalid combination of arguments - got (stride=NoneType, size=NoneType, storage_offset=int, source=NoneType, ), but expected one of:\n * ()\n * (torch.Storage source)\n * (torch.Storage source, int storage_offset, tuple of ints size, tuple of ints stride)\n      didn't match because some of the arguments have invalid types: (\u001b[31;1msource=NoneType\u001b[0m, \u001b[32;1mstorage_offset=int\u001b[0m, \u001b[31;1msize=NoneType\u001b[0m, \u001b[31;1mstride=NoneType\u001b[0m, )\n * (Tensor source)\n * (Tensor source, int storage_offset, tuple of ints size, tuple of ints stride)\n      didn't match because some of the arguments have invalid types: (\u001b[31;1msource=NoneType\u001b[0m, \u001b[32;1mstorage_offset=int\u001b[0m, \u001b[31;1msize=NoneType\u001b[0m, \u001b[31;1mstride=NoneType\u001b[0m, )\n"}
9861
Cov: 28326 -> 28326
Cov: 28326 -> 28326
9862
Cov: 28326 -> 28335
Cov: 28335 -> 28335
9863
Cov: 28335 -> 28335
Cov: 28335 -> 28335
9864
Cov: 28335 -> 28335
Cov: 28335 -> 28335
9865
Cov: 28335 -> 28335
Cov: 28335 -> 28335
9866
Cov: 28335 -> 28335
Cov: 28335 -> 28335
9867
Cov: 28335 -> 28335
Cov: 28335 -> 28335
9868
Cov: 28335 -> 28336
Cov: 28336 -> 28336
9869
Cov: 28336 -> 28337
Cov: 28337 -> 28337
9870
Cov: 28337 -> 28337
Cov: 28337 -> 28337
9871
Cov: 28337 -> 28337
Cov: 28337 -> 28337
9872
Cov: 28337 -> 28337
Cov: 28337 -> 28337
9873
{"exception": "RuntimeError", "msg": "torch.ormqr: Expected tau to have one dimension less than input, but got tau.ndim equal to 2 and input.ndim is equal to 2"}
9874
Cov: 28337 -> 28337
Cov: 28337 -> 28337
9875
Cov: 28337 -> 28337
Cov: 28337 -> 28337
9876
Cov: 28337 -> 28337
Cov: 28337 -> 28337
9877
Cov: 28337 -> 28338
Cov: 28338 -> 28338
9878
Cov: 28338 -> 28338
Cov: 28338 -> 28338
9879
Cov: 28338 -> 28338
Cov: 28338 -> 28338
9880
Cov: 28338 -> 28338
Cov: 28338 -> 28338
9881
Cov: 28338 -> 28338
Cov: 28338 -> 28338
9882
Cov: 28338 -> 28340
Cov: 28340 -> 28340
9883
Cov: 28340 -> 28340
Cov: 28340 -> 28340
9884
Cov: 28340 -> 28342
Cov: 28342 -> 28342
9885
Cov: 28342 -> 28343
Cov: 28343 -> 28343
9886
Cov: 28343 -> 28343
Cov: 28343 -> 28343
9887
Cov: 28343 -> 28343
Cov: 28343 -> 28343
9888
Cov: 28343 -> 28343
Cov: 28343 -> 28343
9889
Cov: 28343 -> 28343
Cov: 28343 -> 28343
9890
Cov: 28343 -> 28343
Cov: 28343 -> 28343
9891
Cov: 28343 -> 28343
Cov: 28343 -> 28343
9892
Cov: 28343 -> 28343
Cov: 28343 -> 28343
9893
Cov: 28343 -> 28345
Cov: 28345 -> 28345
9894
Cov: 28345 -> 28345
Cov: 28345 -> 28345
9895
Cov: 28345 -> 28345
Cov: 28345 -> 28345
9896
Cov: 28345 -> 28345
Cov: 28345 -> 28345
9897
Cov: 28345 -> 28346
Cov: 28346 -> 28346
9898
{"exception": "RuntimeError", "msg": "\"lshift_cpu\" not implemented for 'Float'"}
9899
Cov: 28346 -> 28346
Cov: 28346 -> 28346
9900
Cov: 28346 -> 28346
Cov: 28346 -> 28346
9901
Cov: 28346 -> 28346
Cov: 28346 -> 28346
9902
{"exception": "RuntimeError", "msg": "This function was deprecated since version 1.9 and is now removed. The default behavior has changed from using the upper triangular portion of the matrix by default to using the lower triangular portion.\n\nL, _ = torch.symeig(A, upper=upper) should be replaced with:\nL = torch.linalg.eigvalsh(A, UPLO='U' if upper else 'L')\n\nand\n\nL, V = torch.symeig(A, eigenvectors=True) should be replaced with:\nL, V = torch.linalg.eigh(A, UPLO='U' if upper else 'L')"}
9903
Cov: 28346 -> 28347
Cov: 28347 -> 28347
9904
Cov: 28347 -> 28347
Cov: 28347 -> 28347
9905
Cov: 28347 -> 28347
Cov: 28347 -> 28347
9906
Cov: 28347 -> 28350
Cov: 28350 -> 28350
9907
Cov: 28350 -> 28350
Cov: 28350 -> 28350
9908
Cov: 28350 -> 28350
Cov: 28350 -> 28350
9909
Cov: 28350 -> 28350
Cov: 28350 -> 28350
9910
Cov: 28350 -> 28350
Cov: 28350 -> 28350
9911
Cov: 28350 -> 28350
Cov: 28350 -> 28350
9912
Cov: 28350 -> 28350
Cov: 28350 -> 28350
9913
Cov: 28350 -> 28350
Cov: 28350 -> 28350
9914
Cov: 28350 -> 28350
Cov: 28350 -> 28350
9915
Cov: 28350 -> 28351
Cov: 28351 -> 28351
9916
Cov: 28351 -> 28351
Cov: 28351 -> 28351
9917
Cov: 28351 -> 28351
Cov: 28351 -> 28351
9918
Cov: 28351 -> 28351
Cov: 28351 -> 28351
9919
Cov: 28351 -> 28352
Cov: 28352 -> 28352
9920
Cov: 28352 -> 28352
Cov: 28352 -> 28352
9921
Cov: 28352 -> 28352
Cov: 28352 -> 28352
9922
Cov: 28352 -> 28352
Cov: 28352 -> 28352
9923
Cov: 28352 -> 28352
Cov: 28352 -> 28352
9924
Cov: 28352 -> 28352
Cov: 28352 -> 28352
9925
Cov: 28352 -> 28352
Cov: 28352 -> 28352
9926
Cov: 28352 -> 28352
Cov: 28352 -> 28352
9927
Cov: 28352 -> 28352
Cov: 28352 -> 28352
9928
{"exception": "RuntimeError", "msg": "tensor.sspaddmm(...) can only be called on sparse tensors"}
9929
Cov: 28352 -> 28352
Cov: 28352 -> 28352
9930
Cov: 28352 -> 28352
Cov: 28352 -> 28352
9931
Cov: 28352 -> 28352
Cov: 28352 -> 28352
9932
Cov: 28352 -> 28353
Cov: 28353 -> 28353
9933
Cov: 28353 -> 28353
Cov: 28353 -> 28353
9934
Cov: 28353 -> 28353
Cov: 28353 -> 28353
9935
Cov: 28353 -> 28353
Cov: 28353 -> 28353
9936
Cov: 28353 -> 28353
Cov: 28353 -> 28353
9937
Cov: 28353 -> 28353
Cov: 28353 -> 28353
9938
Cov: 28353 -> 28353
Cov: 28353 -> 28353
9939
Cov: 28353 -> 28353
Cov: 28353 -> 28353
9940
Cov: 28353 -> 28354
Cov: 28354 -> 28354
9941
Cov: 28354 -> 28354
Cov: 28354 -> 28354
9942
Cov: 28354 -> 28354
Cov: 28354 -> 28354
9943
Cov: 28354 -> 28354
Cov: 28354 -> 28354
9944
Cov: 28354 -> 28354
Cov: 28354 -> 28354
9945
Cov: 28354 -> 28354
Cov: 28354 -> 28354
9946
Cov: 28354 -> 28354
Cov: 28354 -> 28354
9947
Cov: 28354 -> 28355
Cov: 28355 -> 28355
9948
Cov: 28355 -> 28355
Cov: 28355 -> 28355
9949
Cov: 28355 -> 28355
Cov: 28355 -> 28355
9950
Cov: 28355 -> 28356
Cov: 28356 -> 28356
9951
Cov: 28356 -> 28356
Cov: 28356 -> 28356
9952
{"exception": "RuntimeError", "msg": "masked_scatter: expected self and source to have same dtypes but gotDouble and Long"}
9953
Cov: 28356 -> 28356
Cov: 28356 -> 28356
9954
Cov: 28356 -> 28356
Cov: 28356 -> 28356
9955
Cov: 28356 -> 28356
Cov: 28356 -> 28356
9956
Cov: 28356 -> 28356
Cov: 28356 -> 28356
9957
Cov: 28356 -> 28356
Cov: 28356 -> 28356
9958
Cov: 28356 -> 28356
Cov: 28356 -> 28356
9959
Cov: 28356 -> 28356
Cov: 28356 -> 28356
9960
Cov: 28356 -> 28356
Cov: 28356 -> 28356
9961
Cov: 28356 -> 28356
Cov: 28356 -> 28356
9962
Cov: 28356 -> 28356
Cov: 28356 -> 28356
9963
{"exception": "RuntimeError", "msg": "Creating a Tensor subclass from a class that does not inherit from Tensor is not possible. Make sure your class inherits from Tensor."}
9964
Cov: 28356 -> 28356
Cov: 28356 -> 28356
9965
Cov: 28356 -> 28357
Cov: 28357 -> 28357
9966
Cov: 28357 -> 28357
Cov: 28357 -> 28357
9967
Cov: 28357 -> 28358
Cov: 28358 -> 28358
9968
Cov: 28358 -> 28358
Cov: 28358 -> 28358
9969
Cov: 28358 -> 28358
Cov: 28358 -> 28358
9970
Cov: 28358 -> 28358
Cov: 28358 -> 28358
9971
Cov: 28358 -> 28358
Cov: 28358 -> 28358
9972
Cov: 28358 -> 28358
Cov: 28358 -> 28358
9973
Cov: 28358 -> 28358
Cov: 28358 -> 28358
9974
{"exception": "TypeError", "msg": "heaviside(): argument 'values' must be Tensor, not float"}
9975
Cov: 28358 -> 28358
Cov: 28358 -> 28358
9976
Cov: 28358 -> 28359
Cov: 28359 -> 28359
9977
Cov: 28359 -> 28359
Cov: 28359 -> 28359
9978
Cov: 28359 -> 28359
Cov: 28359 -> 28359
9979
Cov: 28359 -> 28359
Cov: 28359 -> 28359
9980
{"exception": "TypeError", "msg": "heaviside(): argument 'values' (position 1) must be Tensor, not int"}
9981
Cov: 28359 -> 28360
Cov: 28360 -> 28360
9982
Cov: 28360 -> 28360
Cov: 28360 -> 28360
9983
Cov: 28360 -> 28360
Cov: 28360 -> 28360
9984
Cov: 28360 -> 28363
Cov: 28363 -> 28363
9985
Cov: 28363 -> 28363
Cov: 28363 -> 28363
9986
Cov: 28363 -> 28363
Cov: 28363 -> 28363
9987
Cov: 28363 -> 28363
Cov: 28363 -> 28363
9988
Cov: 28363 -> 28363
Cov: 28363 -> 28363
9989
Cov: 28363 -> 28364
Cov: 28364 -> 28364
9990
Cov: 28364 -> 28364
Cov: 28364 -> 28364
9991
Cov: 28364 -> 28364
Cov: 28364 -> 28364
9992
Cov: 28364 -> 28364
Cov: 28364 -> 28364
9993
Cov: 28364 -> 28364
Cov: 28364 -> 28364
9994
{"exception": "RuntimeError", "msg": "torch.triangular_solve: Expected b to have at least 2 dimensions, but it has 1 dimensions instead"}
9995
Cov: 28364 -> 28364
Cov: 28364 -> 28364
9996
Cov: 28364 -> 28364
Cov: 28364 -> 28364
9997
Cov: 28364 -> 28364
Cov: 28364 -> 28364
9998
Cov: 28364 -> 28364
Cov: 28364 -> 28364
9999
Cov: 28364 -> 28371
Cov: 28371 -> 28371
10000
Cov: 28371 -> 28371
Cov: 28371 -> 28371

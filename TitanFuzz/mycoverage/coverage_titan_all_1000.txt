
1
{"exception": "TypeError", "msg": "index_add() received an invalid combination of arguments - got (tensor2=Tensor, index=Tensor, dim=int, ), but expected one of:\n * (int dim, Tensor index, Tensor source, *, Number alpha)\n * (name dim, Tensor index, Tensor source, *, Number alpha)\n"}
2
Cov: 0 -> 0
Cov: 0 -> 0
3
Cov: 0 -> 0
Cov: 0 -> 0
4
Cov: 0 -> 0
Cov: 0 -> 0
5
Cov: 0 -> 58
Cov: 58 -> 58
6
Cov: 58 -> 58
Cov: 58 -> 58
7
Cov: 58 -> 58
Cov: 58 -> 58
8
Cov: 58 -> 58
Cov: 58 -> 58
9
{"exception": "TypeError", "msg": "new_full(): argument 'size' (position 1) must be tuple of ints, not int"}
10
Cov: 58 -> 58
Cov: 58 -> 58
11
{"exception": "RuntimeError", "msg": "norm(): input dtype should be either floating point or complex. Got Long instead."}
12
Cov: 58 -> 58
Cov: 58 -> 58
13
Cov: 58 -> 58
Cov: 58 -> 58
14
Cov: 58 -> 255
Cov: 255 -> 255
15
{"exception": "RuntimeError", "msg": "1D tensors expected, but got 2D and 2D tensors"}
16
Cov: 255 -> 255
Cov: 255 -> 255
17
Cov: 255 -> 255
Cov: 255 -> 255
18
Cov: 255 -> 255
Cov: 255 -> 255
19
Cov: 255 -> 294
Cov: 294 -> 294
20
Cov: 294 -> 294
Cov: 294 -> 294
21
Cov: 294 -> 294
Cov: 294 -> 294
22
Cov: 294 -> 294
Cov: 294 -> 294
23
{"exception": "RuntimeError", "msg": "setStorage: sizes [8, 2], strides [8, 4], storage offset 0, and itemsize 8 requiring a storage size of 488 are out of bounds for storage of size 128"}
24
{"exception": "RuntimeError", "msg": "result type Float can't be cast to the desired output type Long"}
25
Cov: 294 -> 294
Cov: 294 -> 294
26
Cov: 294 -> 294
Cov: 294 -> 294
27
Cov: 294 -> 294
Cov: 294 -> 294
28
Cov: 294 -> 294
Cov: 294 -> 294
29
Cov: 294 -> 294
Cov: 294 -> 294
30
Cov: 294 -> 294
Cov: 294 -> 294
31
Cov: 294 -> 296
Cov: 296 -> 296
32
Cov: 296 -> 296
Cov: 296 -> 296
33
Cov: 296 -> 296
Cov: 296 -> 296
34
Cov: 296 -> 296
Cov: 296 -> 296
35
Cov: 296 -> 296
Cov: 296 -> 296
36
Cov: 296 -> 296
Cov: 296 -> 296
37
Cov: 296 -> 296
Cov: 296 -> 296
38
Cov: 296 -> 302
Cov: 302 -> 302
39
{"exception": "RuntimeError", "msg": "This function was deprecated since version 1.9 and is now removed. The default behavior has changed from using the upper triangular portion of the matrix by default to using the lower triangular portion.\n\nL, _ = torch.symeig(A, upper=upper) should be replaced with:\nL = torch.linalg.eigvalsh(A, UPLO='U' if upper else 'L')\n\nand\n\nL, V = torch.symeig(A, eigenvectors=True) should be replaced with:\nL, V = torch.linalg.eigh(A, UPLO='U' if upper else 'L')"}
40
Cov: 302 -> 305
Cov: 305 -> 305
41
Cov: 305 -> 380
Cov: 380 -> 380
42
{"exception": "RuntimeError", "msg": "scatter(): Expected self.dtype to be equal to src.dtype"}
43
Cov: 380 -> 413
Cov: 413 -> 413
44
Cov: 413 -> 435
Cov: 435 -> 435
45
Cov: 435 -> 435
Cov: 435 -> 435
46
Cov: 435 -> 435
Cov: 435 -> 435
47
Cov: 435 -> 435
Cov: 435 -> 435
48
Cov: 435 -> 437
Cov: 437 -> 437
49
Cov: 437 -> 437
Cov: 437 -> 437
50
{"exception": "RuntimeError", "msg": "masked_fill_ only supports a 0-dimensional value tensor, but got tensor with 1 dimension(s)."}
51
Cov: 437 -> 437
Cov: 437 -> 437
52
Cov: 437 -> 437
Cov: 437 -> 437
53
Cov: 437 -> 437
Cov: 437 -> 437
54
Cov: 437 -> 437
Cov: 437 -> 437
55
{"exception": "RuntimeError", "msg": "The expanded size of the tensor (5) must match the existing size (4) at non-singleton dimension 2.  Target sizes: [10, 3, 5].  Tensor sizes: [10, 3, 4]"}
56
Cov: 437 -> 437
Cov: 437 -> 437
57
{"exception": "RuntimeError", "msg": "required rank 4 tensor to use channels_last format"}
58
Cov: 437 -> 442
Cov: 442 -> 442
59
Cov: 442 -> 461
Cov: 461 -> 461
60
{"exception": "NotImplementedError", "msg": "Could not run 'aten::q_per_channel_zero_points' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::q_per_channel_zero_points' is only available for these backends: [QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nQuantizedCPU: registered at aten/src/ATen/RegisterQuantizedCPU.cpp:944 [kernel]\nQuantizedCUDA: registered at aten/src/ATen/RegisterQuantizedCUDA.cpp:459 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:153 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:498 [backend fallback]\nFunctionalize: registered at ../aten/src/ATen/FunctionalizeFallbackKernel.cpp:290 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:86 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradMTIA: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_1.cpp:16002 [kernel]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:382 [backend fallback]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:249 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:710 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:203 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:161 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:494 [backend fallback]\nPreDispatch: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:165 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:157 [backend fallback]\n"}
61
{"exception": "TypeError", "msg": "new_full(): argument 'size' (position 1) must be tuple of ints, not int"}
62
Cov: 461 -> 461
Cov: 461 -> 461
63
Cov: 461 -> 468
Cov: 468 -> 468
64
Cov: 468 -> 468
Cov: 468 -> 468
65
Cov: 468 -> 468
Cov: 468 -> 468
66
Cov: 468 -> 468
Cov: 468 -> 468
67
Cov: 468 -> 468
Cov: 468 -> 468
68
Cov: 468 -> 468
Cov: 468 -> 468
69
Cov: 468 -> 608
Cov: 608 -> 608
70
Cov: 608 -> 608
Cov: 608 -> 608
71
{"exception": "RuntimeError", "msg": "size {[2, 3]} is not expandable to size {[5, 5]}."}
72
{"exception": "RuntimeError", "msg": "Please look up dimensions by name, got: name = None."}
73
Cov: 608 -> 608
Cov: 608 -> 608
74
Cov: 608 -> 608
Cov: 608 -> 608
75
Cov: 608 -> 612
Cov: 612 -> 612
76
Cov: 612 -> 612
Cov: 612 -> 612
77
Cov: 612 -> 612
Cov: 612 -> 612
78
Cov: 612 -> 612
Cov: 612 -> 612
79
Cov: 612 -> 612
Cov: 612 -> 612
80
Cov: 612 -> 612
Cov: 612 -> 612
81
Cov: 612 -> 612
Cov: 612 -> 612
82
Cov: 612 -> 612
Cov: 612 -> 612
83
{"exception": "RuntimeError", "msg": "a leaf Variable that requires grad is being used in an in-place operation."}
84
Cov: 612 -> 612
Cov: 612 -> 612
85
Cov: 612 -> 613
Cov: 613 -> 613
86
Cov: 613 -> 613
Cov: 613 -> 613
87
Cov: 613 -> 613
Cov: 613 -> 613
88
Cov: 613 -> 613
Cov: 613 -> 613
89
Cov: 613 -> 613
Cov: 613 -> 613
90
Cov: 613 -> 613
Cov: 613 -> 613
91
Cov: 613 -> 613
Cov: 613 -> 613
92
Cov: 613 -> 618
Cov: 618 -> 618
93
Cov: 618 -> 618
Cov: 618 -> 618
94
Cov: 618 -> 618
Cov: 618 -> 618
95
Cov: 618 -> 618
Cov: 618 -> 618
96
Cov: 618 -> 618
Cov: 618 -> 618
97
Cov: 618 -> 623
Cov: 623 -> 623
98
Cov: 623 -> 623
Cov: 623 -> 623
99
Cov: 623 -> 644
Cov: 644 -> 644
100
Cov: 644 -> 648
Cov: 648 -> 648
101
{"exception": "TypeError", "msg": "descriptor 'new_tensor' for 'torch._C._TensorBase' objects doesn't apply to a 'list' object"}
102
Cov: 648 -> 689
Cov: 689 -> 689
103
Cov: 689 -> 694
Cov: 694 -> 694
104
Cov: 694 -> 694
Cov: 694 -> 694
105
Cov: 694 -> 694
Cov: 694 -> 694
106
Cov: 694 -> 694
Cov: 694 -> 694
107
Cov: 694 -> 694
Cov: 694 -> 694
108
Cov: 694 -> 694
Cov: 694 -> 694
109
Cov: 694 -> 694
Cov: 694 -> 694
110
{"exception": "RuntimeError", "msg": "expand(torch.FloatTensor{[10, 3, 4]}, size=[3, 5]): the number of sizes provided (2) must be greater or equal to the number of dimensions in the tensor (3)"}
111
Cov: 694 -> 694
Cov: 694 -> 694
112
Cov: 694 -> 707
Cov: 707 -> 707
113
Cov: 707 -> 707
Cov: 707 -> 707
114
Cov: 707 -> 707
Cov: 707 -> 707
115
Cov: 707 -> 707
Cov: 707 -> 707
116
Cov: 707 -> 707
Cov: 707 -> 707
117
Cov: 707 -> 707
Cov: 707 -> 707
118
Cov: 707 -> 728
Cov: 728 -> 728
119
Cov: 728 -> 729
Cov: 729 -> 729
120
Cov: 729 -> 729
Cov: 729 -> 729
121
Cov: 729 -> 729
Cov: 729 -> 729
122
Cov: 729 -> 729
Cov: 729 -> 729
123
Cov: 729 -> 729
Cov: 729 -> 729
124
Cov: 729 -> 741
Cov: 741 -> 741
125
{"exception": "RuntimeError", "msg": "torch.ormqr: other.shape[-2] must be equal to input.shape[-2]"}
126
Cov: 741 -> 741
Cov: 741 -> 741
127
Cov: 741 -> 745
Cov: 745 -> 745
128
{"exception": "RuntimeError", "msg": "torch.triangular_solve: Expected b to have at least 2 dimensions, but it has 1 dimensions instead"}
129
Cov: 745 -> 745
Cov: 745 -> 745
130
Cov: 745 -> 745
Cov: 745 -> 745
131
Cov: 745 -> 745
Cov: 745 -> 745
132
{"exception": "RuntimeError", "msg": "Index tensor must have the same number of dimensions as self tensor"}
133
Cov: 745 -> 745
Cov: 745 -> 745
134
Cov: 745 -> 745
Cov: 745 -> 745
135
Cov: 745 -> 751
Cov: 751 -> 751
136
Cov: 751 -> 751
Cov: 751 -> 751
137
Cov: 751 -> 751
Cov: 751 -> 751
138
Cov: 751 -> 754
Cov: 754 -> 754
139
Cov: 754 -> 754
Cov: 754 -> 754
140
Cov: 754 -> 763
Cov: 763 -> 763
141
Cov: 763 -> 764
Cov: 764 -> 764
142
Cov: 764 -> 764
Cov: 764 -> 764
143
Cov: 764 -> 767
Cov: 767 -> 767
144
Cov: 767 -> 770
Cov: 770 -> 770
145
Cov: 770 -> 770
Cov: 770 -> 770
146
Cov: 770 -> 880
Cov: 880 -> 880
147
Cov: 880 -> 880
Cov: 880 -> 880
148
Cov: 880 -> 888
Cov: 888 -> 888
149
Cov: 888 -> 888
Cov: 888 -> 888
150
Cov: 888 -> 888
Cov: 888 -> 888
151
Cov: 888 -> 888
Cov: 888 -> 888
152
Cov: 888 -> 888
Cov: 888 -> 888
153
{"exception": "RuntimeError", "msg": "This function was deprecated since version 1.9 and is now removed. The default behavior has changed from using the upper triangular portion of the matrix by default to using the lower triangular portion.\n\nL, _ = torch.symeig(A, upper=upper) should be replaced with:\nL = torch.linalg.eigvalsh(A, UPLO='U' if upper else 'L')\n\nand\n\nL, V = torch.symeig(A, eigenvectors=True) should be replaced with:\nL, V = torch.linalg.eigh(A, UPLO='U' if upper else 'L')"}
154
Cov: 888 -> 889
Cov: 889 -> 889
155
Cov: 889 -> 889
Cov: 889 -> 889
156
Cov: 889 -> 889
Cov: 889 -> 889
157
Cov: 889 -> 889
Cov: 889 -> 889
158
Cov: 889 -> 892
Cov: 892 -> 892
159
Cov: 892 -> 895
Cov: 895 -> 895
160
Cov: 895 -> 895
Cov: 895 -> 895
161
Cov: 895 -> 900
Cov: 900 -> 900
162
Cov: 900 -> 900
Cov: 900 -> 900
163
Cov: 900 -> 900
Cov: 900 -> 900
164
{"exception": "RuntimeError", "msg": "t_() expects a tensor with <= 2 dimensions, but self is 4D"}
165
Cov: 900 -> 900
Cov: 900 -> 900
166
Cov: 900 -> 900
Cov: 900 -> 900
167
Cov: 900 -> 905
Cov: 905 -> 905
168
Cov: 905 -> 905
Cov: 905 -> 905
169
Cov: 905 -> 905
Cov: 905 -> 905
170
Cov: 905 -> 906
Cov: 906 -> 906
171
Cov: 906 -> 906
Cov: 906 -> 906
172
Cov: 906 -> 906
Cov: 906 -> 906
173
Cov: 906 -> 906
Cov: 906 -> 906
174
Cov: 906 -> 907
Cov: 907 -> 907
175
Cov: 907 -> 907
Cov: 907 -> 907
176
Cov: 907 -> 908
Cov: 908 -> 908
177
{"exception": "ModuleNotFoundError", "msg": "No module named 'PIL'"}
178
Cov: 908 -> 12067
Cov: 12067 -> 12067
179
Cov: 12067 -> 12070
Cov: 12070 -> 12070
180
Cov: 12070 -> 12074
Cov: 12074 -> 12074
181
Cov: 12074 -> 19723
Cov: 19723 -> 19723
182
Cov: 19723 -> 19732
Cov: 19732 -> 19732
183
Cov: 19732 -> 19732
Cov: 19732 -> 19732
184
Cov: 19732 -> 19733
Cov: 19733 -> 19733
185
Cov: 19733 -> 19733
Cov: 19733 -> 19733
186
Cov: 19733 -> 19733
Cov: 19733 -> 19733
187
{"exception": "RuntimeError", "msg": "corrcoef(): expected input to have two or fewer dimensions but got an input with 3 dimensions"}
188
Cov: 19733 -> 19733
Cov: 19733 -> 19733
189
Cov: 19733 -> 19733
Cov: 19733 -> 19733
190
Cov: 19733 -> 19733
Cov: 19733 -> 19733
191
Cov: 19733 -> 19735
Cov: 19735 -> 19735
192
Cov: 19735 -> 19756
Cov: 19756 -> 19756
193
Cov: 19756 -> 19756
Cov: 19756 -> 19756
194
Cov: 19756 -> 19757
Cov: 19757 -> 19757
195
Cov: 19757 -> 19757
Cov: 19757 -> 19757
196
Cov: 19757 -> 19757
Cov: 19757 -> 19757
197
Cov: 19757 -> 19779
Cov: 19779 -> 19779
198
Cov: 19779 -> 19782
Cov: 19782 -> 19782
199
Cov: 19782 -> 19782
Cov: 19782 -> 19782
200
{"exception": "TypeError", "msg": "index_put_(): argument 'indices' (position 1) must be tuple of Tensors, not Tensor"}
201
Cov: 19782 -> 19782
Cov: 19782 -> 19782
202
Cov: 19782 -> 19782
Cov: 19782 -> 19782
203
Cov: 19782 -> 19782
Cov: 19782 -> 19782
204
Cov: 19782 -> 19813
Cov: 19813 -> 19813
205
Cov: 19813 -> 19815
Cov: 19815 -> 19815
206
Cov: 19815 -> 19815
Cov: 19815 -> 19815
207
Cov: 19815 -> 19827
Cov: 19827 -> 19827
208
Cov: 19827 -> 19827
Cov: 19827 -> 19827
209
Cov: 19827 -> 19827
Cov: 19827 -> 19827
210
Cov: 19827 -> 19844
Cov: 19844 -> 19844
211
Cov: 19844 -> 19869
Cov: 19869 -> 19869
212
Cov: 19869 -> 19869
Cov: 19869 -> 19869
213
Cov: 19869 -> 19869
Cov: 19869 -> 19869
214
Cov: 19869 -> 19869
Cov: 19869 -> 19869
215
Cov: 19869 -> 19869
Cov: 19869 -> 19869
216
Cov: 19869 -> 19869
Cov: 19869 -> 19869
217
Cov: 19869 -> 19869
Cov: 19869 -> 19869
218
Cov: 19869 -> 19871
Cov: 19871 -> 19871
219
Cov: 19871 -> 19872
Cov: 19872 -> 19872
220
Cov: 19872 -> 19872
Cov: 19872 -> 19872
221
Cov: 19872 -> 19872
Cov: 19872 -> 19872
222
Cov: 19872 -> 19890
Cov: 19890 -> 19890
223
Cov: 19890 -> 19890
Cov: 19890 -> 19890
224
Cov: 19890 -> 19890
Cov: 19890 -> 19890
225
Cov: 19890 -> 19890
Cov: 19890 -> 19890
226
Cov: 19890 -> 19902
Cov: 19902 -> 19902
227
{"exception": "RuntimeError", "msg": "a leaf Variable that requires grad is being used in an in-place operation."}
228
Cov: 19902 -> 19912
Cov: 19912 -> 19912
229
Cov: 19912 -> 19912
Cov: 19912 -> 19912
230
Cov: 19912 -> 19912
Cov: 19912 -> 19912
231
Cov: 19912 -> 19912
Cov: 19912 -> 19912
232
Cov: 19912 -> 19912
Cov: 19912 -> 19912
233
Cov: 19912 -> 19935
Cov: 19935 -> 19935
234
Cov: 19935 -> 19935
Cov: 19935 -> 19935
235
Cov: 19935 -> 19935
Cov: 19935 -> 19935
236
Cov: 19935 -> 19935
Cov: 19935 -> 19935
237
Cov: 19935 -> 19935
Cov: 19935 -> 19935
238
Cov: 19935 -> 19935
Cov: 19935 -> 19935
239
Cov: 19935 -> 19935
Cov: 19935 -> 19935
240
Cov: 19935 -> 19935
Cov: 19935 -> 19935
241
Cov: 19935 -> 19935
Cov: 19935 -> 19935
242
Cov: 19935 -> 19940
Cov: 19940 -> 19940
243
Cov: 19940 -> 19941
Cov: 19941 -> 19941
244
Cov: 19941 -> 19941
Cov: 19941 -> 19941
245
Cov: 19941 -> 19941
Cov: 19941 -> 19941
246
Cov: 19941 -> 19956
Cov: 19956 -> 19956
247
Cov: 19956 -> 19959
Cov: 19959 -> 19959
248
Cov: 19959 -> 19959
Cov: 19959 -> 19959
249
Cov: 19959 -> 19959
Cov: 19959 -> 19959
250
Cov: 19959 -> 20004
Cov: 20004 -> 20004
251
Cov: 20004 -> 20004
Cov: 20004 -> 20004
252
Cov: 20004 -> 20004
Cov: 20004 -> 20004
253
Cov: 20004 -> 20027
Cov: 20027 -> 20027
254
Cov: 20027 -> 20035
Cov: 20035 -> 20035
255
Cov: 20035 -> 20035
Cov: 20035 -> 20035
256
Cov: 20035 -> 20041
Cov: 20041 -> 20041
257
Cov: 20041 -> 20041
Cov: 20041 -> 20041
258
Cov: 20041 -> 20042
Cov: 20042 -> 20042
259
Cov: 20042 -> 20042
Cov: 20042 -> 20042
260
Cov: 20042 -> 20045
Cov: 20045 -> 20045
261
Cov: 20045 -> 20045
Cov: 20045 -> 20045
262
Cov: 20045 -> 20045
Cov: 20045 -> 20045
263
Cov: 20045 -> 20045
Cov: 20045 -> 20045
264
Cov: 20045 -> 20045
Cov: 20045 -> 20045
265
Cov: 20045 -> 20213
Cov: 20213 -> 20213
266
Cov: 20213 -> 20213
Cov: 20213 -> 20213
267
Cov: 20213 -> 20213
Cov: 20213 -> 20213
268
Cov: 20213 -> 20233
Cov: 20233 -> 20233
269
Cov: 20233 -> 20233
Cov: 20233 -> 20233
270
Cov: 20233 -> 20241
Cov: 20241 -> 20241
271
Cov: 20241 -> 20241
Cov: 20241 -> 20241
272
Cov: 20241 -> 20255
Cov: 20255 -> 20255
273
Cov: 20255 -> 20255
Cov: 20255 -> 20255
274
Cov: 20255 -> 20272
Cov: 20272 -> 20272
275
Cov: 20272 -> 20272
Cov: 20272 -> 20272
276
Cov: 20272 -> 20272
Cov: 20272 -> 20272
277
Cov: 20272 -> 20272
Cov: 20272 -> 20272
278
Cov: 20272 -> 20282
Cov: 20282 -> 20282
279
Cov: 20282 -> 20282
Cov: 20282 -> 20282
280
Cov: 20282 -> 20282
Cov: 20282 -> 20282
281
{"exception": "RuntimeError", "msg": "1D tensors expected, but got 2D and 2D tensors"}
282
Cov: 20282 -> 20282
Cov: 20282 -> 20282
283
{"exception": "TypeError", "msg": "index_put_(): argument 'indices' (position 1) must be tuple of Tensors, not Tensor"}
284
Cov: 20282 -> 20282
Cov: 20282 -> 20282
285
Cov: 20282 -> 20282
Cov: 20282 -> 20282
286
Cov: 20282 -> 20282
Cov: 20282 -> 20282
287
Cov: 20282 -> 20282
Cov: 20282 -> 20282
288
Cov: 20282 -> 20282
Cov: 20282 -> 20282
289
Cov: 20282 -> 20282
Cov: 20282 -> 20282
290
Cov: 20282 -> 20282
Cov: 20282 -> 20282
291
Cov: 20282 -> 20282
Cov: 20282 -> 20282
292
Cov: 20282 -> 20286
Cov: 20286 -> 20286
293
Cov: 20286 -> 20288
Cov: 20288 -> 20288
294
Cov: 20288 -> 20288
Cov: 20288 -> 20288
295
Cov: 20288 -> 20288
Cov: 20288 -> 20288
296
Cov: 20288 -> 20288
Cov: 20288 -> 20288
297
Cov: 20288 -> 20288
Cov: 20288 -> 20288
298
Cov: 20288 -> 20288
Cov: 20288 -> 20288
299
Cov: 20288 -> 20288
Cov: 20288 -> 20288
300
{"exception": "TypeError", "msg": "new_tensor() takes 1 positional argument but 4 were given"}
301
Cov: 20288 -> 20288
Cov: 20288 -> 20288
302
Cov: 20288 -> 20288
Cov: 20288 -> 20288
303
Cov: 20288 -> 20289
Cov: 20289 -> 20289
304
Cov: 20289 -> 20289
Cov: 20289 -> 20289
305
Cov: 20289 -> 20289
Cov: 20289 -> 20289
306
Cov: 20289 -> 20289
Cov: 20289 -> 20289
307
Cov: 20289 -> 20289
Cov: 20289 -> 20289
308
Cov: 20289 -> 20298
Cov: 20298 -> 20298
309
Cov: 20298 -> 20298
Cov: 20298 -> 20298
310
Cov: 20298 -> 20298
Cov: 20298 -> 20298
311
Cov: 20298 -> 20298
Cov: 20298 -> 20298
312
Cov: 20298 -> 20348
Cov: 20348 -> 20348
313
Cov: 20348 -> 20348
Cov: 20348 -> 20348
314
Cov: 20348 -> 20352
Cov: 20352 -> 20352
315
Cov: 20352 -> 20358
Cov: 20358 -> 20358
316
Cov: 20358 -> 20358
Cov: 20358 -> 20358
317
Cov: 20358 -> 20358
Cov: 20358 -> 20358
318
Cov: 20358 -> 20358
Cov: 20358 -> 20358
319
Cov: 20358 -> 20359
Cov: 20359 -> 20359
320
Cov: 20359 -> 20359
Cov: 20359 -> 20359
321
Cov: 20359 -> 20368
Cov: 20368 -> 20368
322
Cov: 20368 -> 20424
Cov: 20424 -> 20424
323
Cov: 20424 -> 20424
Cov: 20424 -> 20424
324
{"exception": "TypeError", "msg": "random_() missing 2 required positional argument: \"from\", \"to\""}
325
Cov: 20424 -> 20424
Cov: 20424 -> 20424
326
Cov: 20424 -> 20424
Cov: 20424 -> 20424
327
Cov: 20424 -> 20424
Cov: 20424 -> 20424
328
Cov: 20424 -> 20424
Cov: 20424 -> 20424
329
Cov: 20424 -> 20424
Cov: 20424 -> 20424
330
Cov: 20424 -> 20424
Cov: 20424 -> 20424
331
Cov: 20424 -> 20424
Cov: 20424 -> 20424
332
Cov: 20424 -> 20424
Cov: 20424 -> 20424
333
Cov: 20424 -> 20424
Cov: 20424 -> 20424
334
Cov: 20424 -> 20424
Cov: 20424 -> 20424
335
Cov: 20424 -> 20431
Cov: 20431 -> 20431
336
Cov: 20431 -> 20431
Cov: 20431 -> 20431
337
{"exception": "TypeError", "msg": "addbmm() received an invalid combination of arguments - got (Tensor), but expected (Tensor batch1, Tensor batch2, *, Number beta, Number alpha)"}
338
Cov: 20431 -> 20431
Cov: 20431 -> 20431
339
Cov: 20431 -> 20431
Cov: 20431 -> 20431
340
Cov: 20431 -> 20431
Cov: 20431 -> 20431
341
Cov: 20431 -> 20431
Cov: 20431 -> 20431
342
Cov: 20431 -> 20432
Cov: 20432 -> 20432
343
Cov: 20432 -> 20432
Cov: 20432 -> 20432
344
Cov: 20432 -> 20432
Cov: 20432 -> 20432
345
{"exception": "RuntimeError", "msg": "This function was deprecated since version 1.9 and is now removed. `torch.lstsq` is deprecated in favor of `torch.linalg.lstsq`.\n`torch.linalg.lstsq` has reversed arguments and does not return the QR decomposition in the returned tuple (although it returns other information about the problem).\n\nTo get the QR decomposition consider using `torch.linalg.qr`.\n\nThe returned solution in `torch.lstsq` stored the residuals of the solution in the last m - n columns of the returned value whenever m > n. In torch.linalg.lstsq, the residuals are in the field 'residuals' of the returned named tuple.\n\nThe unpacking of the solution, as in\nX, _ = torch.lstsq(B, A).solution[:A.size(1)]\nshould be replaced with:\nX = torch.linalg.lstsq(A, B).solution"}
346
Cov: 20432 -> 20437
Cov: 20437 -> 20437
347
Cov: 20437 -> 20437
Cov: 20437 -> 20437
348
Cov: 20437 -> 20437
Cov: 20437 -> 20437
349
Cov: 20437 -> 20437
Cov: 20437 -> 20437
350
Cov: 20437 -> 20437
Cov: 20437 -> 20437
351
{"exception": "RuntimeError", "msg": "torch.ormqr: other.shape[-2] must be greater than or equal to tau.shape[-1]"}
352
{"exception": "TypeError", "msg": "heaviside(): argument 'values' must be Tensor, not float"}
353
Cov: 20437 -> 20441
Cov: 20441 -> 20441
354
Cov: 20441 -> 20441
Cov: 20441 -> 20441
355
Cov: 20441 -> 20441
Cov: 20441 -> 20441
356
Cov: 20441 -> 20442
Cov: 20442 -> 20442
357
Cov: 20442 -> 20443
Cov: 20443 -> 20443
358
Cov: 20443 -> 20443
Cov: 20443 -> 20443
359
Cov: 20443 -> 20443
Cov: 20443 -> 20443
360
Cov: 20443 -> 20443
Cov: 20443 -> 20443
361
Cov: 20443 -> 20443
Cov: 20443 -> 20443
362
Cov: 20443 -> 20443
Cov: 20443 -> 20443
363
Cov: 20443 -> 20450
Cov: 20450 -> 20450
364
Cov: 20450 -> 20915
Cov: 20915 -> 20915
365
Cov: 20915 -> 20917
Cov: 20917 -> 20917
366
Cov: 20917 -> 20918
Cov: 20918 -> 20918
367
Cov: 20918 -> 20918
Cov: 20918 -> 20918
368
Cov: 20918 -> 20927
Cov: 20927 -> 20927
369
Cov: 20927 -> 20927
Cov: 20927 -> 20927
370
Cov: 20927 -> 20927
Cov: 20927 -> 20927
371
{"exception": "RuntimeError", "msg": "index_add_(): self (Double) and source (Long) must have the same scalar type"}
372
Cov: 20927 -> 20927
Cov: 20927 -> 20927
373
Cov: 20927 -> 20942
Cov: 20942 -> 20942
374
Cov: 20942 -> 20943
Cov: 20943 -> 20943
375
Cov: 20943 -> 20943
Cov: 20943 -> 20943
376
Cov: 20943 -> 20943
Cov: 20943 -> 20943
377
Cov: 20943 -> 20943
Cov: 20943 -> 20943
378
Cov: 20943 -> 20943
Cov: 20943 -> 20943
379
Cov: 20943 -> 20943
Cov: 20943 -> 20943
380
Cov: 20943 -> 20946
Cov: 20946 -> 20946
381
Cov: 20946 -> 20946
Cov: 20946 -> 20946
382
Cov: 20946 -> 20946
Cov: 20946 -> 20946
383
Cov: 20946 -> 20946
Cov: 20946 -> 20946
384
Cov: 20946 -> 20946
Cov: 20946 -> 20946
385
Cov: 20946 -> 20946
Cov: 20946 -> 20946
386
Cov: 20946 -> 20946
Cov: 20946 -> 20946
387
Cov: 20946 -> 20949
Cov: 20949 -> 20949
388
Cov: 20949 -> 20949
Cov: 20949 -> 20949
389
Cov: 20949 -> 20953
Cov: 20953 -> 20953
390
Cov: 20953 -> 20953
Cov: 20953 -> 20953
391
Cov: 20953 -> 20953
Cov: 20953 -> 20953
392
Cov: 20953 -> 20953
Cov: 20953 -> 20953
393
Cov: 20953 -> 20953
Cov: 20953 -> 20953
394
Cov: 20953 -> 20953
Cov: 20953 -> 20953
395
Cov: 20953 -> 20953
Cov: 20953 -> 20953
396
Cov: 20953 -> 20953
Cov: 20953 -> 20953
397
Cov: 20953 -> 20953
Cov: 20953 -> 20953
398
Cov: 20953 -> 20953
Cov: 20953 -> 20953
399
Cov: 20953 -> 20953
Cov: 20953 -> 20953
400
Cov: 20953 -> 20953
Cov: 20953 -> 20953
401
Cov: 20953 -> 20953
Cov: 20953 -> 20953
402
Cov: 20953 -> 20953
Cov: 20953 -> 20953
403
Cov: 20953 -> 20953
Cov: 20953 -> 20953
404
Cov: 20953 -> 20953
Cov: 20953 -> 20953
405
Cov: 20953 -> 20953
Cov: 20953 -> 20953
406
Cov: 20953 -> 20955
Cov: 20955 -> 20955
407
{"exception": "RuntimeError", "msg": "scatter(): Expected self.dtype to be equal to src.dtype"}
408
Cov: 20955 -> 20955
Cov: 20955 -> 20955
409
Cov: 20955 -> 20955
Cov: 20955 -> 20955
410
Cov: 20955 -> 20955
Cov: 20955 -> 20955
411
Cov: 20955 -> 20955
Cov: 20955 -> 20955
412
{"exception": "RuntimeError", "msg": "scatter(): Expected self.dtype to be equal to src.dtype"}
413
Cov: 20955 -> 20955
Cov: 20955 -> 20955
414
Cov: 20955 -> 20955
Cov: 20955 -> 20955
415
Cov: 20955 -> 21008
Cov: 21008 -> 21008
416
Cov: 21008 -> 21008
Cov: 21008 -> 21008
417
Cov: 21008 -> 21008
Cov: 21008 -> 21008
418
Cov: 21008 -> 21008
Cov: 21008 -> 21008
419
Cov: 21008 -> 21008
Cov: 21008 -> 21008
420
Cov: 21008 -> 21008
Cov: 21008 -> 21008
421
Cov: 21008 -> 21008
Cov: 21008 -> 21008
422
Cov: 21008 -> 21066
Cov: 21066 -> 21066
423
Cov: 21066 -> 21069
Cov: 21069 -> 21069
424
Cov: 21069 -> 21081
Cov: 21081 -> 21081
425
Cov: 21081 -> 21102
Cov: 21102 -> 21102
426
Cov: 21102 -> 21102
Cov: 21102 -> 21102
427
Cov: 21102 -> 21236
Cov: 21236 -> 21236
428
{"exception": "NotImplementedError", "msg": "Could not run 'aten::q_scale' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::q_scale' is only available for these backends: [QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nQuantizedCPU: registered at aten/src/ATen/RegisterQuantizedCPU.cpp:944 [kernel]\nQuantizedCUDA: registered at aten/src/ATen/RegisterQuantizedCUDA.cpp:459 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:153 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:498 [backend fallback]\nFunctionalize: registered at ../aten/src/ATen/FunctionalizeFallbackKernel.cpp:290 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:86 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradMTIA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:17018 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_4.cpp:13056 [kernel]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:382 [backend fallback]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:249 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:710 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:203 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:161 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:494 [backend fallback]\nPreDispatch: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:165 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:157 [backend fallback]\n"}
429
Cov: 21236 -> 21263
Cov: 21263 -> 21263
430
Cov: 21263 -> 21263
Cov: 21263 -> 21263
431
Cov: 21263 -> 21263
Cov: 21263 -> 21263
432
Cov: 21263 -> 21289
Cov: 21289 -> 21289
433
Cov: 21289 -> 21289
Cov: 21289 -> 21289
434
Cov: 21289 -> 21289
Cov: 21289 -> 21289
435
Cov: 21289 -> 21289
Cov: 21289 -> 21289
436
Cov: 21289 -> 21290
Cov: 21290 -> 21290
437
Cov: 21290 -> 21291
Cov: 21291 -> 21291
438
{"exception": "NotImplementedError", "msg": "Could not run 'aten::int_repr' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::int_repr' is only available for these backends: [QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nQuantizedCPU: registered at aten/src/ATen/RegisterQuantizedCPU.cpp:944 [kernel]\nQuantizedCUDA: registered at aten/src/ATen/RegisterQuantizedCUDA.cpp:459 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:153 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:498 [backend fallback]\nFunctionalize: registered at ../aten/src/ATen/FunctionalizeFallbackKernel.cpp:290 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:86 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradMTIA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_0.cpp:16725 [kernel]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:382 [backend fallback]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:249 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:710 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:203 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:161 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:494 [backend fallback]\nPreDispatch: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:165 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:157 [backend fallback]\n"}
439
Cov: 21291 -> 21295
Cov: 21295 -> 21295
440
Cov: 21295 -> 21299
Cov: 21299 -> 21299
441
Cov: 21299 -> 21301
Cov: 21301 -> 21301
442
Cov: 21301 -> 21301
Cov: 21301 -> 21301
443
Cov: 21301 -> 21301
Cov: 21301 -> 21301
444
Cov: 21301 -> 21301
Cov: 21301 -> 21301
445
Cov: 21301 -> 21301
Cov: 21301 -> 21301
446
Cov: 21301 -> 21303
Cov: 21303 -> 21303
447
Cov: 21303 -> 21303
Cov: 21303 -> 21303
448
Cov: 21303 -> 21303
Cov: 21303 -> 21303
449
Cov: 21303 -> 21303
Cov: 21303 -> 21303
450
Cov: 21303 -> 21303
Cov: 21303 -> 21303
451
Cov: 21303 -> 21303
Cov: 21303 -> 21303
452
Cov: 21303 -> 21303
Cov: 21303 -> 21303
453
Cov: 21303 -> 21303
Cov: 21303 -> 21303
454
Cov: 21303 -> 21303
Cov: 21303 -> 21303
455
{"exception": "RuntimeError", "msg": "torch.ormqr: Expected other to have the same number of dimensions as input, but got other.ndim equal to 2 and input.ndim is equal to 3"}
456
Cov: 21303 -> 21303
Cov: 21303 -> 21303
457
Cov: 21303 -> 21303
Cov: 21303 -> 21303
458
Cov: 21303 -> 21304
Cov: 21304 -> 21304
459
Cov: 21304 -> 21310
Cov: 21310 -> 21310
460
Cov: 21310 -> 21310
Cov: 21310 -> 21310
461
Cov: 21310 -> 21310
Cov: 21310 -> 21310
462
Cov: 21310 -> 21310
Cov: 21310 -> 21310
463
Cov: 21310 -> 21310
Cov: 21310 -> 21310
464
Cov: 21310 -> 21310
Cov: 21310 -> 21310
465
Cov: 21310 -> 21310
Cov: 21310 -> 21310
466
Cov: 21310 -> 21310
Cov: 21310 -> 21310
467
Cov: 21310 -> 21310
Cov: 21310 -> 21310
468
Cov: 21310 -> 21311
Cov: 21311 -> 21311
469
Cov: 21311 -> 21311
Cov: 21311 -> 21311
470
Cov: 21311 -> 21314
Cov: 21314 -> 21314
471
Cov: 21314 -> 21314
Cov: 21314 -> 21314
472
{"exception": "RuntimeError", "msg": "\"lcm_cpu\" not implemented for 'Float'"}
473
Cov: 21314 -> 21314
Cov: 21314 -> 21314
474
Cov: 21314 -> 21316
Cov: 21316 -> 21316
475
Cov: 21316 -> 21316
Cov: 21316 -> 21316
476
Cov: 21316 -> 21316
Cov: 21316 -> 21316
477
Cov: 21316 -> 21316
Cov: 21316 -> 21316
478
Cov: 21316 -> 21316
Cov: 21316 -> 21316
479
Cov: 21316 -> 21316
Cov: 21316 -> 21316
480
{"exception": "TypeError", "msg": "'getset_descriptor' object is not callable"}
481
Cov: 21316 -> 21316
Cov: 21316 -> 21316
482
Cov: 21316 -> 21316
Cov: 21316 -> 21316
483
Cov: 21316 -> 21316
Cov: 21316 -> 21316
484
Cov: 21316 -> 21316
Cov: 21316 -> 21316
485
Cov: 21316 -> 21317
Cov: 21317 -> 21317
486
Cov: 21317 -> 21319
Cov: 21319 -> 21319
487
Cov: 21319 -> 21319
Cov: 21319 -> 21319
488
{"exception": "RuntimeError", "msg": "The size of tensor a (10) must match the size of tensor b (2) at non-singleton dimension 0"}
489
Cov: 21319 -> 21319
Cov: 21319 -> 21319
490
Cov: 21319 -> 21321
Cov: 21321 -> 21321
491
Cov: 21321 -> 21321
Cov: 21321 -> 21321
492
Cov: 21321 -> 21360
Cov: 21360 -> 21360
493
Cov: 21360 -> 21360
Cov: 21360 -> 21360
494
Cov: 21360 -> 21360
Cov: 21360 -> 21360
495
Cov: 21360 -> 21360
Cov: 21360 -> 21360
496
Cov: 21360 -> 21360
Cov: 21360 -> 21360
497
Cov: 21360 -> 21360
Cov: 21360 -> 21360
498
{"exception": "NameError", "msg": "name 'Normal' is not defined"}
499
Cov: 21360 -> 21360
Cov: 21360 -> 21360
500
Cov: 21360 -> 21360
Cov: 21360 -> 21360
501
Cov: 21360 -> 21360
Cov: 21360 -> 21360
502
Cov: 21360 -> 21363
Cov: 21363 -> 21363
503
Cov: 21363 -> 21363
Cov: 21363 -> 21363
504
Cov: 21363 -> 21364
Cov: 21364 -> 21364
505
Cov: 21364 -> 21383
Cov: 21383 -> 21383
506
Cov: 21383 -> 21383
Cov: 21383 -> 21383
507
Cov: 21383 -> 21383
Cov: 21383 -> 21383
508
Cov: 21383 -> 21383
Cov: 21383 -> 21383
509
Cov: 21383 -> 21383
Cov: 21383 -> 21383
510
Cov: 21383 -> 21383
Cov: 21383 -> 21383
511
Cov: 21383 -> 21383
Cov: 21383 -> 21383
512
Cov: 21383 -> 21383
Cov: 21383 -> 21383
513
Cov: 21383 -> 21384
Cov: 21384 -> 21384
514
Cov: 21384 -> 21384
Cov: 21384 -> 21384
515
Cov: 21384 -> 21384
Cov: 21384 -> 21384
516
Cov: 21384 -> 21385
Cov: 21385 -> 21385
517
Cov: 21385 -> 21385
Cov: 21385 -> 21385
518
Cov: 21385 -> 21386
Cov: 21386 -> 21386
519
Cov: 21386 -> 21386
Cov: 21386 -> 21386
520
Cov: 21386 -> 21386
Cov: 21386 -> 21386
521
Cov: 21386 -> 21386
Cov: 21386 -> 21386
522
Cov: 21386 -> 21386
Cov: 21386 -> 21386
523
Cov: 21386 -> 21386
Cov: 21386 -> 21386
524
Cov: 21386 -> 21387
Cov: 21387 -> 21387
525
Cov: 21387 -> 21387
Cov: 21387 -> 21387
526
{"exception": "RuntimeError", "msg": "div expected rounding_mode to be one of None, 'trunc', or 'floor' but found 'ceil'"}
527
Cov: 21387 -> 21392
Cov: 21392 -> 21392
528
Cov: 21392 -> 21392
Cov: 21392 -> 21392
529
Cov: 21392 -> 21392
Cov: 21392 -> 21392
530
Cov: 21392 -> 21392
Cov: 21392 -> 21392
531
Cov: 21392 -> 21392
Cov: 21392 -> 21392
532
Cov: 21392 -> 21392
Cov: 21392 -> 21392
533
Cov: 21392 -> 21392
Cov: 21392 -> 21392
534
Cov: 21392 -> 21392
Cov: 21392 -> 21392
535
Cov: 21392 -> 21392
Cov: 21392 -> 21392
536
Cov: 21392 -> 21392
Cov: 21392 -> 21392
537
Cov: 21392 -> 21392
Cov: 21392 -> 21392
538
Cov: 21392 -> 21396
Cov: 21396 -> 21396
539
Cov: 21396 -> 21397
Cov: 21397 -> 21397
540
Cov: 21397 -> 21397
Cov: 21397 -> 21397
541
Cov: 21397 -> 21397
Cov: 21397 -> 21397
542
Cov: 21397 -> 21410
Cov: 21410 -> 21410
543
Cov: 21410 -> 21410
Cov: 21410 -> 21410
544
Cov: 21410 -> 21413
Cov: 21413 -> 21413
545
Cov: 21413 -> 21414
Cov: 21414 -> 21414
546
Cov: 21414 -> 21414
Cov: 21414 -> 21414
547
Cov: 21414 -> 21414
Cov: 21414 -> 21414
548
Cov: 21414 -> 21421
Cov: 21421 -> 21421
549
Cov: 21421 -> 21423
Cov: 21423 -> 21423
550
Cov: 21423 -> 21425
Cov: 21425 -> 21425
551
Cov: 21425 -> 21425
Cov: 21425 -> 21425
552
Cov: 21425 -> 21428
Cov: 21428 -> 21428
553
Cov: 21428 -> 21428
Cov: 21428 -> 21428
554
Cov: 21428 -> 21428
Cov: 21428 -> 21428
555
Cov: 21428 -> 21428
Cov: 21428 -> 21428
556
Cov: 21428 -> 21428
Cov: 21428 -> 21428
557
Cov: 21428 -> 21428
Cov: 21428 -> 21428
558
Cov: 21428 -> 21428
Cov: 21428 -> 21428
559
Cov: 21428 -> 21428
Cov: 21428 -> 21428
560
Cov: 21428 -> 21428
Cov: 21428 -> 21428
561
Cov: 21428 -> 21428
Cov: 21428 -> 21428
562
Cov: 21428 -> 21428
Cov: 21428 -> 21428
563
{"exception": "TypeError", "msg": "index_put(): argument 'indices' (position 1) must be tuple of Tensors, not Tensor"}
564
Cov: 21428 -> 21429
Cov: 21429 -> 21429
565
Cov: 21429 -> 21429
Cov: 21429 -> 21429
566
Cov: 21429 -> 21429
Cov: 21429 -> 21429
567
Cov: 21429 -> 21429
Cov: 21429 -> 21429
568
Cov: 21429 -> 21429
Cov: 21429 -> 21429
569
Cov: 21429 -> 21429
Cov: 21429 -> 21429
570
Cov: 21429 -> 21429
Cov: 21429 -> 21429
571
Cov: 21429 -> 21430
Cov: 21430 -> 21430
572
Cov: 21430 -> 21431
Cov: 21431 -> 21431
573
Cov: 21431 -> 21431
Cov: 21431 -> 21431
574
Cov: 21431 -> 21432
Cov: 21432 -> 21432
575
Cov: 21432 -> 21436
Cov: 21436 -> 21436
576
Cov: 21436 -> 21437
Cov: 21437 -> 21437
577
Cov: 21437 -> 21437
Cov: 21437 -> 21437
578
Cov: 21437 -> 21438
Cov: 21438 -> 21438
579
Cov: 21438 -> 21438
Cov: 21438 -> 21438
580
Cov: 21438 -> 21438
Cov: 21438 -> 21438
581
Cov: 21438 -> 21464
Cov: 21464 -> 21464
582
Cov: 21464 -> 21464
Cov: 21464 -> 21464
583
Cov: 21464 -> 21464
Cov: 21464 -> 21464
584
Cov: 21464 -> 21464
Cov: 21464 -> 21464
585
Cov: 21464 -> 21467
Cov: 21467 -> 21467
586
Cov: 21467 -> 21471
Cov: 21471 -> 21471
587
Cov: 21471 -> 21471
Cov: 21471 -> 21471
588
Cov: 21471 -> 21502
Cov: 21502 -> 21502
589
Cov: 21502 -> 21502
Cov: 21502 -> 21502
590
Cov: 21502 -> 21502
Cov: 21502 -> 21502
591
Cov: 21502 -> 21502
Cov: 21502 -> 21502
592
Cov: 21502 -> 21502
Cov: 21502 -> 21502
593
Cov: 21502 -> 21503
Cov: 21503 -> 21503
594
Cov: 21503 -> 21503
Cov: 21503 -> 21503
595
Cov: 21503 -> 21503
Cov: 21503 -> 21503
596
Cov: 21503 -> 21503
Cov: 21503 -> 21503
597
Cov: 21503 -> 21503
Cov: 21503 -> 21503
598
Cov: 21503 -> 21503
Cov: 21503 -> 21503
599
Cov: 21503 -> 21503
Cov: 21503 -> 21503
600
Cov: 21503 -> 21503
Cov: 21503 -> 21503
601
Cov: 21503 -> 21504
Cov: 21504 -> 21504
602
Cov: 21504 -> 21507
Cov: 21507 -> 21507
603
Cov: 21507 -> 21508
Cov: 21508 -> 21508
604
Cov: 21508 -> 21508
Cov: 21508 -> 21508
605
Cov: 21508 -> 21508
Cov: 21508 -> 21508
606
Cov: 21508 -> 21508
Cov: 21508 -> 21508
607
Cov: 21508 -> 21508
Cov: 21508 -> 21508
608
Cov: 21508 -> 21513
Cov: 21513 -> 21513
609
Cov: 21513 -> 21513
Cov: 21513 -> 21513
610
Cov: 21513 -> 21513
Cov: 21513 -> 21513
611
Cov: 21513 -> 21626
Cov: 21626 -> 21626
612
Cov: 21626 -> 21626
Cov: 21626 -> 21626
613
Cov: 21626 -> 21626
Cov: 21626 -> 21626
614
Cov: 21626 -> 21626
Cov: 21626 -> 21626
615
Cov: 21626 -> 21626
Cov: 21626 -> 21626
616
Cov: 21626 -> 21626
Cov: 21626 -> 21626
617
Cov: 21626 -> 21786
Cov: 21786 -> 21786
618
Cov: 21786 -> 21787
Cov: 21787 -> 21787
619
Cov: 21787 -> 21787
Cov: 21787 -> 21787
620
Cov: 21787 -> 21788
Cov: 21788 -> 21788
621
Cov: 21788 -> 21788
Cov: 21788 -> 21788
622
{"exception": "RuntimeError", "msg": "The expanded size of the tensor (4) must match the existing size (3) at non-singleton dimension 1.  Target sizes: [2, 4].  Tensor sizes: [2, 3]"}
623
Cov: 21788 -> 21788
Cov: 21788 -> 21788
624
{"exception": "RuntimeError", "msg": "a leaf Variable that requires grad is being used in an in-place operation."}
625
Cov: 21788 -> 21793
Cov: 21793 -> 21793
626
Cov: 21793 -> 21809
Cov: 21809 -> 21809
627
Cov: 21809 -> 21809
Cov: 21809 -> 21809
628
Cov: 21809 -> 21814
Cov: 21814 -> 21814
629
Cov: 21814 -> 21814
Cov: 21814 -> 21814
630
Cov: 21814 -> 21846
Cov: 21846 -> 21846
631
Cov: 21846 -> 21846
Cov: 21846 -> 21846
632
Cov: 21846 -> 22153
Cov: 22153 -> 22153
633
Cov: 22153 -> 22153
Cov: 22153 -> 22153
634
Cov: 22153 -> 22153
Cov: 22153 -> 22153
635
Cov: 22153 -> 22153
Cov: 22153 -> 22153
636
Cov: 22153 -> 22153
Cov: 22153 -> 22153
637
Cov: 22153 -> 22153
Cov: 22153 -> 22153
638
Cov: 22153 -> 22153
Cov: 22153 -> 22153
639
Cov: 22153 -> 22153
Cov: 22153 -> 22153
640
Cov: 22153 -> 22153
Cov: 22153 -> 22153
641
Cov: 22153 -> 22153
Cov: 22153 -> 22153
642
Cov: 22153 -> 22153
Cov: 22153 -> 22153
643
Cov: 22153 -> 22155
Cov: 22155 -> 22155
644
Cov: 22155 -> 22156
Cov: 22156 -> 22156
645
Cov: 22156 -> 22156
Cov: 22156 -> 22156
646
Cov: 22156 -> 22167
Cov: 22167 -> 22167
647
Cov: 22167 -> 22167
Cov: 22167 -> 22167
648
Cov: 22167 -> 22167
Cov: 22167 -> 22167
649
Cov: 22167 -> 22167
Cov: 22167 -> 22167
650
{"exception": "NotImplementedError", "msg": "Could not run 'aten::sparse_mask' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::sparse_mask' is only available for these backends: [SparseCPU, SparseCUDA, SparseCsrCPU, SparseCsrCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nSparseCPU: registered at aten/src/ATen/RegisterSparseCPU.cpp:1387 [kernel]\nSparseCUDA: registered at aten/src/ATen/RegisterSparseCUDA.cpp:1573 [kernel]\nSparseCsrCPU: registered at aten/src/ATen/RegisterSparseCsrCPU.cpp:1135 [kernel]\nSparseCsrCUDA: registered at aten/src/ATen/RegisterSparseCsrCUDA.cpp:1276 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:153 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:498 [backend fallback]\nFunctionalize: registered at ../aten/src/ATen/FunctionalizeFallbackKernel.cpp:290 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:86 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradMTIA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_0.cpp:16725 [kernel]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:382 [backend fallback]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:249 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:710 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:203 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:161 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:494 [backend fallback]\nPreDispatch: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:165 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:157 [backend fallback]\n"}
651
Cov: 22167 -> 22167
Cov: 22167 -> 22167
652
Cov: 22167 -> 22167
Cov: 22167 -> 22167
653
Cov: 22167 -> 22167
Cov: 22167 -> 22167
654
Cov: 22167 -> 22167
Cov: 22167 -> 22167
655
Cov: 22167 -> 22167
Cov: 22167 -> 22167
656
Cov: 22167 -> 22167
Cov: 22167 -> 22167
657
Cov: 22167 -> 22167
Cov: 22167 -> 22167
658
Cov: 22167 -> 22169
Cov: 22169 -> 22169
659
Cov: 22169 -> 22169
Cov: 22169 -> 22169
660
Cov: 22169 -> 22169
Cov: 22169 -> 22169
661
Cov: 22169 -> 22169
Cov: 22169 -> 22169
662
{"exception": "TypeError", "msg": "'getset_descriptor' object is not callable"}
663
Cov: 22169 -> 22169
Cov: 22169 -> 22169
664
Cov: 22169 -> 22169
Cov: 22169 -> 22169
665
Cov: 22169 -> 22170
Cov: 22170 -> 22170
666
Cov: 22170 -> 22174
Cov: 22174 -> 22174
667
Cov: 22174 -> 22174
Cov: 22174 -> 22174
668
Cov: 22174 -> 22174
Cov: 22174 -> 22174
669
Cov: 22174 -> 22174
Cov: 22174 -> 22174
670
Cov: 22174 -> 22174
Cov: 22174 -> 22174
671
Cov: 22174 -> 22175
Cov: 22175 -> 22175
672
Cov: 22175 -> 22175
Cov: 22175 -> 22175
673
Cov: 22175 -> 22175
Cov: 22175 -> 22175
674
Cov: 22175 -> 22175
Cov: 22175 -> 22175
675
Cov: 22175 -> 22175
Cov: 22175 -> 22175
676
Cov: 22175 -> 22181
Cov: 22181 -> 22181
677
Cov: 22181 -> 22183
Cov: 22183 -> 22183
678
Cov: 22183 -> 22183
Cov: 22183 -> 22183
679
Cov: 22183 -> 22183
Cov: 22183 -> 22183
680
Cov: 22183 -> 22186
Cov: 22186 -> 22186
681
Cov: 22186 -> 22186
Cov: 22186 -> 22186
682
Cov: 22186 -> 22186
Cov: 22186 -> 22186
683
Cov: 22186 -> 22186
Cov: 22186 -> 22186
684
Cov: 22186 -> 22186
Cov: 22186 -> 22186
685
{"exception": "RuntimeError", "msg": "quantile() input tensor must be either float or double dtype"}
686
Cov: 22186 -> 22187
Cov: 22187 -> 22187
687
Cov: 22187 -> 22198
Cov: 22198 -> 22198
688
Cov: 22198 -> 22198
Cov: 22198 -> 22198
689
Cov: 22198 -> 22198
Cov: 22198 -> 22198
690
Cov: 22198 -> 22204
Cov: 22204 -> 22204
691
Cov: 22204 -> 22204
Cov: 22204 -> 22204
692
Cov: 22204 -> 22210
Cov: 22210 -> 22210
693
Cov: 22210 -> 22211
Cov: 22211 -> 22211
694
Cov: 22211 -> 22211
Cov: 22211 -> 22211
695
Cov: 22211 -> 22211
Cov: 22211 -> 22211
696
Cov: 22211 -> 22211
Cov: 22211 -> 22211
697
Cov: 22211 -> 22212
Cov: 22212 -> 22212
698
Cov: 22212 -> 22212
Cov: 22212 -> 22212
699
Cov: 22212 -> 22212
Cov: 22212 -> 22212
700
Cov: 22212 -> 22212
Cov: 22212 -> 22212
701
{"exception": "NameError", "msg": "name '_device' is not defined"}
702
Cov: 22212 -> 22212
Cov: 22212 -> 22212
703
Cov: 22212 -> 22212
Cov: 22212 -> 22212
704
Cov: 22212 -> 22212
Cov: 22212 -> 22212
705
Cov: 22212 -> 22215
Cov: 22215 -> 22215
706
Cov: 22215 -> 22215
Cov: 22215 -> 22215
707
Cov: 22215 -> 22216
Cov: 22216 -> 22216
708
Cov: 22216 -> 22281
Cov: 22281 -> 22281
709
Cov: 22281 -> 22281
Cov: 22281 -> 22281
710
{"exception": "NotImplementedError", "msg": "Could not run 'aten::int_repr' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::int_repr' is only available for these backends: [QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nQuantizedCPU: registered at aten/src/ATen/RegisterQuantizedCPU.cpp:944 [kernel]\nQuantizedCUDA: registered at aten/src/ATen/RegisterQuantizedCUDA.cpp:459 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:153 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:498 [backend fallback]\nFunctionalize: registered at ../aten/src/ATen/FunctionalizeFallbackKernel.cpp:290 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:86 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradMTIA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:16838 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_0.cpp:16725 [kernel]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:382 [backend fallback]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:249 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:710 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:203 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:161 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:494 [backend fallback]\nPreDispatch: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:165 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:157 [backend fallback]\n"}
711
Cov: 22281 -> 22281
Cov: 22281 -> 22281
712
Cov: 22281 -> 22281
Cov: 22281 -> 22281
713
Cov: 22281 -> 22281
Cov: 22281 -> 22281
714
Cov: 22281 -> 22293
Cov: 22293 -> 22293
715
Cov: 22293 -> 22294
Cov: 22294 -> 22294
716
Cov: 22294 -> 22294
Cov: 22294 -> 22294
717
Cov: 22294 -> 22294
Cov: 22294 -> 22294
718
Cov: 22294 -> 22294
Cov: 22294 -> 22294
719
Cov: 22294 -> 22294
Cov: 22294 -> 22294
720
Cov: 22294 -> 22294
Cov: 22294 -> 22294
721
Cov: 22294 -> 22294
Cov: 22294 -> 22294
722
{"exception": "RuntimeError", "msg": "a Tensor with 150528 elements cannot be converted to Scalar"}
723
Cov: 22294 -> 22305
Cov: 22305 -> 22305
724
Cov: 22305 -> 22305
Cov: 22305 -> 22305
725
Cov: 22305 -> 22306
Cov: 22306 -> 22306
726
Cov: 22306 -> 22306
Cov: 22306 -> 22306
727
{"exception": "RuntimeError", "msg": "masked_scatter_ only supports boolean masks, but got mask with dtype Byte"}
728
Cov: 22306 -> 22306
Cov: 22306 -> 22306
729
Cov: 22306 -> 22307
Cov: 22307 -> 22307
730
Cov: 22307 -> 22307
Cov: 22307 -> 22307
731
Cov: 22307 -> 22309
Cov: 22309 -> 22309
732
Cov: 22309 -> 22309
Cov: 22309 -> 22309
733
Cov: 22309 -> 22309
Cov: 22309 -> 22309
734
{"exception": "NotImplementedError", "msg": "Could not run 'aten::q_per_channel_zero_points' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::q_per_channel_zero_points' is only available for these backends: [QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nQuantizedCPU: registered at aten/src/ATen/RegisterQuantizedCPU.cpp:944 [kernel]\nQuantizedCUDA: registered at aten/src/ATen/RegisterQuantizedCUDA.cpp:459 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:153 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:498 [backend fallback]\nFunctionalize: registered at ../aten/src/ATen/FunctionalizeFallbackKernel.cpp:290 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:86 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradMTIA: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16017 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_1.cpp:16002 [kernel]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:382 [backend fallback]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:249 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:710 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:203 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:161 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:494 [backend fallback]\nPreDispatch: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:165 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:157 [backend fallback]\n"}
735
Cov: 22309 -> 22309
Cov: 22309 -> 22309
736
Cov: 22309 -> 22309
Cov: 22309 -> 22309
737
Cov: 22309 -> 22309
Cov: 22309 -> 22309
738
Cov: 22309 -> 22309
Cov: 22309 -> 22309
739
{"exception": "TypeError", "msg": "arctanh_() takes no arguments (1 given)"}
740
Cov: 22309 -> 22309
Cov: 22309 -> 22309
741
Cov: 22309 -> 22320
Cov: 22320 -> 22320
742
Cov: 22320 -> 22320
Cov: 22320 -> 22320
743
Cov: 22320 -> 22320
Cov: 22320 -> 22320
744
Cov: 22320 -> 22320
Cov: 22320 -> 22320
745
Cov: 22320 -> 22320
Cov: 22320 -> 22320
746
Cov: 22320 -> 22320
Cov: 22320 -> 22320
747
{"exception": "RuntimeError", "msg": "logdet: A must be batches of square matrices, but they are 3 by 4 matrices"}
748
Cov: 22320 -> 22320
Cov: 22320 -> 22320
749
Cov: 22320 -> 22320
Cov: 22320 -> 22320
750
Cov: 22320 -> 22320
Cov: 22320 -> 22320
751
Cov: 22320 -> 22320
Cov: 22320 -> 22320
752
Cov: 22320 -> 22320
Cov: 22320 -> 22320
753
Cov: 22320 -> 22320
Cov: 22320 -> 22320
754
Cov: 22320 -> 22320
Cov: 22320 -> 22320
755
Cov: 22320 -> 22320
Cov: 22320 -> 22320
756
Cov: 22320 -> 22320
Cov: 22320 -> 22320
757
Cov: 22320 -> 22320
Cov: 22320 -> 22320
758
Cov: 22320 -> 22320
Cov: 22320 -> 22320
759
Cov: 22320 -> 22320
Cov: 22320 -> 22320
760
Cov: 22320 -> 22323
Cov: 22323 -> 22323
761
Cov: 22323 -> 22323
Cov: 22323 -> 22323
762
Cov: 22323 -> 22323
Cov: 22323 -> 22323
763
Cov: 22323 -> 22323
Cov: 22323 -> 22323
764
Cov: 22323 -> 22326
Cov: 22326 -> 22326
765
Cov: 22326 -> 22326
Cov: 22326 -> 22326
766
{"exception": "RuntimeError", "msg": "Creating a Tensor subclass from a class that does not inherit from Tensor is not possible. Make sure your class inherits from Tensor."}
767
Cov: 22326 -> 22336
Cov: 22336 -> 22336
768
Cov: 22336 -> 22337
Cov: 22337 -> 22337
769
Cov: 22337 -> 22337
Cov: 22337 -> 22337
770
Cov: 22337 -> 22337
Cov: 22337 -> 22337
771
Cov: 22337 -> 22337
Cov: 22337 -> 22337
772
Cov: 22337 -> 22337
Cov: 22337 -> 22337
773
Cov: 22337 -> 22342
Cov: 22342 -> 22342
774
Cov: 22342 -> 22351
Cov: 22351 -> 22351
775
Cov: 22351 -> 22351
Cov: 22351 -> 22351
776
{"exception": "TypeError", "msg": "'getset_descriptor' object is not callable"}
777
Cov: 22351 -> 22352
Cov: 22352 -> 22352
778
Cov: 22352 -> 22352
Cov: 22352 -> 22352
779
Cov: 22352 -> 22353
Cov: 22353 -> 22353
780
Cov: 22353 -> 22353
Cov: 22353 -> 22353
781
Cov: 22353 -> 22353
Cov: 22353 -> 22353
782
Cov: 22353 -> 22353
Cov: 22353 -> 22353
783
Cov: 22353 -> 22354
Cov: 22354 -> 22354
784
Cov: 22354 -> 22354
Cov: 22354 -> 22354
785
Cov: 22354 -> 22367
Cov: 22367 -> 22367
786
Cov: 22367 -> 22372
Cov: 22372 -> 22372
787
Cov: 22372 -> 22373
Cov: 22373 -> 22373
788
Cov: 22373 -> 22373
Cov: 22373 -> 22373
789
Cov: 22373 -> 22373
Cov: 22373 -> 22373
790
{"exception": "TypeError", "msg": "'getset_descriptor' object is not callable"}
791
Cov: 22373 -> 22373
Cov: 22373 -> 22373
792
Cov: 22373 -> 22373
Cov: 22373 -> 22373
793
Cov: 22373 -> 22374
Cov: 22374 -> 22374
794
Cov: 22374 -> 22376
Cov: 22376 -> 22376
795
Cov: 22376 -> 22376
Cov: 22376 -> 22376
796
Cov: 22376 -> 22376
Cov: 22376 -> 22376
797
Cov: 22376 -> 22376
Cov: 22376 -> 22376
798
Cov: 22376 -> 22376
Cov: 22376 -> 22376
799
Cov: 22376 -> 22376
Cov: 22376 -> 22376
800
Cov: 22376 -> 22376
Cov: 22376 -> 22376
801
Cov: 22376 -> 22376
Cov: 22376 -> 22376
802
Cov: 22376 -> 22376
Cov: 22376 -> 22376
803
Cov: 22376 -> 22376
Cov: 22376 -> 22376
804
{"exception": "TypeError", "msg": "index_add_() missing 1 required positional arguments: \"source\""}
805
Cov: 22376 -> 22377
Cov: 22377 -> 22377
806
Cov: 22377 -> 22377
Cov: 22377 -> 22377
807
{"exception": "RuntimeError", "msg": "\"bitwise_not_cpu\" not implemented for 'Double'"}
808
Cov: 22377 -> 22377
Cov: 22377 -> 22377
809
Cov: 22377 -> 22377
Cov: 22377 -> 22377
810
Cov: 22377 -> 22377
Cov: 22377 -> 22377
811
Cov: 22377 -> 22377
Cov: 22377 -> 22377
812
Cov: 22377 -> 22377
Cov: 22377 -> 22377
813
Cov: 22377 -> 22377
Cov: 22377 -> 22377
814
Cov: 22377 -> 22378
Cov: 22378 -> 22378
815
{"exception": "TypeError", "msg": "'getset_descriptor' object is not callable"}
816
Cov: 22378 -> 22378
Cov: 22378 -> 22378
817
Cov: 22378 -> 22385
Cov: 22385 -> 22385
818
Cov: 22385 -> 22385
Cov: 22385 -> 22385
819
Cov: 22385 -> 22386
Cov: 22386 -> 22386
820
Cov: 22386 -> 22404
Cov: 22404 -> 22404
821
Cov: 22404 -> 22404
Cov: 22404 -> 22404
822
Cov: 22404 -> 22404
Cov: 22404 -> 22404
823
Cov: 22404 -> 22404
Cov: 22404 -> 22404
824
Cov: 22404 -> 22412
Cov: 22412 -> 22412
825
Cov: 22412 -> 22412
Cov: 22412 -> 22412
826
Cov: 22412 -> 22413
Cov: 22413 -> 22413
827
Cov: 22413 -> 22452
Cov: 22452 -> 22452
828
Cov: 22452 -> 22452
Cov: 22452 -> 22452
829
Cov: 22452 -> 22452
Cov: 22452 -> 22452
830
Cov: 22452 -> 22453
Cov: 22453 -> 22453
831
Cov: 22453 -> 22458
Cov: 22458 -> 22458
832
Cov: 22458 -> 22458
Cov: 22458 -> 22458
833
{"exception": "RuntimeError", "msg": "Creating a Tensor subclass from a class that does not inherit from Tensor is not possible. Make sure your class inherits from Tensor."}
834
Cov: 22458 -> 22458
Cov: 22458 -> 22458
835
Cov: 22458 -> 22458
Cov: 22458 -> 22458
836
Cov: 22458 -> 22458
Cov: 22458 -> 22458
837
{"exception": "TypeError", "msg": "index_put_(): argument 'indices' (position 1) must be tuple of Tensors, not Tensor"}
838
Cov: 22458 -> 22458
Cov: 22458 -> 22458
839
Cov: 22458 -> 22458
Cov: 22458 -> 22458
840
Cov: 22458 -> 22458
Cov: 22458 -> 22458
841
Cov: 22458 -> 22458
Cov: 22458 -> 22458
842
Cov: 22458 -> 22465
Cov: 22465 -> 22465
843
Cov: 22465 -> 22465
Cov: 22465 -> 22465
844
Cov: 22465 -> 22465
Cov: 22465 -> 22465
845
Cov: 22465 -> 22466
Cov: 22466 -> 22466
846
Cov: 22466 -> 22472
Cov: 22472 -> 22472
847
Cov: 22472 -> 22473
Cov: 22473 -> 22473
848
Cov: 22473 -> 22473
Cov: 22473 -> 22473
849
Cov: 22473 -> 22473
Cov: 22473 -> 22473
850
Cov: 22473 -> 22473
Cov: 22473 -> 22473
851
Cov: 22473 -> 22473
Cov: 22473 -> 22473
852
{"exception": "TypeError", "msg": "'getset_descriptor' object is not callable"}
853
Cov: 22473 -> 22476
Cov: 22476 -> 22476
854
Cov: 22476 -> 22476
Cov: 22476 -> 22476
855
Cov: 22476 -> 22476
Cov: 22476 -> 22476
856
Cov: 22476 -> 22476
Cov: 22476 -> 22476
857
Cov: 22476 -> 22477
Cov: 22477 -> 22477
858
Cov: 22477 -> 22477
Cov: 22477 -> 22477
859
Cov: 22477 -> 22477
Cov: 22477 -> 22477
860
Cov: 22477 -> 22477
Cov: 22477 -> 22477
861
Cov: 22477 -> 22477
Cov: 22477 -> 22477
862
Cov: 22477 -> 22477
Cov: 22477 -> 22477
863
{"exception": "RuntimeError", "msg": "1D tensors expected, but got 2D and 2D tensors"}
864
Cov: 22477 -> 22478
Cov: 22478 -> 22478
865
Cov: 22478 -> 22478
Cov: 22478 -> 22478
866
Cov: 22478 -> 22478
Cov: 22478 -> 22478
867
Cov: 22478 -> 22478
Cov: 22478 -> 22478
868
Cov: 22478 -> 22478
Cov: 22478 -> 22478
869
Cov: 22478 -> 22478
Cov: 22478 -> 22478
870
Cov: 22478 -> 22478
Cov: 22478 -> 22478
871
Cov: 22478 -> 22479
Cov: 22479 -> 22479
872
Cov: 22479 -> 22479
Cov: 22479 -> 22479
873
Cov: 22479 -> 22479
Cov: 22479 -> 22479
874
Cov: 22479 -> 22479
Cov: 22479 -> 22479
875
Cov: 22479 -> 22486
Cov: 22486 -> 22486
876
Cov: 22486 -> 22487
Cov: 22487 -> 22487
877
Cov: 22487 -> 22488
Cov: 22488 -> 22488
878
Cov: 22488 -> 22488
Cov: 22488 -> 22488
879
Cov: 22488 -> 22495
Cov: 22495 -> 22495
880
Cov: 22495 -> 22495
Cov: 22495 -> 22495
881
Cov: 22495 -> 22495
Cov: 22495 -> 22495
882
Cov: 22495 -> 22495
Cov: 22495 -> 22495
883
Cov: 22495 -> 22495
Cov: 22495 -> 22495
884
Cov: 22495 -> 22495
Cov: 22495 -> 22495
885
Cov: 22495 -> 22495
Cov: 22495 -> 22495
886
Cov: 22495 -> 22495
Cov: 22495 -> 22495
887
Cov: 22495 -> 22495
Cov: 22495 -> 22495
888
Cov: 22495 -> 22495
Cov: 22495 -> 22495
889
Cov: 22495 -> 22500
Cov: 22500 -> 22500
890
Cov: 22500 -> 22501
Cov: 22501 -> 22501
891
Cov: 22501 -> 22501
Cov: 22501 -> 22501
892
Cov: 22501 -> 22501
Cov: 22501 -> 22501
893
Cov: 22501 -> 22502
Cov: 22502 -> 22502
894
Cov: 22502 -> 22507
Cov: 22507 -> 22507
895
Cov: 22507 -> 22507
Cov: 22507 -> 22507
896
Cov: 22507 -> 22507
Cov: 22507 -> 22507
897
Cov: 22507 -> 22507
Cov: 22507 -> 22507
898
Cov: 22507 -> 22535
Cov: 22535 -> 22535
899
Cov: 22535 -> 22535
Cov: 22535 -> 22535
900
Cov: 22535 -> 22535
Cov: 22535 -> 22535
901
Cov: 22535 -> 22535
Cov: 22535 -> 22535
902
Cov: 22535 -> 22535
Cov: 22535 -> 22535
903
Cov: 22535 -> 22535
Cov: 22535 -> 22535
904
Cov: 22535 -> 22541
Cov: 22541 -> 22541
905
Cov: 22541 -> 22541
Cov: 22541 -> 22541
906
Cov: 22541 -> 22542
Cov: 22542 -> 22542
907
Cov: 22542 -> 22542
Cov: 22542 -> 22542
908
Cov: 22542 -> 22543
Cov: 22543 -> 22543
909
Cov: 22543 -> 22543
Cov: 22543 -> 22543
910
Cov: 22543 -> 22543
Cov: 22543 -> 22543
911
Cov: 22543 -> 22543
Cov: 22543 -> 22543
912
Cov: 22543 -> 22543
Cov: 22543 -> 22543
913
Cov: 22543 -> 22543
Cov: 22543 -> 22543
914
Cov: 22543 -> 22543
Cov: 22543 -> 22543
915
Cov: 22543 -> 22543
Cov: 22543 -> 22543
916
Cov: 22543 -> 22544
Cov: 22544 -> 22544
917
{"exception": "TypeError", "msg": "type_as(): argument 'other' (position 1) must be Tensor, not torch.tensortype"}
918
Cov: 22544 -> 22550
Cov: 22550 -> 22550
919
Cov: 22550 -> 22550
Cov: 22550 -> 22550
920
Cov: 22550 -> 22550
Cov: 22550 -> 22550
921
Cov: 22550 -> 22550
Cov: 22550 -> 22550
922
Cov: 22550 -> 22555
Cov: 22555 -> 22555
923
Cov: 22555 -> 22555
Cov: 22555 -> 22555
924
Cov: 22555 -> 22555
Cov: 22555 -> 22555
925
Cov: 22555 -> 22555
Cov: 22555 -> 22555
926
Cov: 22555 -> 22556
Cov: 22556 -> 22556
927
Cov: 22556 -> 22556
Cov: 22556 -> 22556
928
Cov: 22556 -> 22556
Cov: 22556 -> 22556
929
Cov: 22556 -> 22556
Cov: 22556 -> 22556
930
Cov: 22556 -> 22557
Cov: 22557 -> 22557
931
Cov: 22557 -> 22558
Cov: 22558 -> 22558
932
{"exception": "RuntimeError", "msg": "Bad in-place call: input tensor size [2, 3] and output tensor size [3, 3] should match"}
933
Cov: 22558 -> 22558
Cov: 22558 -> 22558
934
{"exception": "TypeError", "msg": "addbmm() missing 1 required positional arguments: \"batch2\""}
935
Cov: 22558 -> 22558
Cov: 22558 -> 22558
936
Cov: 22558 -> 22558
Cov: 22558 -> 22558
937
Cov: 22558 -> 22558
Cov: 22558 -> 22558
938
Cov: 22558 -> 22558
Cov: 22558 -> 22558
939
Cov: 22558 -> 22558
Cov: 22558 -> 22558
940
Cov: 22558 -> 22558
Cov: 22558 -> 22558
941
Cov: 22558 -> 22558
Cov: 22558 -> 22558
942
Cov: 22558 -> 22558
Cov: 22558 -> 22558
943
Cov: 22558 -> 22563
Cov: 22563 -> 22563
944
Cov: 22563 -> 22563
Cov: 22563 -> 22563
945
Cov: 22563 -> 22565
Cov: 22565 -> 22565
946
Cov: 22565 -> 22565
Cov: 22565 -> 22565
947
Cov: 22565 -> 22565
Cov: 22565 -> 22565
948
Cov: 22565 -> 22566
Cov: 22566 -> 22566
949
Cov: 22566 -> 22566
Cov: 22566 -> 22566
950
Cov: 22566 -> 22567
Cov: 22567 -> 22567
951
Cov: 22567 -> 22567
Cov: 22567 -> 22567
952
{"exception": "TypeError", "msg": "addbmm() received an invalid combination of arguments - got (Tensor), but expected (Tensor batch1, Tensor batch2, *, Number beta, Number alpha)"}
953
Cov: 22567 -> 22567
Cov: 22567 -> 22567
954
Cov: 22567 -> 22568
Cov: 22568 -> 22568
955
Cov: 22568 -> 22568
Cov: 22568 -> 22568
956
Cov: 22568 -> 22571
Cov: 22571 -> 22571
957
Cov: 22571 -> 22571
Cov: 22571 -> 22571
958
Cov: 22571 -> 22571
Cov: 22571 -> 22571
959
Cov: 22571 -> 22577
Cov: 22577 -> 22577
960
Cov: 22577 -> 22577
Cov: 22577 -> 22577
961
Cov: 22577 -> 22577
Cov: 22577 -> 22577
962
Cov: 22577 -> 22577
Cov: 22577 -> 22577
963
Cov: 22577 -> 22581
Cov: 22581 -> 22581
964
Cov: 22581 -> 22581
Cov: 22581 -> 22581
965
Cov: 22581 -> 22581
Cov: 22581 -> 22581
966
Cov: 22581 -> 22586
Cov: 22586 -> 22586
967
Cov: 22586 -> 22586
Cov: 22586 -> 22586
968
Cov: 22586 -> 22587
Cov: 22587 -> 22587
969
Cov: 22587 -> 22587
Cov: 22587 -> 22587
970
Cov: 22587 -> 22587
Cov: 22587 -> 22587
971
Cov: 22587 -> 22588
Cov: 22588 -> 22588
972
Cov: 22588 -> 22588
Cov: 22588 -> 22588
973
Cov: 22588 -> 22588
Cov: 22588 -> 22588
974
Cov: 22588 -> 22588
Cov: 22588 -> 22588
975
Cov: 22588 -> 22588
Cov: 22588 -> 22588
976
Cov: 22588 -> 22588
Cov: 22588 -> 22588
977
Cov: 22588 -> 22592
Cov: 22592 -> 22592
978
Cov: 22592 -> 22593
Cov: 22593 -> 22593
979
Cov: 22593 -> 22593
Cov: 22593 -> 22593
980
Cov: 22593 -> 22595
Cov: 22595 -> 22595
981
Cov: 22595 -> 22595
Cov: 22595 -> 22595
982
Cov: 22595 -> 22595
Cov: 22595 -> 22595
983
Cov: 22595 -> 22597
Cov: 22597 -> 22597
984
Cov: 22597 -> 22597
Cov: 22597 -> 22597
985
Cov: 22597 -> 22597
Cov: 22597 -> 22597
986
Cov: 22597 -> 22597
Cov: 22597 -> 22597
987
Cov: 22597 -> 22597
Cov: 22597 -> 22597
988
Cov: 22597 -> 22597
Cov: 22597 -> 22597
989
Cov: 22597 -> 22597
Cov: 22597 -> 22597
990
Cov: 22597 -> 22597
Cov: 22597 -> 22597
991
Cov: 22597 -> 22597
Cov: 22597 -> 22597
992
Cov: 22597 -> 22597
Cov: 22597 -> 22597
993
Cov: 22597 -> 22597
Cov: 22597 -> 22597
994
Cov: 22597 -> 22597
Cov: 22597 -> 22597
995
Cov: 22597 -> 22597
Cov: 22597 -> 22597
996
Cov: 22597 -> 22597
Cov: 22597 -> 22597
997
Cov: 22597 -> 22599
Cov: 22599 -> 22599
998
{"exception": "RuntimeError", "msg": "masked_scatter_ only supports boolean masks, but got mask with dtype Byte"}
999
Cov: 22599 -> 22599
Cov: 22599 -> 22599
1000
Cov: 22599 -> 22600
Cov: 22600 -> 22600
